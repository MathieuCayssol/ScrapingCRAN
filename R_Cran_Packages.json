{"SpotSampling": {"categories": ["Spatial"], "description": "In spatial data, information of two neighboring units are generally very similar. \n  For spatial sampling, it is therefore more efficient to select samples that are well spread out in space. Often, the interest lies not only in estimating a measure at one point in time, but rather in estimating several points in time to also study the evolution. \n  Three new methods called Orfs (Optimal Rotation with Fixed sample Size), Orsp (Optimal Rotation with Spread sample), and Spot (Spatial and Optimally Temporal Sampling) are implemented in this package. Orfs allows to select temporal samples with fixed size. Orsp selects spatio-temporal samples with random size that are well spread out in space at each point in time. And Spot generates spread sample with fixed sample size at each wave. These methods provide an optimal time rotation of the selected units using the systematic sampling."}, "plotKML": {"categories": ["WebTechnologies"], "description": "Writes spatial-class, spacetime-class, raster-class and similar spatial and spatiotemporal objects to KML following some basic cartographic rules."}, "AzureStor": {"categories": ["WebTechnologies"], "description": "Manage storage in Microsoft's 'Azure' cloud: <https://azure.microsoft.com/en-us/product-categories/storage/>. On the admin side, 'AzureStor' includes features to create, modify and delete storage accounts. On the client side, it includes an interface to blob storage, file storage, and 'Azure Data Lake Storage Gen2': upload and download files and blobs; list containers and files/blobs; create containers; and so on. Authenticated access to storage is supported, via either a shared access key or a shared access signature (SAS). Part of the 'AzureR' family of packages."}, "semtree": {"categories": ["Psychometrics"], "description": "SEM Trees and SEM Forests \u2013 an extension of model-based decision\n    trees and forests to Structural Equation Models (SEM). SEM trees hierarchically\n    split empirical data into homogeneous groups each sharing similar data patterns\n    with respect to a SEM by recursively selecting optimal predictors of these\n    differences. SEM forests are an extension of SEM trees. They are ensembles of\n    SEM trees each built on a random sample of the original data. By aggregating\n    over a forest, we obtain measures of variable importance that are more robust\n    than measures from single trees. A description of the method was published by\n    Brandmaier, von Oertzen, McArdle, & Lindenberger (2013) <doi:10.1037/a0030001> \n    and Arnold, Voelkle, & Brandmaier (2020) <doi:10.3389/fpsyg.2020.564403>."}, "RiskPortfolios": {"categories": ["Finance"], "description": "Collection of functions designed to compute risk-based portfolios as described \n    in Ardia et al. (2017) <doi:10.1007/s10479-017-2474-7> and Ardia et al. (2017) <doi:10.21105/joss.00171>."}, "dde": {"categories": ["DifferentialEquations", "Epidemiology"], "description": "Solves ordinary and delay differential equations, where\n    the objective function is written in either R or C.  Suitable only\n    for non-stiff equations, the solver uses a 'Dormand-Prince' method\n    that allows interpolation of the solution at any point.  This\n    approach is as described by Hairer, Norsett and Wanner (1993)\n    <ISBN:3540604529>.  Support is also included for iterating\n    difference equations."}, "gmeta": {"categories": ["MetaAnalysis"], "description": "An implementation of an all-in-one function for a wide range of meta-analysis problems. It contains three functions. The gmeta() function unifies all standard meta-analysis methods and also several newly developed ones under a framework of combining confidence distributions (CDs). Specifically, the package can perform classical p-value combination methods (such as methods of Fisher, Stouffer, Tippett, etc.), fit meta-analysis fixed-effect and random-effects models, and synthesizes 2x2 tables. Furthermore, it can perform robust meta-analysis, which provides protection against model-misspecifications, and limits the impact of any unknown outlying studies. In addition, the package implements two exact meta-analysis methods from synthesizing 2x2 tables with rare events (e.g., zero total event). The np.gmeta() function summarizes information obtained from multiple studies and makes inference for study-level parameters with no distributional assumption. Specifically, it can construct confidence intervals for unknown, fixed study-level parameters via confidence distribution. Furthermore, it can perform  estimation via asymptotic confidence distribution whether tie or near tie condition exist or not. The plot.gmeta() function to visualize individual and combined CDs through extended forest plots is also available. Compared to version 2.2-6, version 2.3-0 contains a new function np.gmeta()."}, "Rcpp": {"categories": ["HighPerformanceComputing", "NumericalMathematics"], "description": "The 'Rcpp' package provides R functions as well as C++ classes which\n offer a seamless integration of R and C++. Many R data types and objects can be\n mapped back and forth to C++ equivalents which facilitates both writing of new\n code as well as easier integration of third-party libraries. Documentation\n about 'Rcpp' is provided by several vignettes included in this package, via the\n 'Rcpp Gallery' site at <https://gallery.rcpp.org>, the paper by Eddelbuettel and\n Francois (2011, <doi:10.18637/jss.v040.i08>), the book by Eddelbuettel (2013,\n <doi:10.1007/978-1-4614-6868-4>) and the paper by Eddelbuettel and Balamuta (2018,\n <doi:10.1080/00031305.2017.1375990>); see 'citation(\"Rcpp\")' for details."}, "skpr": {"categories": ["ExperimentalDesign"], "description": "Generates and evaluates D, I, A, Alias, E, T, and G optimal designs. Supports generation and evaluation of blocked and split/split-split/.../N-split plot designs. Includes parametric and Monte Carlo power evaluation functions, and supports calculating power for censored responses. Provides a framework to evaluate power using functions provided in other packages or written by the user. Includes a Shiny graphical user interface that displays the underlying code used to create and evaluate the design to improve ease-of-use and make analyses more reproducible. For details, see Morgan-Wall et al. (2021) <doi:10.18637/jss.v099.i01>."}, "quantregForest": {"categories": ["MachineLearning"], "description": "Quantile Regression Forests is a tree-based ensemble\n        method for estimation of conditional quantiles. It is\n        particularly well suited for high-dimensional data. Predictor\n        variables of mixed classes can be handled. The package is\n        dependent on the package 'randomForest', written by Andy Liaw."}, "ebdbNet": {"categories": ["Bayesian"], "description": "Infer the adjacency matrix of a\n\tnetwork from time course data using an empirical Bayes\n\testimation procedure based on Dynamic Bayesian Networks."}, "RNAseqNet": {"categories": ["MissingData"], "description": "Infer log-linear Poisson Graphical Model with an auxiliary data\n    set. Hot-deck multiple imputation method is used to improve the reliability\n    of the inference with an auxiliary dataset. Standard log-linear Poisson \n    graphical model can also be used for the inference and the Stability \n    Approach for Regularization Selection (StARS) is implemented to drive the \n    selection of the regularization parameter. The method is fully described in\n    <doi:10.1093/bioinformatics/btx819>."}, "features": {"categories": ["NumericalMathematics"], "description": "Discretely-sampled function is first smoothed.  Features\n        of the smoothed function are then extracted.  Some of the key\n        features include mean value, first and second derivatives,\n        critical points (i.e. local maxima and minima), curvature of\n        cunction at critical points, wiggliness of the function, noise\n        in data, and outliers in data."}, "psd": {"categories": ["TimeSeries"], "description": "Produces power spectral density estimates through iterative\n    refinement of the optimal number of sine-tapers at each frequency. This\n    optimization procedure is based on the method of Riedel and Sidorenko\n    (1995), which minimizes the Mean Square Error (sum of variance and bias)\n    at each frequency, but modified for computational stability. The same\n    procedure can now be used to calculate the cross spectrum (multivariate\n    analyses)."}, "RMKdiscrete": {"categories": ["Distributions"], "description": "Sundry discrete probability distributions and helper functions."}, "imguR": {"categories": ["WebTechnologies"], "description": "A complete API client for the image hosting service Imgur.com, including the an imgur graphics device, enabling the easy upload and sharing of plots."}, "mathpix": {"categories": ["WebTechnologies"], "description": "Given an image of a formula (typeset or handwritten) this package\n    provides calls to the 'Mathpix' service to produce the 'LaTeX' code which should\n    generate that image, and pastes it into a (e.g. an 'rmarkdown') document. \n    See <https://docs.mathpix.com/> for full details. 'Mathpix' is an external service \n    and use of the API is subject to their terms and conditions."}, "mombf": {"categories": ["Bayesian"], "description": "Bayesian model selection and averaging for regression and mixtures for non-local and selected local priors."}, "cobalt": {"categories": ["CausalInference", "MissingData"], "description": "Generate balance tables and plots for covariates of groups preprocessed through matching, weighting or subclassification, for example, using propensity scores. Includes integration with 'MatchIt', 'twang', 'Matching', 'optmatch', 'CBPS', 'ebal', 'WeightIt', 'cem', 'sbw', and 'designmatch' for assessing balance on the output of their preprocessing functions. Users can also specify data for balance assessment not generated through the above packages. Also included are methods for assessing balance in clustered or multiply imputed data sets or data sets with longitudinal treatments."}, "greybox": {"categories": ["Distributions", "TimeSeries"], "description": "Implements functions and instruments for regression model building and its\n             application to forecasting. The main scope of the package is in variables selection\n             and models specification for cases of time series data. This includes promotional\n             modelling, selection between different dynamic regressions with non-standard\n             distributions of errors, selection based on cross validation, solutions to the fat\n             regression model problem and more. Models developed in the package are tailored\n             specifically for forecasting purposes. So as a results there are several methods\n             that allow producing forecasts from these models and visualising them."}, "NADA": {"categories": ["Epidemiology", "Survival"], "description": "Contains methods described by Dennis Helsel in \n             his book \"Nondetects And Data Analysis: Statistics \n             for Censored Environmental Data\"."}, "frailtypack": {"categories": ["Survival"], "description": "The following several classes of frailty models using a penalized likelihood estimation on the hazard function but also a parametric estimation can be fit using this R package:\n        1) A shared frailty model (with gamma or log-normal frailty distribution) and Cox proportional hazard model. Clustered and recurrent survival times can be studied.\n        2) Additive frailty models for proportional hazard models with two correlated random effects (intercept random effect with random slope).\n        3) Nested frailty models for hierarchically clustered data (with 2 levels of clustering) by including two iid gamma random effects.\n        4) Joint frailty models in the context of the joint modelling for recurrent events with terminal event for clustered data or not. A joint frailty model for two semi-competing risks and clustered data is also proposed.\n\t\t5) Joint general frailty models in the context of the joint modelling for recurrent events with terminal event data with two independent frailty terms.\n\t\t6) Joint Nested frailty models in the context of the joint modelling for recurrent events with terminal event, for hierarchically clustered data (with two levels of clustering) by including two iid gamma random effects.\n\t\t7) Multivariate joint frailty models for two types of recurrent events and a terminal event.\n\t\t8) Joint models for longitudinal data and a terminal event.\n\t\t9) Trivariate joint models for longitudinal data, recurrent events and a terminal event.\n\t\t10) Joint frailty models for the validation of surrogate endpoints in multiple randomized clinical trials with failure-time endpoints\n\t\twith the possibility to use a mediation analysis model.\n    11) Conditional and Marginal two-part joint models for longitudinal semicontinuous data and a terminal event.\n    12) Joint frailty-copula models for the validation of surrogate endpoints in multiple randomized clinical trials with failure-time endpoints.\n\t\t13) Generalized shared and joint frailty models for recurrent and terminal events. Proportional hazards (PH), additive hazard (AH), proportional odds (PO) and probit models are available in a fully parametric framework. For PH and AH models, it is possible to consider type-varying coefficients and flexible semiparametric hazard function.\n\t\tPrediction values are available (for a terminal event or for a new recurrent event). Left-truncated (not for Joint model), right-censored data, interval-censored data (only for Cox proportional hazard and shared frailty model) and strata are allowed. In each model, the random effects have the gamma or normal distribution. Now, you can also consider time-varying covariates effects in Cox, shared and joint frailty models (1-5). The package includes concordance measures for Cox proportional hazards models and for shared frailty models.\n\t\tMoreover, the package can be used with its shiny application, in a local mode or by following the link below."}, "BB": {"categories": ["NumericalMathematics", "Optimization"], "description": "Barzilai-Borwein spectral methods for solving nonlinear\n        system of equations, and for optimizing nonlinear objective\n        functions subject to simple constraints. A tutorial style\n        introduction to this package is available in a vignette on the\n        CRAN download page or, when the package is loaded in an R\n        session, with vignette(\"BB\")."}, "BART": {"categories": ["Bayesian", "MachineLearning"], "description": "Bayesian Additive Regression Trees (BART) provide flexible nonparametric modeling of covariates for continuous, binary, categorical and time-to-event outcomes.  For more information see Sparapani, Spanbauer and McCulloch <doi:10.18637/jss.v097.i01>."}, "WienR": {"categories": ["Distributions"], "description": "First, we provide functions to calculate the partial derivative of the first-passage time diffusion probability density function (PDF) and cumulative\n    distribution function (CDF)\twith respect to the first-passage time t (only for PDF), the upper barrier a, the drift rate v, the relative starting point w, the\n    non-decision time t0, the inter-trial variability of the drift rate sv, the inter-trial variability of the rel. starting point sw, and the inter-trial variability\n    of the non-decision time st0. In addition the PDF and CDF themselves are also provided. Most calculations are done on the logarithmic scale to make it more stable.\n    Since the PDF, CDF, and their derivatives are represented as infinite series, we give the user the option to control the approximation errors with the argument\n    'precision'. For the numerical integration we used the C library cubature by Johnson, S. G. (2005-2013) <https://github.com/stevengj/cubature>. Numerical integration is\n    required whenever sv, sw, and/or st0 is not zero. Note that numerical integration reduces speed of the computation and the precision cannot be guaranteed\n    anymore. Therefore, whenever numerical integration is used an estimate of the approximation error is provided in the output list.\n    Note: The large number of contributors (ctb) is due to copying a lot of C/C++ code chunks from the GNU Scientific Library (GSL).\n    Second, we provide methods to sample from the first-passage time distribution with or without user-defined truncation from above. The first method is a new adaptive\n    rejection sampler building on the works of Gilks and Wild (1992; <doi:10.2307/2347565>) and Hartmann and Klauer (in press). The second method is a rejection\n    sampler provided by Drugowitsch (2016; <doi:10.1038/srep20490>). The third method is an inverse transformation sampler. The fourth method is a\n    \"pseudo\" adaptive rejection sampler that builds on the first method. For more details see the corresponding help files."}, "lgarch": {"categories": ["Finance"], "description": "Simulation and estimation of univariate and multivariate log-GARCH models. The main functions of the package are: lgarchSim(), mlgarchSim(), lgarch() and mlgarch(). The first two functions simulate from a univariate and a multivariate log-GARCH model, respectively, whereas the latter two estimate a univariate and multivariate log-GARCH model, respectively."}, "sde": {"categories": ["DifferentialEquations", "Finance", "TimeSeries"], "description": "Companion package to the book Simulation and Inference for\n        Stochastic Differential Equations With R Examples, ISBN\n        978-0-387-75838-1, Springer, NY."}, "pbv": {"categories": ["Distributions"], "description": "\n    Computes probabilities of the bivariate normal distribution\n    in a vectorized R function (Drezner & Wesolowsky, 1990, \n    <doi:10.1080/00949659008811236>)."}, "smcfcs": {"categories": ["MissingData"], "description": "Implements multiple imputation of missing covariates by\n    Substantive Model Compatible Fully Conditional Specification.\n    This is a modification of the popular FCS/chained equations\n    multiple imputation approach, and allows imputation of missing\n    covariate values from models which are compatible with the user\n    specified substantive model."}, "iptools": {"categories": ["WebTechnologies"], "description": "A toolkit for manipulating, validating and testing 'IP' addresses and\n    ranges, along with datasets relating to 'IP' addresses. Tools are also provided\n    to map 'IPv4' blocks to country codes. While it primarily has support for the 'IPv4'\n    address space, more extensive 'IPv6' support is intended."}, "rsm": {"categories": ["ExperimentalDesign"], "description": "Provides functions to generate response-surface designs, \n    fit first- and second-order response-surface models, \n    make surface plots, obtain the path of steepest ascent, \n    and do canonical analysis. A good reference on these methods \n    is Chapter 10 of Wu, C-F J and Hamada, M (2009) \n    \"Experiments: Planning, Analysis, and Parameter Design Optimization\"\n    ISBN 978-0-471-69946-0. An early version of the package is\n    documented in Journal of Statistical Software <doi:10.18637/jss.v032.i07>."}, "SCBmeanfd": {"categories": ["FunctionalData"], "description": "Statistical methods for estimating and inferring the mean of functional data. The methods include simultaneous confidence bands, local polynomial fitting,  bandwidth selection by plug-in and cross-validation, goodness-of-fit tests for parametric models, equality tests for two-sample problems, and plotting functions.   "}, "cclust": {"categories": ["Cluster"], "description": "Convex Clustering methods, including K-means algorithm,\n  On-line Update algorithm (Hard Competitive Learning) and Neural Gas\n  algorithm (Soft Competitive Learning), and calculation of several\n  indexes for finding the number of clusters in a data set."}, "tidyLPA": {"categories": ["Psychometrics"], "description": "An interface to the 'mclust' package to easily\n    carry out latent profile analysis (\"LPA\"). Provides functionality to\n    estimate commonly-specified models. Follows a tidy approach, in that\n    output is in the form of a data frame that can subsequently be\n    computed on. Also has functions to interface to the commercial 'MPlus'\n    software via the 'MplusAutomation' package."}, "PReMiuM": {"categories": ["Bayesian", "Cluster", "MissingData", "Spatial", "Survival"], "description": "Bayesian clustering using a Dirichlet process mixture model. This model is an alternative to regression models, non-parametrically linking a response vector to covariate data through cluster membership. The package allows Bernoulli, Binomial, Poisson, Normal, survival and categorical response, as well as Normal and discrete covariates. It also allows for fixed effects in the response model, where a spatial CAR (conditional autoregressive) term can be also included. Additionally, predictions may be made for the response, and missing values for the covariates are handled. Several samplers and label switching moves are implemented along with diagnostic tools to assess convergence. A number of R functions for post-processing of the output are also provided. In addition to fitting mixtures, it may additionally be of interest to determine which covariates actively drive the mixture components. This is implemented in the package as variable selection. The main reference for the package is Liverani, Hastie, Azizi, Papathomas and Richardson (2015) <doi:10.18637/jss.v064.i07>."}, "fpow": {"categories": ["Distributions"], "description": "Returns the noncentrality parameter of the noncentral F\n        distribution if probability of type I and type II error,\n        degrees of freedom of the numerator and the denominator are\n        given.  It may be useful for computing minimal detectable\n        differences for general ANOVA models.  This program is\n        documented in the paper of A. Baharev, S. Kemeny, On the\n        computation of the noncentral F and noncentral beta\n        distribution; Statistics and Computing, 2008, 18 (3), 333-340."}, "ChainLadder": {"categories": ["Finance"], "description": "Various statistical methods and models which are\n    typically used for the estimation of outstanding claims reserves\n    in general insurance, including those to estimate the claims\n    development result as required under Solvency II."}, "biclust": {"categories": ["Cluster"], "description": "The main function biclust() provides several algorithms to\n        find biclusters in two-dimensional data: Cheng and Church (2000, ISBN:1-57735-115-0),\n        spectral (2003) <doi:10.1101/gr.648603>, plaid model (2005) <doi:10.1016/j.csda.2004.02.003>, xmotifs (2003) <doi:10.1142/9789812776303_0008> and bimax (2006) <doi:10.1093/bioinformatics/btl060>. In addition, the\n        package provides methods for data preprocessing (normalization\n        and discretisation), visualisation, and validation of bicluster\n        solutions."}, "cort": {"categories": ["Distributions"], "description": "Provides S4 classes and methods to fit several copula models: The classic empirical checkerboard copula and the empirical checkerboard copula with known margins, see Cuberos, Masiello and Maume-Deschamps (2019) <doi:10.1080/03610926.2019.1586936> are proposed. These two models allow to fit copulas in high dimension with a small number of observations, and they are always proper copulas. Some flexibility is added via a possibility to differentiate the checkerboard parameter by dimension. The last model consist of the implementation of the Copula Recursive Tree algorithm proposed by Laverny, Maume-Deschamps, Masiello and Rulli\u00e8re (2020) <arXiv:2005.02912>, including the localised dimension reduction, which fits a copula by recursive splitting of the copula domain. We also provide an efficient way of mixing copulas, allowing to bag the algorithm into a forest, and a generic way of measuring d-dimensional boxes with a copula."}, "PortRisk": {"categories": ["Finance"], "description": "Risk Attribution of a portfolio with Volatility Risk Analysis."}, "lori": {"categories": ["MissingData"], "description": "Analysis, imputation, and multiple imputation of count data using covariates. LORI uses a log-linear Poisson model where main row and column effects, as well as effects of known covariates and interaction terms can be fitted. The estimation procedure is based on the convex optimization of the Poisson loss penalized by a Lasso type penalty and a nuclear norm. LORI returns estimates of main effects, covariate effects and interactions, as well as an imputed count table. The package also contains a multiple imputation procedure. The methods are described in Robin, Josse, Moulines and Sardy (2019) <arXiv:1703.02296v4>."}, "gma": {"categories": ["CausalInference"], "description": "Performs Granger mediation analysis (GMA) for time series. This package includes a single level GMA model and a two-level GMA model, for time series with hierarchically nested structure. The single level GMA model for the time series of a single participant performs the causal mediation analysis which integrates the structural equation modeling and the Granger causality frameworks. A vector autoregressive model of order p is employed to account for the spatiotemporal dependencies in the data. Meanwhile, the model introduces the unmeasured confounding effect through a nonzero correlation parameter. Under the two-level model, by leveraging the variabilities across participants, the parameters are identifiable and consistently estimated based on a full conditional likelihood or a two-stage method. See Zhao, Y., & Luo, X. (2017), Granger Mediation Analysis of Multiple Time Series with an Application to fMRI, <arXiv:1709.05328> for details."}, "padr": {"categories": ["MissingData"], "description": "Transforms datetime data into a format ready for analysis.\n    It offers two core functionalities; aggregating data to a higher level interval\n    (thicken) and imputing records where observations were absent (pad). "}, "timeseriesdb": {"categories": ["TimeSeries"], "description": "Archive and manage times series data from official statistics. The 'timeseriesdb' package was designed to manage a large catalog of time series from official statistics which are typically published on a monthly, quarterly or yearly basis. Thus timeseriesdb is optimized to handle updates caused by data revision as well as elaborate, multi-lingual meta information. "}, "drc": {"categories": ["ChemPhys"], "description": "Analysis of dose-response data is made available through a suite of flexible and versatile model fitting and after-fitting functions."}, "DTWBI": {"categories": ["MissingData"], "description": "Functions to impute large gaps within time series based on Dynamic Time Warping methods. It contains all required functions to create large missing consecutive values within time series and to fill them, according to the paper Phan et al. (2017), <doi:10.1016/j.patrec.2017.08.019>. Performance criteria are added to compare similarity between two signals (query and reference)."}, "saws": {"categories": ["Survival"], "description": "Tests coefficients with sandwich estimator of variance and with small samples. Regression types supported are gee, linear regression, and conditional logistic regression."}, "vkR": {"categories": ["WebTechnologies"], "description": "Provides an interface to the VK API <https://vk.com/dev/methods>.\n      VK <https://vk.com/> is the largest European online social networking\n      service, based in Russia."}, "simsurv": {"categories": ["Survival"], "description": "Simulate survival times from standard parametric survival\n    distributions (exponential, Weibull, Gompertz), 2-component mixture\n    distributions, or a user-defined hazard, log hazard, cumulative hazard,\n    or log cumulative hazard function. Baseline covariates can be included\n    under a proportional hazards assumption.\n    Time dependent effects (i.e. non-proportional hazards) can be included by\n    interacting covariates with linear time or a user-defined function of time.\n    Clustered event times are also accommodated.\n    The 2-component mixture distributions can allow for a variety of flexible\n    baseline hazard functions reflecting those seen in practice.\n    If the user wishes to provide a user-defined\n    hazard or log hazard function then this is possible, and the resulting\n    cumulative hazard function does not need to have a closed-form solution.\n    For details see the supporting paper <doi:10.18637/jss.v097.i03>.\n    Note that this package is modelled on the 'survsim' package available in\n    the 'Stata' software (see Crowther and Lambert (2012)\n    <https://www.stata-journal.com/sjpdf.html?articlenum=st0275> or\n    Crowther and Lambert (2013) <doi:10.1002/sim.5823>)."}, "BNSP": {"categories": ["Bayesian"], "description": "MCMC algorithms & processing functions for: 1. single response multiple regression, see Papageorgiou, G. (2018) <doi:10.32614/RJ-2018-069>, 2. multivariate response multiple regression, with nonparametric models for the means, the variances and the correlation matrix, with variable selection, see Papageorgiou, G. and Marshall, B. C. (2020) <doi:10.1080/10618600.2020.1739534>, 3. joint mean-covariance models for multivariate responses, see Papageorgiou, G. (2020), and 4.Dirichlet process mixtures, see Papageorgiou, G. (2019) <doi:10.1111/anzs.12273>."}, "httpRequest": {"categories": ["WebTechnologies"], "description": "HTTP Request protocols. Implements the GET, POST and multipart POST request."}, "interval": {"categories": ["Survival"], "description": "Functions to fit nonparametric survival curves, plot them, and perform logrank or Wilcoxon type tests [see Fay and Shaw <doi:10.18637/jss.v036.i02>]."}, "caRamel": {"categories": ["Optimization"], "description": "Multi-objective optimizer initially developed for the calibration of hydrological models.\n     The algorithm is a hybrid of the MEAS algorithm (Efstratiadis and Koutsoyiannis (2005) <doi:10.13140/RG.2.2.32963.81446>) by using the directional search method based on the simplexes of the objective space\n     and the epsilon-NGSA-II algorithm with the method of classification of the parameter vectors archiving management by epsilon-dominance (Reed and Devireddy <doi:10.1142/9789812567796_0004>)."}, "tm.plugin.factiva": {"categories": ["NaturalLanguageProcessing"], "description": "Provides a 'tm' Source to create corpora from\n  articles exported from the Dow Jones 'Factiva' content provider as\n  XML or HTML files. It is able to read both text content and meta-data\n  information (including source, date, title, author, subject,\n  geographical coverage, company, industry, and various\n  provider-specific fields)."}, "LogicReg": {"categories": ["Survival"], "description": "Routines for fitting Logic Regression models. Logic Regression is described\n\tin Ruczinski, Kooperberg, and LeBlanc (2003) <doi:10.1198/1061860032238>. Monte\n        Carlo Logic Regression is described in and Kooperberg and Ruczinski (2005)\n        <doi:10.1002/gepi.20042>."}, "psymetadata": {"categories": ["MetaAnalysis"], "description": "Data and examples from meta-analyses in psychology research. "}, "mlVAR": {"categories": ["Psychometrics", "TimeSeries"], "description": "Estimates the multi-level vector autoregression model on time-series data.\n             Three network structures are obtained: temporal networks, contemporaneous\n             networks and between-subjects networks."}, "ubiquity": {"categories": ["Pharmacokinetics"], "description": "Complete work flow for the analysis of pharmacokinetic pharmacodynamic (PKPD), physiologically-based pharmacokinetic (PBPK) and systems pharmacology models including: creation of ordinary differential equation-based models, pooled parameter estimation, individual/population based simulations, rule-based simulations for clinical trial design and modeling assays, deployment with a customizable 'Shiny' app, and non-compartmental analysis. System-specific analysis templates can be generated and each element includes integrated reporting with 'PowerPoint' and 'Word'. "}, "robustlmm": {"categories": ["Robust"], "description": "A method to fit linear mixed effects models robustly.\n    Robustness is achieved by modification of the scoring equations\n    combined with the Design Adaptive Scale approach."}, "thief": {"categories": ["TimeSeries"], "description": "Methods and tools for generating forecasts at different temporal\n    frequencies using a hierarchical time series approach."}, "influence.SEM": {"categories": ["Psychometrics"], "description": "A set of tools for evaluating several measures of case influence for structural equation models. "}, "MLDS": {"categories": ["Psychometrics"], "description": "Difference scaling is a method for scaling perceived \n  supra-threshold differences.  The package contains functions that\n  allow the user to design and run a difference scaling experiment, \n  to fit the resulting data by maximum likelihood and test the\n  internal validity of the estimated scale."}, "CompQuadForm": {"categories": ["Distributions"], "description": "Computes the distribution function of quadratic forms in normal variables using Imhof's method, Davies's algorithm, Farebrother's algorithm or Liu et al.'s algorithm."}, "strand": {"categories": ["Finance"], "description": "Provides a framework for performing discrete (share-level) simulations of\n  investment strategies. Simulated portfolios optimize exposure to an input signal subject\n  to constraints such as position size and factor exposure. For background see L. Chincarini\n  and D. Kim (2010, ISBN:978-0-07-145939-6) \"Quantitative Equity Portfolio Management\"."}, "R4CouchDB": {"categories": ["Databases"], "description": "Provides a collection of functions for basic\n    database and document management operations such as add, get, list access\n    or delete. Every cdbFunction() gets and returns a list() containing the\n    connection setup. Such a list can be generated by cdbIni()."}, "rim": {"categories": ["NumericalMathematics"], "description": "Provides an interface to the powerful and fairly complete computer algebra system maxima.\n    It can be used to start and control 'Maxima' from within R by entering 'Maxima' commands. \n    It facilitates outputting results from 'Maxima' in 'LaTeX' and 'MathML'. 2D and 3D plots can be displayed directly. This package also registers a 'knitr'-engine \n    enabling 'Maxima' code chunks to be written in 'RMarkdown' documents."}, "conquestr": {"categories": ["Psychometrics"], "description": "Extends 'ACER ConQuest' through a family of functions\n    designed to improve graphical outputs and help with advanced analysis\n    (e.g., differential item functioning).  Allows R users to call \n    'ACER ConQuest' from within R and read 'ACER ConQuest' System Files\n    (generated by the command 'put').  Requires 'ACER ConQuest' version\n    5.19.5 or later. A demonstration version can be downloaded from\n    <https://shop.acer.org/acer-conquest-5.html>."}, "yaImpute": {"categories": ["MissingData"], "description": "Performs nearest neighbor-based imputation using one or more alternative \n approaches to processing multivariate data. These include methods based on canonical \n correlation analysis, canonical correspondence analysis, and a multivariate adaptation \n of the random forest classification and regression techniques of Leo Breiman and Adele \n Cutler. Additional methods are also offered. The package includes functions for \n comparing the results from running alternative techniques, detecting imputation targets \n that are notably distant from reference observations, detecting and correcting \n for bias, bootstrapping and building ensemble imputations, and mapping results."}, "BayesianGLasso": {"categories": ["Bayesian"], "description": "Implements a data-augmented block Gibbs sampler for simulating the posterior distribution of concentration matrices for specifying the topology and parameterization of a Gaussian Graphical Model (GGM). This sampler was originally proposed in Wang (2012) <doi:10.1214/12-BA729>."}, "borrowr": {"categories": ["CausalInference"], "description": "Estimate population average treatment effects from a primary data source \n  with borrowing from supplemental sources. Causal estimation is done with either a \n  Bayesian linear model or with Bayesian additive regression trees (BART) to adjust \n  for confounding. Borrowing is done with multisource exchangeability models (MEMs). For \n  information on BART, see Chipman, George, & McCulloch (2010) <doi:10.1214/09-AOAS285>. \n  For information on MEMs, see Kaizer, Koopmeiners, & \n  Hobbs (2018) <doi:10.1093/biostatistics/kxx031>."}, "missRanger": {"categories": ["MissingData"], "description": "Alternative implementation of the beautiful 'MissForest'\n    algorithm used to impute mixed-type data sets by chaining random\n    forests, introduced by Stekhoven, D.J. and Buehlmann, P. (2012)\n    <doi:10.1093/bioinformatics/btr597>. Under the hood, it uses the\n    lightning fast random jungle package 'ranger'. Between the iterative\n    model fitting, we offer the option of using predictive mean matching.\n    This firstly avoids imputation with values not already present in the\n    original data (like a value 0.3334 in 0-1 coded variable).  Secondly,\n    predictive mean matching tries to raise the variance in the resulting\n    conditional distributions to a realistic level. This would allow e.g.\n    to do multiple imputation when repeating the call to missRanger().  A\n    formula interface allows to control which variables should be imputed\n    by which."}, "polycor": {"categories": ["Psychometrics"], "description": "Computes polychoric and polyserial correlations by quick \"two-step\" methods or ML, \n  optionally with standard errors; tetrachoric and biserial correlations are special cases."}, "mets": {"categories": ["Survival"], "description": "Implementation of various statistical models for multivariate\n    event history data <doi:10.1007/s10985-013-9244-x>. Including multivariate\n    cumulative incidence models <doi:10.1002/sim.6016>, and  bivariate random\n    effects probit models (Liability models) <doi:10.1016/j.csda.2015.01.014>.\n    Also contains two-stage binomial modelling that can do pairwise odds-ratio\n    dependence modelling based marginal logistic regression models. This is an\n    alternative to the alternating logistic regression approach (ALR)."}, "systemfit": {"categories": ["Econometrics", "Psychometrics"], "description": "Econometric estimation of simultaneous\n systems of linear and nonlinear equations using Ordinary Least\n Squares (OLS), Weighted Least Squares (WLS), Seemingly Unrelated\n Regressions (SUR), Two-Stage Least Squares (2SLS), Weighted\n Two-Stage Least Squares (W2SLS), and Three-Stage Least Squares (3SLS)\n as suggested, e.g., by Zellner (1962) <doi:10.2307/2281644>,\n Zellner and Theil (1962) <doi:10.2307/1911287>, and\n Schmidt (1990) <doi:10.1016/0304-4076(90)90127-F>."}, "tm.plugin.lexisnexis": {"categories": ["NaturalLanguageProcessing"], "description": "Provides a 'tm' Source to create corpora from\n  articles exported from the 'LexisNexis' content provider as\n  HTML files. It is able to read both text content and meta-data\n  information (including source, date, title, author and pages).\n  Note that the file format is highly unstable: there is no warranty\n  that this package will work for your corpus, and you may have\n  to adjust the code to adapt it to your particular format."}, "multiwayvcov": {"categories": ["Econometrics"], "description": "Exports two functions implementing\n    multi-way clustering using the method suggested by Cameron, Gelbach, &\n    Miller (2011) and cluster (or block)\n    bootstrapping for estimating variance-covariance matrices. Normal one and\n    two-way clustering matches the results of other common statistical\n    packages.  Missing values are handled transparently and rudimentary\n    parallelization support is provided."}, "vcr": {"categories": ["WebTechnologies"], "description": "Record test suite 'HTTP' requests and replays them during\n    future runs. A port of the Ruby gem of the same name\n    (<https://github.com/vcr/vcr/>). Works by hooking into the 'webmockr'\n    R package for matching 'HTTP' requests by various rules ('HTTP' method,\n    'URL', query parameters, headers, body, etc.), and then caching\n    real 'HTTP' responses on disk in 'cassettes'. Subsequent 'HTTP' requests\n    matching any previous requests in the same 'cassette' use a cached\n    'HTTP' response."}, "rmgarch": {"categories": ["Finance"], "description": "Feasible multivariate GARCH models including DCC, GO-GARCH and Copula-GARCH."}, "RQuantLib": {"categories": ["Finance"], "description": "The 'RQuantLib' package makes parts of 'QuantLib' accessible from R\n The 'QuantLib' project aims to provide a comprehensive software framework\n for quantitative finance. The goal is to provide a standard open source library\n for quantitative analysis, modeling, trading, and risk management of financial\n assets."}, "akima": {"categories": ["NumericalMathematics"], "description": "Several cubic spline interpolation methods of H. Akima for irregular and\n  regular gridded data are available through this package, both for the bivariate case\n  (irregular data: ACM 761, regular data: ACM 760) and univariate case (ACM 433 and ACM 697).\n  Linear interpolation of irregular gridded data is also covered by reusing D. J. Renkas\n  triangulation code which is part of Akimas Fortran code. A bilinear interpolator\n  for regular grids was also added for comparison with the bicubic interpolator on\n  regular grids."}, "Rchoice": {"categories": ["Econometrics"], "description": "An implementation of simulated maximum likelihood method for the estimation of Binary (Probit and Logit), Ordered (Probit and Logit) and Poisson models with random parameters for cross-sectional and longitudinal data as presented in Sarrias (2016) <doi:10.18637/jss.v074.i10>."}, "uniah": {"categories": ["Survival"], "description": "Nonparametric estimation of a unimodal or U-shape covariate effect under additive hazards model."}, "SPAtest": {"categories": ["MetaAnalysis"], "description": "Performs score test using saddlepoint approximation to estimate the null distribution. Also prepares summary statistics for meta-analysis and performs meta-analysis to combine multiple association results. For the latest version, please check <https://github.com/leeshawn/SPAtest>."}, "crfsuite": {"categories": ["NaturalLanguageProcessing"], "description": "Wraps the 'CRFsuite' library <https://github.com/chokkan/crfsuite> allowing users \n    to fit a Conditional Random Field model and to apply it on existing data.\n    The focus of the implementation is in the area of Natural Language Processing where this R package allows you to easily build and apply models \n    for named entity recognition, text chunking, part of speech tagging, intent recognition or classification of any category you have in mind. Next to training, a small web application\n    is included in the package to allow you to easily construct training data."}, "gravity": {"categories": ["Econometrics"], "description": "A wrapper of different standard estimation methods for gravity models. \n  This package provides estimation methods for log-log models and multiplicative models."}, "sazedR": {"categories": ["TimeSeries"], "description": "Spectral and Average Autocorrelation Zero Distance Density\n    ('sazed') is a method for estimating the season length of a \n    seasonal time series. 'sazed' is aimed at practitioners, as it employs only \n    domain-agnostic preprocessing and does not depend on parameter tuning or \n    empirical constants. The computation of 'sazed' relies on the efficient \n    autocorrelation computation methods suggested by Thibauld Nion (2012, URL: \n    <https://etudes.tibonihoo.net/literate_musing/autocorrelations.html>) and by \n    Bob Carpenter (2012, URL: \n    <https://lingpipe-blog.com/2012/06/08/autocorrelation-fft-kiss-eigen/>)."}, "recurse": {"categories": ["Tracking"], "description": "Computes revisitation metrics for trajectory data, such as the number of revisitations for each location as well as the time spent for that visit and the time since the previous visit. Also includes functions to plot data."}, "webmockr": {"categories": ["WebTechnologies"], "description": "Stubbing and setting expectations on 'HTTP' requests.\n    Includes tools for stubbing 'HTTP' requests, including expected\n    request conditions and response conditions. Match on\n    'HTTP' method, query parameters, request body, headers and\n    more. Can be used for unit tests or outside of a testing \n    context."}, "EbayesThresh": {"categories": ["Bayesian"], "description": "Empirical Bayes thresholding using the methods developed\n    by I. M. Johnstone and B. W. Silverman. The basic problem is to\n    estimate a mean vector given a vector of observations of the mean\n    vector plus white noise, taking advantage of possible sparsity in\n    the mean vector. Within a Bayesian formulation, the elements of\n    the mean vector are modelled as having, independently, a\n    distribution that is a mixture of an atom of probability at zero\n    and a suitable heavy-tailed distribution. The mixing parameter can\n    be estimated by a marginal maximum likelihood approach. This leads\n    to an adaptive thresholding approach on the original data.\n    Extensions of the basic method, in particular to wavelet\n    thresholding, are also implemented within the package."}, "imbibe": {"categories": ["MedicalImaging"], "description": "Provides a set of fast, chainable image-processing operations\n             which are applicable to images of two, three or four dimensions,\n             particularly medical images."}, "ITRLearn": {"categories": ["CausalInference"], "description": "Maximin-projection learning (MPL, Shi, et al., 2018) is implemented for \n\trecommending a meaningful and reliable individualized treatment regime for future \n\tgroups of patients based on the observed data from different populations with\n\theterogeneity in individualized decision making. Q-learning and A-learning are\n\timplemented for estimating the groupwise contrast function that shares the same\n\tmarginal treatment effects. The packages contains classical Q-learning and A-learning\n\talgorithms for a single stage study as a byproduct. More functions will be added\n\tat later versions."}, "Rmosek": {"categories": ["Optimization"], "description": "This is a meta-package designed to support the\n        installation of Rmosek (>= 6.0) and bring the optimization\n        facilities of MOSEK (>= 6.0) to the R-language. The interface\n        supports large-scale optimization of many kinds: Mixed-integer\n        and continuous linear, second-order cone, exponential cone and\n        power cone optimization, as well as continuous semidefinite\n        optimization. Rmosek and the R-language are open-source\n        projects. MOSEK is a proprietary product, but unrestricted\n        trial and academic licenses are available."}, "openair": {"categories": ["Environmetrics", "Hydrology", "SpatioTemporal"], "description": "Tools to analyse, interpret and understand air\n    pollution data. Data are typically hourly time series\n    and both monitoring data and dispersion model output\n    can be analysed.  Many functions can also be applied to\n    other data, including meteorological and traffic data."}, "gss": {"categories": ["Survival"], "description": "A comprehensive package for structural multivariate\n        function estimation using smoothing splines."}, "CircStats": {"categories": ["Distributions", "Environmetrics"], "description": "Circular Statistics, from \"Topics in Circular Statistics\"\n        (2001) S. Rao Jammalamadaka and A. SenGupta, World Scientific."}, "elasticnet": {"categories": ["ChemPhys", "MachineLearning", "Psychometrics"], "description": "Provides functions for fitting the entire\n        solution path of the Elastic-Net and also provides functions\n        for doing sparse PCA.  "}, "here": {"categories": ["ReproducibleResearch"], "description": "Constructs paths to your project's files.\n    Declare the relative path of a file within your project with 'i_am()'.\n    Use the 'here()' function as a drop-in replacement for 'file.path()',\n    it will always locate the files relative to your project root."}, "fpp2": {"categories": ["TimeSeries"], "description": "All data sets required for the examples and exercises\n  in the book \"Forecasting: principles and practice\" (2nd ed, 2018)\n  by Rob J Hyndman and George Athanasopoulos <https://otexts.com/fpp2/>.\n  All packages required to run the examples are also loaded."}, "gratis": {"categories": ["TimeSeries"], "description": "\n  Generates synthetic time series based on various univariate time series models \n  including MAR and ARIMA processes. Kang, Y., Hyndman, R.J., Li, F.(2020) <doi:10.1002/sam.11461>."}, "lpSolve": {"categories": ["Optimization"], "description": "Lp_solve is freely available (under LGPL 2) software for\n        solving linear, integer and mixed integer programs. In this\n        implementation we supply a \"wrapper\" function in C and some R\n        functions that solve general linear/integer problems,\n        assignment problems, and transportation problems. This version\n        calls lp_solve version 5.5."}, "BRugs": {"categories": ["GraphicalModels"], "description": "Fully-interactive R interface to the 'OpenBUGS' software for Bayesian analysis using MCMC sampling.  Runs natively and stably in 32-bit R under Windows.  Versions running on Linux and on 64-bit R under Windows are in \"beta\" status and less efficient."}, "gb": {"categories": ["Distributions"], "description": "A collection of algorithms and functions \n  for fitting data to a generalized lambda distribution \n  via moment matching methods, and generalized \n  bootstrapping."}, "caracas": {"categories": ["NumericalMathematics"], "description": "Computer algebra via the 'SymPy' library (<https://www.sympy.org/>). \n  This makes it possible to solve equations symbolically, \n  find symbolic integrals, symbolic sums and other important quantities. "}, "dgumbel": {"categories": ["Distributions"], "description": "Gumbel distribution functions (De Haan L. (2007)\n    <doi:10.1007/0-387-34471-3>) implemented with the techniques of automatic\n    differentiation (Griewank A. (2008) <isbn:978-0-89871-659-7>).\n    With this tool, a user should be able to quickly model extreme\n    events for which the Gumbel distribution is the domain of attraction.\n    The package makes available the density function, the distribution\n    function the quantile function and a random generating function. In\n    addition, it supports gradient functions. The package combines 'Adept'\n    (C++ templated automatic differentiation) (Hogan R. (2017)\n    <doi:10.5281/zenodo.1004730>) and 'Eigen' (templated matrix-vector\n    library) for fast computations of both objective functions and exact\n    gradients. It relies on 'RcppEigen' for easy access to 'Eigen' and\n    bindings to R."}, "ffscrapr": {"categories": ["SportsAnalytics"], "description": "Helps access various Fantasy Football APIs by handling\n    authentication and rate-limiting, forming appropriate calls, and\n    returning tidy dataframes which can be easily connected to other data\n    sources."}, "cit": {"categories": ["CausalInference"], "description": "A likelihood-based hypothesis testing approach is implemented for\n  assessing causal mediation. Described in Millstein, Chen, and Breton (2016),\n  <doi:10.1093/bioinformatics/btw135>, it could be used to test for mediation\n  of a known causal association between a DNA variant, the 'instrumental variable',\n  and a clinical outcome or phenotype by gene expression or DNA methylation, the\n  potential mediator. Another example would be testing mediation of the effect\n  of a drug on a clinical outcome by the molecular target. The hypothesis test\n  generates a p-value or permutation-based FDR value with confidence intervals\n  to quantify uncertainty in the causal inference. The outcome can be represented\n  by either a continuous or binary variable, the potential mediator is continuous,\n  and the instrumental variable can be continuous or binary and is not limited to\n  a single variable but may be a design matrix representing multiple variables."}, "klaR": {"categories": ["MachineLearning"], "description": "Miscellaneous functions for classification and visualization,\n     e.g. regularized discriminant analysis, sknn() kernel-density naive Bayes, \n     an interface to 'svmlight' and stepclass() wrapper variable selection \n     for supervised classification, partimat() visualization of classification rules \n         and shardsplot() of cluster results as well as kmodes() clustering for categorical data, \n     corclust() variable clustering, variable extraction from different variable clustering models \n         and weight of evidence preprocessing."}, "hoopR": {"categories": ["SportsAnalytics"], "description": "A utility to quickly obtain clean and tidy men's\n    basketball play by play data. Provides functions to access\n    live play by play and box score data from ESPN<https://www.espn.com> with shot locations\n    when available. It is also a full NBA Stats API<https://www.nba.com/stats/> wrapper.\n    It is also a scraping and aggregating interface for Ken Pomeroy's \n    men's college basketball statistics website<https://kenpom.com>. It provides users with an\n    active subscription the capability to scrape the website tables and\n    analyze the data for themselves."}, "relliptical": {"categories": ["Distributions"], "description": "It offers random numbers generation from members of the truncated multivariate elliptical family of distribution such as the truncated versions of the Normal, Student-t, Pearson VII, Slash, Logistic, among others. Particular distributions can be provided by specifying the density generating function. It also computes the first two moments (covariance matrix as well) for some particular distributions.\n  References used for this package: Galarza, C. E., Matos, L. A., Castro, L. M., & Lachos, V. H. (2022). Moments of the doubly truncated selection elliptical distributions with emphasis on the unified multivariate skew-t distribution. Journal of Multivariate Analysis, 189, 104944 <doi:10.1016/j.jmva.2021.104944>; Ho, H. J., Lin, T. I., Chen, H. Y., & Wang, W. L. (2012). Some results on the truncated multivariate t distribution. Journal of Statistical Planning and Inference, 142(1), 25-40 <doi:10.1016/j.jspi.2011.06.006>; Valeriano, K. A., Galarza, C. E., & Matos, L. A. (2021). Moments and random number generation for the truncated elliptical family of distributions. arXiv preprint <arXiv:2112.09319>."}, "latdiag": {"categories": ["Psychometrics"], "description": "A graph\n  proposed by Rosenbaum is useful\n  for checking some properties of various\n  sorts of latent scale, this program generates commands\n  to obtain the graph using 'dot' from 'graphviz'."}, "mlrMBO": {"categories": ["Optimization"], "description": "Flexible and comprehensive R toolbox for model-based optimization\n    ('MBO'), also known as Bayesian optimization. It implements the Efficient\n    Global Optimization Algorithm and is designed for both single- and multi-\n    objective optimization with mixed continuous, categorical and conditional\n    parameters. The machine learning toolbox 'mlr' provide dozens of regression\n    learners to model the performance of the target algorithm with respect to\n    the parameter settings. It provides many different infill criteria to guide\n    the search process. Additional features include multi-point batch proposal,\n    parallel execution as well as visualization and sophisticated logging\n    mechanisms, which is especially useful for teaching and understanding of\n    algorithm behavior. 'mlrMBO' is implemented in a modular fashion, such that\n    single components can be easily replaced or adapted by the user for specific\n    use cases."}, "GSSE": {"categories": ["Survival"], "description": "We propose a fully efficient sieve maximum likelihood method to estimate genotype-specific distribution of time-to-event outcomes under a nonparametric model. We can handle missing genotypes in pedigrees.  We estimate the time-dependent hazard ratio between two genetic mutation groups using B-splines, while applying nonparametric maximum likelihood estimation to the reference baseline hazard function.  The estimators are calculated via an expectation-maximization algorithm."}, "degreenet": {"categories": ["Distributions"], "description": "Likelihood-based inference for skewed count distributions used in network modeling. \"degreenet\" is a part of the \"statnet\" suite of packages for network analysis."}, "ctmm": {"categories": ["SpatioTemporal", "Tracking"], "description": "Functions for identifying, fitting, and applying continuous-space, continuous-time stochastic movement models to animal tracking data.\n  The package is described in Calabrese et al (2016) <doi:10.1111/2041-210X.12559>, with models and methods based on those introduced in\n  Fleming & Calabrese et al (2014) <doi:10.1086/675504>,\n  Fleming et al (2014) <doi:10.1111/2041-210X.12176>,\n  Fleming et al (2015) <doi:10.1103/PhysRevE.91.032107>,\n  Fleming et al (2015) <doi:10.1890/14-2010.1>,\n  Fleming et al (2016) <doi:10.1890/15-1607>,\n  P\u00e9ron & Fleming et al (2016) <doi:10.1186/s40462-016-0084-7>,\n  Fleming & Calabrese (2017) <doi:10.1111/2041-210X.12673>,\n  P\u00e9ron et al (2017) <doi:10.1002/ecm.1260>,\n  Fleming et al (2017) <doi:10.1016/j.ecoinf.2017.04.008>,\n  Fleming et al (2018) <doi:10.1002/eap.1704>,\n  Winner & Noonan et al (2018) <doi:10.1111/2041-210X.13027>,\n  Fleming et al (2019) <doi:10.1111/2041-210X.13270>,\n  Noonan & Fleming et al (2019) <doi:10.1186/s40462-019-0177-1>,\n  Fleming et al (2020) <doi:10.1101/2020.06.12.130195>,\n  Noonan et al (2021) <doi:10.1111/2041-210X.13597>,\n  and\n  Fleming et al (2022) <doi:10.1111/2041-210X.13815>."}, "RcmdrPlugin.RMTCJags": {"categories": ["MetaAnalysis"], "description": "Mixed Treatment Comparison is a methodology to compare directly and/or indirectly health strategies (drugs, treatments, devices). This package provides an 'Rcmdr' plugin to perform Mixed Treatment Comparison for binary outcome using BUGS code from Bristol University (Lu and Ades)."}, "MCMCpack": {"categories": ["Bayesian", "Distributions", "Psychometrics", "Survival"], "description": "Contains functions to perform Bayesian\n        inference using posterior simulation for a number of\n        statistical models. Most simulation is done in compiled C++\n        written in the Scythe Statistical Library Version 1.0.3. All\n        models return 'coda' mcmc objects that can then be summarized\n        using the 'coda' package. Some useful\n        utility functions such as density functions,\n\tpseudo-random number generators for statistical\n        distributions, a general purpose Metropolis sampling algorithm,\n        and tools for visualization are provided."}, "twang": {"categories": ["CausalInference"], "description": "Provides functions for propensity score\n        estimating and weighting, nonresponse weighting, and diagnosis\n        of the weights."}, "CALIBERrfimpute": {"categories": ["MissingData"], "description": "Functions to impute using random forest under full conditional specifications (multivariate imputation by chained equations). The methods are described in Shah and others (2014) <doi:10.1093/aje/kwt312>."}, "gimme": {"categories": ["Psychometrics"], "description": "Data-driven approach for arriving at person-specific time series models. The method first identifies which relations replicate across the majority of individuals to detect signal from noise. These group-level relations are then used as a foundation for starting the search for person-specific (or individual-level) relations. See Gates & Molenaar (2012) <doi:10.1016/j.neuroimage.2012.06.026>. "}, "RcppArmadillo": {"categories": ["NumericalMathematics"], "description": "'Armadillo' is a templated C++ linear algebra library (by Conrad\n Sanderson) that aims towards a good balance between speed and ease of\n use. Integer, floating point and complex numbers are supported, as\n well as a subset of trigonometric and statistics functions. Various\n matrix decompositions are provided through optional integration with\n LAPACK and ATLAS libraries.  The 'RcppArmadillo' package includes the\n header files from the templated 'Armadillo' library. Thus users do\n not need to install 'Armadillo' itself in order to use\n 'RcppArmadillo'. From release 7.800.0 on, 'Armadillo' is licensed\n under Apache License 2; previous releases were under licensed as MPL\n 2.0 from version 3.800.0 onwards and LGPL-3 prior to that;\n 'RcppArmadillo' (the 'Rcpp' bindings/bridge to Armadillo) is licensed\n under the GNU GPL version 2 or later, as is the rest of 'Rcpp'.\n Armadillo requires a C++11 compiler."}, "fourPNO": {"categories": ["Psychometrics"], "description": "Estimate Barton & Lord's (1981) <doi:10.1002/j.2333-8504.1981.tb01255.x> \n    four parameter IRT model with lower and upper asymptotes using Bayesian\n    formulation described by Culpepper (2016) <doi:10.1007/s11336-015-9477-6>."}, "sparsevar": {"categories": ["TimeSeries"], "description": "A wrapper for sparse VAR/VECM time series models estimation\n             using penalties like ENET (Elastic Net), SCAD (Smoothly Clipped \n             Absolute Deviation) and MCP (Minimax Concave Penalty). \n             Based on the work of Sumanta Basu and George Michailidis \n             <doi:10.1214/15-AOS1315>."}, "mc.heterogeneity": {"categories": ["MetaAnalysis"], "description": "Implements a Monte Carlo Based Heterogeneity Test for standardized mean differences (d), Fisher-transformed Pearson's correlations (r), and natural-logarithm-transformed odds ratio (OR) in Meta-Analysis Studies. Depending on the presence of moderators, this Monte Carlo Based Test can be implemented in the random or mixed-effects model. This package uses rma() function from the R package 'metafor' to obtain parameter estimates and likelihood, so installation of R package 'metafor' is required. This approach refers to the studies of Hedges (1981) <doi:10.3102/10769986006002107>, Hedges & Olkin (1985, ISBN:978-0123363800), Silagy, Lancaster, Stead, Mant, & Fowler (2004) <doi:10.1002/14651858.CD000146.pub2>, Viechtbauer (2010) <doi:10.18637/jss.v036.i03>, and Zuckerman (1994, ISBN:978-0521432009)."}, "SportsTour": {"categories": ["SportsAnalytics"], "description": "Use of Knock Out and Round Robin Techniques in preparing tournament fixtures as discussed in the Book Health and Physical Education by 'Dr. V K Sharma'(2018,ISBN:978-93-5272-134-4)."}, "SpatialEpi": {"categories": ["Epidemiology", "Spatial"], "description": "Methods and data for cluster detection and disease mapping."}, "gmvarkit": {"categories": ["TimeSeries"], "description": "Unconstrained and constrained maximum likelihood estimation of structural and reduced form \n    Gaussian mixture vector autoregressive, Student's t mixture vector autoregressive, and Gaussian and Student's t\n    mixture vector autoregressive models, quantile residual tests, graphical diagnostics,\n    simulations, forecasting, and estimation of generalized impulse response function and generalized \n    forecast error variance decomposition.\n    Leena Kalliovirta, Mika Meitz, Pentti Saikkonen (2016) <doi:10.1016/j.jeconom.2016.02.012>,\n    Savi Virolainen (2022) <arXiv:2007.04713>,\n    Savi Virolainen (2022) <arXiv:2109.13648>."}, "squashinformr": {"categories": ["SportsAnalytics"], "description": "Scrape SquashInfo <http://www.squashinfo.com/> for data on the Professional Squash Association World Tour and other squash events. 'squashinformr' functions scrape, parse, and clean data associated with players, tournaments, and rankings."}, "bst": {"categories": ["MachineLearning"], "description": "Functional gradient descent algorithm for a variety of convex and non-convex loss functions, for both classical and robust regression and classification problems. See Wang (2011) <doi:10.2202/1557-4679.1304>, Wang (2012) <doi:10.3414/ME11-02-0020>, Wang (2018) <doi:10.1080/10618600.2018.1424635>, Wang (2018) <doi:10.1214/18-EJS1404>."}, "runstats": {"categories": ["TimeSeries"], "description": "Provides methods for fast computation of running sample \n    statistics for time series. These include: (1) mean, (2) \n    standard deviation, and (3) variance over a fixed-length window \n    of time-series, (4) correlation, (5) covariance, and (6) \n    Euclidean distance (L2 norm) between short-time pattern and \n    time-series. Implemented methods utilize Convolution Theorem to \n    compute convolutions via Fast Fourier Transform (FFT)."}, "eyelinker": {"categories": ["SpatioTemporal"], "description": "Imports plain-text ASC data files from EyeLink eye trackers \n    into (relatively) tidy data frames for analysis and visualization."}, "gsignal": {"categories": ["TimeSeries"], "description": "R implementation of the 'Octave' package 'signal', containing\n    a variety of signal processing tools, such as signal generation and\n    measurement, correlation and convolution, filtering, filter design,\n    filter analysis and conversion, power spectrum analysis, system\n    identification, decimation and sample rate change, and windowing."}, "Amelia": {"categories": ["MissingData"], "description": "A tool that \"multiply imputes\" missing data in a single cross-section\n  (such as a survey), from a time series (like variables collected for\n  each year in a country), or from a time-series-cross-sectional data\n  set (such as collected by years for each of several countries).\n  Amelia II implements our bootstrapping-based algorithm that gives\n  essentially the same answers as the standard IP or EMis approaches,\n  is usually considerably faster than existing approaches and can\n  handle many more variables.  Unlike Amelia I and other statistically\n  rigorous imputation software, it virtually never crashes (but please\n  let us know if you find to the contrary!).  The program also\n  generalizes existing approaches by allowing for trends in time series\n  across observations within a cross-sectional unit, as well as priors\n  that allow experts to incorporate beliefs they have about the values\n  of missing cells in their data.  Amelia II also includes useful\n  diagnostics of the fit of multiple imputation models.  The program\n  works from the R command line or via a graphical user interface that\n  does not require users to know R."}, "downloader": {"categories": ["WebTechnologies"], "description": "Provides a wrapper for the download.file function,\n    making it possible to download files over HTTPS on Windows, Mac OS X, and\n    other Unix-like platforms. The 'RCurl' package provides this functionality\n    (and much more) but can be difficult to install because it must be compiled\n    with external dependencies. This package has no external dependencies, so\n    it is much easier to install."}, "qtlnet": {"categories": ["CausalInference"], "description": "Functions to Simultaneously Infer Causal Graphs and Genetic Architecture.\n  Includes acyclic and cyclic graphs for data from an experimental cross with a modest number (<10) of phenotypes driven by\n  a few genetic loci (QTL).\n  Chaibub Neto E, Keller MP, Attie AD, Yandell BS (2010)\n  Causal Graphical Models in Systems Genetics: a unified framework for joint inference of causal network and genetic architecture for correlated phenotypes.\n  Annals of Applied Statistics 4: 320-339.\n  <doi:10.1214/09-AOAS288>."}, "odin": {"categories": ["Epidemiology"], "description": "Generate systems of ordinary differential equations\n    (ODE) and integrate them, using a domain specific language\n    (DSL).  The DSL uses R's syntax, but compiles to C in order to\n    efficiently solve the system.  A solver is not provided, but\n    instead interfaces to the packages 'deSolve' and 'dde' are\n    generated.  With these, while solving the differential equations,\n    no allocations are done and the calculations remain entirely in\n    compiled code.  Alternatively, a model can be transpiled to R for\n    use in contexts where a C compiler is not present.  After\n    compilation, models can be inspected to return information about\n    parameters and outputs, or intermediate values after calculations.\n    'odin' is not targeted at any particular domain and is suitable\n    for any system that can be expressed primarily as mathematical\n    expressions.  Additional support is provided for working with\n    delays (delay differential equations, DDE), using interpolated\n    functions during interpolation, and for integrating quantities\n    that represent arrays."}, "boutliers": {"categories": ["MetaAnalysis"], "description": "A R package for implementing outlier detection and influence diagnostics for meta-analysis. Bootstrap distributions of the influence statistics are calculated, and the thresholds to determine influential outliers are provided explicitly."}, "siplab": {"categories": ["Environmetrics", "Spatial"], "description": "A platform for computing competition indices and experimenting\n    with spatially explicit individual-based vegetation models."}, "FinancialMath": {"categories": ["Finance"], "description": "Contains financial math functions and introductory derivative functions included in the Society of Actuaries and Casualty Actuarial Society 'Financial Mathematics' exam, and some topics in the 'Models for Financial Economics' exam."}, "emoa": {"categories": ["Optimization"], "description": "Collection of building blocks for the design and analysis\n        of evolutionary multiobjective optimization algorithms."}, "experiment": {"categories": ["CausalInference", "ClinicalTrials", "ExperimentalDesign", "MissingData"], "description": "Provides various statistical methods for\n  designing and analyzing randomized experiments. One functionality\n  of the package is the implementation of randomized-block and\n  matched-pair designs based on possibly multivariate pre-treatment\n  covariates. The package also provides the tools to analyze various\n  randomized experiments including cluster randomized experiments,\n  two-stage randomized experiments, randomized experiments with \n  noncompliance, and randomized experiments with missing data."}, "sugrrants": {"categories": ["TimeSeries"], "description": "Provides 'ggplot2' graphics for analysing time\n    series data. It aims to fit into the 'tidyverse' and grammar of\n    graphics framework for handling temporal data."}, "workflowr": {"categories": ["ReproducibleResearch"], "description": "Provides a workflow for your analysis projects by combining\n  literate programming ('knitr' and 'rmarkdown') and version control\n  ('Git', via 'git2r') to generate a website containing time-stamped,\n  versioned, and documented results."}, "airGRdatassim": {"categories": ["Hydrology"], "description": "Add-on to the 'airGR' package which provides the tools to assimilate observed discharges in daily GR hydrological models. The package consists in two functions allowing to perform the assimilation of observed discharges via the Ensemble Kalman filter or the Particle filter as described in Piazzi et al. (2021) <doi:10.1029/2020WR028390>."}, "riskParityPortfolio": {"categories": ["Finance"], "description": "Fast design of risk parity portfolios for financial investment.\n    The goal of the risk parity portfolio formulation is to equalize or distribute\n    the risk contributions of the different assets, which is missing if we simply\n    consider the overall volatility of the portfolio as in the mean-variance\n    Markowitz portfolio. In addition to the vanilla formulation, where the risk\n    contributions are perfectly equalized subject to no shortselling and budget\n    constraints, many other formulations are considered that allow for box\n    constraints and shortselling, as well as the inclusion of additional\n    objectives like the expected return and overall variance. See vignette for\n    a detailed documentation and comparison, with several illustrative examples.\n    The package is based on the papers:\n    Y. Feng, and D. P. Palomar (2015). SCRIP: Successive Convex Optimization Methods\n    for Risk Parity Portfolio Design. IEEE Trans. on Signal Processing, vol. 63,\n    no. 19, pp. 5285-5300. <doi:10.1109/TSP.2015.2452219>.\n    F. Spinu (2013), An Algorithm for Computing Risk Parity Weights.\n    <doi:10.2139/ssrn.2297383>.\n    T. Griveau-Billion, J. Richard, and T. Roncalli (2013). A fast algorithm for computing\n    High-dimensional risk parity portfolios. <arXiv:1311.4057>."}, "RDota2": {"categories": ["SportsAnalytics"], "description": "An R API Client for Valve's Dota2. RDota2 can be easily used \n    to connect to the Steam API and retrieve data for Valve's popular video \n    game Dota2. You can find out more about Dota2 at \n    <http://store.steampowered.com/app/570/>."}, "cNORM": {"categories": ["Psychometrics"], "description": "Conventional methods for producing standard scores or percentiles \n    in psychometrics or biometrics are often plagued with 'jumps' or 'gaps' \n    (i.e., discontinuities) in norm tables and low confidence for assessing \n    extreme scores. The continuous norming method introduced by A. Lenhard et al.\n    (2016, <doi:10.1177/1073191116656437>; 2019, <doi:10.1371/journal.pone.0222279>;\n    2021 <doi:10.1177/0013164420928457>) estimates percentile development \n    (e. g. over age) and generates continuous test norm scores on the basis of \n    the raw data from standardization samples, without requiring assumptions \n    about the distribution of the raw data: Norm scores are directly established \n    from raw data by modeling the latter ones as a function of both percentile \n    scores and an explanatory variable (e.g., age). The method minimizes bias \n    arising from sampling and measurement error, while handling marked deviations \n    from normality, addressing bottom or ceiling effects and capturing almost \n    all of the variance in the original norm data sample. It includes procedures \n    for post stratification of norm samples to overcome bias in data collection \n    and to mitigate violations of representativeness. An online demonstration is \n    available via <https://cnorm.shinyapps.io/cNORM/>."}, "bfast": {"categories": ["TimeSeries"], "description": "Decomposition of time series into\n    trend, seasonal, and remainder components with methods for detecting and\n    characterizing abrupt changes within the trend and seasonal components. 'BFAST'\n    can be used to analyze different types of satellite image time series and can\n    be applied to other disciplines dealing with seasonal or non-seasonal time\n    series, such as hydrology, climatology, and econometrics. The algorithm can be\n    extended to label detected changes with information on the parameters of the\n    fitted piecewise linear models. 'BFAST' monitoring functionality is described\n    in Verbesselt et al. (2010) <doi:10.1016/j.rse.2009.08.014>. 'BFAST monitor'\n    provides functionality to detect disturbance in near real-time based on 'BFAST'-\n    type models, and is described in Verbesselt et al. (2012) <doi:10.1016/j.rse.2012.02.022>.\n    'BFAST Lite' approach is a flexible approach that handles missing data\n    without interpolation, and will be described in an upcoming paper.\n    Furthermore, different models can now be used to fit the\n    time series data and detect structural changes (breaks)."}, "GLMMRR": {"categories": ["Psychometrics"], "description": "Generalized Linear Mixed Model (GLMM) for Binary Randomized Response Data.\n    Includes Cauchit, Compl. Log-Log, Logistic, and Probit link functions for Bernoulli Distributed RR data.\n    RR Designs: Warner, Forced Response, Unrelated Question, Kuk, Crosswise, and Triangular. \n    Reference: Fox, J-P, Veen, D. and Klotzke, K. (2018). Generalized Linear Mixed Models for Randomized Responses. Methodology. <doi:10.1027/1614-2241/a000153>."}, "oddsapiR": {"categories": ["SportsAnalytics"], "description": "A utility to quickly obtain clean and tidy sports\n    odds from The Odds API <https://the-odds-api.com>."}, "SurvLong": {"categories": ["Survival"], "description": "Provides kernel weighting methods for estimation of proportional \n    hazards models with intermittently observed longitudinal covariates. \n    Cao H., Churpek M. M., Zeng D., and Fine J. P. (2015) \n    <doi:10.1080/01621459.2014.957289>."}, "GLDEX": {"categories": ["Cluster", "Distributions"], "description": "The fitting algorithms considered in this package have two major objectives. One is to provide a smoothing device to fit distributions to data using the weight and unweighted discretised approach based on the bin width of the histogram. The other is to provide a definitive fit to the data set using the maximum likelihood and quantile matching estimation. Other methods such as moment matching, starship method, L moment matching are also provided. Diagnostics on goodness of fit can be done via qqplots, KS-resample tests and comparing mean, variance, skewness and kurtosis of the data with the fitted distribution. References include the following: Karvanen and Nuutinen (2008) \"Characterizing the generalized lambda distribution by L-moments\" <doi:10.1016/j.csda.2007.06.021>, King and MacGillivray (1999) \"A starship method for fitting the generalised lambda distributions\" <doi:10.1111/1467-842X.00089>, Su (2005) \"A Discretized Approach to Flexibly Fit Generalized Lambda Distributions to Data\" <doi:10.22237/jmasm/1130803560>, Su (2007) \"Nmerical Maximum Log Likelihood Estimation for Generalized Lambda Distributions\" <doi:10.1016/j.csda.2006.06.008>, Su (2007) \"Fitting Single and Mixture of Generalized Lambda Distributions to Data via Discretized and Maximum Likelihood Methods: GLDEX in R\" <doi:10.18637/jss.v021.i09>, Su (2009) \"Confidence Intervals for Quantiles Using Generalized Lambda Distributions\" <doi:10.1016/j.csda.2009.02.014>, Su (2010) \"Chapter 14: Fitting GLDs and Mixture of GLDs to Data using Quantile Matching Method\" <doi:10.1201/b10159>, Su (2010) \"Chapter 15: Fitting GLD to data using GLDEX 1.0.4 in R\" <doi:10.1201/b10159>, Su (2015)   \"Flexible Parametric Quantile Regression Model\" <doi:10.1007/s11222-014-9457-1>, Su (2021) \"Flexible parametric accelerated failure time model\"<doi:10.1080/10543406.2021.1934854>."}, "bayesmeta": {"categories": ["Bayesian", "MetaAnalysis"], "description": "A collection of functions allowing to derive the posterior distribution of the two parameters in a random-effects meta-analysis, and providing functionality to evaluate joint and marginal posterior probability distributions, predictive distributions, shrinkage effects, posterior predictive p-values, etc.; For more details, see also Roever C (2020) <doi:10.18637/jss.v093.i06>."}, "TIMP": {"categories": ["ChemPhys"], "description": "A problem-solving environment (PSE) for fitting\n         separable nonlinear models to measurements arising in physics\n         and chemistry experiments; has been extensively applied to\n         time-resolved spectroscopy and FLIM-FRET data."}, "mcclust": {"categories": ["Cluster"], "description": "Implements methods for processing a sample of (hard)\n        clusterings, e.g. the MCMC output of a Bayesian clustering\n        model. Among them are methods that find a single best\n        clustering to represent the sample, which are based on the\n        posterior similarity matrix or a relabelling algorithm."}, "WeightIt": {"categories": ["CausalInference"], "description": "Generates balancing weights for causal effect estimation in observational studies with\n             binary, multi-category, or continuous point or longitudinal treatments by easing and\n             extending the functionality of several R packages and providing in-house estimation methods.\n             Available methods include propensity score weighting using generalized linear models, gradient\n             boosting machines, the covariate balancing propensity score algorithm, Bayesian additive regression trees, and\n             SuperLearner, and directly estimating balancing weights using entropy balancing, empirical\n             balancing calibration weights, energy balancing, and optimization-based weights. Also\n             allows for assessment of weights and checking of covariate balance by interfacing directly\n             with the 'cobalt' package. See the vignette \"Installing Supporting Packages\" for instructions on how\n             to install any package 'WeightIt' uses, including those that may not be on CRAN."}, "DAAG": {"categories": ["Survival"], "description": "Functions and data sets used in examples and exercises in the\n        text Maindonald, J.H. and Braun, W.J. (2003, 2007, 2010) \"Data\n        Analysis and Graphics Using R\", and in an upcoming Maindonald,\n        Braun, Andrews, and Narayan text that builds on this earlier text."}, "CTTShiny": {"categories": ["Psychometrics"], "description": "Interactive shiny application for running classical test theory (item analysis)."}, "fastICA": {"categories": ["ChemPhys", "Psychometrics"], "description": "Implementation of FastICA algorithm to perform Independent\n        Component Analysis (ICA) and Projection Pursuit."}, "bacondecomp": {"categories": ["CausalInference"], "description": "Decomposition for differences-in-differences with variation in\n    treatment timing from Goodman-Bacon (2018) <doi:10.3386/w25018>."}, "gear": {"categories": ["Spatial"], "description": "Implements common geostatistical methods in a clean, straightforward, efficient manner. The methods are discussed in Schabenberger and Gotway (2004, <ISBN:9781584883227>) and Waller and Gotway (2004, <ISBN:9780471387718>)."}, "aprof": {"categories": ["HighPerformanceComputing"], "description": "Assists the evaluation of whether and\n    where to focus code optimization, using Amdahl's law and visual aids\n    based on line profiling. Amdahl's profiler organizes profiling output\n    files (including memory profiling) in a visually appealing way.\n    It is meant to help to balance development\n    vs. execution time by helping to identify the most promising sections\n    of code to optimize and projecting potential gains. The package is\n    an addition to R's standard profiling tools and is not a wrapper for them."}, "metap": {"categories": ["MetaAnalysis"], "description": "The canonical way to perform meta-analysis involves using effect sizes.\n   When they are not available this package provides a number of methods for\n   meta-analysis of significance values including the methods of Edgington, Fisher,\n   Lancaster, Stouffer, Tippett, and Wilkinson; a number of data-sets to replicate published results;\n   and routines for graphical display."}, "PKconverter": {"categories": ["Pharmacokinetics"], "description": "Pharmacokinetics is the study of drug absorption, distribution, \n   metabolism, and excretion. The pharmacokinetics model explains that how the \n   drug concentration change as the drug moves through the different compartments\n   of the body. For pharmacokinetic modeling and analysis, it is essential\n   to understand the basic pharmacokinetic parameters. All parameters are \n   considered, but only some of parameters are used in the model. Therefore, \n   we need to convert the estimated parameters to the other parameters after \n   fitting the specific pharmacokinetic model. This package is developed to help\n   this converting work. For more detailed explanation of pharmacokinetic \n   parameters, see \"Gabrielsson and Weiner\" (2007), \"ISBN-10: 9197651001\";\n   \"Benet and Zia-Amirhosseini\" (1995) <doi:10.1177/019262339502300203>; \n   \"Mould and Upton\" (2012) <doi:10.1038/psp.2012.4>;\n   \"Mould and Upton\" (2013) <doi:10.1038/psp.2013.14>."}, "TSP": {"categories": ["Optimization"], "description": "Basic infrastructure and some algorithms for the traveling\n    salesperson problem (also traveling salesman problem; TSP).\n    The package provides some simple algorithms and\n    an interface to the Concorde TSP solver and its implementation of the\n    Chained-Lin-Kernighan heuristic. The code for Concorde\n    itself is not included in the package and has to be obtained separately.\n    Hahsler and Hornik (2007) <doi:10.18637/jss.v023.i02>."}, "storr": {"categories": ["ReproducibleResearch"], "description": "Creates and manages simple key-value stores.  These can\n    use a variety of approaches for storing the data.  This package\n    implements the base methods and support for file system, in-memory\n    and DBI-based database stores."}, "SMR": {"categories": ["Distributions"], "description": "Computes the studentized midrange distribution (pdf, cdf and quantile) and generates random numbers."}, "DrImpute": {"categories": ["MissingData"], "description": "R codes for imputing dropout events. Many statistical methods in cell type identification, visualization and lineage reconstruction do not account for dropout events ('PCAreduce', 'SC3', 'PCA', 't-SNE', 'Monocle', 'TSCAN', etc). 'DrImpute' can improve the performance of such software by imputing dropout events."}, "aster2": {"categories": ["Survival"], "description": "Aster models are exponential family regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    Unlike the aster package, this package does dependence groups (nodes of\n    the graph need not be conditionally independent given their predecessor\n    node), including multinomial and two-parameter normal as families.  Thus\n    this package also generalizes mark-capture-recapture analysis."}, "gk": {"categories": ["Distributions"], "description": "Functions for the g-and-k and generalised g-and-h distributions."}, "stlplus": {"categories": ["MissingData", "TimeSeries"], "description": "Decompose a time series into seasonal, trend, and remainder\n    components using an implementation of Seasonal Decomposition of Time\n    Series by Loess (STL) that provides several enhancements over the STL\n    method in the stats package.  These enhancements include handling missing\n    values, providing higher order (quadratic) loess smoothing with automated\n    parameter choices, frequency component smoothing beyond the seasonal and\n    trend components, and some basic plot methods for diagnostics."}, "RcppRedis": {"categories": ["Databases"], "description": "Connection to the 'Redis' key/value store using the\n C-language client library 'hiredis' (included as a fallback) with\n 'MsgPack' encoding provided via 'RcppMsgPack' headers. It now also\n includes the pub/sub functions from the 'rredis' package."}, "RcmdrPlugin.EZR": {"categories": ["MetaAnalysis"], "description": "EZR (Easy R) adds a variety of statistical functions, including survival analyses, ROC analyses, metaanalyses, sample size calculation, and so on, to the R commander. EZR enables point-and-click easy access to statistical functions, especially for medical statistics. EZR is platform-independent and runs on Windows, Mac OS X, and UNIX. Its complete manual is available only in Japanese (Chugai Igakusha, ISBN: 978-4-498-10918-6, Nankodo, ISBN: 978-4-524-26158-1, Ohmsha, ISBN: 978-4-274-22632-8), but an report that introduced the investigation of EZR was published in Bone Marrow Transplantation (Nature Publishing Group) as an Open article. This report can be used as a simple manual. It can be freely downloaded from the journal website as shown below. This report has been cited in more than 3,000 scientific articles."}, "stochvol": {"categories": ["Bayesian", "Finance", "TimeSeries"], "description": "Efficient algorithms for fully Bayesian estimation of stochastic volatility (SV) models with and without asymmetry (leverage) via Markov chain Monte Carlo (MCMC) methods. Methodological details are given in Kastner and Fr\u00fchwirth-Schnatter (2014) <doi:10.1016/j.csda.2013.01.002> and Hosszejni and Kastner (2019) <doi:10.1007/978-3-030-30611-3_8>; the most common use cases are described in Hosszejni and Kastner (2021) <doi:10.18637/jss.v100.i12> and Kastner (2016) <doi:10.18637/jss.v069.i05> and the package examples."}, "sanon": {"categories": ["MissingData"], "description": "There are several functions to implement the method for analysis in a randomized clinical trial with strata with following key features. A stratified Mann-Whitney estimator addresses the comparison between two randomized groups for a strictly ordinal response variable. The multivariate vector of such stratified Mann-Whitney estimators for multivariate response variables can be considered for one or more response variables such as in repeated measurements and these can have missing completely at random (MCAR) data. Non-parametric covariance adjustment is also considered with the minimal assumption of randomization. The p-value for hypothesis test and confidence interval are provided."}, "BMTAR": {"categories": ["MissingData", "TimeSeries"], "description": "Implements parameter estimation using a Bayesian approach for Multivariate Threshold Autoregressive (MTAR) models with missing data using Markov Chain Monte Carlo methods. Performs the simulation of MTAR processes (mtarsim()), estimation of matrix parameters and the threshold values (mtarns()),  identification of the autoregressive orders using Bayesian variable selection (mtarstr()), identification of the number of regimes using Metropolised Carlin and Chib (mtarnumreg()) and estimate missing data, coefficients and covariance matrices conditional on the autoregressive orders, the threshold values and the number of regimes (mtarmissing()). Calderon and Nieto (2017) <doi:10.1080/03610926.2014.990758>."}, "glpkAPI": {"categories": ["Optimization"], "description": "R Interface to C API of GLPK, depends on GLPK Version >= 4.42."}, "httptest": {"categories": ["WebTechnologies"], "description": "Testing and documenting code that communicates with remote servers\n    can be painful. Dealing with authentication, server state,\n    and other complications can make testing seem too costly to\n    bother with. But it doesn't need to be that hard. This package enables one\n    to test all of the logic on the R sides of the API in your package without\n    requiring access to the remote service. Importantly, it provides three\n    contexts that mock the network connection in different ways, as well as\n    testing functions to assert that HTTP requests were\u2014or were\n    not\u2014made. It also allows one to safely record real API responses to use as\n    test fixtures. The ability to save responses and load them offline also\n    enables one to write vignettes and other dynamic documents that can be\n    distributed without access to a live server."}, "bnma": {"categories": ["MetaAnalysis"], "description": "Network meta-analyses using Bayesian framework following Dias et al. (2013) <doi:10.1177/0272989X12458724>. Based on the data input, creates prior, model file, and initial values needed to run models in 'rjags'. Able to handle binomial, normal and multinomial arm-level data. Can handle multi-arm trials and includes methods to incorporate covariate and baseline risk effects. Includes standard diagnostics and visualization tools to evaluate the results."}, "desirability": {"categories": ["ExperimentalDesign", "Optimization"], "description": "S3 classes for multivariate optimization using the desirability function by Derringer and Suich (1980)."}, "R.cache": {"categories": ["ReproducibleResearch"], "description": "Memoization can be used to speed up repetitive and computational expensive function calls.  The first time a function that implements memoization is called the results are stored in a cache memory.  The next time the function is called with the same set of parameters, the results are momentarily retrieved from the cache avoiding repeating the calculations.  With this package, any R object can be cached in a key-value storage where the key can be an arbitrary set of R objects.  The cache memory is persistent (on the file system)."}, "NetworkToolbox": {"categories": ["Psychometrics"], "description": "Implements network analysis and graph theory measures used in neuroscience, cognitive science, and psychology. Methods include various filtering methods and approaches such as threshold, dependency (Kenett, Tumminello, Madi, Gur-Gershgoren, Mantegna, & Ben-Jacob, 2010 <doi:10.1371/journal.pone.0015032>), Information Filtering Networks (Barfuss, Massara, Di Matteo, & Aste, 2016 <doi:10.1103/PhysRevE.94.062306>), and Efficiency-Cost Optimization (Fallani, Latora, & Chavez, 2017 <doi:10.1371/journal.pcbi.1005305>). Brain methods include the recently developed Connectome Predictive Modeling (see references in package). Also implements several network measures including local network characteristics (e.g., centrality), community-level network characteristics (e.g., community centrality), global network characteristics (e.g., clustering coefficient), and various other measures associated with the reliability and reproducibility of network analysis. "}, "RTransferEntropy": {"categories": ["TimeSeries"], "description": "Measuring information flow between time series with Shannon and R\u00e9nyi transfer entropy. See also Dimpfl and Peter (2013) <doi:10.1515/snde-2012-0044> and Dimpfl and Peter (2014) <doi:10.1016/j.intfin.2014.03.004> for theory and applications to financial time series. Additional references can be found in the theory part of the vignette."}, "gogarch": {"categories": ["Finance"], "description": "Provision of classes and methods for estimating generalized\n   orthogonal GARCH models. This is an alternative approach to CC-GARCH models\n   in the context of multivariate volatility modeling."}, "TSHRC": {"categories": ["Survival"], "description": "Two-stage procedure compares hazard rate functions,\n    which may or may not cross each other."}, "EEM": {"categories": ["ChemPhys"], "description": "Read raw EEM data and prepares them for further analysis."}, "bhm": {"categories": ["CausalInference"], "description": "Contains tools to fit both predictive and prognostic biomarker effects using biomarker threshold models and continuous threshold models. Evaluate the treatment effect, biomarker effect and treatment-biomarker interaction using probability index measurement. Test for treatment-biomarker interaction using residual bootstrap method."}, "stellaR": {"categories": ["ChemPhys"], "description": "Manages and display stellar tracks and isochrones from \n\t     Pisa low-mass database. Includes tools for isochrones\n\t     construction and tracks interpolation. "}, "NMADiagT": {"categories": ["MetaAnalysis", "MissingData"], "description": "Implements HSROC (hierarchical summary receiver operating characteristic) model developed by Ma, Lian, Chu, Ibrahim, and Chen (2018) <doi:10.1093/biostatistics/kxx025>\n  and hierarchical model developed by Lian, Hodges, and Chu (2019) <doi:10.1080/01621459.2018.1476239> for performing meta-analysis for 1-5 diagnostic tests to simultaneously \n    compare multiple tests within a missing data framework. This package evaluates the accuracy of \n    multiple diagnostic tests and also gives graphical representation of the results."}, "RobKF": {"categories": ["TimeSeries"], "description": "Implements a series of robust Kalman filtering approaches. It implements the additive outlier robust filters of Ruckdeschel et al. (2014) <arXiv:1204.3358> and Agamennoni et al. (2018) <doi:10.1109/ICRA.2011.5979605>, the innovative outlier robust filter of Ruckdeschel et al. (2014) <arXiv:1204.3358>, as well as the innovative and additive outlier robust filter of Fisch et al. (2020) <arXiv:2007.03238>."}, "MatchIt": {"categories": ["CausalInference", "OfficialStatistics"], "description": "Selects matched samples of the original treated and\n    control groups with similar covariate distributions \u2013 can be\n    used to match exactly on covariates, to match on propensity\n    scores, or perform a variety of other matching procedures.  The\n    package also implements a series of recommendations offered in\n    Ho, Imai, King, and Stuart (2007) <doi:10.1093/pan/mpl013>. (The \n    'gurobi' package, which is not on CRAN, is optional and comes with \n    an installation of the Gurobi Optimizer, available at \n    <https://www.gurobi.com>.)"}, "autohd": {"categories": ["Bayesian"], "description": "Perform mediation analysis for time to event high-dimensional data.\n             Mediation Analysis proposed by Miocevic et al.(2017)  <doi:10.1080/10705511.2017.1342541> \n             as a statistical tool in the Bayesian framework. \n             Time to event data analysis methods like Cox proportional hazard model, \n             accelerated failure time model to work with high dimensional data \n             with Bayesian approaches are provided. Missing data imputation techniques tool\n             to work with high dimensional data coupled for mediation analysis by \n             presented by the active mediator variables."}, "wk": {"categories": ["Spatial"], "description": "Provides a minimal R and C++ API for parsing\n  well-known binary and well-known text representation of\n  geometries to and from R-native formats. \n  Well-known binary is compact\n  and fast to parse; well-known text is human-readable\n  and is useful for writing tests. These formats are only\n  useful in R if the information they contain can be \n  accessed in R, for which high-performance functions \n  are provided here."}, "wql": {"categories": ["Hydrology"], "description": "Functions to assist in the processing and\n    exploration of data from environmental monitoring programs.\n    The package name stands for \"water quality\" and reflects the\n    original focus on time series data for physical and chemical\n    properties of water, as well as the biota. Intended for\n    programs that sample approximately monthly, quarterly or\n    annually at discrete stations, a feature of many legacy data\n    sets. Most of the functions should be useful for analysis of\n    similar-frequency time series regardless of the subject\n    matter."}, "pmxTools": {"categories": ["Pharmacokinetics"], "description": "Pharmacometric tools for common data analytical tasks; closed-form solutions for calculating concentrations at given \n    times after dosing based on compartmental PK models (1-compartment, 2-compartment and 3-compartment, covering infusions, zero- \n    and first-order absorption, and lag times, after single doses and at steady state, per Bertrand & Mentre (2008) \n    <http://lixoft.com/wp-content/uploads/2016/03/PKPDlibrary.pdf>); parametric simulation from NONMEM-generated parameter estimates \n    and other output; and parsing, tabulating and plotting results generated by Perl-speaks-NONMEM (PsN)."}, "getspres": {"categories": ["MetaAnalysis"], "description": "An implementation of SPRE (standardised predicted random-effects)\n    statistics in R to explore heterogeneity in genetic association meta-\n    analyses, as described by Magosi et al. (2019) \n    <doi:10.1093/bioinformatics/btz590>. SPRE statistics are precision \n    weighted residuals that indicate the direction and extent with which \n    individual study-effects in a meta-analysis deviate from the average \n    genetic effect. Overly influential positive outliers have the potential \n    to inflate average genetic effects in a meta-analysis whilst negative \n    outliers might lower or change the direction of effect. See the 'getspres' \n    website for documentation and examples \n    <https://magosil86.github.io/getspres/>."}, "gmp": {"categories": ["NumericalMathematics"], "description": "Multiple Precision Arithmetic (big integers and rationals,\n prime number tests, matrix computation), \"arithmetic without limitations\"\n using the C library GMP (GNU Multiple Precision Arithmetic)."}, "earth": {"categories": ["Environmetrics", "MachineLearning"], "description": "Build regression models using the techniques in Friedman's\n    papers \"Fast MARS\" and \"Multivariate Adaptive Regression\n    Splines\" <doi:10.1214/aos/1176347963>.\n    (The term \"MARS\" is trademarked and thus not used in\n    the name of the package.)"}, "bigmemory": {"categories": ["HighPerformanceComputing"], "description": "Create, store, access, and manipulate massive matrices.\n    Matrices are allocated to shared memory and may use memory-mapped\n    files.  Packages 'biganalytics', 'bigtabulate', 'synchronicity', and\n    'bigalgebra' provide advanced functionality."}, "areal": {"categories": ["MissingData", "Spatial"], "description": "A pipeable, transparent implementation of areal weighted interpolation\n    with support for interpolating multiple variables in a single function call.\n    These tools provide a full-featured workflow for validation and estimation\n    that fits into both modern data management (e.g. tidyverse) and spatial \n    data (e.g. sf) frameworks."}, "STARTS": {"categories": ["Psychometrics"], "description": "\n    Contains functions for estimating the STARTS model of\n    Kenny and Zautra (1995, 2001) <doi:10.1037/0022-006X.63.1.52>,\n    <doi:10.1037/10409-008>. Penalized maximum likelihood\n    estimation and Markov Chain Monte Carlo estimation are\n    also provided, see Luedtke, Robitzsch and Wagner (2018) \n    <doi:10.1037/met0000155>."}, "Sleuth2": {"categories": ["TeachingStatistics"], "description": "Data sets from Ramsey, F.L. and Schafer, D.W. (2002), \"The\n    Statistical Sleuth: A Course in Methods of Data Analysis (2nd\n    ed)\", Duxbury. "}, "spectral": {"categories": ["TimeSeries"], "description": "On discrete data spectral analysis is performed by Fourier and Hilbert\n    transforms as well as with model based analysis called Lomb-Scargle method.\n    Fragmented and irregularly spaced data can be processed in almost all methods. Both,\n    FFT as well as LOMB methods take multivariate data and return standardized PSD. \n    For didactic reasons an analytical approach for deconvolution of noise spectra and \n    sampling function is provided.\n    A user friendly interface helps to interpret the results."}, "elasdics": {"categories": ["FunctionalData"], "description": "Provides functions to align curves and to compute mean curves based on the \n    elastic distance defined in the square-root-velocity framework. For more details on \n    this framework see Srivastava and Klassen (2016, <doi:10.1007/978-1-4939-4020-2>). \n    For more theoretical details on our methods and algorithms see \n    Steyer et al. (2021, <arXiv:2104.11039>)."}, "ForeCA": {"categories": ["TimeSeries"], "description": "Implementation of Forecastable Component Analysis ('ForeCA'),\n    including main algorithms and auxiliary function (summary, plotting, etc.) to\n    apply 'ForeCA' to multivariate time series data. 'ForeCA' is a novel dimension\n    reduction (DR) technique for temporally dependent signals. Contrary to other\n    popular DR methods, such as 'PCA' or 'ICA', 'ForeCA' takes time dependency\n    explicitly into account and searches for the most \u201dforecastable\u201d signal.\n    The measure of forecastability is based on the Shannon entropy of the spectral\n    density of the transformed signal."}, "topmodel": {"categories": ["Environmetrics", "Hydrology"], "description": "Set of hydrological functions including an R\n        implementation of the hydrological model TOPMODEL, which is\n        based on the 1995 FORTRAN version by Keith Beven. From version\n        0.7.0, the package is put into maintenance mode."}, "climextRemes": {"categories": ["ExtremeValue"], "description": "Functions for fitting GEV and POT (via point process fitting)\n    models for extremes in climate data, providing return values, return\n    probabilities, and return periods for stationary and nonstationary models.\n    Also provides differences in return values and differences in log return\n    probabilities for contrasts of covariate values. Functions for estimating risk\n    ratios for event attribution analyses, including uncertainty. Under the hood,\n    many of the functions use functions from 'extRemes', including for fitting the\n    statistical models. Details are given in Paciorek, Stone, and Wehner (2018)\n    <doi:10.1016/j.wace.2018.01.002>."}, "gtfs2gps": {"categories": ["Tracking"], "description": "Convert general transit feed specification (GTFS) data to global positioning system (GPS) records in 'data.table' format. It also has some functions to subset GTFS data in time and space and to convert both representations to simple feature format."}, "fso": {"categories": ["Environmetrics"], "description": "Fuzzy set ordination is a multivariate analysis used in ecology to\n    relate the composition of samples to possible explanatory variables.  While\n    differing in theory and method, in practice, the use is similar to 'constrained\n    ordination.'  The package contains plotting and summary functions as well as\n    the analyses.  "}, "optmatch": {"categories": ["CausalInference"], "description": "Distance based bipartite matching using minimum cost flow, oriented\n    to matching of treatment and control groups in observational studies (Hansen\n    and Klopfer 2006 <doi:10.1198/106186006X137047>). Routines are provided to\n    generate distances from generalised linear models (propensity score\n    matching), formulas giving variables on which to limit matched distances,\n    stratified or exact matching directives, or calipers, alone or in\n    combination."}, "MaOEA": {"categories": ["Optimization"], "description": "A set of evolutionary algorithms to solve many-objective optimization. \n    Hybridization between the algorithms are also facilitated. Available algorithms are:\n    'SMS-EMOA' <doi:10.1016/j.ejor.2006.08.008>\n    'NSGA-III' <doi:10.1109/TEVC.2013.2281535>\n    'MO-CMA-ES' <doi:10.1145/1830483.1830573>\n    The following many-objective benchmark problems are also provided: \n    'DTLZ1'-'DTLZ4' from Deb, et al. (2001) <doi:10.1007/1-84628-137-7_6> and\n    'WFG4'-'WFG9' from Huband, et al. (2005) <doi:10.1109/TEVC.2005.861417>."}, "Rsymphony": {"categories": ["Optimization"], "description": "An R interface to the SYMPHONY solver for mixed-integer linear programs."}, "redcapAPI": {"categories": ["WebTechnologies"], "description": "Access data stored in 'REDCap' databases using the Application\n    Programming Interface (API).  'REDCap' (Research Electronic Data CAPture;\n    <https://projectredcap.org>) is\n    a web application for building and managing online surveys and databases\n    developed at Vanderbilt University.  The API allows users to access data\n    and project meta data (such as the data dictionary) from the web\n    programmatically.  The 'redcapAPI' package facilitates the process of\n    accessing data with options to prepare an analysis-ready data set\n    consistent with the definitions in a database's data dictionary."}, "fsn": {"categories": ["MetaAnalysis"], "description": "Estimation of Rosenthal's fail safe number including confidence intervals. The relevant papers are the following. Konstantinos C. Fragkos, Michail Tsagris and Christos C. Frangos (2014). \"Publication Bias in Meta-Analysis: Confidence Intervals for Rosenthal's Fail-Safe Number\". International Scholarly Research Notices, Volume 2014. <doi:10.1155/2014/825383>. Konstantinos C. Fragkos, Michail Tsagris and Christos C. Frangos (2017). \"Exploring the distribution for the estimator of Rosenthal's fail-safe number of unpublished studies in meta-analysis\". Communications in Statistics-Theory and Methods, 46(11):5672\u20135684. <doi:10.1080/03610926.2015.1109664>."}, "openEBGM": {"categories": ["Bayesian"], "description": "An implementation of DuMouchel's (1999) \n  <doi:10.1080/00031305.1999.10474456> Bayesian data mining method for the\n  market basket problem. Calculates Empirical Bayes Geometric Mean (EBGM) and\n  quantile scores from the posterior distribution using the Gamma-Poisson\n  Shrinker (GPS) model to find unusually large cell counts in large, sparse\n  contingency tables. Can be used to find unusually high reporting rates of\n  adverse events associated with products. In general, can be used to mine any\n  database where the co-occurrence of two variables or items is of interest.\n  Also calculates relative and proportional reporting ratios. Builds on the work\n  of the 'PhViD' package, from which much of the code is derived. Some of the\n  added features include stratification to adjust for confounding variables and\n  data squashing to improve computational efficiency. Now includes an\n  implementation of the EM algorithm for hyperparameter estimation loosely\n  derived from the 'mederrRank' package."}, "tbrf": {"categories": ["TimeSeries"], "description": "Provides rolling statistical functions based\n    on date and time windows instead of n-lagged observations."}, "mrds": {"categories": ["Environmetrics"], "description": "Animal abundance estimation via conventional, multiple covariate\n    and mark-recapture distance sampling (CDS/MCDS/MRDS). Detection function\n    fitting is performed via maximum likelihood. Also included are diagnostics\n    and plotting for fitted detection functions. Abundance estimation is via a\n    Horvitz-Thompson-like estimator."}, "survAUC": {"categories": ["Survival"], "description": "Provides a variety of functions to estimate\n        time-dependent true/false positive rates and AUC curves from a\n        set of censored survival data."}, "bpca": {"categories": ["Psychometrics"], "description": "Implements biplot (2d and 3d) of multivariate data based\n             on principal components analysis and diagnostic tools of the quality of the reduction."}, "funFEM": {"categories": ["Cluster", "FunctionalData"], "description": "The funFEM algorithm (Bouveyron et al., 2014) allows to cluster functional data by modeling the curves within a common and discriminative functional subspace."}, "RJSONIO": {"categories": ["WebTechnologies"], "description": "This is a package that allows conversion to and from \n  data in Javascript object notation (JSON) format.\n  This allows R objects to be inserted into Javascript/ECMAScript/ActionScript code\n  and allows R programmers to read and convert JSON content to R objects.\n  This is an alternative to rjson package. Originally, that was too slow for converting large R objects to JSON\n  and was not extensible.  rjson's performance is now similar to this package, and perhaps slightly faster in some cases.\n  This package uses methods and is readily extensible by defining methods for different classes, \n  vectorized operations, and C code and callbacks to R functions for deserializing JSON objects to R. \n  The two packages intentionally share the same basic interface. This package (RJSONIO) has many additional\n  options to allow customizing the generation and processing of JSON content.\n  This package uses libjson rather than implementing yet another JSON parser. The aim is to support\n  other general projects by building on their work, providing feedback and benefit from their ongoing development."}, "MBC": {"categories": ["Hydrology"], "description": "Calibrate and apply multivariate bias correction algorithms\n    for climate model simulations of multiple climate variables. Three methods\n    described by Cannon (2016) <doi:10.1175/JCLI-D-15-0679.1> and \n    Cannon (2018) <doi:10.1007/s00382-017-3580-6> are implemented \u2014\n    (i) MBC Pearson correlation (MBCp), (ii) MBC rank correlation (MBCr),\n    and (iii) MBC N-dimensional PDF transform (MBCn) \u2014 as is the Rank\n    Resampling for Distributions and Dependences (R2D2) method."}, "cabinets": {"categories": ["ReproducibleResearch"], "description": "Creates project specific directory and file templates that are \n written to a .Rprofile file. Upon starting a new R session, these templates \n can be used to streamline the creation of new directories that are \n standardized to the user's preferences and can include the initiation of a \n git repository, an RStudio R project, and project-local dependency management \n with the 'renv' package.  "}, "RobPer": {"categories": ["ChemPhys", "Robust"], "description": "Calculates periodograms based on (robustly) fitting periodic functions to light curves (irregularly observed time series, possibly with measurement accuracies, occurring in astroparticle physics). Three main functions are included: RobPer() calculates the periodogram. Outlying periodogram bars (indicating a period) can be detected with betaCvMfit(). Artificial light curves can be generated using the function tsgen(). For more details see the corresponding article: Thieler, Fried and Rathjens (2016), Journal of Statistical Software 69(9), 1-36, <doi:10.18637/jss.v069.i09>."}, "AIPW": {"categories": ["CausalInference"], "description": "The 'AIPW' pacakge implements the augmented inverse probability weighting, a doubly robust estimator, for average causal effect estimation with user-defined stacked machine learning algorithms. To cite the 'AIPW' package, please use: \"Yongqi Zhong, Edward H. Kennedy, Lisa M. Bodnar, Ashley I. Naimi (2021, In Press). AIPW: An R Package for Augmented Inverse Probability Weighted Estimation of Average Causal Effects. American Journal of Epidemiology\". Visit: <https://yqzhong7.github.io/AIPW/> for more information."}, "metafor": {"categories": ["ClinicalTrials", "MetaAnalysis"], "description": "A comprehensive collection of functions for conducting meta-analyses in R. The package includes functions to calculate various effect sizes or outcome measures, fit equal-, fixed-, random-, and mixed-effects models to such data, carry out moderator and meta-regression analyses, and create various types of meta-analytical plots (e.g., forest, funnel, radial, L'Abbe, Baujat, bubble, and GOSH plots). For meta-analyses of binomial and person-time data, the package also provides functions that implement specialized methods, including the Mantel-Haenszel method, Peto's method, and a variety of suitable generalized linear (mixed-effects) models (i.e., mixed-effects logistic and Poisson regression models). Finally, the package provides functionality for fitting meta-analytic multivariate/multilevel models that account for non-independent sampling errors and/or true effects (e.g., due to the inclusion of multiple treatment studies, multiple endpoints, or other forms of clustering). Network meta-analyses and meta-analyses accounting for known correlation structures (e.g., due to phylogenetic relatedness) can also be conducted. An introduction to the package can be found in Viechtbauer (2010) <doi:10.18637/jss.v036.i03>."}, "stringi": {"categories": ["NaturalLanguageProcessing"], "description": "A collection of character string/text/natural language\n    processing tools for pattern searching (e.g., with 'Java'-like regular\n    expressions or the 'Unicode' collation algorithm), random string generation,\n    case mapping, string transliteration, concatenation, sorting, padding,\n    wrapping, Unicode normalisation, date-time formatting and parsing,\n    and many more. They are fast, consistent, convenient, and -\n    thanks to 'ICU' (International Components for Unicode) -\n    portable across all locales and platforms.\n    Documentation about 'stringi' is provided\n    via its website at <https://stringi.gagolewski.com/> and\n    the paper by Gagolewski (2022, <doi:10.18637/jss.v103.i02>)."}, "mapview": {"categories": ["Spatial"], "description": "Quickly and conveniently create interactive\n    visualisations of spatial data with or without background maps.\n    Attributes of displayed features are fully queryable via pop-up\n    windows. Additional functionality includes methods to visualise true-\n    and false-color raster images and bounding boxes."}, "mvLSW": {"categories": ["TimeSeries"], "description": "Tools for analysing multivariate time series with wavelets. This includes: simulation of a multivariate locally stationary wavelet (mvLSW) process from a multivariate evolutionary wavelet spectrum (mvEWS); estimation of the mvEWS, local coherence and local partial coherence. See Park, Eckley and Ombao (2014) <doi:10.1109/TSP.2014.2343937> for details."}, "micEconIndex": {"categories": ["OfficialStatistics"], "description": "Tools for calculating Laspeyres, Paasche, and Fisher\n  price and quantity indices."}, "ltmle": {"categories": ["CausalInference"], "description": "Targeted Maximum Likelihood Estimation (TMLE) of\n    treatment/censoring specific mean outcome or marginal structural model for\n    point-treatment and longitudinal data."}, "bkmr": {"categories": ["Epidemiology"], "description": "Implementation of a statistical approach \n  for estimating the joint health effects of multiple \n  concurrent exposures, as described in Bobb et al (2015) \n  <doi:10.1093/biostatistics/kxu058>."}, "lme4": {"categories": ["Econometrics", "Environmetrics", "Psychometrics", "SpatioTemporal"], "description": "Fit linear and generalized linear mixed-effects models.\n    The models and their components are represented using S4 classes and\n    methods.  The core computational algorithms are implemented using the\n    'Eigen' C++ library for numerical linear algebra and 'RcppEigen' \"glue\"."}, "SiMRiv": {"categories": ["SpatioTemporal", "Tracking"], "description": "Provides functions to generate and analyze spatially-explicit individual-based multistate movements in rivers,\n  heterogeneous and homogeneous spaces. This is done by incorporating landscape bias on local behaviour, based on\n  resistance rasters. Although originally conceived and designed to simulate trajectories of species constrained to\n  linear habitats/dendritic ecological networks (e.g. river networks), the simulation algorithm is built to be\n  highly flexible and can be applied to any (aquatic, semi-aquatic or terrestrial) organism, independently on the\n  landscape in which it moves. Thus, the user will be able to use the package to simulate movements either in\n  homogeneous landscapes, heterogeneous landscapes (e.g. semi-aquatic animal moving mainly along rivers but also using\n  the matrix), or even in highly contrasted landscapes (e.g. fish in a river network). The algorithm and its input\n  parameters are the same for all cases, so that results are comparable. Simulated trajectories can then be used as\n  mechanistic null models (Potts & Lewis 2014, <doi:10.1098/rspb.2014.0231>) to test a variety of 'Movement Ecology'\n  hypotheses (Nathan et al. 2008, <doi:10.1073/pnas.0800375105>), including landscape effects (e.g. resources, \n  infrastructures) on animal movement and species site fidelity, or for predictive purposes (e.g. road mortality risk,\n  dispersal/connectivity). The package should be relevant to explore a broad spectrum of ecological phenomena, such as\n  those at the interface of animal behaviour, management, landscape and movement ecology, disease and invasive species\n  spread, and population dynamics."}, "metadat": {"categories": ["MetaAnalysis"], "description": "A collection of meta-analysis datasets for teaching purposes, illustrating/testing meta-analytic methods, and validating published analyses."}, "RSEIS": {"categories": ["TimeSeries"], "description": "Multiple interactive codes to view and analyze seismic data, via spectrum analysis, wavelet transforms, particle motion, hodograms.  Includes general time-series tools, plotting, filtering, interactive display."}, "gistr": {"categories": ["WebTechnologies"], "description": "Work with 'GitHub' 'gists' from 'R' (e.g., \n    <https://en.wikipedia.org/wiki/GitHub#Gist>, \n    <https://docs.github.com/en/github/writing-on-github/creating-gists/>). A 'gist'\n    is simply one or more files with code/text/images/etc. This package allows\n    the user to create new 'gists', update 'gists' with new files, rename files,\n    delete files, get and delete 'gists', star and 'un-star' 'gists', fork 'gists',\n    open a 'gist' in your default browser, get embed code for a 'gist', list\n    'gist' 'commits', and get rate limit information when 'authenticated'. Some\n    requests require authentication and some do not. 'Gists' website: \n    <https://gist.github.com/>."}, "reproducible": {"categories": ["ReproducibleResearch"], "description": "Collection of high-level, machine- and OS-independent tools\n    for making deeply reproducible and reusable content in R.\n    The two workhorse functions are Cache and prepInputs; \n    these allow for: nested caching, robust to environments, and objects with\n    environments (like functions); and data retrieval and processing \n    in continuous workflow environments. In all cases,\n    efforts are made to make the first and subsequent calls of functions have \n    the same result, but vastly faster at subsequent times by way of checksums\n    and digesting. Several features are still under active development, including\n    cloud storage of cached objects, allowing for sharing between users. Several\n    advanced options are available, see ?reproducibleOptions."}, "rucrdtw": {"categories": ["TimeSeries"], "description": "R bindings for functions from the UCR Suite by Rakthanmanon et al. (2012) <doi:10.1145/2339530.2339576>, which enables ultrafast subsequence\n      search for a best match under Dynamic Time Warping and Euclidean Distance."}, "RODBC": {"categories": ["Databases"], "description": "An ODBC database interface."}, "TreeSim": {"categories": ["MissingData"], "description": "Simulation methods for phylogenetic trees where (i) all tips are sampled at one time point or (ii) tips are sampled sequentially through time. (i) For sampling at one time point, simulations are performed under a constant rate birth-death process, conditioned on having a fixed number of final tips (sim.bd.taxa()), or a fixed age (sim.bd.age()), or a fixed age and number of tips (sim.bd.taxa.age()). When conditioning on the number of final tips, the method allows for shifts in rates and mass extinction events during the birth-death process (sim.rateshift.taxa()). The function sim.bd.age() (and sim.rateshift.taxa() without extinction) allow the speciation rate to change in a density-dependent way. The LTT plots of the simulations can be displayed using LTT.plot(), LTT.plot.gen() and LTT.average.root(). TreeSim further samples trees with n final tips from a set of trees generated by the common sampling algorithm stopping when a fixed number m>>n of tips is first reached (sim.gsa.taxa()). This latter method is appropriate for m-tip trees generated under a big class of models (details in the sim.gsa.taxa() man page). For incomplete phylogeny, the missing speciation events can be added through simulations (corsim()). (ii) sim.rateshifts.taxa() is generalized to sim.bdsky.stt() for serially sampled trees, where the trees are conditioned on either the number of sampled tips or the age. Furthermore, for a multitype-branching process with sequential sampling, trees on a fixed number of tips can be simulated using sim.bdtypes.stt.taxa(). This function further allows to simulate under epidemiological models with an exposed class. The function sim.genespeciestree() simulates coalescent gene trees within birth-death species trees, and sim.genetree() simulates coalescent gene trees."}, "samplesize": {"categories": ["ClinicalTrials"], "description": "Computes sample size for Student's t-test and for the Wilcoxon-Mann-Whitney test for categorical data. The t-test function allows paired and unpaired (balanced / unbalanced) designs as well as homogeneous and heterogeneous variances. The Wilcoxon function allows for ties."}, "ui": {"categories": ["CausalInference"], "description": "Implements functions to derive uncertainty intervals for (i) regression (linear and probit) parameters when outcome is missing not at random (non-ignorable missingness) introduced in Genbaeck, M., Stanghellini, E., de Luna, X. (2015) <doi:10.1007/s00362-014-0610-x> and Genbaeck, M., Ng, N., Stanghellini, E., de Luna, X. (2018) <doi:10.1007/s10433-017-0448-x>; and (ii) double robust and outcome regression estimators of average causal effects (on the treated) with possibly unobserved confounding introduced in Genbaeck, M., de Luna, X. (2018) <doi:10.1111/biom.13001>."}, "LCAvarsel": {"categories": ["Cluster", "Psychometrics"], "description": "Variable selection for latent class analysis for model-based clustering of multivariate categorical data. The package implements a general framework for selecting the subset of variables with relevant clustering information and discard those that are redundant and/or not informative. The variable selection method is based on the approach of Fop et al. (2017) <doi:10.1214/17-AOAS1061> and Dean and Raftery (2010) <doi:10.1007/s10463-009-0258-9>. Different algorithms are available to perform the selection: stepwise, swap-stepwise and evolutionary stochastic search. Concomitant covariates used to predict the class membership probabilities can also be included in the latent class analysis model. The selection procedure can be run in parallel on multiple cores machines."}, "NonNorMvtDist": {"categories": ["Distributions"], "description": "Implements calculation of probability density function, cumulative distribution function, equicoordinate quantile function and survival function, and random numbers generation for the following multivariate distributions: Lomax (Pareto Type II), generalized Lomax, Mardia\u2019s Pareto of Type I, Logistic, Burr, Cook-Johnson\u2019s uniform, F and Inverted Beta. See Tapan Nayak (1987) <doi:10.2307/3214068>."}, "FBFsearch": {"categories": ["GraphicalModels"], "description": "We propose an objective Bayesian algorithm for searching the space of Gaussian directed acyclic graph (DAG) models. The algorithm proposed makes use of moment fractional Bayes factors (MFBF) and thus it is suitable for learning sparse graph. The algorithm is implemented by using Armadillo: an open-source C++ linear algebra library. "}, "eurostat": {"categories": ["OfficialStatistics"], "description": "Tools to download data from the Eurostat database\n    <https://ec.europa.eu/eurostat> together with search and manipulation\n    utilities."}, "nvmix": {"categories": ["Finance"], "description": "Functions for working with (grouped) multivariate normal variance mixture\n  distributions (evaluation of distribution functions and densities,\n  random number generation and parameter estimation), including\n  Student's t distribution for non-integer degrees-of-freedom as well as the grouped t\n  distribution and copula with multiple degrees-of-freedom parameters.\n  See <doi:10.18637/jss.v102.i02> for a high-level description of select functionality."}, "tseriesChaos": {"categories": ["Finance", "TimeSeries"], "description": "Routines for the analysis of nonlinear time series. This\n        work is largely inspired by the TISEAN project, by Rainer\n        Hegger, Holger Kantz and Thomas Schreiber:\n        <http://www.mpipks-dresden.mpg.de/~tisean/>."}, "clue": {"categories": ["Cluster", "Optimization"], "description": "CLUster Ensembles."}, "MBHdesign": {"categories": ["ExperimentalDesign", "OfficialStatistics", "Spatial"], "description": "Provides spatially survey balanced designs using the quasi-random number method described Robinson et al. (2013) <doi:10.1111/biom.12059>. These designs can accommodate, without substantial detrimental effects on spatial balance, legacy sites (Foster et al., 2017 <doi:10.1111/2041-210X.12782>), and for both point samples and transect samples (Foster et al. 2020, <doi:10.1111/2041-210X.13321>).  Additional information about the package use itself is given in Foster (2021) <doi:10.1111/2041-210X.13535>."}, "soilwater": {"categories": ["Hydrology"], "description": "It implements parametric formulas of soil\n    water retention or conductivity curve. At the moment, only Van Genuchten\n    (for soil water retention curve) and Mualem (for hydraulic conductivity)\n    were implemented. \n    See reference (<http://en.wikipedia.org/wiki/Water_retention_curve>)."}, "prabclus": {"categories": ["Cluster", "Environmetrics"], "description": "Distance-based parametric bootstrap tests for clustering with \n  spatial neighborhood information. Some distance measures, \n  Clustering of presence-absence, abundance and multilocus genetic data \n  for species delimitation, nearest neighbor \n  based noise detection. Genetic distances between communities.\n  Tests whether various distance-based regressions\n  are equal. Try package?prabclus for on overview. "}, "wsrf": {"categories": ["MachineLearning"], "description": "\n    A parallel implementation of Weighted Subspace Random Forest.  The\n    Weighted Subspace Random Forest algorithm was proposed in the\n    International Journal of Data Warehousing and Mining by Baoxun Xu,\n    Joshua Zhexue Huang, Graham Williams, Qiang Wang, and Yunming Ye\n    (2012) <doi:10.4018/jdwm.2012040103>.  The algorithm can classify\n    very high-dimensional data with random forests built using small\n    subspaces.  A novel variable weighting method is used for variable\n    subspace selection in place of the traditional random variable\n    sampling.This new approach is particularly useful in building\n    models from high-dimensional data."}, "vetiver": {"categories": ["ModelDeployment"], "description": "The goal of 'vetiver' is to provide fluent tooling to\n    version, share, deploy, and monitor a trained model. Functions handle\n    both recording and checking the model's input data prototype, and\n    predicting from a remote API endpoint. The 'vetiver' package is\n    extensible, with generics that can support many kinds of models."}, "influxdbr": {"categories": ["Databases", "TimeSeries"], "description": "An R interface to the InfluxDB time series database <https://www.influxdata.com>. This package allows you to fetch and write time series data from/to an InfluxDB server. Additionally, handy wrappers for the Influx Query Language (IQL) to manage and explore a remote database are provided. "}, "batch": {"categories": ["HighPerformanceComputing"], "description": "Functions to allow you to easily pass command-line arguments into R, and functions to aid in submitting your R code in parallel on a cluster and joining the results afterward (e.g. multiple parameter values for simulations running in parallel, splitting up a permutation test in parallel, etc.). See \u2018parseCommandArgs(...)\u2019 for the main example of how to use this package."}, "dbplyr": {"categories": ["Databases", "ModelDeployment"], "description": "A 'dplyr' back end for databases that allows you to work with\n    remote database tables as if they are in-memory data frames.  Basic\n    features works with any database that has a 'DBI' back end; more\n    advanced features require 'SQL' translation to be provided by the\n    package author."}, "ipw": {"categories": ["MissingData"], "description": "Functions to estimate the probability to receive the observed treatment, based on\n    individual characteristics. The inverse of these probabilities can be used as weights when\n\testimating causal effects from observational data via marginal structural models. Both point\n\ttreatment situations and longitudinal studies can be analysed. The same functions can be used to\n\tcorrect for informative censoring.\t"}, "lhs": {"categories": ["Distributions", "ExperimentalDesign"], "description": "Provides a number of methods for creating and augmenting Latin Hypercube Samples and Orthogonal Array Latin Hypercube Samples."}, "fastRhockey": {"categories": ["SportsAnalytics"], "description": "A utility to scrape and load play-by-play data\n    and statistics from the Premier Hockey Federation (PHF)<https://www.premierhockeyfederation.com/>, formerly\n    known as the National Women's Hockey League (NWHL). Additionally, allows access to the National Hockey League's\n    stats API <https://www.nhl.com/>."}, "nflfastR": {"categories": ["SportsAnalytics"], "description": "A set of functions to access National Football\n    League play-by-play data from <https://www.nfl.com/>."}, "xpose": {"categories": ["Pharmacokinetics"], "description": "Diagnostics for non-linear mixed-effects (population) \n    models from 'NONMEM' <https://www.iconplc.com/innovation/nonmem/>. \n    'xpose' facilitates data import, creation of numerical run summary \n    and provide 'ggplot2'-based graphics for data exploration and model \n    diagnostics."}, "mgm": {"categories": ["GraphicalModels", "Psychometrics", "TimeSeries"], "description": "Estimation of k-Order time-varying Mixed Graphical Models and mixed VAR(p) models via elastic-net regularized neighborhood regression. For details see Haslbeck & Waldorp (2020) <doi:10.18637/jss.v093.i08>."}, "ITRSelect": {"categories": ["CausalInference"], "description": "Sequential advantage selection (SAS, Fan, Lu and Song, 2016) <arXiv:1405.5239> \n\tand penalized A-learning (PAL, Shi, et al., 2018) methods are implement for selecting \n\timportant variables involved in optimal individualized (dynamic) treatment regime in \n\tboth single-stage or multi-stage studies."}, "BaPreStoPro": {"categories": ["Bayesian"], "description": "Bayesian estimation and prediction for stochastic processes based\n    on the Euler approximation. Considered processes are: jump diffusion,\n    (mixed) diffusion models, hidden (mixed) diffusion models, non-homogeneous\n    Poisson processes (NHPP), (mixed) regression models for comparison and a\n    regression model including a NHPP."}, "TSdist": {"categories": ["TimeSeries"], "description": "A set of commonly used distance measures and some additional functions which, although initially not designed for this purpose, can be used to measure the dissimilarity between time series. These measures can be used to perform clustering, classification or other data mining tasks which require the definition of a distance measure between time series. U. Mori, A. Mendiburu and J.A. Lozano (2016), <doi:10.32614/RJ-2016-058>."}, "PLmixed": {"categories": ["Psychometrics"], "description": "Utilizes the 'lme4' and 'optimx' packages (previously the optim()\n    function from 'stats') to estimate (generalized) linear mixed models (GLMM)\n    with factor structures using a profile likelihood approach, as outlined in\n    Jeon and Rabe-Hesketh (2012) <doi:10.3102/1076998611417628> and\n    Rockwood and Jeon (2019) <doi:10.1080/00273171.2018.1516541>.\n    Factor analysis and item response models can be extended to allow\n    for an arbitrary number of nested and crossed random effects,\n    making it useful for multilevel and cross-classified models."}, "dejaVu": {"categories": ["MissingData"], "description": "Performs reference based multiple imputation of recurrent event data\n    based on a negative binomial regression model, as described\n    by Keene et al (2014) <doi:10.1002/pst.1624>."}, "smooth": {"categories": ["TimeSeries"], "description": "Functions implementing Single Source of Error state space models for purposes of time series analysis and forecasting.\n             The package includes ADAM (Svetunkov, 2021, <https://openforecast.org/adam/>),\n             Exponential Smoothing (Hyndman et al., 2008, <doi:10.1007/978-3-540-71918-2>),\n             SARIMA (Svetunkov & Boylan, 2019 <doi:10.1080/00207543.2019.1600764>),\n             Complex Exponential Smoothing (Svetunkov & Kourentzes, 2018, <doi:10.13140/RG.2.2.24986.29123>),\n             Simple Moving Average (Svetunkov & Petropoulos, 2018 <doi:10.1080/00207543.2017.1380326>)\n             and several simulation functions. It also allows dealing with intermittent demand based on the\n             iETS framework (Svetunkov & Boylan, 2019, <doi:10.13140/RG.2.2.35897.06242>)."}, "ratesci": {"categories": ["MetaAnalysis"], "description": "Computes confidence intervals for the rate (or risk)\n    difference ('RD') or rate ratio (or relative risk, 'RR') for \n    binomial proportions or Poisson rates, or for odds ratio \n    ('OR', binomial only). Also confidence intervals for a single \n    binomial or Poisson rate, and intervals for matched pairs. \n    Includes skewness-corrected asymptotic score ('SCAS') methods, \n    which have been developed in Laud (2017) <doi:10.1002/pst.1813>\n    from Miettinen & Nurminen (1985) <doi:10.1002/sim.4780040211> and \n    Gart & Nam (1988) <doi:10.2307/2531848>. The same score produces \n    hypothesis tests analogous to the test for binomial RD and RR by \n    Farrington & Manning (1990) <doi:10.1002/sim.4780091208>. \n    The package also includes MOVER methods\n    (Method Of Variance Estimates Recovery) for all contrasts, derived \n    from the Newcombe method but using equal-tailed Jeffreys intervals,\n    and generalised for Bayesian applications incorporating prior \n    information. So-called 'exact' methods for strictly conservative \n    coverage are approximated using continuity corrections.\n    Also includes methods for stratified calculations (e.g. meta-analysis),\n    either assuming fixed effects (matching the CMH test) or incorporating \n    stratum heterogeneity."}, "humidity": {"categories": ["Hydrology"], "description": "Vapor pressure, relative humidity, absolute humidity, specific humidity, and mixing ratio are commonly used water vapor measures in meteorology. This R package provides functions for calculating saturation vapor pressure (hPa), partial water vapor pressure (Pa), relative humidity (%), absolute humidity (kg/m^3), specific humidity (kg/kg), and mixing ratio (kg/kg) from temperature (K) and dew point (K). Conversion functions between humidity measures are also provided."}, "parma": {"categories": ["Finance", "Optimization"], "description": "Provision of a set of models and methods for use in the allocation and management of capital in financial portfolios."}, "bentcableAR": {"categories": ["TimeSeries"], "description": "\n\tIncluded are two main interfaces, bentcable.ar() and\n\tbentcable.dev.plot(), for fitting and diagnosing bent-cable\n\tregressions for autoregressive time-series data (Chiu and\n\tLockhart 2010, <doi:10.1002/cjs.10070>) or independent data (time\n\tseries or otherwise - Chiu, Lockhart and Routledge 2006,\n\t<doi:10.1198/016214505000001177>). Some components in the package\n\tcan also be used as stand-alone functions. The bent cable\n\t(linear-quadratic-linear) generalizes the broken stick\n\t(linear-linear), which is also handled by this package. Version\n\t0.2 corrected a glitch in the computation of confidence intervals\n\tfor the CTP. References that were updated from Versions 0.2.1 and\n\t0.2.2 appear in Version 0.2.3 and up. Version 0.3.0 improved\n\trobustness of the error-message producing mechanism. Version 0.3.1\n\timproves the NAMESPACE file of the package. It is the author's\n\tintention to distribute any future updates via GitHub."}, "cmaes": {"categories": ["Optimization"], "description": "Single objective optimization using a CMA-ES."}, "tukeyGH": {"categories": ["Distributions"], "description": "It provides distribution, density and quantile functions of the\n    Tukey's g-and-h probability distribution, as well as functions for random\n    number generation, parameter estimation and testing."}, "RCzechia": {"categories": ["Spatial"], "description": "Administrative regions and other spatial objects of the Czech Republic."}, "ipred": {"categories": ["Environmetrics", "MachineLearning", "Survival"], "description": "Improved predictive models by indirect classification and\n  bagging for classification, regression and survival problems \n  as well as resampling based estimators of prediction error. "}, "nmathresh": {"categories": ["MetaAnalysis"], "description": "Calculation and presentation of decision-invariant bias adjustment\n    thresholds and intervals for Network Meta-Analysis, as described by \n    Phillippo et al. (2018) <doi:10.1111/rssa.12341>. These describe the \n    smallest changes to the data that would result in a change of decision."}, "tsdisagg2": {"categories": ["TimeSeries"], "description": "Disaggregates low frequency time series data to higher frequency series. Implements the following methods for temporal disaggregation: Boot, Feibes and Lisman (1967) <doi:10.2307/2985238>, Chow and Lin (1971) <doi:10.2307/1928739>, Fernandez (1981) <doi:10.2307/1924371> and Litterman (1983) <doi:10.2307/1391858>."}, "marginaleffects": {"categories": ["CausalInference", "Econometrics"], "description": "Compute, summarize, and plot marginal effects, adjusted predictions, contrasts, and marginal means for a wide variety of models."}, "oro.pet": {"categories": ["MedicalImaging"], "description": "Image analysis techniques for positron emission tomography\n    (PET) that form part of the Rigorous Analytics bundle."}, "EpiILM": {"categories": ["Epidemiology"], "description": "Provides tools for simulating from discrete-time individual level models for infectious disease data analysis. This epidemic model class contains spatial and contact-network based models with two disease types: Susceptible-Infectious (SI) and Susceptible-Infectious-Removed (SIR)."}, "MSwM": {"categories": ["TimeSeries"], "description": "Estimation, inference and diagnostics for Univariate Autoregressive Markov Switching Models for Linear and Generalized Models. Distributions for the series include gaussian, Poisson, binomial and gamma cases. The EM algorithm is used for estimation (see Perlin (2012) <doi:10.2139/ssrn.1714016>)."}, "GWI": {"categories": ["Distributions"], "description": "Firstly, both functions of the univariate Poisson dispersion index (DI) for count data and the univariate exponential variation index (VI) for nonnegative continuous data are performed. Next, other functions of univariate indexes such the binomial dispersion index (DIb), the negative binomial dispersion index (DInb) and the inverse Gaussian variation index (VIiG) are given. Finally, we are computed some multivariate versions of these functions such that the generalized dispersion index (GDI) with its marginal one (MDI) and the generalized variation index (GVI) with its marginal one (MVI) too."}, "PBSmapping": {"categories": ["Spatial"], "description": "This software has evolved from fisheries research conducted at the\n   Pacific Biological Station (PBS) in 'Nanaimo', British Columbia, Canada. It\n   extends the R language to include two-dimensional plotting features similar\n   to those commonly available in a Geographic Information System (GIS).\n   Embedded C code speeds algorithms from computational geometry, such as\n   finding polygons that contain specified point events or converting between\n   longitude-latitude and Universal Transverse Mercator (UTM) coordinates.\n   Additionally, we include 'C++' code developed by Angus Johnson for the\n   'Clipper' library, data for a global shoreline, and other data sets in the\n   public domain. Under the user's R library directory '.libPaths()',\n   specifically in './PBSmapping/doc', a complete user's guide is offered and\n   should be consulted to use package functions effectively."}, "forestplot": {"categories": ["MetaAnalysis"], "description": "A forest plot that allows for\n    multiple confidence intervals per row,\n    custom fonts for each text element,\n    custom confidence intervals,\n    text mixed with expressions, and more.\n    The aim is to extend the use of forest plots beyond meta-analyses.\n    This is a more general version of the original 'rmeta' package's forestplot()\n    function and relies heavily on the 'grid' package."}, "osmextract": {"categories": ["Spatial"], "description": "Match, download, convert and import Open Street Map data extracts \n    obtained from several providers. "}, "enpls": {"categories": ["ChemPhys"], "description": "An algorithmic framework for measuring feature importance,\n    outlier detection, model applicability domain evaluation,\n    and ensemble predictive modeling with (sparse)\n    partial least squares regressions."}, "ARCensReg": {"categories": ["TimeSeries"], "description": "It fits an univariate left or right censored linear regression model\n    with autoregressive errors under the normal distribution. It provides estimates\n    and standard errors of the parameters, prediction of future observations and\n    it supports missing values on the dependent variable.\n    It also performs influence diagnostic through local influence for three possible\n    perturbation schemes."}, "ROpenDota": {"categories": ["SportsAnalytics"], "description": "Provides a client for the API of OpenDota. OpenDota is a web service which is provide DOTA2 real time data. Data is collected through the Steam WebAPI. With ROpenDota you can easily grab the latest DOTA2 statistics in R programming such as latest match on official international competition, analyzing your or enemy performance to learn their strategies,etc. Please see <https://github.com/rosdyana/ROpenDota> for more information."}, "googleLanguageR": {"categories": ["WebTechnologies"], "description": "Call 'Google Cloud' machine learning APIs for text and speech tasks.\n  Call the 'Cloud Translation' API <https://cloud.google.com/translate/> for detection \n  and translation of text, the 'Natural Language' API <https://cloud.google.com/natural-language/> to \n  analyse text for sentiment, entities or syntax, the 'Cloud Speech' API \n  <https://cloud.google.com/speech/> to transcribe sound files to text and \n  the 'Cloud Text-to-Speech' API <https://cloud.google.com/text-to-speech/> to turn text \n  into sound files."}, "htmlTable": {"categories": ["ReproducibleResearch"], "description": "Tables with state-of-the-art layout elements such as row spanners,\n    column spanners, table spanners, zebra striping, and more. While allowing\n    advanced layout, the underlying css-structure is simple in order to maximize\n    compatibility with common word processors.  The package also contains a few \n    text formatting functions that help outputting text compatible with HTML/LaTeX."}, "fastcluster": {"categories": ["Cluster"], "description": "This is a two-in-one package which provides interfaces to\n        both R and 'Python'. It implements fast hierarchical, agglomerative\n        clustering routines. Part of the functionality is designed as drop-in\n        replacement for existing routines: linkage() in the 'SciPy' package\n        'scipy.cluster.hierarchy', hclust() in R's 'stats' package, and the\n        'flashClust' package. It provides the same functionality with the\n        benefit of a much faster implementation. Moreover, there are\n        memory-saving routines for clustering of vector data, which go beyond\n        what the existing packages provide. For information on how to install\n        the 'Python' files, see the file INSTALL in the source distribution.\n        Based on the present package, Christoph Dalitz also wrote a pure 'C++'\n        interface to 'fastcluster':\n        <https://lionel.kr.hs-niederrhein.de/~dalitz/data/hclust/>."}, "tfestimators": {"categories": ["HighPerformanceComputing", "ModelDeployment"], "description": "Interface to 'TensorFlow' Estimators \n    <https://www.tensorflow.org/guide/estimator>, a high-level \n    API that provides implementations of many different model types \n    including linear models and deep neural networks. "}, "lars": {"categories": ["MachineLearning"], "description": "Efficient procedures for fitting an entire lasso\n\t\tsequence with the cost of a single least squares\n\t\tfit. Least angle regression and infinitesimal forward\n\t\tstagewise regression are related to the lasso, as\n\t\tdescribed in the paper below."}, "fracdiff": {"categories": ["Finance", "TimeSeries"], "description": "Maximum likelihood estimation of the parameters of a fractionally\n   differenced ARIMA(p,d,q) model (Haslett and Raftery, Appl.Statistics, 1989);\n   including inference and basic methods.  Some alternative algorithms to estimate \"H\"."}, "mexhaz": {"categories": ["Survival"], "description": "Fit flexible (excess) hazard regression models with the possibility of including non-proportional effects of covariables and of adding a random effect at the cluster level (corresponding to a shared frailty). A detailed description of the package functionalities is provided in Charvat and Belot (2021) <doi:10.18637/jss.v098.i14>."}, "doMC": {"categories": ["HighPerformanceComputing"], "description": "Provides a parallel backend for the %dopar% function using\n        the multicore functionality of the parallel package."}, "anesrake": {"categories": ["OfficialStatistics"], "description": "Provides a comprehensive system for selecting\n    variables and weighting data to match the specifications of the American\n    National Election Studies. The package includes methods for identifying\n    discrepant variables, raking data, and assessing the effects of the raking\n    algorithm. It also allows automated re-raking if target variables fall\n    outside identified bounds and allows greater user specification than other\n    available raking algorithms. A variety of simple weighted statistics that\n    were previously in this package (version .55 and earlier) have been moved to\n    the package 'weights.'"}, "untb": {"categories": ["Environmetrics"], "description": "Hubbell's Unified Neutral Theory of Biodiversity."}, "etm": {"categories": ["Epidemiology", "Survival"], "description": "The etm (empirical transition matrix) package permits to estimate the matrix of transition probabilities for any time-inhomogeneous multi-state model with finite state space using the Aalen-Johansen estimator. Functions for data preparation and for displaying are also included (Allignol et al., 2011 <doi:10.18637/jss.v038.i04>). Functionals of the Aalen-Johansen estimator, e.g., excess length-of-stay in an intermediate state, can also be computed (Allignol et al. 2011 <doi:10.1007/s00180-010-0200-x>)."}, "mousetrap": {"categories": ["SpatioTemporal"], "description": "Mouse-tracking, the analysis of mouse movements in computerized\n    experiments, is a method that is becoming increasingly popular in the\n    cognitive sciences. The mousetrap package offers functions for importing,\n    preprocessing, analyzing, aggregating, and visualizing mouse-tracking data.\n    An introduction into mouse-tracking analyses using mousetrap can be found\n    in Wulff, Kieslich, Henninger, Haslbeck, & Schulte-Mecklenbeck (2021)\n    <doi:10.31234/osf.io/v685r> (preprint: <https://psyarxiv.com/v685r>)."}, "sfsmisc": {"categories": ["Distributions"], "description": "Useful utilities ['goodies'] from Seminar fuer Statistik ETH Zurich,\n   some of which were ported from S-plus in the 1990s.\n For graphics, have pretty (Log-scale) axes, an enhanced Tukey-Anscombe\n   plot, combining histogram and boxplot, 2d-residual plots, a 'tachoPlot()',\n   pretty arrows, etc.\n For robustness, have a robust F test and robust range().\n For system support, notably on Linux, provides 'Sys.*()' functions with\n   more access to system and CPU information.\n Finally, miscellaneous utilities such as simple efficient prime numbers,\n   integer codes, Duplicated(), toLatex.numeric() and is.whole()."}, "Tcomp": {"categories": ["TimeSeries"], "description": "The 1311 time series from the tourism forecasting competition conducted in 2010 and described in Athanasopoulos et al. (2011) <doi:10.1016/j.ijforecast.2010.04.009>."}, "OpenCL": {"categories": ["HighPerformanceComputing"], "description": "This package provides an interface to OpenCL, allowing R to leverage computing power of GPUs and other HPC accelerator devices."}, "ivmte": {"categories": ["CausalInference"], "description": "The marginal treatment effect was introduced by Heckman and\n    Vytlacil (2005) <doi:10.1111/j.1468-0262.2005.00594.x> to provide a\n    choice-theoretic interpretation to instrumental variables models that\n    maintain the monotonicity condition of Imbens and Angrist (1994)\n    <doi:10.2307/2951620>. This interpretation can be used to extrapolate from\n    the compliers to estimate treatment effects for other subpopulations. This\n    package provides a flexible set of methods for conducting this\n    extrapolation. It allows for parametric or nonparametric sieve estimation,\n    and allows the user to maintain shape restrictions such as monotonicity. The\n    package operates in the general framework developed by Mogstad, Santos and\n    Torgovitsky (2018) <doi:10.3982/ECTA15463>, and accommodates either point\n    identification or partial identification (bounds). In the partially\n    identified case, bounds are computed using either linear programming\n    or quadratically constrained quadratic programming. Support for\n    four solvers is provided. Gurobi and the Gurobi R API\n    can be obtained from <http://www.gurobi.com/index>. CPLEX can be obtained\n    from <https://www.ibm.com/analytics/cplex-optimizer>. CPLEX R APIs 'Rcplex'\n    and 'cplexAPI' are available from CRAN. MOSEK and the MOSEK R API can be\n    obtained from <https://www.mosek.com/>. The lp_solve library is freely\n    available from <http://lpsolve.sourceforge.net/5.5/>, and is included when\n    installing its API 'lpSolveAPI', which is available from CRAN."}, "bayesdfa": {"categories": ["Bayesian"], "description": "Implements Bayesian dynamic factor analysis with 'Stan'. Dynamic \n    factor analysis is a dimension reduction tool for multivariate time series.\n    'bayesdfa' extends conventional dynamic factor models in several ways. \n    First, extreme events may be estimated in the latent trend by modeling\n    process error with a student-t distribution. Second, alternative constraints\n    (including proportions are allowed). Third, the estimated\n    dynamic factors can be analyzed with hidden Markov models to evaluate\n    support for latent regimes."}, "stepPlr": {"categories": ["ChemPhys"], "description": "L2 penalized logistic regression for both continuous and discrete predictors, with forward stagewise/forward stepwise variable selection procedure."}, "HardyWeinberg": {"categories": ["MissingData"], "description": "Contains tools for exploring Hardy-Weinberg equilibrium (Hardy, 1908;  Weinberg, 1908) for bi and multi-allelic genetic marker data. All classical tests (chi-square, exact, likelihood-ratio and permutation tests) with bi-allelic variants are included in the package, as well as functions for power computation and for the simulation of marker data under equilibrium and disequilibrium. Routines for dealing with markers on the X-chromosome are included (Graffelman & Weir, 2016) <doi:10.1038/hdy.2016.20>, including Bayesian procedures. Some exact and permutation procedures also work with multi-allelic variants. Special test procedures that jointly address Hardy-Weinberg equilibrium and equality of allele frequencies in both sexes are supplied, for the bi and multi-allelic case. Functions for testing equilibrium in the presence of missing data by using multiple imputation are also provided. Implements several graphics for exploring the equilibrium status of a large set of bi-allelic markers: ternary plots with acceptance regions, log-ratio plots and Q-Q plots. The functionality of the package is explained in detail in a related JSS paper <doi:10.18637/jss.v064.i03>. "}, "tesseract": {"categories": ["NaturalLanguageProcessing"], "description": "Bindings to 'Tesseract': \n     a powerful optical character recognition (OCR) engine that supports over 100 languages.\n     The engine is highly configurable in order to tune the detection algorithms and\n     obtain the best possible results."}, "FPLdata": {"categories": ["SportsAnalytics"], "description": "This data contains a large variety of information on players and their\n  current attributes on Fantasy Premier League\n  <https://fantasy.premierleague.com/>. In particular, it contains a\n  \u2018next_gw_points' (next gameweek points) value for each player\n  given their attributes in the current week. Rows represent player-gameweeks,\n  i.e. for each player there is a row for each gameweek. This\n  makes the data suitable for modelling a player\u2019s next gameweek points, given\n  attributes such as form, total points, and cost at the current gameweek.\n  This data can therefore be used to create Fantasy Premier League bots that\n  may use a machine learning algorithm and a linear programming solver\n  (for example) to return the best possible transfers and team to pick for\n  each gameweek, thereby fully automating the decision making process in\n  Fantasy Premier League. This function simply supplies the required data\n  for such a task."}, "vines": {"categories": ["Distributions"], "description": "Implementation of the vine graphical model for building\n    high-dimensional probability distributions as a factorization of\n    bivariate copulas and marginal density functions. This package\n    provides S4 classes for vines (C-vines and D-vines) and methods\n    for inference, goodness-of-fit tests, density/distribution\n    function evaluation, and simulation."}, "httping": {"categories": ["WebTechnologies"], "description": "A suite of functions to ping 'URLs' and to time\n    'HTTP' 'requests'. Designed to work with 'httr'."}, "derivmkts": {"categories": ["Finance"], "description": "A set of pricing and expository functions that should\n    be useful in teaching a course on financial derivatives."}, "paleoTS": {"categories": ["TimeSeries"], "description": "Facilitates analysis of paleontological sequences of trait values.  \n    Functions are provided to fit, using maximum likelihood, simple \n    evolutionary models (including unbiased random walks, directional \n    evolution,stasis, Ornstein-Uhlenbeck, covariate-tracking) and \n    complex models (punctuation, mode shifts)."}, "etma": {"categories": ["MetaAnalysis"], "description": "Traditional meta-regression based method has been developed for using meta-analysis data, but it faced the challenge of inconsistent estimates. This package purpose a new statistical method to detect epistasis using incomplete information summary, and have proven it not only successfully let consistency of evidence, but also increase the power compared with traditional method (Detailed tutorial is shown in website)."}, "caret": {"categories": ["HighPerformanceComputing"], "description": "Misc functions for training and plotting classification and\n    regression models."}, "blogdown": {"categories": ["ReproducibleResearch"], "description": "Write blog posts and web pages in R Markdown. This package\n    supports the static site generator 'Hugo' (<https://gohugo.io>) best,\n    and it also supports 'Jekyll' (<https://jekyllrb.com>) and 'Hexo'\n    (<https://hexo.io>)."}, "Cprob": {"categories": ["Survival"], "description": "Permits to estimate the conditional probability function of a competing event, and to fit, using the temporal process regression or the pseudo-value approach, a proportional-odds model to the conditional probability function (or other models by specifying another link function). See <doi:10.1111/j.1467-9876.2010.00729.x>."}, "fma": {"categories": ["Econometrics", "TimeSeries"], "description": "All data sets from \"Forecasting: methods and applications\" by Makridakis, Wheelwright & Hyndman (Wiley, 3rd ed., 1998) <https://robjhyndman.com/forecasting/>."}, "Sim.DiffProc": {"categories": ["DifferentialEquations", "Finance", "HighPerformanceComputing", "Psychometrics", "TimeSeries"], "description": "It provides users with a wide range of tools to simulate, estimate, analyze, and visualize the dynamics of stochastic differential systems in both forms Ito and Stratonovich. Statistical analysis with parallel Monte Carlo and moment equations methods of SDEs <doi:10.18637/jss.v096.i02>. Enabled many searchers in different domains to use these equations to modeling practical problems in financial and actuarial modeling and other areas of application, e.g., modeling and simulate of first passage time problem in shallow water using the attractive center (Boukhetala K, 1996) ISBN:1-56252-342-2. "}, "switchr": {"categories": ["ReproducibleResearch"], "description": "Provides an abstraction for managing, installing,\n    and switching between sets of installed R packages. This allows users to\n    maintain multiple package libraries simultaneously, e.g. to maintain\n    strict, package-version-specific reproducibility of many analyses, or\n    work within a development/production release paradigm. Introduces a\n    generalized package installation process which supports multiple repository\n    and non-repository sources and tracks package provenance."}, "lctools": {"categories": ["Spatial"], "description": "Provides researchers and educators with easy-to-learn user friendly tools for calculating \n    key spatial statistics and to apply simple as well as advanced methods of spatial analysis in real data. \n    These include: Local Pearson and Geographically Weighted Pearson Correlation Coefficients, \n    Spatial Inequality Measures (Gini, Spatial Gini, LQ, Focal LQ), Spatial Autocorrelation \n    (Global and Local Moran's I), several Geographically Weighted Regression techniques and other \n    Spatial Analysis tools (other geographically weighted statistics). This package also contains functions for \n    measuring the significance of each statistic calculated, mainly based on Monte Carlo simulations."}, "ICAOD": {"categories": ["ExperimentalDesign"], "description": "Finds optimal designs for nonlinear models using a metaheuristic algorithm called Imperialist Competitive Algorithm (ICA). See, for details, Masoudi et al. (2017) <doi:10.1016/j.csda.2016.06.014> and Masoudi et al. (2019) <doi:10.1080/10618600.2019.1601097>."}, "spmoran": {"categories": ["Spatial"], "description": "Functions for estimating spatial varying coefficient models, mixed models, and other spatial regression models for Gaussian and non-Gaussian data. Moran eigenvectors are used to an approximate Gaussian process modeling which is interpretable in terms of the Moran coefficient. The GP is used for modeling the spatial processes in residuals and regression coefficients. For details see Murakami (2021) <arXiv:1703.04467>."}, "chyper": {"categories": ["Distributions"], "description": "An implementation of the probability mass function, cumulative density function, quantile function, random number generator, maximum likelihood estimator, and p-value generator from a conditional hypergeometric distribution: the distribution of how many items are in the overlap of all samples when samples of arbitrary size are each taken without replacement from populations of arbitrary size."}, "hydroPSO": {"categories": ["Hydrology", "Optimization"], "description": "State-of-the-art version of the Particle Swarm Optimisation (PSO) algorithm (SPSO-2011 and SPSO-2007 capable). hydroPSO can be used as a replacement of the 'optim' R function for (global) optimization of non-smooth and non-linear functions. However, the main focus of hydroPSO is the calibration of environmental and other real-world models that need to be executed from the system console. hydroPSO is model-independent, allowing the user to easily interface any computer simulation model with the calibration engine (PSO). hydroPSO  communicates with the model through the model's own input and output files, without requiring access to the model's source code. Several PSO variants and controlling options are included to fine-tune the performance of the calibration engine to different calibration problems. An advanced sensitivity analysis function together with user-friendly plotting summaries facilitate the interpretation and assessment of the calibration results. hydroPSO is parallel-capable, to alleviate the computational burden of complex models with \"long\" execution time. Bugs reports/comments/questions are very welcomed (in English, Spanish or Italian). See Zambrano-Bigiarini and Rojas (2013) <doi:10.1016/j.envsoft.2013.01.004> for more details."}, "TrackReconstruction": {"categories": ["SpatioTemporal", "Tracking"], "description": "Reconstructs animal tracks from magnetometer, accelerometer, depth and optional speed data.  Designed primarily using data from Wildlife Computers Daily Diary tags deployed on northern fur seals."}, "powerSurvEpi": {"categories": ["Epidemiology", "Survival"], "description": "Functions to calculate power and\n                sample size for testing main effect or interaction effect in\n                the survival analysis of epidemiological studies\n                (non-randomized studies), taking into account the \n                correlation between the covariate of the\n                interest and other covariates. Some calculations also take \n                into account the competing risks and stratified analysis. \n                This package also includes\n                a set of functions to calculate power and sample size\n                for testing main effect in the survival analysis of \n                randomized clinical trials and conditional logistic regression for nested case-control study."}, "rtop": {"categories": ["Environmetrics", "Hydrology", "MissingData", "Spatial"], "description": "Geostatistical interpolation of data with irregular spatial support such as runoff related data or data from administrative units."}, "chemometrics": {"categories": ["ChemPhys"], "description": "R companion to the book \"Introduction to Multivariate Statistical Analysis in Chemometrics\" written by K. Varmuza and P. Filzmoser (2009)."}, "cricketr": {"categories": ["SportsAnalytics"], "description": "Tools for analyzing performances of cricketers based on stats in\n    ESPN Cricinfo Statsguru. The toolset can  be used for analysis of Tests,ODIs \n    and Twenty20 matches of both batsmen and bowlers. The package can also be used to\n    analyze team performances."}, "ROptRegTS": {"categories": ["Robust"], "description": "Optimally robust estimation for regression-type models using S4 classes and\n            methods."}, "pointblank": {"categories": ["Databases"], "description": "Validate data in data frames, 'tibble' objects, 'Spark'\n    'DataFrames', and database tables. Validation pipelines can be made using\n    easily-readable, consecutive validation steps. Upon execution of the\n    validation plan, several reporting options are available. User-defined\n    thresholds for failure rates allow for the determination of appropriate\n    reporting actions. Many other workflows are available including an\n    information management workflow, where the aim is to record, collect, and\n    generate useful information on data tables."}, "tools4uplift": {"categories": ["CausalInference"], "description": "Uplift modeling aims at predicting the causal effect of an action such as a marketing campaign on a particular individual. In order to simplify the task for practitioners in uplift modeling, we propose a combination of tools that can be separated into the following ingredients: i) quantization, ii) visualization, iii) variable selection, iv) parameters estimation and, v) model validation. For more details, see <https://dms.umontreal.ca/~murua/research/UpliftRegression.pdf>."}, "nor1mix": {"categories": ["Cluster", "Distributions"], "description": "Onedimensional Normal (i.e. Gaussian) Mixture Models Classes,\n   for, e.g., density estimation or clustering algorithms research and teaching;\n   providing the widely used Marron-Wand densities.  Efficient random\n   number generation and graphics. Fitting to data by efficient ML (Maximum\n   Likelihood) or traditional EM estimation."}, "FRK": {"categories": ["Spatial"], "description": "Fixed Rank Kriging is a tool for spatial/spatio-temporal modelling and prediction with large datasets. The approach models the field, and hence the covariance function, using a set of r basis functions, where r is typically much smaller than the number of data points (or polygons) m. This low-rank basis-function representation facilitates the modelling of 'big' spatial/spatio-temporal data. The method naturally allows for non-stationary, anisotropic covariance functions. Discretisation of the spatial domain into so-called basic areal units (BAUs) facilitates the use of observations with varying support (i.e., both point-referenced and areal supports, potentially simultaneously), and prediction over arbitrary user-specified regions. 'FRK' also supports inference over various manifolds, including the 2D plane and 3D sphere, and it provides helper functions to model, fit, predict, and plot with relative ease. Version 2.0.0 and above of the package 'FRK' also supports modelling of non-Gaussian data, by employing a spatial generalised linear mixed model (GLMM) framework  to cater for Poisson, binomial, negative-binomial, gamma, and inverse-Gaussian distributions.  Zammit-Mangion and Cressie <doi:10.18637/jss.v098.i04> describe 'FRK' in a Gaussian setting, and detail its use of basis functions and BAUs."}, "BMA": {"categories": ["Bayesian", "Econometrics", "Survival"], "description": "Package for Bayesian model averaging and variable selection for linear models,\n        generalized linear models and survival models (cox\n        regression)."}, "SIS": {"categories": ["MachineLearning"], "description": "Variable selection techniques are essential tools for model\n    selection and estimation in high-dimensional statistical models. Through this\n    publicly available package, we provide a unified environment to carry out\n    variable selection using iterative sure independence screening (SIS) (Fan and Lv (2008)<doi:10.1111/j.1467-9868.2008.00674.x>) and all\n    of its variants in generalized linear models (Fan and Song (2009)<doi:10.1214/10-AOS798>) and the Cox proportional hazards\n    model (Fan, Feng and Wu (2010)<doi:10.1214/10-IMSCOLL606>)."}, "rwt": {"categories": ["TimeSeries"], "description": "Provides a set of functions for 1D and 2D wavelet and filter\n  bank design, analysis, and processing. Functions for denoising are also\n  included. The package can be used for image denoising or deconvolution."}, "adehabitatLT": {"categories": ["Spatial", "SpatioTemporal", "Tracking"], "description": "A collection of tools for the analysis of animal movements."}, "esaBcv": {"categories": ["Psychometrics"], "description": "These functions estimate the latent factors of a given matrix, no matter it is high-dimensional or not. It tries to first estimate the number of factors using bi-cross-validation and then estimate the latent factor matrix and the noise variances. For more information about the method, see Art B. Owen and Jingshu Wang 2015 archived article on factor model (<arXiv:1503.03515>). "}, "tscount": {"categories": ["TimeSeries"], "description": "Likelihood-based methods for model fitting and assessment, prediction and intervention analysis of count time series following generalized linear models are provided. Models with the identity and with the logarithmic link function are allowed. The conditional distribution can be Poisson or Negative Binomial."}, "clubSandwich": {"categories": ["Econometrics", "MetaAnalysis", "Robust"], "description": "Provides several cluster-robust variance estimators (i.e.,\n    sandwich estimators) for ordinary and weighted least squares linear regression\n    models, including the bias-reduced linearization estimator introduced by Bell\n    and McCaffrey (2002) \n    <https://www150.statcan.gc.ca/n1/pub/12-001-x/2002002/article/9058-eng.pdf> and \n    developed further by Pustejovsky and Tipton (2017) \n    <doi:10.1080/07350015.2016.1247004>. The package includes functions for estimating\n    the variance- covariance matrix and for testing single- and multiple-\n    contrast hypotheses based on Wald test statistics. Tests of single regression\n    coefficients use Satterthwaite or saddle-point corrections. Tests of multiple-\n    contrast hypotheses use an approximation to Hotelling's T-squared distribution.\n    Methods are provided for a variety of fitted models, including lm() and mlm\n    objects, glm(), ivreg() (from package 'AER'), plm() (from package 'plm'), gls()\n    and lme() (from 'nlme'), lmer() (from \u2018lme4'), robu() (from \u2019robumeta'), and \n    rma.uni() and rma.mv() (from 'metafor')."}, "sorvi": {"categories": ["OfficialStatistics"], "description": "Misc support functions for rOpenGov and open data downloads."}, "qcv": {"categories": ["Psychometrics"], "description": "Primarily, the 'qcv' package computes key indices related to the Quantifying Construct\n    Validity procedure (QCV; Westen & Rosenthal, 2003 <doi:10.1037/0022-3514.84.3.608>; \n    see also Furr & Heuckeroth, in press). The qcv() function is the heart of the 'qcv' package, \n    but additional functions in the package provide useful ancillary information related to the QCV procedure."}, "logconcens": {"categories": ["Survival"], "description": "Based on right or interval censored data, compute the maximum likelihood estimator of a (sub)probability density under the assumption that it is log-concave. For further information see Duembgen, Rufibach and Schuhmacher (2014) <doi:10.1214/14-EJS930>.  "}, "onlineforecast": {"categories": ["TimeSeries"], "description": "A framework for fitting adaptive forecasting models. Provides a way to use forecasts as input to models, e.g. weather forecasts for energy related forecasting. The models can be fitted recursively and can easily be setup for updating parameters when new data arrives. See the included vignettes, the website <https://onlineforecasting.org> and the pre-print paper \"onlineforecast: An R package for adaptive and recursive forecasting\" <arXiv:2109.12915>."}, "spiderbar": {"categories": ["WebTechnologies"], "description": "The 'Robots Exclusion Protocol' <https://www.robotstxt.org/orig.html> documents\n    a set of standards for allowing or excluding robot/spider crawling of different areas of\n    site content. Tools are provided which wrap The 'rep-cpp' <https://github.com/seomoz/rep-cpp>\n    C++ library for processing these 'robots.txt' files."}, "crseEventStudy": {"categories": ["Finance"], "description": "Based on Dutta et al. (2018) <doi:10.1016/j.jempfin.2018.02.004>, this package provides their standardized test for abnormal returns in long-horizon event studies. The methods used improve the major weaknesses of size, power, and robustness of long-run statistical tests described in Kothari/Warner (2007) <doi:10.1016/B978-0-444-53265-7.50015-9>. Abnormal returns are weighted by their statistical precision (i.e., standard deviation), resulting in abnormal standardized returns. This procedure efficiently captures the heteroskedasticity problem. Clustering techniques following Cameron et al. (2011) <doi:10.1198/jbes.2010.07136> are adopted for computing cross-sectional correlation robust standard errors. The statistical tests in this package therefore accounts for potential biases arising from returns' cross-sectional correlation, autocorrelation, and volatility clustering without power loss."}, "BiodiversityR": {"categories": ["Environmetrics"], "description": "Graphical User Interface (via the R-Commander) and utility functions (often based on the vegan package) for statistical analysis of biodiversity and ecological communities, including species accumulation curves, diversity indices, Renyi profiles, GLMs for analysis of species abundance and presence-absence, distance matrices, Mantel tests, and cluster, constrained and unconstrained ordination analysis. A book on biodiversity and community ecology analysis is available for free download from the website. In 2012, methods for (ensemble) suitability modelling and mapping were expanded in the package."}, "catmap": {"categories": ["MetaAnalysis"], "description": "Although many software tools can perform meta-analyses on genetic case-control data,\n    none of these apply to combined case-control and family-based (TDT) studies. This package conducts\n    fixed-effects (with inverse variance weighting) and random-effects [DerSimonian and Laird (1986)\n    <doi:10.1016/0197-2456(86)90046-2>] meta-analyses on combined genetic data. Specifically, this package\n    implements a fixed-effects model [Kazeem and Farrall (2005) <doi:10.1046/j.1529-8817.2005.00156.x>]\n    and a random-effects model [Nicodemus (2008) <doi:10.1186/1471-2105-9-130>] for combined studies."}, "MetaIntegrator": {"categories": ["MetaAnalysis"], "description": "A pipeline for the meta-analysis  of gene expression data. We have\n\tassembled several analysis and plot functions to\n    perform integrated multi-cohort analysis of gene expression data (meta-\n    analysis). Methodology described in:\n\t<http://biorxiv.org/content/early/2016/08/25/071514>."}, "hackeRnews": {"categories": ["WebTechnologies"], "description": "Use the <https://hacker-news.firebaseio.com/v0/> API through R. Retrieve\n    posts, articles and other items in form of convenient R objects."}, "gdalUtilities": {"categories": ["Spatial"], "description": "R's 'sf' package ships with self-contained 'GDAL'\n    executables, including a bare bones interface to several\n    'GDAL'-related utility programs collectively known as the 'GDAL\n    utilities'. For each of those utilities, this package provides an\n    R wrapper whose formal arguments closely mirror those of the\n    'GDAL' command line interface. The utilities operate on data\n    stored in files and typically write their output to other\n    files. Therefore, to process data stored in any of R's more common\n    spatial formats (i.e. those supported by the 'sp', 'sf', and\n    'raster' packages), first write them to disk, then process them\n    with the package's wrapper functions before reading the outputted\n    results back into R. GDAL function arguments introduced in GDAL\n    version 3.2.1 or earlier are supported."}, "causaleffect": {"categories": ["CausalInference"], "description": "Functions for identification and transportation of causal effects. Provides a conditional causal effect identification algorithm (IDC) by Shpitser, I. and Pearl, J. (2006) <http://ftp.cs.ucla.edu/pub/stat_ser/r329-uai.pdf>, an algorithm for transportability from multiple domains with limited experiments by Bareinboim, E. and Pearl, J. (2014) <http://ftp.cs.ucla.edu/pub/stat_ser/r443.pdf>, and a selection bias recovery algorithm by Bareinboim, E. and Tian, J. (2015) <http://ftp.cs.ucla.edu/pub/stat_ser/r445.pdf>. All of the previously mentioned algorithms are based on a causal effect identification algorithm by Tian , J. (2002) <http://ftp.cs.ucla.edu/pub/stat_ser/r309.pdf>."}, "fdadensity": {"categories": ["FunctionalData"], "description": "An implementation of the methodology described in\n    Petersen and Mueller (2016) <doi:10.1214/15-AOS1363> for the functional\n    data analysis of samples of density functions.  Densities are first\n    transformed to their corresponding log quantile densities, followed by\n    ordinary Functional Principal Components Analysis (FPCA).  Transformation\n    modes of variation yield improved interpretation of the variability in the\n    data as compared to FPCA on the densities themselves.  The standard\n    fraction of variance explained (FVE) criterion commonly used for functional\n    data is adapted to the transformation setting, also allowing for an\n    alternative quantification of variability for density data through the\n    Wasserstein metric of optimal transport."}, "LiblineaR": {"categories": ["MachineLearning"], "description": "A wrapper around the LIBLINEAR C/C++ library for machine\n        learning (available at\n        <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>). LIBLINEAR is a\n        simple library for solving large-scale regularized linear\n        classification and regression. It currently supports\n        L2-regularized classification (such as logistic regression,\n        L2-loss linear SVM and L1-loss linear SVM) as well as\n        L1-regularized classification (such as L2-loss linear SVM and\n        logistic regression) and L2-regularized support vector\n        regression (with L1- or L2-loss). The main features of\n        LiblineaR include multi-class classification (one-vs-the rest,\n        and Crammer & Singer method), cross validation for model\n        selection, probability estimates (logistic regression only) or\n        weights for unbalanced data. The estimation of the models is\n        particularly fast as compared to other libraries."}, "empichar": {"categories": ["Distributions"], "description": "Evaluates the empirical characteristic function of univariate and multivariate samples.\n    This package uses 'RcppArmadillo' for fast evaluation. It is also possible to export the code to be used in other packages at 'C++' level."}, "memisc": {"categories": ["MissingData", "OfficialStatistics", "ReproducibleResearch"], "description": "An infrastructure for the management of survey data including\n        value labels, definable missing values, recoding of variables,\n        production of code books, and import of (subsets of) 'SPSS' and\n        'Stata' files is provided. Further, the package allows to produce\n        tables and data frames of arbitrary descriptive statistics and\n        (almost) publication-ready tables of regression model\n        estimates, which can be exported to 'LaTeX' and HTML."}, "VGAM": {"categories": ["Distributions", "Econometrics", "Environmetrics", "ExtremeValue", "Psychometrics", "Survival"], "description": "An implementation of about 6 major classes of\n    statistical regression models. The central algorithm is\n    Fisher scoring and iterative reweighted least squares.\n    At the heart of this package are the vector generalized linear\n    and additive model (VGLM/VGAM) classes. VGLMs can be loosely\n    thought of as multivariate GLMs. VGAMs are data-driven\n    VGLMs that use smoothing. The book \"Vector Generalized\n    Linear and Additive Models: With an Implementation in R\"\n    (Yee, 2015) <doi:10.1007/978-1-4939-2818-7> gives details of\n    the statistical framework and the package. Currently only\n    fixed-effects models are implemented. Many (100+) models and\n    distributions are estimated by maximum likelihood estimation\n    (MLE) or penalized MLE. The other classes are RR-VGLMs\n    (reduced-rank VGLMs), quadratic RR-VGLMs, reduced-rank VGAMs,\n    RCIMs (row-column interaction models)\u2014these classes perform\n    constrained and unconstrained quadratic ordination (CQO/UQO)\n    models in ecology, as well as constrained additive ordination\n    (CAO). Hauck-Donner effect detection is implemented.\n    Note that these functions are subject to change;\n    see the NEWS and ChangeLog files for latest changes."}, "greeks": {"categories": ["Finance"], "description": "Methods to calculate sensitivities of financial option prices for\n European, Asian, American and Digital Options options in the Black Scholes\n model, and in more general jump diffusion models. Furthermore, methods to\n compute implied volatilities are provided for a wide range of option types and\n custom payoff functions. Classical formulas are implemented for European\n options in the Black Scholes Model, as is presented in Hull, J. C. (2017).\n Options, Futures, and Other Derivatives, Global Edition (9th Edition). Pearson.\n In the case of Asian options, Malliavin Monte Carlo Greeks are implemented, see\n Hudde, A. & R\u00fcschendorf, L. (2016). European and Asian Greeks for exponential\n L\u00e9vy processes. <arXiv:1603.00920>. For American options, the Binomial Tree\n Method is implemented, as is presented in Hull, J. C. (2017). "}, "fdapace": {"categories": ["FunctionalData"], "description": "A versatile package that provides implementation of various\n    methods of Functional Data Analysis (FDA) and Empirical Dynamics. The core of this\n    package is Functional Principal Component Analysis (FPCA), a key technique for\n    functional data analysis, for sparsely or densely sampled random trajectories\n    and time courses, via the Principal Analysis by Conditional Estimation\n    (PACE) algorithm. This core algorithm yields covariance and mean functions,\n    eigenfunctions and principal component (scores), for both functional data and\n    derivatives, for both dense (functional) and sparse (longitudinal) sampling designs.\n    For sparse designs, it provides fitted continuous trajectories with confidence bands,\n    even for subjects with very few longitudinal observations. PACE is a viable and\n    flexible alternative to random effects modeling of longitudinal data. There is also a\n    Matlab version (PACE) that contains some methods not available on fdapace and vice\n    versa. Updates to fdapace were supported by grants from NIH Echo and NSF DMS-1712864 and DMS-2014626. Please cite our package if you use it (You may run the command citation(\"fdapace\") to get the citation format and bibtex entry).\n    References: Wang, J.L., Chiou, J., M\u00fcller, H.G. (2016) <doi:10.1146/annurev-statistics-041715-033624>;\n    Chen, K., Zhang, X., Petersen, A., M\u00fcller, H.G. (2017) <doi:10.1007/s12561-015-9137-5>."}, "RcppAlgos": {"categories": ["NumericalMathematics"], "description": "Provides optimized functions and flexible combinatorial iterators\n    implemented in C++ for solving problems in combinatorics and\n    computational mathematics. Utilizes the RMatrix class from 'RcppParallel'\n    for thread safety. There are combination/permutation functions with\n    constraint parameters that allow for generation of all results of a vector\n    meeting specific criteria (e.g. generating integer partitions\n    or finding all combinations such that the sum is between two bounds).\n    Capable of generating specific combinations/permutations (e.g. retrieve\n    only the nth lexicographical result) which sets up nicely for\n    parallelization as well as random sampling. Gmp support permits exploration\n    where the total number of results is large (e.g. comboSample(10000, 500,\n    n = 4)). Additionally, there are several high performance number theoretic\n    functions that are useful for problems common in computational mathematics.\n    Some of these functions make use of the fast integer division library\n    'libdivide'. The primeSieve function is based on the segmented sieve of\n    Eratosthenes implementation by Kim Walisch. It is also efficient for large\n    numbers by using the cache friendly improvements originally developed by\n    Tom\u00e1s Oliveira. Finally, there is a prime counting function that implements\n    Legendre's formula based on the work of Kim Walisch."}, "mirtCAT": {"categories": ["Psychometrics"], "description": "Provides tools to generate an HTML interface for creating adaptive\n    and non-adaptive educational and psychological tests using the shiny\n    package (Chalmers (2016) <doi:10.18637/jss.v071.i05>). \n    Suitable for applying unidimensional and multidimensional\n    computerized adaptive tests (CAT) using item response theory methodology and for\n    creating simple questionnaires forms to collect response data directly in R.\n    Additionally, optimal test designs (e.g., \"shadow testing\") are supported\n    for tests which contain a large number of item selection constraints.\n    Finally, package contains tools useful for performing Monte Carlo simulations \n    for studying the behavior of computerized adaptive test banks."}, "smacpod": {"categories": ["Spatial"], "description": "Statistical methods for analyzing case-control point data.  Methods include the ratio of kernel densities, the difference in K Functions, the spatial scan statistic, and q nearest neighbors of cases."}, "crch": {"categories": ["Distributions", "Econometrics"], "description": "Different approaches to censored or truncated regression with \n  conditional heteroscedasticity are provided. First, continuous \n  distributions can be used for the (right and/or left censored or truncated)\n  response with separate linear predictors for the mean and variance. \n  Second, cumulative link models for ordinal data\n  (obtained by interval-censoring continuous data) can be employed for\n  heteroscedastic extended logistic regression (HXLR). In the latter type of\n  models, the intercepts depend on the thresholds that define the intervals. "}, "SuperLearner": {"categories": ["MachineLearning"], "description": "Implements the super learner prediction method and contains a\n    library of prediction algorithms to be used in the super learner."}, "irtDemo": {"categories": ["Psychometrics"], "description": "\n  Includes a collection of shiny applications to demonstrate\n  or to explore fundamental item response theory (IRT) concepts\n  such as estimation, scoring, and multidimensional IRT models."}, "basad": {"categories": ["Bayesian"], "description": "Provides a Bayesian variable selection approach using continuous spike and slab prior distributions. The prior choices here are motivated by the shrinking and diffusing priors studied in Narisetty & He (2014) <doi:10.1214/14-AOS1207>."}, "DTWUMI": {"categories": ["MissingData"], "description": "Functions to impute large gaps within multivariate time series based on Dynamic Time Warping methods. Gaps of size 1 or inferior to a defined threshold are filled using simple average and weighted moving average respectively. Larger gaps are filled using the methodology provided by Phan et al. (2017) <doi:10.1109/MLSP.2017.8168165>: a query is built immediately before/after a gap and a moving window is used to find the most similar sequence to this query using Dynamic Time Warping. To lower the calculation time, similar sequences are pre-selected using global features. Contrary to the univariate method (package 'DTWBI'), these global features are not estimated over the sequence containing the gap(s), but a feature matrix is built to summarize general features of the whole multivariate signal. Once the most similar sequence to the query has been identified, the adjacent sequence to this window is used to fill the gap considered. This function can deal with multiple gaps over all the sequences componing the input multivariate signal. However, for better consistency, large gaps at the same location over all sequences should be avoided."}, "mapedit": {"categories": ["Spatial"], "description": "Suite of interactive functions and helpers for selecting and editing\n    geospatial data."}, "tempdisagg": {"categories": ["TimeSeries"], "description": "Temporal disaggregation methods are used to disaggregate and\n    interpolate a low frequency time series to a higher frequency series, where\n    either the sum, the mean, the first or the last value of the resulting\n    high frequency series is consistent with the low frequency series. Temporal\n    disaggregation can be performed with or without one or more high frequency\n    indicator series. Contains the methods of Chow-Lin, Santos-Silva-Cardoso,\n    Fernandez, Litterman, Denton and Denton-Cholette, summarized in Sax and\n    Steiner (2013) <doi:10.32614/RJ-2013-028>. Supports most R time series\n    classes."}, "spsur": {"categories": ["Econometrics", "Spatial"], "description": "A collection of functions to test and estimate Seemingly \n    Unrelated Regression (usually called SUR) models, with spatial structure, by maximum \n    likelihood and three-stage least squares. The package estimates the \n    most common spatial specifications, that is, SUR with Spatial Lag of \n    X regressors (called SUR-SLX), SUR with Spatial Lag Model (called SUR-SLM), \n    SUR with Spatial Error Model (called SUR-SEM), SUR with Spatial Durbin Model (called SUR-SDM), \n    SUR with Spatial Durbin Error Model (called SUR-SDEM), \n    SUR with Spatial Autoregressive terms and Spatial Autoregressive \n    Disturbances (called SUR-SARAR), SUR-SARAR with Spatial Lag of X \n    regressors (called SUR-GNM) and SUR with Spatially Independent \n    Model (called SUR-SIM). The methodology of these models can be found \n    in next references: Mur, J., Lopez, F., and Herrera, M. (2010) \n    <doi:10.1080/17421772.2010.516443>; \n    Lopez, F.A., Mur, J., and Angulo, A. (2014) \n    <doi:10.1007/s00168-014-0624-2> and \n    Lopez, F.A., Minguez, R. and Mur, J. (2020) \n    <doi:10.1007/s00168-019-00914-1>."}, "bayesbio": {"categories": ["Bayesian"], "description": "A hodgepodge of hopefully helpful functions. Two of these perform\n    shrinkage estimation: one using a simple weighted method where the user can\n    specify the degree of shrinkage required, and one using James-Stein shrinkage\n    estimation for the case of unequal variances."}, "discSurv": {"categories": ["Survival"], "description": "Provides data transformations, estimation utilities,\n    predictive evaluation measures and simulation functions for discrete time\n    survival analysis."}, "polspline": {"categories": ["Survival"], "description": "Routines for the polynomial spline fitting routines\n  hazard regression, hazard estimation with flexible tails, logspline,\n  lspec, polyclass, and polymars, by C. Kooperberg and co-authors."}, "BCE": {"categories": ["Bayesian"], "description": "Function to estimate taxonomic compositions from biomarker data, using a Bayesian approach."}, "implyr": {"categories": ["Databases"], "description": "'SQL' back-end to 'dplyr' for Apache Impala, the massively\n    parallel processing query engine for Apache 'Hadoop'. Impala enables\n    low-latency 'SQL' queries on data stored in the 'Hadoop' Distributed\n    File System '(HDFS)', Apache 'HBase', Apache 'Kudu', Amazon Simple \n    Storage Service '(S3)', Microsoft Azure Data Lake Store '(ADLS)', \n    and Dell 'EMC' 'Isilon'. See <https://impala.apache.org> for more\n    information about Impala."}, "rhosa": {"categories": ["TimeSeries"], "description": "Higher-order spectra or polyspectra of time series, such as bispectrum and bicoherence, have been investigated in abundant literature and applied to problems of signal detection in a wide range of fields. This package aims to provide a simple API to estimate and analyze them. The current implementation is based on Brillinger and Irizarry (1998) <doi:10.1016/S0165-1684(97)00217-X> for estimating bispectrum or bicoherence, Lii and Helland (1981) <doi:10.1145/355958.355961> for cross-bispectrum, and Kim and Powers (1979) <doi:10.1109/TPS.1979.4317207> for cross-bicoherence."}, "tsibbledata": {"categories": ["TimeSeries"], "description": "Provides diverse datasets in the 'tsibble' data structure. These datasets are useful for learning and demonstrating how tidy temporal data can tidied, visualised, and forecasted."}, "ORIClust": {"categories": ["Cluster"], "description": "A user-friendly R-based software package for\n        gene clustering. Clusters are given by genes matched to\n        prespecified profiles across various ordered treatment groups.\n        It is particularly useful for analyzing data obtained from\n        short time-course or dose-response microarray experiments."}, "quadprog": {"categories": ["Optimization"], "description": "This package contains routines and documentation for\n        solving quadratic programming problems."}, "DiPs": {"categories": ["CausalInference"], "description": "Improves the balance of optimal matching with near-fine balance by giving penalties on the unbalanced covariates with the unbalanced directions. Many directional penalties can also be viewed as Lagrange multipliers, pushing a matched sample in the direction of satisfying a linear constraint that would not be satisfied without penalization.\n    Yu and Rosenbaum (2019) <doi:10.1111/biom.13098>. "}, "quanteda": {"categories": ["NaturalLanguageProcessing"], "description": "A fast, flexible, and comprehensive framework for \n    quantitative text analysis in R.  Provides functionality for corpus management,\n    creating and manipulating tokens and ngrams, exploring keywords in context, \n    forming and manipulating sparse matrices\n    of documents by features and feature co-occurrences, analyzing keywords, computing feature similarities and\n    distances, applying content dictionaries, applying supervised and unsupervised machine learning, \n    visually representing text and text analyses, and more. "}, "RM2006": {"categories": ["Finance"], "description": "Estimation of the conditional covariance matrix using the RiskMetrics 2006 methodology of Zumbach (2007) <doi:10.2139/ssrn.1420185>."}, "nflseedR": {"categories": ["SportsAnalytics"], "description": "A set of functions to simulate National Football\n    League seasons including the sophisticated tie-breaking procedures."}, "teamcolors": {"categories": ["SportsAnalytics"], "description": "Provides color palettes corresponding to professional and amateur, \n    sports teams. These can be useful in creating data graphics that are themed \n    for particular teams. "}, "episensr": {"categories": ["Epidemiology"], "description": "Basic sensitivity analysis of the observed relative risks adjusting\n    for unmeasured confounding and misclassification of the\n    exposure/outcome, or both. It follows the bias analysis methods and\n    examples from the book by Lash T.L, Fox M.P, and Fink A.K.\n    \"Applying Quantitative Bias Analysis to Epidemiologic Data\",\n    ('Springer', 2009)."}, "grpreg": {"categories": ["MachineLearning"], "description": "Efficient algorithms for fitting the regularization path of linear\n  regression, GLM, and Cox regression models with grouped penalties.  This\n  includes group selection methods such as group lasso, group MCP, and\n  group SCAD as well as bi-level selection methods such as the group\n  exponential lasso, the composite MCP, and the group bridge.  For more\n  information, see Breheny and Huang (2009) <doi:10.4310/sii.2009.v2.n3.a10>,\n  Huang, Breheny, and Ma (2012) <doi:10.1214/12-sts392>, Breheny and Huang\n  (2015) <doi:10.1007/s11222-013-9424-2>, and Breheny (2015)\n  <doi:10.1111/biom.12300>, or visit the package homepage\n  <https://pbreheny.github.io/grpreg/>."}, "MSGARCH": {"categories": ["Finance"], "description": "Fit (by Maximum Likelihood or MCMC/Bayesian), simulate, and forecast various Markov-Switching GARCH models as described in Ardia et al. (2019) <doi:10.18637/jss.v091.i04>."}, "distill": {"categories": ["ReproducibleResearch"], "description": "Scientific and technical article format for the web.\n    'Distill' articles feature attractive, reader-friendly typography,\n    flexible layout options for visualizations, and full support for\n    footnotes and citations."}, "volleystat": {"categories": ["SportsAnalytics"], "description": "Volleyball match statistics of the German volleyball first division league (seasons 2013/2014 to \n    2018/2019). The data has been collected from the official volleyball first division homepage \n    (<www.volleyball-bundesliga.de>) and contains information on teams, staff, sets, matches, and player-in-match \n    statistics (extracted automatically from the official match reports)."}, "googleComputeEngineR": {"categories": ["WebTechnologies"], "description": "Interact with the 'Google Compute Engine' API in R. Lets you create, \n  start and stop instances in the 'Google Cloud'.  Support for preconfigured instances, \n  with templates for common R needs. "}, "jstor": {"categories": ["WebTechnologies"], "description": "Functions and helpers to import metadata, ngrams and full-texts \n    delivered by Data for Research by JSTOR. "}, "Bmix": {"categories": ["Bayesian", "Cluster"], "description": "This is a bare-bones implementation of sampling algorithms\n        for a variety of Bayesian stick-breaking (marginally DP)\n        mixture models, including particle learning and Gibbs sampling\n        for static DP mixtures, particle learning for dynamic BAR\n        stick-breaking, and DP mixture regression.  The software is\n        designed to be easy to customize to suit different situations\n        and for experimentation with stick-breaking models.  Since\n        particles are repeatedly copied, it is not an especially\n        efficient implementation."}, "tau": {"categories": ["NaturalLanguageProcessing"], "description": "Utilities for text analysis."}, "OpenStreetMap": {"categories": ["Spatial"], "description": "Accesses high resolution raster maps using the OpenStreetMap\n    protocol. Dozens of road, satellite, and topographic map servers are directly\n    supported, including Apple, Mapnik, Bing, and stamen. Additionally raster maps\n    may be constructed using custom tile servers.  Maps can be\n    plotted using either base graphics, or ggplot2. This package is not affiliated\n    with the OpenStreetMap.org mapping project."}, "kdist": {"categories": ["Distributions"], "description": "Density, distribution function, quantile function and random generation\n             for the K-distribution. A plotting function that plots data on Weibull\n             paper and another function to draw additional lines. See results from package in T Lamont-Smith (2018), submitted J. R. Stat. Soc."}, "R2wd": {"categories": ["ReproducibleResearch"], "description": "This package uses either the statconnDCOM server (via the\n        rcom package) or the RDCOMClient to communicate with MS-Word\n        via the COM interface."}, "FactoMineR": {"categories": ["Psychometrics"], "description": "Exploratory data analysis methods to summarize, visualize and describe datasets. The main principal component methods are available, those with the largest potential in terms of applications: principal component analysis (PCA) when variables are quantitative, correspondence analysis (CA) and multiple correspondence analysis (MCA) when variables are categorical, Multiple Factor Analysis when variables are structured in groups, etc. and hierarchical cluster analysis. F. Husson, S. Le and J. Pages (2017)."}, "idem": {"categories": ["CausalInference", "MissingData"], "description": "In randomized studies involving severely ill patients, functional\n    outcomes are often unobserved due to missed clinic visits, premature\n    withdrawal or death. It is well known that if these unobserved functional\n    outcomes are not handled properly, biased treatment comparisons can be\n    produced. In this package, we implement a procedure for comparing treatments\n    that is based on the composite endpoint of both the functional outcome and\n    survival. The procedure was proposed in Wang et al. (2016) <doi:10.1111/biom.12594>\n    and Wang et al. (2020) <doi:10.18637/jss.v093.i12>. It considers missing data\n    imputation with different sensitivity\n    analysis strategies to handle the unobserved functional outcomes not due to\n    death."}, "doSNOW": {"categories": ["HighPerformanceComputing"], "description": "Provides a parallel backend for the %dopar% function using\n        the snow package of Tierney, Rossini, Li, and Sevcikova."}, "sgt": {"categories": ["Distributions"], "description": "Density, distribution function, quantile function and random generation for the skewed generalized t distribution. This package also provides a function that can fit data to the skewed generalized t distribution using maximum likelihood estimation."}, "RGF": {"categories": ["MachineLearning"], "description": "Regularized Greedy Forest wrapper of the 'Regularized Greedy Forest' <https://github.com/RGF-team/rgf/tree/master/python-package> 'python' package, which also includes a Multi-core implementation (FastRGF) <https://github.com/RGF-team/rgf/tree/master/FastRGF>."}, "insee": {"categories": ["OfficialStatistics"], "description": "Using embedded sdmx queries, get the data of more than 150 000 insee series from bdm database. Have a look at the detailed sdmx web service page with the following link : <https://www.insee.fr/en/information/2868055>."}, "protoclust": {"categories": ["Cluster"], "description": "Performs minimax linkage hierarchical clustering.  Every cluster\n    has an associated prototype element that represents that cluster as\n    described in Bien, J., and Tibshirani, R. (2011), \"Hierarchical Clustering \n    with Prototypes via Minimax Linkage,\" The Journal of the American \n    Statistical Association, 106(495), 1075-1084."}, "PDQutils": {"categories": ["Distributions"], "description": "A collection of tools for approximating the 'PDQ' functions\n    (respectively, the cumulative distribution, density, and quantile) of\n    probability distributions via classical expansions involving moments and\n    cumulants."}, "trendeval": {"categories": ["Epidemiology"], "description": "Provides a coherent interface for evaluating models fit with the\n  trending package.  This package is part of the RECON\n  (<https://www.repidemicsconsortium.org/>) toolkit for outbreak analysis."}, "Rdsm": {"categories": ["HighPerformanceComputing"], "description": "Provides a threads-type programming environment for R.\n   The package gives the R programmer the clearer, more concise\n   shared memory world view, and in some cases gives superior\n   performance as well.  In addition, it enables parallel processing on\n   very large, out-of-core matrices.  "}, "mime": {"categories": ["WebTechnologies"], "description": "Guesses the MIME type from a filename extension using the data\n    derived from /etc/mime.types in UNIX-type systems."}, "isni": {"categories": ["MissingData"], "description": "The current version provides functions to compute, print and summarize the Index of Sensitivity to Nonignorability (ISNI) in the generalized linear model for independent data, and in the marginal multivariate Gaussian model and the mixed-effects models for continuous and binary longitudinal/clustered data. It allows for arbitrary patterns of missingness in the regression outcomes  caused by dropout and/or intermittent missingness. One can compute the sensitivity index without estimating any nonignorable models or positing specific magnitude of nonignorability. Thus ISNI provides a simple quantitative assessment of how robust the standard estimates assuming missing at random  is with respect to the assumption of ignorability. For a tutorial, download at <https://huixie.people.uic.edu/Research/ISNI_R_tutorial.pdf>.\tFor more details, see Troxel Ma and Heitjan (2004) and Xie and Heitjan (2004) <doi:10.1191/1740774504cn005oa> and Ma Troxel and Heitjan (2005) <doi:10.1002/sim.2107> and  Xie (2008) <doi:10.1002/sim.3117> and  Xie (2012) <doi:10.1016/j.csda.2010.11.021> and Xie and Qian (2012) <doi:10.1002/jae.1157>."}, "amap": {"categories": ["Cluster", "Environmetrics"], "description": "Tools for Clustering and Principal Component Analysis\n        (With robust methods, and parallelized functions)."}, "dplyr": {"categories": ["Databases", "Epidemiology", "ModelDeployment"], "description": "A fast, consistent tool for working with data frame\n    like objects, both in memory and out of memory."}, "copulaedas": {"categories": ["Optimization"], "description": "Provides a platform where EDAs (estimation of\n    distribution algorithms) based on copulas can be implemented and\n    studied. The package offers complete implementations of various\n    EDAs based on copulas and vines, a group of well-known\n    optimization problems, and utility functions to study the\n    performance of the algorithms. Newly developed EDAs can be easily\n    integrated into the package by extending an S4 class with generic\n    functions for their main components."}, "Lahman": {"categories": ["SportsAnalytics"], "description": "Provides the tables from the 'Sean Lahman Baseball Database' as\n    a set of R data.frames. It uses the data on pitching, hitting and fielding\n    performance and other tables from 1871 through 2021, as recorded in the 2022\n    version of the database. Documentation examples show how many baseball\n    questions can be investigated."}, "nfl4th": {"categories": ["SportsAnalytics"], "description": "A set of functions to estimate outcomes of fourth down\n    plays in the National Football League and obtain fourth down plays\n    from <https://www.nfl.com/> and <https://www.espn.com/>."}, "tripack": {"categories": ["NumericalMathematics"], "description": "A constrained two-dimensional Delaunay triangulation package\n  providing both triangulation and generation of voronoi mosaics of \n  irregular spaced data."}, "bama": {"categories": ["Bayesian"], "description": "Perform mediation analysis in the presence of high-dimensional\n    mediators based on the potential outcome framework. Bayesian Mediation\n    Analysis (BAMA), developed by Song et al (2019) <doi:10.1111/biom.13189> and\n    Song et al (2020) <arXiv:2009.11409>,\n    relies on two Bayesian sparse linear mixed models to simultaneously analyze\n    a relatively large number of mediators for a continuous exposure and outcome\n    assuming a small number of mediators are truly active. This sparsity\n    assumption also allows the extension of univariate mediator analysis by\n    casting the identification of active mediators as a variable selection\n    problem and applying Bayesian methods with continuous shrinkage priors on\n    the effects."}, "fpcb": {"categories": ["TimeSeries"], "description": "Functions to represent functional objects under a Reproducing Kernel Hilbert Space (RKHS) framework as described \n  in Mu\u00f1oz & Gonz\u00e1lez (2010). Autoregressive Hilbertian Model for functional time series using RKHS and predictive confidence bands construction \n  as proposed in Hern\u00e1ndez et al (2021)."}, "readsdmx": {"categories": ["OfficialStatistics"], "description": "Read Statistical Data and Metadata Exchange (SDMX) XML data. \n    This the main transmission format used in official statistics. Data can be imported from\n    local SDMX-ML files or a SDMX web-service and will be read in 'as is' into a dataframe object.\n    The 'RapidXML' C++ library <http://rapidxml.sourceforge.net> is used to parse the XML data."}, "cds": {"categories": ["Psychometrics"], "description": "This is an implementation of constrained dual scaling for\n    detecting response styles in categorical data, including utility functions. The\n    procedure involves adding additional columns to the data matrix representing the\n    boundaries between the rating categories. The resulting matrix is then doubled\n    and analyzed by dual scaling. One-dimensional solutions are sought which provide\n    optimal scores for the rating categories. These optimal scores are constrained\n    to follow monotone quadratic splines. Clusters are introduced within which the\n    response styles can vary. The type of response style present in a cluster can\n    be diagnosed from the optimal scores for said cluster, and this can be used to\n    construct an imputed version of the data set which adjusts for response styles."}, "markdown": {"categories": ["ReproducibleResearch"], "description": "Provides R bindings to the 'Sundown' Markdown rendering library\n    (<https://github.com/vmg/sundown>). Markdown is a plain-text formatting\n    syntax that can be converted to 'XHTML' or other formats. See\n    <http://en.wikipedia.org/wiki/Markdown> for more information about Markdown."}, "RHMS": {"categories": ["Hydrology"], "description": "Hydrologic modelling system is an object oriented tool for simulation and analysis of hydrologic events. The package proposes functions and methods for construction, simulation, visualization, and calibration of a hydrologic model."}, "bigtime": {"categories": ["TimeSeries"], "description": "Estimation of large Vector AutoRegressive (VAR), Vector AutoRegressive with Exogenous Variables X (VARX) and Vector AutoRegressive Moving Average (VARMA) Models with Structured Lasso Penalties, see Nicholson, Wilms, Bien and Matteson (2020) <https://jmlr.org/papers/v21/19-777.html> and Wilms, Basu, Bien and Matteson (2021) <doi:10.1080/01621459.2021.1942013>."}, "TrendInTrend": {"categories": ["CausalInference"], "description": "Estimation of causal odds ratio and power calculation given trends in exposure prevalence    and outcome frequencies of stratified data."}, "mda": {"categories": ["Environmetrics"], "description": "Mixture and flexible discriminant analysis, multivariate\n        adaptive regression splines (MARS), BRUTO, and vector-response smoothing splines.\n\tHastie, Tibshirani and Friedman (2009) \"Elements of Statistical Learning (second edition, chap 12)\" Springer, New York. "}, "otsad": {"categories": ["TimeSeries"], "description": "Implements a set of online fault detectors for time-series, called: PEWMA see M. Carter\n             et al. (2012) <doi:10.1109/SSP.2012.6319708>, SD-EWMA and TSSD-EWMA see H. Raza et al. \n             (2015) <doi:10.1016/j.patcog.2014.07.028>, KNN-CAD see E. Burnaev et al. (2016)\n             <arXiv:1608.04585>, KNN-LDCD see V. Ishimtsev et al. (2017) <arXiv:1706.03412> and \n             CAD-OSE see M. Smirnov (2018) <https://github.com/smirmik/CAD>. The first three \n             algorithms belong to prediction-based techniques and the last three belong to \n             window-based techniques. In addition, the SD-EWMA and PEWMA algorithms are algorithms \n             designed to work in stationary environments, while the other four \n             are algorithms designed to work in non-stationary environments."}, "DAKS": {"categories": ["Psychometrics"], "description": "Functions and an example dataset for the psychometric theory of\n  knowledge spaces.  This package implements data analysis methods and\n  procedures for simulating data and quasi orders and transforming different\n  formulations in knowledge space theory.  See package?DAKS for an overview."}, "htm2txt": {"categories": ["WebTechnologies"], "description": "Convert a html document to plain texts by stripping off all html tags."}, "signal": {"categories": ["NumericalMathematics"], "description": "A set of signal processing functions originally written for 'Matlab' and 'Octave'.\n  Includes filter generation utilities, filtering functions,\n  resampling routines, and visualization of filter models. It also\n  includes interpolation functions."}, "mded": {"categories": ["Distributions"], "description": "Provides a function for measuring the difference between two independent or non-independent empirical distributions and returning a significance level of the difference."}, "frechet": {"categories": ["FunctionalData"], "description": "Provides implementation of statistical methods for random objects \n    lying in various metric spaces, which are not necessarily linear spaces. \n    The core of this package is Fr\u00e9chet regression for random objects with \n    Euclidean predictors, which allows one to perform regression analysis \n    for non-Euclidean responses under some mild conditions. \n    Examples include distributions in L^2-Wasserstein space, \n    covariance matrices endowed with power metric (with Frobenius metric \n    as a special case), Cholesky and log-Cholesky metrics.  \n    References: Petersen, A., & M\u00fcller, H.-G. (2019) <doi:10.1214/17-AOS1624>."}, "gsynth": {"categories": ["CausalInference", "MissingData"], "description": "Provides causal inference with interactive fixed-effect models. It imputes counterfactuals for each treated unit using control group information based on a linear interactive fixed effects model that incorporates unit-specific intercepts interacted with time-varying coefficients. This method generalizes the synthetic control method to the case of multiple treated units and variable treatment periods, and improves efficiency and interpretability. This version supports unbalanced panels and implements the matrix completion method. "}, "Dire": {"categories": ["Psychometrics"], "description": "Fit latent variable linear models, estimating score distributions for groups of people, following Cohen and Jiang (1999) <doi:10.2307/2669917>. In this model, a latent distribution is conditional on students item response, item characteristics, and conditioning variables the user includes. This latent trait is then integrated out. This software is intended to fit the same models as the existing software 'AM' <https://am.air.org/>. As of version 2, also allows the user to draw plausible values."}, "bigleaf": {"categories": ["Hydrology"], "description": "Calculation of physical (e.g. aerodynamic conductance, surface temperature), \n             and physiological (e.g. canopy conductance, water-use efficiency) ecosystem properties\n\t\t\t from eddy covariance data and accompanying meteorological measurements. Calculations\n\t\t\t assume the land surface to behave like a 'big-leaf' and return bulk ecosystem/canopy variables."}, "Rsolnp": {"categories": ["Optimization"], "description": "General Non-linear Optimization Using Augmented Lagrange Multiplier Method."}, "drake": {"categories": ["HighPerformanceComputing", "ReproducibleResearch"], "description": "A general-purpose computational engine for data\n  analysis, drake rebuilds intermediate data objects when their\n  dependencies change, and it skips work when the results are already up\n  to date.  Not every execution starts from scratch, there is native\n  support for parallel and distributed computing, and completed projects\n  have tangible evidence that they are reproducible.  Extensive\n  documentation, from beginner-friendly tutorials to practical examples\n  and more, is available at the reference website\n  <https://docs.ropensci.org/drake/> and the online manual\n  <https://books.ropensci.org/drake/>."}, "slider": {"categories": ["TimeSeries"], "description": "Provides type-stable rolling window functions over any R data\n    type. Cumulative and expanding windows are also supported. For more\n    advanced usage, an index can be used as a secondary vector that\n    defines how sliding windows are to be created."}, "clifford": {"categories": ["NumericalMathematics"], "description": "A suite of routines for Clifford algebras, using the\n   'Map' class of the Standard Template Library.  Canonical\n   reference: Hestenes (1987, ISBN 90-277-1673-0, \"Clifford algebra\n   to geometric calculus\").  Special cases including Lorentz transforms,\n   quaternion multiplication, and Grassman algebra, are discussed.\n   Conformal geometric algebra theory is implemented."}, "rankhazard": {"categories": ["Survival"], "description": "Rank-hazard plots Karvanen and Harrell (2009) <doi:10.1002/sim.3591> visualize the relative importance of covariates in a proportional hazards model. The key idea is to rank the covariate values and plot the relative hazard as a function of ranks scaled to interval [0,1]. The relative hazard is plotted in respect to the reference hazard, which can bee.g. the hazard related to the median of the covariate."}, "itsmr": {"categories": ["TimeSeries"], "description": "Provides functions for modeling and forecasting time series data. Forecasting is based on the innovations algorithm. A description of the innovations algorithm can be found in the textbook \"Introduction to Time Series and Forecasting\" by Peter J. Brockwell and Richard A. Davis. <http://www.springer.com/us/book/9781475777505>."}, "nlsr": {"categories": ["Optimization"], "description": "Provides tools for working with nonlinear least squares problems.\n      It is intended to eventually supersede the 'nls()' function in the R\n      distribution. For example, 'nls()' specifically does NOT deal with small \n      or zero residual problems as its Gauss-Newton method frequently stops with\n      'singular gradient' messages. 'nlsr' is based on the now-deprecated package\n      'nlmrt', and has refactored functions and R-language symbolic derivative\n      features."}, "jsonvalidate": {"categories": ["WebTechnologies"], "description": "Uses the node library 'is-my-json-valid' or 'ajv' to\n    validate 'JSON' against a 'JSON' schema.  Drafts 04, 06 and 07 of\n    'JSON' schema are supported."}, "R.matlab": {"categories": ["NumericalMathematics"], "description": "Methods readMat() and writeMat() for reading and writing MAT files.  For user with MATLAB v6 or newer installed (either locally or on a remote host), the package also provides methods for controlling MATLAB (trademark) via R and sending and retrieving data between R and MATLAB."}, "validatetools": {"categories": ["OfficialStatistics"], "description": "Rule sets with validation rules may contain redundancies or contradictions. \n  Functions for finding redundancies and problematic rules are provided, \n  given a set a rules formulated with 'validate'."}, "RJDBC": {"categories": ["Databases"], "description": "The RJDBC package is an implementation of R's DBI interface using JDBC as a back-end. This allows R to connect to any DBMS that has a JDBC driver."}, "powerGWASinteraction": {"categories": ["ExperimentalDesign"], "description": "Analytical power calculations for GxE and GxG interactions for case-control studies of candidate genes and genome-wide association studies (GWAS). This includes power calculation for four two-step screening and testing procedures.  It can also calculate power for GxE and GxG without any screening.    "}, "costat": {"categories": ["TimeSeries"], "description": "Contains functions that can determine whether a time series\n\tis second-order stationary or not (and hence evidence for\n\tlocally stationarity). Given two non-stationary series (i.e.\n\tlocally stationary series) this package can then discover\n\ttime-varying linear combinations that are second-order stationary."}, "AnaCoDa": {"categories": ["Bayesian"], "description": "Is a collection of models to analyze genome scale codon\n        data using a Bayesian framework. Provides visualization\n        routines and checkpointing for model fittings. Currently\n        published models to analyze gene data for selection on codon\n        usage based on Ribosome Overhead Cost (ROC) are: ROC (Gilchrist\n        et al. (2015) <doi:10.1093/gbe/evv087>), and ROC with phi\n        (Wallace & Drummond (2013) <doi:10.1093/molbev/mst051>). In\n        addition 'AnaCoDa' contains three currently unpublished models.\n        The FONSE (First order approximation On NonSense Error) model\n        analyzes gene data for selection on codon usage against of\n        nonsense error rates. The PA (PAusing time) and PANSE (PAusing\n        time + NonSense Error) models use ribosome footprinting data to\n        analyze estimate ribosome pausing times with and without\n        nonsense error rate from ribosome footprinting data."}, "RSGHB": {"categories": ["Bayesian", "Econometrics"], "description": "Functions for estimating models using a Hierarchical Bayesian (HB) framework. The flexibility comes in allowing the user to specify the likelihood function directly instead of assuming predetermined model structures. Types of models that can be estimated with this code include the family of discrete choice models (Multinomial Logit, Mixed Logit, Nested Logit, Error Components Logit and Latent Class) as well ordered response models like ordered probit and ordered logit. In addition, the package allows for flexibility in specifying parameters as either fixed (non-varying across individuals) or random with continuous distributions. Parameter distributions supported include normal, positive/negative log-normal, positive/negative censored normal, and the Johnson SB distribution. Kenneth Train's Matlab and Gauss code for doing Hierarchical Bayesian estimation has served as the basis for a few of the functions included in this package. These Matlab/Gauss functions have been rewritten to be optimized within R. Considerable code has been added to increase the flexibility and usability of the code base. Train's original Gauss and Matlab code can be found here: <http://elsa.berkeley.edu/Software/abstracts/train1006mxlhb.html> See Train's chapter on HB in Discrete Choice with Simulation here: <http://elsa.berkeley.edu/books/choice2.html>; and his paper on using HB with non-normal distributions here: <http://eml.berkeley.edu//~train/trainsonnier.pdf>. The authors would also like to thank the invaluable contributions of Stephane Hess and the Choice Modelling Centre: <https://cmc.leeds.ac.uk/>."}, "sigclust": {"categories": ["Cluster"], "description": "SigClust is a statistical method for testing the\n        significance of clustering results. SigClust can be applied to\n        assess the statistical significance of splitting a data set\n        into two clusters. For more than two clusters, SigClust can be\n        used iteratively."}, "ProbitSpatial": {"categories": ["Spatial"], "description": "Fast estimation of binomial spatial probit regression models with spatial autocorrelation for big datasets."}, "diveMove": {"categories": ["Tracking"], "description": "Utilities to represent, visualize, filter, analyse, and summarize\n\t     time-depth recorder (TDR) data.  Miscellaneous functions for\n\t     handling location data are also provided."}, "Rfacebook": {"categories": ["WebTechnologies"], "description": "Provides an interface to the Facebook API."}, "pscl": {"categories": ["Bayesian", "Econometrics", "Environmetrics", "Psychometrics"], "description": "Bayesian analysis of item-response theory (IRT) models,\n\t     roll call analysis; computing highest density regions; maximum\n\t     likelihood estimation of zero-inflated and hurdle models for count\n\t     data; goodness-of-fit measures for GLMs; data sets used\n\t     in writing\tand teaching at the Political Science\n\t     Computational Laboratory; seats-votes curves."}, "ahaz": {"categories": ["MachineLearning", "Survival"], "description": "Computationally efficient procedures for regularized\n        estimation with the semiparametric additive hazards regression\n        model."}, "svrep": {"categories": ["OfficialStatistics"], "description": "Provides tools for creating and working with survey replicate weights,\n  extending functionality of the 'survey' package from Lumley (2004) <doi:10.18637/jss.v009.i08>.\n  Methods are provided for applying nonresponse adjustments to\n  both full-sample and replicate weights as suggested by \n  Rust and Rao (1996) <doi:10.1177/096228029600500305>.\n  Implements methods for sample-based calibration described by Opsomer and Erciulescu (2021) \n  <https://www150.statcan.gc.ca/n1/pub/12-001-x/2021002/article/00006-eng.htm>.\n  Diagnostic functions are included to compare weights and weighted estimates\n  from different sets of replicate weights."}, "RAdwords": {"categories": ["WebTechnologies"], "description": "Aims at loading Google Adwords data into R. Adwords is an online\n    advertising service that enables advertisers to display advertising copy to web\n    users (see <https://developers.google.com/adwords/> for more information). \n    Therefore the package implements three main features. First, the package\n    provides an authentication process for R with the Google Adwords API (see \n    <https://developers.google.com/adwords/api/> for more information) via OAUTH2.\n    Second, the package offers an interface to apply the Adwords query language in\n    R and query the Adwords API with ad-hoc reports. Third, the received data are\n    transformed into suitable data formats for further data processing and data\n    analysis."}, "DriftBurstHypothesis": {"categories": ["Finance"], "description": "Calculates the T-Statistic for the drift burst hypothesis from the working paper Christensen, Oomen and Reno (2018) <doi:10.2139/ssrn.2842535>. The authors' MATLAB code is available upon request, see: <https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2842535>."}, "matlab": {"categories": ["NumericalMathematics"], "description": "Emulate 'MATLAB' code using 'R'."}, "optweight": {"categories": ["CausalInference"], "description": "Use optimization to estimate weights that balance covariates for binary, multinomial, and continuous treatments in the spirit of Zubizarreta (2015) <doi:10.1080/01621459.2015.1023805>. The degree of balance can be specified for each covariate. In addition, sampling weights can be estimated that allow a sample to generalize to a population specified with given target moments of covariates."}, "robustreg": {"categories": ["Robust"], "description": "Linear regression functions using Huber and bisquare psi functions. Optimal weights are calculated using IRLS algorithm."}, "harmonicmeanp": {"categories": ["MetaAnalysis"], "description": "The harmonic mean p-value (HMP) test combines p-values and corrects for multiple testing while controlling the strong-sense family-wise error rate. It is more powerful than common alternatives including Bonferroni and Simes procedures when combining large proportions of all the p-values, at the cost of slightly lower power when combining small proportions of all the p-values. It is more stringent than controlling the false discovery rate, and possesses theoretical robustness to positive correlations between tests and unequal weights. It is a multi-level test in the sense that a superset of one or more significant tests is certain to be significant and conversely when the superset is non-significant, the constituent tests are certain to be non-significant. It is based on MAMML (model averaging by mean maximum likelihood), a frequentist analogue to Bayesian model averaging, and is theoretically grounded in generalized central limit theorem. For detailed examples type vignette(\"harmonicmeanp\") after installation. Version 3.0 addresses errors in versions 1.0 and 2.0 that led function p.hmp to control the familywise error rate only in the weak sense, rather than the strong sense as intended."}, "Rcgmin": {"categories": ["Optimization"], "description": "Conjugate gradient minimization of nonlinear functions with box constraints using Dai/Yuan update."}, "uroot": {"categories": ["TimeSeries"], "description": "Seasonal unit roots and seasonal stability tests.\n    P-values based on response surface regressions are available for both tests.\n    P-values based on bootstrap are available for seasonal unit root tests."}, "R2WinBUGS": {"categories": ["Bayesian", "GraphicalModels"], "description": "Invoke a 'BUGS' model in 'OpenBUGS' or 'WinBUGS', a class \"bugs\" for 'BUGS' \n  results and functions to work with that class.\n  Function write.model() allows a 'BUGS' model file to be written.  \n  The class and auxiliary functions could be used with other MCMC programs, including 'JAGS'."}, "s2": {"categories": ["Spatial"], "description": "Provides R bindings for Google's s2 library for geometric calculations on\n    the sphere. High-performance constructors and exporters provide high compatibility\n    with existing spatial packages, transformers construct new geometries from existing\n    geometries, predicates provide a means to select geometries based on spatial \n    relationships, and accessors extract information about geometries."}, "NFCP": {"categories": ["Finance"], "description": "Commodity pricing models are (systems of) stochastic differential equations that are utilized for the valuation and hedging of commodity contingent claims (i.e. derivative products on the commodity) and other commodity related investments. Commodity pricing models that capture market dynamics are of great importance to commodity market participants in order to exercise sound investment and risk-management strategies. Parameters of commodity pricing models are estimated through maximum likelihood estimation, using available term structure futures data of a commodity. 'NFCP' (n-factor commodity pricing) provides a framework for the modeling, parameter estimation, probabilistic forecasting, option valuation and simulation of commodity prices through state space and Monte Carlo methods, risk-neutral valuation and Kalman filtering. 'NFCP' allows the commodity pricing model to consist of n correlated factors, with both random walk and mean-reverting elements. The n-factor commodity pricing model framework was first presented in the work of Cortazar and Naranjo (2006) <doi:10.1002/fut.20198>. Examples presented in 'NFCP' replicate the two-factor crude oil commodity pricing model presented in the prolific work of Schwartz and Smith (2000) <doi:10.1287/mnsc.46.7.893.12034> with the approximate term structure futures data applied within this study provided in the 'NFCP' package."}, "rminizinc": {"categories": ["Optimization"], "description": "Constraint optimization, or constraint programming, is the name given to identifying\n  feasible solutions out of a very large set of candidates, where the problem can be modeled in terms \n  of arbitrary constraints. 'MiniZinc' is a free and open-source constraint modeling language. \n  Constraint satisfaction and discrete optimization problems can be formulated in a high-level \n  modeling language. Models are compiled into an intermediate representation that is understood by a\n  wide range of solvers. 'MiniZinc' itself provides several solvers, for instance 'GeCode'. R users \n  can use the package to solve constraint programming problems without using 'MiniZinc' directly, \n  modify existing 'MiniZinc' models and also create their own models."}, "ggm": {"categories": ["GraphicalModels"], "description": "Tools for marginalization, conditioning and fitting by maximum likelihood."}, "BatchJobs": {"categories": ["HighPerformanceComputing"], "description": "Provides Map, Reduce and Filter variants to generate jobs on batch\n    computing systems like PBS/Torque, LSF, SLURM and Sun Grid Engine.\n    Multicore and SSH systems are also supported. For further details see the\n    project web page."}, "optimx": {"categories": ["Optimization"], "description": "Provides a replacement and extension of the optim()\n    function to call to several function minimization codes in R in a single\n    statement. These methods handle smooth, possibly box constrained functions \n    of several or many parameters. Note that function 'optimr()' was prepared to\n    simplify the incorporation of minimization codes going forward. Also implements some\n    utility codes and some extra solvers, including safeguarded Newton methods.\n    Many methods previously separate are now included here.\n    This is the version for CRAN."}, "ktsolve": {"categories": ["NumericalMathematics"], "description": "This is designed for use with an arbitrary set of equations with\n an arbitrary set of unknowns.\n The user selects \"fixed\" values for enough unknowns to leave as many variables as\n there are equations, which in most cases means the system is properly\n defined and a unique solution exists. The function, the fixed values\n and initial values for the remaining unknowns are fed to a nonlinear backsolver.  \n The original version of \"TK!Solver\" , now a product of Universal Technical Systems (<https://www.uts.com>) was the inspiration for this function."}, "plotmo": {"categories": ["MachineLearning"], "description": "Plot model surfaces for a wide variety of models\n        using partial dependence plots and other techniques.\n        Also plot model residuals and other information on the model."}, "seg": {"categories": ["Spatial"], "description": "Measuring spatial segregation. The methods implemented in this \n        package include White's P index (1983) <doi:10.1086/227768>, \n        Morrill's D(adj) (1991), Wong's D(w) and D(s) (1993) \n        <doi:10.1080/00420989320080551>, and Reardon and O'Sullivan's set of \n        spatial segregation measures (2004) \n        <doi:10.1111/j.0081-1750.2004.00150.x>."}, "highlight": {"categories": ["ReproducibleResearch"], "description": "Syntax highlighter for R code based on the results\n    of the R parser. Rendering in HTML and latex markup. Custom Sweave\n    driver performing syntax highlighting of R code chunks."}, "MicSim": {"categories": ["OfficialStatistics", "Survival"], "description": "This entry-level toolkit allows performing continuous-time microsimulation for a wide range of life science (demography, social sciences, epidemiology) applications. Individual life-courses are specified by a continuous-time multi-state model. "}, "openxlsx": {"categories": ["ReproducibleResearch"], "description": "Simplifies the creation of Excel .xlsx files by providing a\n    high level interface to writing, styling and editing worksheets.\n    Through the use of 'Rcpp', read/write times are comparable to the\n    'xlsx' and 'XLConnect' packages with the added benefit of removing the\n    dependency on Java."}, "fabletools": {"categories": ["TimeSeries"], "description": "Provides tools, helpers and data structures for\n    developing models and time series functions for 'fable' and extension\n    packages. These tools support a consistent and tidy interface for time\n    series modelling and analysis."}, "simglm": {"categories": ["ClinicalTrials", "MissingData"], "description": "Simulates regression models,\n    including both simple regression and generalized linear mixed\n    models with up to three level of nesting. Power simulations that are\n    flexible allowing the specification of missing data, unbalanced designs,\n    and different random error distributions are built into the package."}, "bsam": {"categories": ["SpatioTemporal", "Tracking"], "description": "Tools to fit Bayesian state-space models to animal tracking data. Models are provided for location \n    filtering, location filtering and behavioural state estimation, and their hierarchical versions. \n    The models are primarily intended for fitting to ARGOS satellite tracking data but options exist to fit \n    to other tracking data types. For Global Positioning System data, consider the 'moveHMM' package. \n    Simplified Markov Chain Monte Carlo convergence diagnostic plotting is provided but users are encouraged \n    to explore tools available in packages such as 'coda' and 'boa'."}, "micEconCES": {"categories": ["Econometrics"], "description": "Tools for econometric analysis and economic modelling\n   with the traditional two-input Constant Elasticity of Substitution (CES) function\n   and with nested CES functions with three and four inputs.\n   The econometric estimation can be done by the Kmenta approximation,\n   or non-linear least-squares\n   using various gradient-based or global optimisation algorithms.\n   Some of these algorithms can constrain the parameters to certain ranges,\n   e.g. economically meaningful values.\n   Furthermore, the non-linear least-squares estimation\n   can be combined with a grid-search for the rho-parameter(s)."}, "regtools": {"categories": ["TeachingStatistics"], "description": "Tools for linear, nonlinear and nonparametric regression\n             and classification.  Novel graphical methods for assessment \n             of parametric models using nonparametric methods. One \n             vs. All and All vs. All multiclass classification, optional\n             class probabilities adjustment.  Nonparametric regression \n             (k-NN) for general dimension, local-linear option.  Nonlinear \n             regression with Eickert-White method for dealing with \n             heteroscedasticity.  Utilities for converting time series\n             to rectangular form.  Utilities for conversion between\n             factors and indicator variables.  Some code related to\n             \"Statistical Regression and Classification: from Linear\n             Models to Machine Learning\", N. Matloff, 2017, CRC,\n             ISBN 9781498710916."}, "dfpk": {"categories": ["Bayesian", "ClinicalTrials", "ExperimentalDesign", "Pharmacokinetics"], "description": "Statistical methods involving PK measures are provided, in the dose allocation process during a Phase I clinical trials. These methods, proposed by Ursino et al, (2017) <doi:10.1002/bimj.201600084>, enter pharmacokinetics (PK) in the dose finding designs in different ways, including covariates models, dependent variable or hierarchical models. This package provides functions to generate data from several scenarios and functions to run simulations which their objective is to determine the maximum tolerated dose (MTD)."}, "metacom": {"categories": ["Environmetrics"], "description": "Functions to analyze coherence, boundary clumping, and turnover\n    following the pattern-based metacommunity analysis of Leibold and Mikkelson\n    2002  <doi:10.1034/j.1600-0706.2002.970210.x>. The package also includes \n\t\tfunctions to visualize ecological networks, and to calculate modularity \n\t\tas a replacement to boundary clumping."}, "glmnet": {"categories": ["MachineLearning", "Survival"], "description": "Extremely efficient procedures for fitting the entire lasso or elastic-net regularization path for linear regression, logistic and multinomial regression models, Poisson regression, Cox model,  multiple-response Gaussian, and the grouped multinomial regression. There are two new and important additions. The family argument can be a GLM family object, which opens the door to any programmed family. This comes with a modest computational cost, so when the built-in families suffice, they should be used instead.  The other novelty is the relax option, which refits each of the active sets in the path unpenalized. The algorithm uses cyclical coordinate descent in a path-wise fashion, as described in the papers listed in the URL below."}, "metaSurvival": {"categories": ["MetaAnalysis"], "description": "To assess a summary survival curve from survival probabilities and number of at-risk patients collected at various points in time in various studies, and to test the between-strata heterogeneity."}, "x13binary": {"categories": ["TimeSeries"], "description": "The US Census Bureau provides a seasonal adjustment program now\n called 'X-13ARIMA-SEATS' building on both earlier programs called X-11 and\n X-12 as well as the SEATS program by the Bank of Spain. The US Census Bureau\n offers both source and binary versions \u2013 which this package integrates for\n use by other R packages."}, "flexsurv": {"categories": ["Distributions", "Survival"], "description": "Flexible parametric models for time-to-event data,\n    including the Royston-Parmar spline model, generalized gamma and\n    generalized F distributions.  Any user-defined parametric\n    distribution can be fitted, given at least an R function defining\n    the probability density or hazard. There are also tools for\n    fitting and predicting from fully parametric multi-state models,\n    based on either cause-specific hazards or mixture models."}, "VineCopula": {"categories": ["Distributions"], "description": "Provides tools for the statistical analysis of regular vine copula \n    models, see Aas et al. (2009) <doi:10.1016/j.insmatheco.2007.02.001> and \n    Dissman et al. (2013) <doi:10.1016/j.csda.2012.08.010>.\n    The package includes tools for parameter estimation, model selection,\n    simulation, goodness-of-fit tests, and visualization. Tools for estimation,\n    selection and exploratory data analysis of bivariate copula models are also\n    provided."}, "locfit": {"categories": ["Survival"], "description": "Local regression, likelihood and density estimation methods as described in the 1999 book by Loader."}, "m2b": {"categories": ["Tracking"], "description": "Prediction of behaviour from movement \n\tcharacteristics using observation and random forest for the analyses of movement\n\tdata in ecology.\n\tFrom movement information (speed, bearing...) the model predicts the\n\tobserved behaviour (movement, foraging...) using random forest. The\n\tmodel can then extrapolate behavioural information to movement data\n\twithout direct observation of behaviours.\n\tThe specificity of this method relies on the derivation of multiple predictor variables from the\n\tmovement data over a range of temporal windows. This procedure allows to capture\n\tas much information as possible on the changes and variations of movement and\n\tensures the use of the random forest algorithm to its best capacity. The method\n\tis very generic, applicable to any set of data providing movement data together with\n\tobservation of behaviour."}, "freegroup": {"categories": ["NumericalMathematics"], "description": "The free group in R; juxtaposition is represented by a plus.  Includes inversion, multiplication by a scalar, group-theoretic power operation, and Tietze forms.  "}, "BAS": {"categories": ["Bayesian"], "description": "Package for Bayesian Variable Selection and  Model Averaging \n    in linear models and generalized linear models using stochastic or\n    deterministic sampling without replacement from posterior\n    distributions.  Prior distributions on coefficients are\n    from Zellner's g-prior or mixtures of g-priors\n    corresponding to the Zellner-Siow Cauchy Priors or the\n    mixture of g-priors from Liang et al (2008)\n    <doi:10.1198/016214507000001337>\n    for linear models or mixtures of g-priors from  Li and Clyde\n    (2019) <doi:10.1080/01621459.2018.1469992> in generalized linear models.\n    Other model selection criteria include AIC, BIC and Empirical Bayes \n    estimates of g. Sampling probabilities may be updated based on the sampled\n    models using sampling w/out replacement or an efficient MCMC algorithm which\n    samples models using a tree structure of the model space \n    as an efficient hash table.  See  Clyde, Ghosh and Littman (2010) \n    <doi:10.1198/jcgs.2010.09049> for  details on the sampling algorithms.\n    Uniform priors over all models or beta-binomial prior distributions on\n    model size are allowed, and for large p truncated priors on the model\n    space may be used to enforce sampling models that are full rank.  \n    The user may force variables to always be included in addition to imposing\n    constraints that higher order interactions are included only if their \n    parents are included in the model.\n    This material is based upon work supported by the National Science\n    Foundation under Division of Mathematical Sciences grant 1106891.\n    Any opinions, findings, and\n    conclusions or recommendations expressed in this material are those of\n    the author(s) and do not necessarily reflect the views of the\n    National Science Foundation."}, "EffectLiteR": {"categories": ["CausalInference"], "description": "Use structural equation modeling to estimate average and\n    conditional effects of a treatment variable on an outcome variable, taking into\n    account multiple continuous and categorical covariates."}, "jrt": {"categories": ["Psychometrics"], "description": "Psychometric analysis and scoring of judgment data using polytomous Item-Response Theory (IRT) models, as described in Myszkowski and Storme (2019) <doi:10.1037/aca0000225> and Myszkowski (2021) <doi:10.1037/aca0000287>. A function is used to automatically compare and select models, as well as to present a variety of model-based statistics. Plotting functions are used to present category curves, as well as information, reliability and standard error functions."}, "DirectEffects": {"categories": ["CausalInference"], "description": "A set of functions to estimate the controlled direct effect of treatment fixing a potential mediator to a specific value. Implements the sequential g-estimation estimator described in Vansteelandt (2009) <doi:10.1097/EDE.0b013e3181b6f4c9> and Acharya, Blackwell, and Sen (2016) <doi:10.1017/S0003055416000216>."}, "rnaturalearth": {"categories": ["Spatial"], "description": "Facilitates mapping by making natural earth map data from <http://www.naturalearthdata.com/> more easily available to R users."}, "mapmisc": {"categories": ["Spatial"], "description": "A minimal, light-weight set of tools for producing nice looking maps in R, with support for map projections."}, "PwrGSD": {"categories": ["ExperimentalDesign"], "description": "Tools for the evaluation of interim analysis plans for sequentially\n monitored trials on a survival endpoint; tools to construct efficacy and \n futility boundaries, for deriving power of a sequential design at a specified\n alternative, template for evaluating the performance of candidate plans at a \n set of time varying alternatives. See Izmirlian, G. (2014) <doi:10.4310/SII.2014.v7.n1.a4>."}, "C50": {"categories": ["MachineLearning"], "description": "C5.0 decision trees and rule-based models for pattern recognition that extend the work of Quinlan (1993, ISBN:1-55860-238-0)."}, "stratification": {"categories": ["OfficialStatistics"], "description": "Univariate stratification of survey populations with a generalization of the \n  Lavallee-Hidiroglou method of stratum construction. The generalized method takes into account \n  a discrepancy between the stratification variable and the survey variable. The determination \n  of the optimal boundaries also incorporate, if desired, an anticipated non-response, a take-all \n  stratum for large units, a take-none stratum for small units, and a certainty stratum to ensure \n  that some specific units are in the sample. The well known cumulative root frequency rule of \n  Dalenius and Hodges and the geometric rule of Gunning and Horgan are also implemented. "}, "coin": {"categories": ["ClinicalTrials", "Survival"], "description": "Conditional inference procedures for the general independence\n             problem including two-sample, K-sample (non-parametric ANOVA),\n             correlation, censored, ordered and multivariate problems described\n             in <doi:10.18637/jss.v028.i08>."}, "complmrob": {"categories": ["Robust"], "description": "Robust regression methods for compositional data.\n    The distribution of the estimates can be approximated with various bootstrap\n    methods. These bootstrap methods are available for the compositional as well\n    as for standard robust regression estimates. This allows for direct\n    comparison between them."}, "rflexscan": {"categories": ["Spatial"], "description": "Functions for the detection of spatial clusters using the flexible\n    spatial scan statistic developed by Tango and Takahashi (2005) <doi:10.1186/1476-072X-4-11>.\n    This package implements a wrapper for the C routine used in the FleXScan 3.1.2 \n    <https://sites.google.com/site/flexscansoftware/home> developed by Takahashi,\n    Yokoyama, and Tango. For details, see Otani et al. (2021) <doi:10.18637/jss.v099.i13>."}, "setRNG": {"categories": ["Distributions"], "description": "Provides utilities to help set and record the setting of\n\tthe seed and the uniform and normal generators used when a random\n\texperiment is run. The utilities can be used in other functions \n\tthat do random experiments to simplify recording and/or setting all the \n\tnecessary information for reproducibility. \n\tSee the vignette and reference manual for examples."}, "causaldrf": {"categories": ["CausalInference"], "description": "Functions and data to estimate causal dose response functions given continuous, ordinal, or binary treatments."}, "greta": {"categories": ["Bayesian"], "description": "Write statistical models in R and fit them by MCMC and\n    optimisation on CPUs and GPUs, using Google 'TensorFlow'.  greta lets\n    you write your own model like in BUGS, JAGS and Stan, except that you\n    write models right in R, it scales well to massive datasets, and it\u2019s\n    easy to extend and build on.  See the website for more information,\n    including tutorials, examples, package documentation, and the greta\n    forum."}, "reReg": {"categories": ["Survival"], "description": "A comprehensive collection of practical and easy-to-use tools for regression analysis of recurrent events, with or without the presence of a (possibly) informative terminal event. The modeling framework is based on a joint frailty scale-change model, that includes models described in Wang et al. (2001) <doi:10.1198/016214501753209031>, Huang and Wang (2004) <doi:10.1198/016214504000001033>, Xu et al. (2017) <doi:10.1080/01621459.2016.1173557>, and Xu et al. (2019) <doi:10.5705/SS.202018.0224> as special cases. The implemented estimating procedure does not require any parametric assumption on the frailty distribution. The package also allows the users to specify different model forms for both the recurrent event process and the terminal event. "}, "expint": {"categories": ["NumericalMathematics"], "description": "The exponential integrals E_1(x), E_2(x), E_n(x) and\n  Ei(x), and the incomplete gamma function G(a, x) defined for\n  negative values of its first argument. The package also gives easy\n  access to the underlying C routines through an API; see the package\n  vignette for details. A test package included in sub-directory\n  example_API provides an implementation. C routines derived from the\n  GNU Scientific Library <https://www.gnu.org/software/gsl/>."}, "bayesammi": {"categories": ["Bayesian"], "description": "Performs Bayesian estimation of the additive main effects and multiplicative interaction (AMMI) model. The method is explained in Crossa, J., Perez-Elizalde, S., Jarquin, D., Cotes, J.M., Viele, K., Liu, G. and Cornelius, P.L. (2011) (<doi:10.2135/cropsci2010.06.0343>)."}, "prodlim": {"categories": ["Survival"], "description": "Fast and user friendly implementation of nonparametric estimators\n    for censored event history (survival) analysis. Kaplan-Meier and\n    Aalen-Johansen method."}, "ncappc": {"categories": ["Pharmacokinetics"], "description": "A flexible tool that can perform\n    (i) traditional non-compartmental analysis (NCA) and\n    (ii) Simulation-based posterior predictive checks for population\n    pharmacokinetic (PK) and/or pharmacodynamic (PKPD) models using NCA metrics. "}, "rprintf": {"categories": ["ReproducibleResearch"], "description": "Provides a set of functions to facilitate building formatted strings\n    under various replacement rules: C-style formatting, variable-based formatting,\n    and number-based formatting. C-style formatting is basically identical to built-in\n    function 'sprintf'. Variable-based formatting allows users to put variable names\n    in a formatted string which will be replaced by variable values. Number-based\n    formatting allows users to use index numbers to represent the corresponding\n    argument value to appear in the string."}, "logOfGamma": {"categories": ["NumericalMathematics"], "description": "Uses approximations to compute the natural logarithm of the Gamma\n    function for large values."}, "qmap": {"categories": ["Distributions", "Hydrology"], "description": "Empirical adjustment of the distribution of variables originating from (regional) climate model simulations using quantile mapping."}, "midasr": {"categories": ["Econometrics"], "description": "Methods and tools for mixed frequency time series data analysis.\n    Allows estimation, model selection and forecasting for MIDAS regressions."}, "AzureVision": {"categories": ["WebTechnologies"], "description": "An interface to 'Azure Computer Vision' <https://docs.microsoft.com/azure/cognitive-services/Computer-vision/Home> and 'Azure Custom Vision' <https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/home>, building on the low-level functionality provided by the 'AzureCognitive' package. These services allow users to leverage the cloud to carry out visual recognition tasks using advanced image processing models, without needing powerful hardware of their own. Part of the 'AzureR' family of packages."}, "measurements": {"categories": ["ChemPhys"], "description": "Collection of tools to make working with physical measurements\n\t\teasier. Convert between metric and imperial units, or calculate a dimension's\n\t\tunknown value from other dimensions' measurements."}, "mind": {"categories": ["OfficialStatistics"], "description": "Allows users to produce estimates and MSE for multivariate variables using Linear Mixed Model. The package follows the approach of Datta, Day and Basawa (1999) <doi:10.1016/S0378-3758(98)00147-5>."}, "feisr": {"categories": ["Econometrics"], "description": "Provides the function feis() to estimate fixed effects individual \n    slope (FEIS) models. The FEIS model constitutes a more general version of \n    the often-used fixed effects (FE) panel model, as implemented in the \n    package 'plm' by Croissant and Millo (2008) <doi:10.18637/jss.v027.i02>. \n    In FEIS models, data are not only person demeaned like in conventional \n    FE models, but detrended by the predicted individual slope of each \n    person or group. Estimation is performed by applying least squares lm() \n    to the transformed data. For more details on FEIS models see Bruederl and \n    Ludwig (2015, ISBN:1446252442); Frees (2001) <doi:10.2307/3316008>; \n    Polachek and Kim (1994) <doi:10.1016/0304-4076(94)90075-2>; \n\tRuettenauer and Ludwig (2020) <doi:10.1177/0049124120926211>;\n    Wooldridge (2010, ISBN:0262294354). To test consistency of conventional FE \n    and random effects estimators against heterogeneous slopes, the package \n    also provides the functions feistest() for an artificial regression test \n    and bsfeistest() for a bootstrapped version of the Hausman test."}, "metaBMA": {"categories": ["MetaAnalysis"], "description": "Computes the posterior model probabilities for standard meta-analysis models \n    (null model vs. alternative model assuming either fixed- or random-effects, respectively).\n    These posterior probabilities are used to estimate the overall mean effect size \n    as the weighted average of the mean effect size estimates of the random- and \n    fixed-effect model as proposed by Gronau, Van Erp, Heck, Cesario, Jonas, & \n    Wagenmakers (2017, <doi:10.1080/23743603.2017.1326760>). The user can define \n    a wide range of non-informative or informative priors for the mean effect size \n    and the heterogeneity coefficient. Moreover, using pre-compiled Stan models, \n    meta-analysis with continuous and discrete moderators with Jeffreys-Zellner-Siow (JZS) \n    priors can be fitted and tested. This allows to compute Bayes factors and \n    perform Bayesian model averaging across random- and fixed-effects meta-analysis \n    with and without moderators. For a primer on Bayesian model-averaged meta-analysis, \n    see Gronau, Heck, Berkhout, Haaf, & Wagenmakers (2020, <doi:10.31234/osf.io/97qup>)."}, "rgrass7": {"categories": ["Spatial"], "description": "Interpreted interface between 'GRASS' geographical \n    information system and R, based on starting R from within the 'GRASS' 'GIS'\n    environment, or running free-standing R in a temporary 'GRASS' location;\n    the package provides facilities for using all 'GRASS' commands from the \n    R command line. This package may not be used for 'GRASS' 6, for which\n    'spgrass6' should be used."}, "Synth": {"categories": ["CausalInference"], "description": "Implements the synthetic control group method for comparative case studies as described in Abadie and Gardeazabal (2003) and Abadie, Diamond, and Hainmueller (2010, 2011, 2014). The synthetic control method allows for effect estimation in settings where a single unit (a state, country, firm, etc.) is exposed to an event or intervention. It provides a data-driven procedure to construct synthetic control units based on a weighted combination of comparison units that approximates the characteristics of the unit that is exposed to the intervention. A combination of comparison units often provides a better comparison for the unit exposed to the intervention than any comparison unit alone."}, "lifecontingencies": {"categories": ["Finance"], "description": "Classes and methods that allow the user to manage life table,\n    actuarial tables (also multiple decrements tables). Moreover, functions to easily\n    perform demographic, financial and actuarial mathematics on life contingencies\n    insurances calculations are contained therein. See Spedicato (2013)\t<doi:10.18637/jss.v055.i10>."}, "sirt": {"categories": ["Psychometrics"], "description": "\n    Supplementary functions for item response models aiming\n    to complement existing R packages. The functionality includes among others\n    multidimensional compensatory and noncompensatory IRT models\n    (Reckase, 2009, <doi:10.1007/978-0-387-89976-3>), \n    MCMC for hierarchical IRT models and testlet models\n    (Fox, 2010, <doi:10.1007/978-1-4419-0742-4>), \n    NOHARM (McDonald, 1982, <doi:10.1177/014662168200600402>), \n    Rasch copula model (Braeken, 2011, <doi:10.1007/s11336-010-9190-4>;\n    Schroeders, Robitzsch & Schipolowski, 2014, <doi:10.1111/jedm.12054>),\n    faceted and hierarchical rater models (DeCarlo, Kim & Johnson, 2011,\n    <doi:10.1111/j.1745-3984.2011.00143.x>),\n    ordinal IRT model (ISOP; Scheiblechner, 1995, <doi:10.1007/BF02301417>), \n    DETECT statistic (Stout, Habing, Douglas & Kim, 1996, \n    <doi:10.1177/014662169602000403>), local structural equation modeling \n    (LSEM; Hildebrandt, Luedtke, Robitzsch, Sommer & Wilhelm, 2016,\n    <doi:10.1080/00273171.2016.1142856>)."}, "ClustImpute": {"categories": ["MissingData"], "description": "This k-means algorithm is able to cluster data with missing values and as a by-product completes the data set. The implementation can deal with missing values in multiple variables and is computationally efficient since it iteratively uses the current cluster assignment to define a plausible distribution for missing value imputation. Weights are used to shrink early random draws for missing values (i.e., draws based on the cluster assignments after few iterations) towards the global mean of each feature. This shrinkage slowly fades out after a fixed number of iterations to reflect the increasing credibility of cluster assignments. See the vignette for details."}, "MEPDF": {"categories": ["Distributions"], "description": "Based on the input data an n-dimensional cube with sub cells of user specified side length is created.\n    The number of sample points which fall in each sub cube is counted, and with the cell volume and overall sample\n    size an empirical probability can be computed. A number of cubes of higher resolution can be superimposed. The\n    basic method stems from J.L. Bentley in \"Multidimensional Divide and Conquer\".\n    J. L. Bentley (1980) <doi:10.1145/358841.358850>.\n    Furthermore a simple kernel density estimation method is made available, as well as an expansion of Bentleys\n    method, which offers a kernel approach for the grid method."}, "RMySQL": {"categories": ["Databases"], "description": "Legacy 'DBI' interface to 'MySQL' / 'MariaDB' based on old code\n    ported from S-PLUS. A modern 'MySQL' client based on 'Rcpp' is available \n    from the 'RMariaDB' package."}, "mc2d": {"categories": ["Distributions"], "description": "A complete framework to build and study Two-Dimensional Monte-Carlo simulations, aka Second-Order Monte-Carlo simulations. Also includes various distributions (pert, triangular, Bernoulli, empirical discrete and continuous)."}, "rwunderground": {"categories": ["Hydrology"], "description": "Tools for getting historical weather information and forecasts \n    from wunderground.com. Historical weather and forecast data includes, but \n    is not limited to, temperature, humidity, windchill, wind speed, dew point, \n    heat index. Additionally, the weather underground weather API also includes \n    information on sunrise/sunset, tidal conditions, satellite/webcam imagery, \n    weather alerts, hurricane alerts and historical high/low temperatures."}, "metR": {"categories": ["Hydrology"], "description": "Many useful functions and extensions for dealing\n    with meteorological data in the tidy data framework. Extends 'ggplot2'\n    for better plotting of scalar and vector fields and provides commonly\n    used analysis methods in the atmospheric sciences."}, "fImport": {"categories": ["Finance"], "description": "Provides a collection of utility functions \n    to download and manage data sets from the Internet or from other \n    sources."}, "eechidna": {"categories": ["MissingData"], "description": "Data from the seven Australian Federal Elections (House of\n    Representatives) between 2001 and 2019, and from the four Australian\n    Censuses over the same period. Includes tools for visualizing and\n    analysing the data, as well as imputing Census data for years in\n    which a Census does not occur. This package incorporates\n    data that is copyright Commonwealth of Australia (Australian\n    Electoral Commission and Australian Bureau of Statistics) 2019."}, "soc.ca": {"categories": ["Psychometrics"], "description": "Specific and class specific multiple correspondence analysis on\n    survey-like data. Soc.ca is optimized to the needs of the social scientist and\n    presents easily interpretable results in near publication ready quality."}, "ineq": {"categories": ["Econometrics", "OfficialStatistics"], "description": "Inequality, concentration, and poverty measures. Lorenz curves (empirical and theoretical)."}, "CausalQueries": {"categories": ["CausalInference"], "description": "Users can declare binary causal models, update beliefs about causal types given data and calculate arbitrary estimands.  Model definition makes use of 'dagitty' functionality. Updating is implemented in 'stan'. The approach used in 'CausalQueries' is a generalization of the 'biqq' models described in \"Mixing Methods: A Bayesian Approach\" (Humphreys and Jacobs, 2015, <doi:10.1017/S0003055415000453>). The conceptual extension makes use of work on probabilistic causal models described in Pearl's Causality (Pearl, 2009, <doi:10.1017/CBO9780511803161>)."}, "sitmo": {"categories": ["HighPerformanceComputing"], "description": "Provided within are two high quality and fast PPRNGs that may be\n    used in an 'OpenMP' parallel environment. In addition, there is a generator\n    for one dimensional low-discrepancy sequence. The objective of this library\n    to consolidate the distribution of the 'sitmo' (C++98 & C++11), 'threefry' and\n    'vandercorput' (C++11-only) engines on CRAN by enabling others to link to the\n    header files inside of 'sitmo' instead of including a copy of each engine\n    within their individual package. Lastly, the package contains example\n    implementations using the 'sitmo' package and three accompanying vignette that\n    provide additional information."}, "tsDyn": {"categories": ["Econometrics", "Finance", "TimeSeries"], "description": "Implements nonlinear autoregressive (AR) time series models. For univariate series, a non-parametric approach is available through additive nonlinear AR. Parametric modeling and testing for regime switching dynamics is available when the transition is either direct (TAR: threshold AR) or smooth (STAR: smooth transition AR, LSTAR). For multivariate series, one can estimate a range of TVAR or threshold cointegration TVECM models with two or three regimes. Tests can be conducted for TVAR as well as for TVECM (Hansen and Seo 2002 and Seo 2006). "}, "mrgsolve": {"categories": ["DifferentialEquations", "Pharmacokinetics"], "description": "Fast simulation from ordinary differential equation\n    (ODE) based models typically employed in quantitative pharmacology and\n    systems biology."}, "plsRcox": {"categories": ["Survival"], "description": "Provides Partial least squares Regression and various regular, sparse or kernel, techniques for fitting Cox models in high dimensional settings <doi:10.1093/bioinformatics/btu660>, Bastien, P., Bertrand, F., Meyer N., Maumy-Bertrand, M. (2015), Deviance residuals-based sparse PLS and sparse kernel PLS regression for censored data, Bioinformatics, 31(3):397-404. Cross validation criteria were studied in <arXiv:1810.02962>, Bertrand, F., Bastien, Ph. and Maumy-Bertrand, M. (2018), Cross validating extensions of kernel, sparse or regular partial least squares regression models to censored data."}, "astrodatR": {"categories": ["ChemPhys"], "description": "A collection of 19 datasets from contemporary astronomical research.  They are described the textbook \u2018Modern Statistical Methods for Astronomy with R Applications\u2019 by Eric D. Feigelson and G. Jogesh Babu (Cambridge University Press, 2012, Appendix C) or on the website of Penn State's Center for Astrostatistics (http://astrostatistics.psu.edu/datasets).  These datasets can be used to exercise methodology involving: density estimation; heteroscedastic measurement errors; contingency tables; two-sample hypothesis tests; spatial point processes; nonlinear regression; mixture models; censoring and truncation; multivariate analysis; classification and clustering; inhomogeneous Poisson processes; periodic and stochastic time series analysis.  "}, "RestRserve": {"categories": ["ModelDeployment"], "description": "\n  Allows to easily create high-performance full featured HTTP APIs from R\n  functions. Provides high-level classes such as 'Request', 'Response',\n  'Application', 'Middleware' in order to streamline server side\n  application development. Out of the box allows to serve requests using\n  'Rserve' package, but flexible enough to integrate with other HTTP servers\n  such as 'httpuv'."}, "GGIR": {"categories": ["Tracking"], "description": "A tool to process and analyse data collected with wearable raw acceleration sensors as described in Migueles and colleagues (JMPB 2019), and van Hees and colleagues (JApplPhysiol 2014; PLoSONE 2015). The package has been developed and tested for binary data from 'GENEActiv' <https://www.activinsights.com/> and GENEA devices (not for sale), .csv-export data from  'Actigraph' <https://actigraphcorp.com> devices, and .cwa and .wav-format data from 'Axivity' <https://axivity.com>. These devices are currently widely used in research on human daily physical activity. Further, the package can handle accelerometer data file from any other sensor brand providing that the data is stored in csv format and has either no header or a two column header. Also the package allows for external function embedding."}, "MaxPro": {"categories": ["ExperimentalDesign"], "description": "Generate maximum projection (MaxPro) designs for quantitative and/or qualitative factors. Details of the MaxPro criterion can be found in: (1) Joseph, Gul, and Ba. (2015) \"Maximum Projection Designs for Computer Experiments\", Biometrika, 102, 371-380, and (2) Joseph, Gul, and Ba. (2018) \"Designing Computer Experiments with Multiple Types of Factors: The MaxPro Approach\", Journal of Quality Technology, to appear."}, "OpenMx": {"categories": ["MissingData", "Psychometrics"], "description": "Create structural equation models that can be manipulated programmatically.\n    Models may be specified with matrices or paths (LISREL or RAM)\n    Example models include confirmatory factor, multiple group, mixture\n    distribution, categorical threshold, modern test theory, differential\n    Fit functions include full information maximum likelihood, maximum likelihood, and weighted least squares.\n    equations, state space, and many others.\n\tSupport and advanced package binaries available at <http://openmx.ssri.psu.edu>.\n    The software is described in Neale, Hunter, Pritikin, Zahery, Brick,\n    Kirkpatrick, Estabrook, Bates, Maes, & Boker (2016) <doi:10.1007/s11336-014-9435-8>."}, "PRIMME": {"categories": ["NumericalMathematics"], "description": "\n    R interface to 'PRIMME' <https://www.cs.wm.edu/~andreas/software/>, a C library for computing a few\n    eigenvalues and their corresponding eigenvectors of a real symmetric or complex\n    Hermitian matrix, or generalized Hermitian eigenproblem.  It can also compute\n    singular values and vectors of a square or rectangular matrix. 'PRIMME' finds\n    largest, smallest, or interior singular/eigenvalues and can use preconditioning\n    to accelerate convergence. General description of the methods are provided in the papers\n    Stathopoulos (2010, <doi:10.1145/1731022.1731031>) and Wu (2017, <doi:10.1137/16M1082214>).\n    See 'citation(\"PRIMME\")' for details."}, "BMAmevt": {"categories": ["ExtremeValue"], "description": "Toolkit for Bayesian estimation of the dependence structure in multivariate extreme value parametric models, following Sabourin and Naveau (2014) <doi:10.1016/j.csda.2013.04.021> and Sabourin, Naveau and Fougeres (2013) <doi:10.1007/s10687-012-0163-0>."}, "MLEcens": {"categories": ["Survival"], "description": "We provide functions to compute the nonparametric \n  maximum likelihood estimator (MLE) for \n  the bivariate distribution of (X,Y), when \n  realizations of (X,Y) cannot be observed directly. \n  To be more precise, we consider the situation \n  where we observe a set of rectangles in R^2 that are known \n  to contain the unobservable realizations of (X,Y). We\n  compute the MLE based on such a set of rectangles. \n  The methods can also be used for univariate censored data (see data set\n  'cosmesis'), and for \n  censored data with competing risks (see data set 'menopause'). \n  We also provide functions to visualize the observed data and the MLE. "}, "STMedianPolish": {"categories": ["SpatioTemporal"], "description": "Analyses spatio-temporal data, decomposing data in n-dimensional arrays and using the median polish technique."}, "PenCoxFrail": {"categories": ["Survival"], "description": "A regularization approach for Cox Frailty Models by penalization methods is provided."}, "AdMit": {"categories": ["Bayesian", "Cluster", "Distributions"], "description": "Provides functions to perform the fitting of an adaptive mixture\n    of Student-t distributions to a target density through its kernel function as described in\n    Ardia et al. (2009) <doi:10.18637/jss.v029.i03>. The\n    mixture approximation can then be used as the importance density in importance\n    sampling or as the candidate density in the Metropolis-Hastings algorithm to\n    obtain quantities of interest for the target density itself. "}, "snapshot": {"categories": ["ChemPhys"], "description": "Functions for reading and writing Gadget N-body snapshots. The Gadget code is popular in astronomy for running N-body / hydrodynamical cosmological and merger simulations. To find out more about Gadget see the main distribution page at www.mpa-garching.mpg.de/gadget/"}, "partykit": {"categories": ["MachineLearning"], "description": "A toolkit with infrastructure for representing, summarizing, and\n  visualizing tree-structured regression and classification models. This\n  unified infrastructure can be used for reading/coercing tree models from\n  different sources ('rpart', 'RWeka', 'PMML') yielding objects that share\n  functionality for print()/plot()/predict() methods. Furthermore, new and improved\n  reimplementations of conditional inference trees (ctree()) and model-based\n  recursive partitioning (mob()) from the 'party' package are provided based\n  on the new infrastructure. A description of this package was published\n  by Hothorn and Zeileis (2015) <https://jmlr.org/papers/v16/hothorn15a.html>."}, "CSGo": {"categories": ["SportsAnalytics"], "description": "An implementation of calls designed to collect and organize in an easy way the data from the Steam API specifically for the Counter-Strike Global Offensive Game (CS Go) <https://developer.valvesoftware.com/wiki/Steam_Web_API>."}, "FFdownload": {"categories": ["Finance"], "description": "Downloads all the datasets (you can exclude the daily ones or specify a list of those you are targeting specifically) from Kenneth French's Website at <https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html>, process them and convert them to list of 'xts' (time series)."}, "e1071": {"categories": ["Cluster", "Distributions", "Environmetrics", "MachineLearning", "Psychometrics"], "description": "Functions for latent class analysis, short time Fourier\n\t     transform, fuzzy clustering, support vector machines,\n\t     shortest path computation, bagged clustering, naive Bayes\n\t     classifier, generalized k-nearest neighbour ..."}, "tth": {"categories": ["ReproducibleResearch"], "description": "C source code and R wrappers for the tth/ttm TeX-to-HTML/MathML translators."}, "mixsmsn": {"categories": ["Cluster"], "description": "Functions to fit finite mixture of scale mixture of skew-normal (FM-SMSN) distributions, details in Prates, Lachos and Cabral (2013) <doi:10.18637/jss.v054.i12>, Cabral, Lachos and Prates (2012) <doi:10.1016/j.csda.2011.06.026> and Basso, Lachos, Cabral and Ghosh (2010) <doi:10.1016/j.csda.2009.09.031>."}, "eRm": {"categories": ["MissingData", "Psychometrics"], "description": "Fits Rasch models (RM), linear logistic test models (LLTM), rating scale model (RSM), linear rating scale models (LRSM), partial credit models (PCM), and linear partial credit models (LPCM).  Missing values are allowed in the data matrix.  Additional features are the ML estimation of the person parameters, Andersen's LR-test, item-specific Wald test, Martin-Loef-Test, nonparametric Monte-Carlo Tests, itemfit and personfit statistics including infit and outfit measures, ICC and other plots, automated stepwise item elimination, simulation module for various binary data matrices."}, "DIFtree": {"categories": ["Psychometrics"], "description": "Item focussed recursive partitioning for simultaneous selection of items and variables that induce Differential Item Functioning (DIF) in dichotomous or polytomous items. "}, "koRpus": {"categories": ["NaturalLanguageProcessing"], "description": "A set of tools to analyze texts. Includes, amongst others, functions for\n          automatic language detection, hyphenation, several indices of lexical diversity\n          (e.g., type token ratio, HD-D/vocd-D, MTLD) and readability (e.g., Flesch,\n          SMOG, LIX, Dale-Chall). Basic import functions for language corpora are also\n          provided, to enable frequency analyses (supports Celex and Leipzig Corpora\n          Collection file formats) and measures like tf-idf. Note: For full functionality\n          a local installation of TreeTagger is recommended. It is also recommended to\n          not load this package directly, but by loading one of the available language\n          support packages from the 'l10n' repository\n          <https://undocumeantit.github.io/repos/l10n/>. 'koRpus' also includes a plugin\n          for the R GUI and IDE RKWard, providing graphical dialogs for its basic\n          features. The respective R package 'rkward' cannot be installed directly from a\n          repository, as it is a part of RKWard. To make full use of this feature, please\n          install RKWard from <https://rkward.kde.org> (plugins are detected\n          automatically). Due to some restrictions on CRAN, the full package sources are\n          only available from the project homepage. To ask for help, report bugs, request\n          features, or discuss the development of the package, please subscribe to the\n          koRpus-dev mailing list (<https://korpusml.reaktanz.de>)."}, "pxweb": {"categories": ["OfficialStatistics"], "description": "Generic interface for the PX-Web/PC-Axis API. The\n    PX-Web/PC-Axis API is used by organizations such as Statistics Sweden\n    and Statistics Finland to disseminate data. The R package can interact\n    with all PX-Web/PC-Axis APIs to fetch information about the data\n    hierarchy, extract metadata and extract and parse statistics to R\n    data.frame format. PX-Web is a solution to disseminate PC-Axis data\n    files in dynamic tables on the web.  Since 2013 PX-Web contains an API\n    to disseminate PC-Axis files."}, "TreeBUGS": {"categories": ["Psychometrics"], "description": "User-friendly analysis of hierarchical multinomial processing tree (MPT) \n    models that are often used in cognitive psychology. Implements the latent-trait \n    MPT approach (Klauer, 2010) <doi:10.1007/s11336-009-9141-0> and the beta-MPT \n    approach (Smith & Batchelder, 2010) <doi:10.1016/j.jmp.2009.06.007> to model \n    heterogeneity of participants. MPT models are conveniently specified by an\n    .eqn-file as used by other MPT software and data are provided by a .csv-file \n    or directly in R. Models are either fitted by calling JAGS or by an MPT-tailored \n    Gibbs sampler in C++ (only for nonhierarchical and beta MPT models). Provides \n    tests of heterogeneity and MPT-tailored summaries and plotting functions.\n    A detailed documentation is available in Heck, Arnold, & Arnold (2018) \n    <doi:10.3758/s13428-017-0869-7>."}, "Gifi": {"categories": ["Psychometrics"], "description": "Implements categorical principal component analysis ('PRINCALS'), multiple correspondence analysis ('HOMALS'), monotone regression analysis ('MORALS'). It replaces the 'homals' package. "}, "coda": {"categories": ["Bayesian", "GraphicalModels"], "description": "Provides functions for summarizing and plotting the\n\toutput from Markov Chain Monte Carlo (MCMC) simulations, as\n\twell as diagnostic tests of convergence to the equilibrium\n\tdistribution of the Markov chain."}, "GENMETA": {"categories": ["MetaAnalysis"], "description": "Generalized meta-analysis is a technique for estimating parameters associated with a multiple regression model through meta-analysis of studies which may have information only on partial sets of the regressors. It estimates the effects of each variable while fully adjusting for all other variables that are measured in at least one of the studies. Using algebraic relationships between regression parameters in different dimensions, a set of moment equations is specified for estimating the parameters of a maximal model through information available on sets of parameter estimates from a series of reduced models available from the different studies. The specification of the equations requires a reference dataset to estimate the joint distribution of the covariates. These equations are solved using the generalized method of moments approach, with the optimal weighting of the equations taking into account uncertainty associated with estimates of the parameters of the reduced models. The proposed framework is implemented using iterated reweighted least squares algorithm for fitting generalized linear regression models. For more details about the method, please see pre-print version of the manuscript on generalized meta-analysis by Prosenjit Kundu, Runlong Tang and Nilanjan Chatterjee (2018) <arXiv:1708.03818>."}, "ora": {"categories": ["Databases"], "description": "Easy-to-use functions to explore Oracle databases and import data\n  into R. User interface for the ROracle package."}, "humanFormat": {"categories": ["ReproducibleResearch"], "description": "Format quantities of time or bytes into human-friendly strings."}, "lmForc": {"categories": ["Finance"], "description": "Introduces in-sample, out-of-sample, pseudo out-of-sample, and\n    benchmark linear model forecast tests and a new class for working with \n    forecast data: Forecast."}, "BayesBinMix": {"categories": ["Bayesian"], "description": "Fully Bayesian inference for estimating the number of clusters and related parameters to heterogeneous binary data."}, "irtrees": {"categories": ["Psychometrics"], "description": "Helper functions and example data sets to facilitate the estimation of IRTree models from data with different shape and using different software."}, "pubmed.mineR": {"categories": ["WebTechnologies"], "description": "Text mining of PubMed Abstracts (text and XML) from <https://pubmed.ncbi.nlm.nih.gov/>."}, "occ": {"categories": ["MedicalImaging"], "description": "Generic function for estimating positron emission tomography (PET) neuroreceptor occupancies from the total volumes of distribution of a set of regions of interest. Fittings methods include the simple 'reference region' and 'ordinary least squares' (sometimes known as occupancy plot) methods, as well as the more efficient 'restricted maximum likelihood estimation'."}, "PTAk": {"categories": ["ChemPhys", "MedicalImaging", "Psychometrics"], "description": "A multiway method to decompose a tensor (array) of any order, as a generalisation of SVD also supporting non-identity metrics and penalisations. 2-way SVD with these extensions is also available. The package includes also some other multiway methods: PCAn (Tucker-n) and PARAFAC/CANDECOMP with these extensions. "}, "gt": {"categories": ["ReproducibleResearch"], "description": "Build display tables from tabular data with an easy-to-use set of\n    functions. With its progressive approach, we can construct display tables\n    with a cohesive set of table parts. Table values can be formatted using any\n    of the included formatting functions. Footnotes and cell styles can be \n    precisely added through a location targeting system. The way in which 'gt'\n    handles things for you means that you don't often have to worry about the\n    fine details."}, "simecol": {"categories": ["ChemPhys", "DifferentialEquations", "Environmetrics"], "description": "An object oriented framework to simulate\n  ecological (and other) dynamic systems. It can be used for\n  differential equations, individual-based (or agent-based) and other\n  models as well. It supports structuring of simulation scenarios (to avoid copy\n  and paste) and aims to improve readability and re-usability of code."}, "MixedTS": {"categories": ["Distributions"], "description": "We provide detailed functions for univariate Mixed Tempered Stable distribution. "}, "AlgDesign": {"categories": ["ExperimentalDesign"], "description": "Algorithmic experimental designs. Calculates exact and\n        approximate theory experimental designs for D,A, and I\n        criteria. Very large designs may be created. Experimental\n        designs may be blocked or blocked designs created from a\n        candidate list, using several criteria.  The blocking can be\n        done when whole and within plot factors interact."}, "VIM": {"categories": ["MissingData", "OfficialStatistics"], "description": "New tools for the visualization of missing and/or imputed values\n    are introduced, which can be used for exploring the data and the structure of\n    the missing and/or imputed values. Depending on this structure of the missing\n    values, the corresponding methods may help to identify the mechanism generating\n    the missing values and allows to explore the data including missing values.\n    In addition, the quality of imputation can be visually explored using various\n    univariate, bivariate, multiple and multivariate plot methods. A graphical user\n    interface available in the separate package VIMGUI allows an easy handling of\n    the implemented plot methods."}, "outbreaker2": {"categories": ["Epidemiology"], "description": "Bayesian reconstruction of disease outbreaks using epidemiological\n    and genetic information. Jombart T, Cori A, Didelot X, Cauchemez S, Fraser\n    C and Ferguson N. 2014. <doi:10.1371/journal.pcbi.1003457>. Campbell, F,\n    Cori A, Ferguson N, Jombart T. 2019. <doi:10.1371/journal.pcbi.1006930>."}, "tgp": {"categories": ["Bayesian", "ExperimentalDesign", "MachineLearning", "Spatial"], "description": "Bayesian nonstationary, semiparametric nonlinear regression \n and design by treed Gaussian processes (GPs) with jumps to the limiting \n linear model (LLM).  Special cases also implemented include Bayesian \n linear models, CART, treed linear models, stationary separable and \n isotropic GPs, and GP single-index models.  Provides 1-d and 2-d plotting functions \n (with projection and slice capabilities) and tree drawing, designed for \n visualization of tgp-class output.  Sensitivity analysis and \n multi-resolution models are supported. Sequential experimental \n design and adaptive sampling functions are also provided, including ALM, \n ALC, and expected improvement.  The latter supports derivative-free\n optimization of noisy black-box functions.  For details and tutorials, \n see Gramacy (2007) <doi:10.18637/jss.v019.i09> and Gramacy & Taddy (2010) \n <doi:10.18637/jss.v033.i06>.  "}, "xts": {"categories": ["Econometrics", "Finance", "MissingData", "SpatioTemporal", "TimeSeries"], "description": "Provide for uniform handling of R's different time-based data classes by extending zoo, maximizing native format information preservation and allowing for user level customization and extension, while simplifying cross-class interoperability."}, "optrcdmaeAT": {"categories": ["ExperimentalDesign"], "description": "Computes A-, MV-, D- and E-optimal or near-optimal row-column designs for two-colour cDNA microarray experiments using the linear fixed effects and mixed effects models where the interest is in a comparison of all pairwise treatment contrasts. The algorithms used in this package are based on the array exchange and treatment exchange algorithms adopted from Debusho, Gemechu and Haines (2016, unpublished) algorithms after adjusting for the row-column designs setup. The package also provides an optional method of using the graphical user interface (GUI) R package tcltk to ensure that it is user friendly."}, "miceRanger": {"categories": ["MissingData"], "description": "Multiple Imputation has been shown to \n  be a flexible method to impute missing values by \n  Van Buuren (2007) <doi:10.1177/0962280206074463>. \n  Expanding on this, random forests have been shown \n  to be an accurate model by Stekhoven and Buhlmann \n  <arXiv:1105.0828> to impute missing values in datasets. \n  They have the added benefits of returning out of bag \n  error and variable importance estimates, as well as \n  being simple to run in parallel."}, "geonapi": {"categories": ["Spatial", "WebTechnologies"], "description": "Provides an R interface to the 'GeoNetwork' API (<https://geonetwork-opensource.org/#api>) allowing to upload and publish metadata in a 'GeoNetwork' web-application and expose it to OGC CSW."}, "trackeRapp": {"categories": ["SpatioTemporal", "SportsAnalytics"], "description": "Provides an integrated user interface and workflow for\n             the analysis of running, cycling and swimming data from GPS-enabled\n             tracking devices through the 'trackeR' <https://CRAN.R-project.org/package=trackeR> R package."}, "PDFEstimator": {"categories": ["Distributions"], "description": "Farmer, J., D. Jacobs (2108) <doi:10.1371/journal.pone.0196937>. A multivariate nonparametric density estimator based on the maximum-entropy method.  Accurately predicts a probability density function (PDF) for random data using a novel iterative scoring function to determine the best fit without overfitting to the sample. "}, "SparseGrid": {"categories": ["NumericalMathematics"], "description": "SparseGrid is a package to create sparse grids for numerical integration, based on code from www.sparse-grids.de"}, "survminer": {"categories": ["Survival"], "description": "Contains the function 'ggsurvplot()' for drawing easily beautiful\n    and 'ready-to-publish' survival curves with the 'number at risk' table\n    and 'censoring count plot'. Other functions are also available to plot \n    adjusted curves for \u2018Cox' model and to visually examine \u2019Cox' model assumptions."}, "mritc": {"categories": ["Cluster", "MedicalImaging"], "description": "Various methods for MRI tissue classification."}, "survexp.fr": {"categories": ["Survival"], "description": "It computes Relative survival, AER and SMR based on French death rates."}, "REndo": {"categories": ["Econometrics"], "description": "Fits linear models with endogenous regressor using latent instrumental variable approaches. \n    The methods included in the package are Lewbel's (1997) <doi:10.2307/2171884> higher moments approach as well as \n    Lewbel's (2012) <doi:10.1080/07350015.2012.643126> heteroscedasticity approach, Park and Gupta's (2012) <doi:10.1287/mksc.1120.0718> joint estimation method \n    that uses Gaussian copula and Kim and Frees's (2007) <doi:10.1007/s11336-007-9008-1> multilevel generalized\n    method of moment approach that deals with endogeneity in a multilevel setting.\n    These are statistical techniques to address the endogeneity problem where no external instrumental variables are needed.\n    Note that with version 2.0.0 sweeping changes were introduced which greatly improve functionality and usability but break backwards compatibility."}, "LOST": {"categories": ["MissingData"], "description": "Functions for simulating missing morphometric\n\tdata randomly, with taxonomic bias and with anatomical bias. LOST also \n\tincludes functions for estimating linear and geometric morphometric data. "}, "surveysd": {"categories": ["OfficialStatistics"], "description": "Calculate point estimates and their standard errors in complex household surveys using bootstrap replicates. Bootstrapping considers survey design with a rotating panel. A comprehensive description of the methodology can be found under <https://statistikat.github.io/surveysd/articles/methodology.html>."}, "rnmamod": {"categories": ["MetaAnalysis"], "description": "A comprehensive suite of functions to perform and visualise \n    pairwise and network meta-analysis with aggregate binary or continuous\n    missing participant outcome data. The package covers core Bayesian one-stage\n    models implemented in a systematic review with multiple interventions, \n    including fixed-effect and random-effects network meta-analysis, \n    meta-regression, evaluation of the consistency assumption via the \n    node-splitting approach and the unrelated mean effects model, and \n    sensitivity analysis. Missing participant outcome data are addressed in all \n    models of the package. The package also offers a rich, user-friendly \n    visualisation toolkit that aids in appraising and interpreting the results \n    thoroughly and preparing the manuscript for journal submission. \n    The visualisation tools comprise the network plot, forest plots, panel of \n    diagnostic plots, heatmaps on the extent of missing participant outcome data\n    in the network, league heatmaps on estimation and prediction, rankograms, \n    Bland-Altman plot, leverage plot, deviance scatterplot, heatmap of \n    robustness, and barplot of Kullback-Leibler divergence. The package also \n    allows the user to export the results to an Excel file at the working \n    directory."}, "gambin": {"categories": ["Distributions"], "description": "Fits unimodal and multimodal gambin distributions to species-abundance distributions\n    from ecological data, as in in Matthews et al. (2014) <doi:10.1111/ecog.00861>. \n    'gambin' is short for 'gamma-binomial'. The main function is fit_abundances(), which estimates \n    the 'alpha' parameter(s) of the gambin distribution using maximum likelihood. Functions are \n    also provided to generate the gambin distribution and for calculating likelihood statistics."}, "warpMix": {"categories": ["FunctionalData"], "description": "Mixed effects modeling with warping for functional data using B-\n    spline. Warping coefficients are considered as random effects, and warping\n    functions are general functions, parameters representing the projection onto B-\n    spline basis of a part of the warping functions. Warped data are modelled by a\n    linear mixed effect functional model, the noise is Gaussian and independent from\n    the warping functions."}, "brew": {"categories": ["ReproducibleResearch"], "description": "Implements a templating framework for mixing text and\n        R code for report generation. brew template syntax is similar\n        to PHP, Ruby's erb module, Java Server Pages, and Python's psp\n        module."}, "stm": {"categories": ["NaturalLanguageProcessing"], "description": "The Structural Topic Model (STM) allows researchers \n  to estimate topic models with document-level covariates. \n  The package also includes tools for model selection, visualization,\n  and estimation of topic-covariate regressions. Methods developed in\n  Roberts et. al. (2014) <doi:10.1111/ajps.12103> and \n  Roberts et. al. (2016) <doi:10.1080/01621459.2016.1141684>. Vignette\n  is Roberts et. al. (2019) <doi:10.18637/jss.v091.i02>."}, "MARSS": {"categories": ["TimeSeries"], "description": "The MARSS package provides maximum-likelihood parameter estimation for constrained and unconstrained linear multivariate autoregressive state-space (MARSS) models fit to multivariate time-series data.  Fitting is primarily via an Expectation-Maximization (EM) algorithm, although fitting via the BFGS algorithm (using the optim function) is also provided.  MARSS models are a class of dynamic linear model (DLM) and vector autoregressive model (VAR) model.  Functions are provided for parametric and innovations bootstrapping, Kalman filtering and smoothing, bootstrap model selection criteria (AICb), confidences intervals via the Hessian approximation and via bootstrapping and calculation of auxiliary residuals for detecting outliers and shocks.  The user guide shows examples of using MARSS for parameter estimation for a variety of applications, model selection, dynamic factor analysis, outlier and shock detection, and addition of covariates.  Online workshops (lectures, eBook, and computer labs) at <https://atsa-es.github.io/>  See the NEWS file for update information."}, "fdaoutlier": {"categories": ["FunctionalData"], "description": "A collection of functions for outlier detection in functional data analysis. \n  Methods implemented include directional outlyingness by \n  Dai and Genton (2019) <doi:10.1016/j.csda.2018.03.017>,\n  MS-plot by Dai and Genton (2018) <doi:10.1080/10618600.2018.1473781>,\n  total variation depth and modified shape similarity index by \n  Huang and Sun (2019) <doi:10.1080/00401706.2019.1574241>, and sequential transformations by\n  Dai et al. (2020) <doi:10.1016/j.csda.2020.106960 among others. Additional outlier detection\n  tools and depths for functional data like functional boxplot, (modified) band depth etc.,\n  are also available. "}, "CRM": {"categories": ["ClinicalTrials"], "description": "Functions for phase I clinical trials using the continual reassessment method."}, "gendist": {"categories": ["Distributions"], "description": "Computes the probability density function (pdf), cumulative distribution function (cdf), quantile function (qf) and generates random values (rg) for the following general models : mixture models, composite models, folded models, skewed symmetric models and arc tan models."}, "riskSimul": {"categories": ["Finance"], "description": "Implements efficient simulation procedures to estimate tail loss probabilities and conditional excess for a stock portfolio. The log-returns are assumed to follow a t-copula model with generalized hyperbolic or t marginals. "}, "nleqslv": {"categories": ["NumericalMathematics"], "description": "Solve a system of nonlinear equations using a Broyden or a Newton method\n             with a choice of global strategies such as line search and trust region.\n             There are options for using a numerical or user supplied Jacobian,\n             for specifying a banded numerical Jacobian and for allowing\n             a singular or ill-conditioned Jacobian."}, "psychomix": {"categories": ["Cluster", "Psychometrics"], "description": "Psychometric mixture models based on 'flexmix' infrastructure. At the moment Rasch mixture models\n  with different parameterizations of the score distribution (saturated vs. mean/variance specification),\n  Bradley-Terry mixture models, and MPT mixture models are implemented. These mixture models can be estimated\n  with or without concomitant variables. See vignette('raschmix', package = 'psychomix') for details on the\n  Rasch mixture models."}, "PublicationBias": {"categories": ["MetaAnalysis"], "description": "Performs sensitivity analysis for publication bias in meta-analyses (per Mathur & VanderWeele, 2020 [<https://osf.io/s9dp6>]).\n    These analyses enable statements such as: \"For publication bias to shift the observed point estimate to the null,\n    'significant' results would need to be at least 30-fold more likely to be published than negative or 'nonsignificant'\n    results.\" Comparable statements can be made regarding shifting to a chosen non-null value or shifting the confidence\n    interval. Provides a worst-case meta-analytic point estimate under maximal publication bias obtained simply by conducting\n    a standard meta-analysis of only the negative and \"nonsignificant\" studies."}, "rStrava": {"categories": ["SportsAnalytics"], "description": "Functions to access data from the 'Strava v3 API' <https://developers.strava.com/>."}, "proffer": {"categories": ["HighPerformanceComputing"], "description": "Like similar profiling tools,\n  the 'proffer' package automatically detects\n  sources of slowness in R code.\n  The distinguishing feature of 'proffer' is its utilization of\n  'pprof', which supplies interactive visualizations\n  that are efficient and easy to interpret.\n  Behind the scenes, the 'profile' package converts\n  native Rprof() data to a protocol buffer\n  that 'pprof' understands.\n  For the documentation of 'proffer',\n  visit <https://r-prof.github.io/proffer/>.\n  To learn about the implementations and methodologies of\n  'pprof', 'profile', and protocol buffers,\n  visit <https://github.com/google/pprof>.\n  <https://developers.google.com/protocol-buffers>,\n  and <https://github.com/r-prof/profile>, respectively."}, "RND": {"categories": ["Finance"], "description": "Extract the implied risk neutral density from options using various methods."}, "surveydata": {"categories": ["OfficialStatistics"], "description": "Data obtained from surveys contains information not only about the\n    survey responses, but also the survey metadata, e.g. the original survey\n    questions and the answer options. The 'surveydata' package makes it easy to\n    keep track of this metadata, and to easily extract columns with\n    specific questions."}, "genSurv": {"categories": ["Survival"], "description": "Generation of survival data with one (binary)\n  time-dependent covariate.  Generation of survival data arising\n  from a progressive illness-death model."}, "otrimle": {"categories": ["Robust"], "description": "Performs robust cluster analysis allowing for outliers and noise that cannot be fitted by any cluster. The data are modelled by a mixture of Gaussian distributions and a noise component, which is an improper uniform  distribution covering the whole Euclidean space. Parameters are estimated by  (pseudo) maximum likelihood. This is fitted by a EM-type algorithm. See Coretto and Hennig (2016) <doi:10.1080/01621459.2015.1100996>, and Coretto and Hennig (2017) <https://jmlr.org/papers/v18/16-382.html>."}, "opencpu": {"categories": ["ModelDeployment", "WebTechnologies"], "description": "A system for embedded scientific computing and reproducible research with R.\n    The OpenCPU server exposes a simple but powerful HTTP api for RPC and data interchange\n    with R. This provides a reliable and scalable foundation for statistical services or \n    building R web applications. The OpenCPU server runs either as a single-user development\n    server within the interactive R session, or as a multi-user Linux stack based on Apache2. \n    The entire system is fully open source and permissively licensed. The OpenCPU website\n    has detailed documentation and example apps."}, "Frames2": {"categories": ["OfficialStatistics"], "description": "Point and interval estimation in dual frame surveys. In contrast\n    to classic sampling theory, where only one sampling frame is considered,\n    dual frame methodology assumes that there are two frames available for\n    sampling and that, overall, they cover the entire target population. Then,\n    two probability samples (one from each frame) are drawn and information\n    collected is suitably combined to get estimators of the parameter of\n    interest."}, "BCBCSF": {"categories": ["Bayesian"], "description": "Fully Bayesian Classification with a subset of high-dimensional features, such as expression levels of genes. The data are modeled with a hierarchical Bayesian models using heavy-tailed t distributions as priors. When a large number of features are available, one may like to select only a subset of features to use, typically those features strongly correlated with the response in training cases. Such a feature selection procedure is however invalid since the relationship between the response and the features has be exaggerated by feature selection. This package provides a way to avoid this bias and yield better-calibrated predictions for future cases when one uses F-statistic to select features. "}, "request": {"categories": ["WebTechnologies"], "description": "High level and easy 'HTTP' client for 'R'. Provides functions for\n    building 'HTTP' queries, including query parameters, body requests, headers,\n    authentication, and more."}, "rrcovNA": {"categories": ["MissingData", "Robust"], "description": "Robust Location and Scatter Estimation and Robust\n        Multivariate Analysis with High Breakdown Point for Incomplete\n        Data (missing values) (Todorov et al. (2010) <doi:10.1007/s11634-010-0075-2>)."}, "bbricks": {"categories": ["Bayesian"], "description": "A set of frequently used Bayesian parametric and nonparametric model structures, as well as a set of tools for common analytical tasks. Structures include linear Gaussian systems, Gaussian and Normal-Inverse-Wishart conjugate structure, Gaussian and Normal-Inverse-Gamma conjugate structure, Categorical and Dirichlet conjugate structure, Dirichlet Process on positive integers, Dirichlet Process in general, Hierarchical Dirichlet Process ... Tasks include updating posteriors, sampling from posteriors, calculating marginal likelihood, calculating posterior predictive densities, sampling from posterior predictive distributions, calculating \"Maximum A Posteriori\" (MAP) estimates ... See <https://chenhaotian.github.io/Bayesian-Bricks/> to get started."}, "EMD": {"categories": ["TimeSeries"], "description": "For multiscale analysis, this package carries out empirical mode decomposition and Hilbert spectral\n        analysis. For usage of EMD, see Kim and Oh, 2009 (Kim, D and Oh, H.-S. (2009) EMD: A Package for Empirical \n        Mode Decomposition and Hilbert Spectrum, The R Journal, 1, 40-46). "}, "qdap": {"categories": ["NaturalLanguageProcessing"], "description": "Automates many of the tasks associated with quantitative discourse analysis of transcripts containing discourse\n              including frequency counts of sentence types, words, sentences, turns of talk, syllables and other assorted\n              analysis tasks. The package provides parsing tools for preparing transcript data. Many functions enable the user\n              to aggregate data by any number of grouping variables, providing analysis and seamless integration with other R\n              packages that undertake higher level analysis and visualization of text. This affords the user a more efficient\n              and targeted analysis. 'qdap' is designed for transcript analysis, however, many functions are applicable to other\n              areas of Text Mining/ Natural Language Processing."}, "trajectories": {"categories": ["SpatioTemporal", "Tracking"], "description": "Classes and methods for trajectory data, with support for nesting individual Track objects in track sets (Tracks) and track sets for different entities in collections of Tracks. Methods include selection, generalization, aggregation, intersection, simulation, and plotting."}, "SQUAREM": {"categories": ["NumericalMathematics"], "description": "Algorithms for accelerating the convergence of slow,\n        monotone sequences from smooth, contraction mapping such as the\n        EM algorithm. It can be used to accelerate any smooth, linearly\n        convergent acceleration scheme.  A tutorial style introduction\n        to this package is available in a vignette on the CRAN download\n        page or, when the package is loaded in an R session, with\n        vignette(\"SQUAREM\"). Refer to the J Stat Software article: <doi:10.18637/jss.v092.i07>. "}, "flexclust": {"categories": ["Cluster"], "description": "The main function kcca implements a general framework for\n  k-centroids cluster analysis supporting arbitrary distance measures\n  and centroid computation. Further cluster methods include hard\n  competitive learning, neural gas, and QT clustering. There are\n  numerous visualization methods for cluster results (neighborhood\n  graphs, convex cluster hulls, barcharts of centroids, ...), and\n  bootstrap methods for the analysis of cluster stability."}, "dsm": {"categories": ["Environmetrics"], "description": "Density surface modelling of line transect data. A Generalized\n    Additive Model-based approach is used to calculate spatially-explicit estimates\n    of animal abundance from distance sampling (also presence/absence and strip\n    transect) data. Several utility functions are provided for model checking,\n    plotting and variance estimation."}, "lwgeom": {"categories": ["Spatial"], "description": "Access to selected functions found in 'liblwgeom' <https://github.com/postgis/postgis/tree/master/liblwgeom>, the light-weight geometry library used by 'PostGIS' <http://postgis.net/>."}, "languageR": {"categories": ["NaturalLanguageProcessing"], "description": "Data sets exemplifying statistical methods, and some\n        facilitatory utility functions used in \u201cAnalyzing Linguistic\n        Data: A practical introduction to statistics using R\u201d,\n        Cambridge University Press, 2008."}, "nonlinearTseries": {"categories": ["TimeSeries"], "description": "Functions for nonlinear time series analysis. This package permits\n    the computation of the  most-used nonlinear statistics/algorithms\n    including generalized correlation dimension, information dimension,\n    largest Lyapunov exponent, sample entropy and Recurrence\n    Quantification Analysis (RQA), among others. Basic routines\n    for surrogate data testing are also included. Part of this work\n    was based on the  book \"Nonlinear time series analysis\" by\n    Holger Kantz and Thomas Schreiber (ISBN: 9780521529020)."}, "dynlm": {"categories": ["Econometrics", "Environmetrics", "Finance", "TimeSeries"], "description": "Dynamic linear models and time series regression."}, "geometa": {"categories": ["Spatial"], "description": "Provides facilities to handle reading and writing of geographic metadata \n defined with OGC/ISO 19115, 11119 and 19110 geographic information metadata standards,\n and encoded using the ISO 19139 (XML) standard. It includes also a facility to check\n the validity of ISO 19139 XML encoded metadata."}, "lbfgsb3c": {"categories": ["Optimization"], "description": "Interfacing to Nocedal et al. L-BFGS-B.3.0 \n        (See <http://users.iems.northwestern.edu/~nocedal/lbfgsb.html>)\n        limited memory BFGS minimizer with bounds on parameters. \n        This is a fork of 'lbfgsb3'. \n\tThis registers a 'R' compatible 'C' interface to L-BFGS-B.3.0 that uses the same \n\tfunction types and optimization as the optim() function (see writing 'R' extensions\n        and source for details).  This package also adds more stopping criteria as well \n        as allowing the adjustment of more tolerances."}, "TTR": {"categories": ["Finance"], "description": "A collection of over 50 technical indicators for creating technical trading rules. The package also provides fast implementations of common rolling-window functions, and several volatility calculations."}, "DTDA": {"categories": ["Survival"], "description": "Implementation of different algorithms for analyzing\n        randomly truncated data, one-sided and two-sided (i.e. doubly)\n        truncated data. It serves to compute empirical cumulative \n        distributions and also kernel density and hazard functions \n        using different bandwidth selectors.\n      Several real data sets are included."}, "pls": {"categories": ["ChemPhys", "HighPerformanceComputing", "Psychometrics"], "description": "Multivariate regression methods\n\tPartial Least Squares Regression (PLSR), Principal Component\n\tRegression (PCR) and Canonical Powered Partial Least Squares (CPPLS)."}, "Rsfar": {"categories": ["TimeSeries"], "description": "This is a collection of functions designed for simulating, estimating and forecasting seasonal functional autoregressive time series of order one. These methods are addressed in the manuscript: <https://www.monash.edu/business/ebs/research/publications/ebs/wp16-2019.pdf>."}, "schumaker": {"categories": ["NumericalMathematics"], "description": "This is a shape preserving spline <doi:10.1137/0720057>\n    which is guaranteed to be monotonic and concave or convex if the\n    data is monotonic and concave or convex. It does not use any \n    optimisation and is therefore quick and smoothly converges to a \n    fixed point in economic dynamics problems including value function \n    iteration. It also automatically gives the first two derivatives \n    of the spline and options for determining behaviour when evaluated \n    outside the interpolation domain."}, "readabs": {"categories": ["OfficialStatistics", "TimeSeries"], "description": "Downloads, imports, and tidies time series data from the \n    Australian Bureau of Statistics <https://www.abs.gov.au/>."}, "mdendro": {"categories": ["Cluster"], "description": "A comprehensive collection of linkage methods for agglomerative\n  hierarchical clustering on a matrix of proximity data (distances or\n  similarities), returning a multifurcated dendrogram or multidendrogram.\n  Multidendrograms can group more than two clusters when ties in proximity data\n  occur, and therefore they do not depend on the order of the input data.\n  Descriptive measures to analyze the resulting dendrogram are additionally\n  provided."}, "binseqtest": {"categories": ["ExperimentalDesign"], "description": "For a series of binary responses, create stopping boundary with exact results after stopping, allowing updating for missing assessments."}, "BatchExperiments": {"categories": ["ExperimentalDesign", "HighPerformanceComputing"], "description": "Extends the BatchJobs package to run statistical experiments on\n    batch computing clusters. For further details see the project web page."}, "CollocInfer": {"categories": ["DifferentialEquations"], "description": "These functions implement collocation-inference\n    for continuous-time and discrete-time stochastic processes.\n    They provide model-based smoothing, gradient-matching,\n    generalized profiling and forwards prediction error methods."}, "mcmc": {"categories": ["Bayesian"], "description": "Simulates continuous distributions of random vectors using\n    Markov chain Monte Carlo (MCMC).  Users specify the distribution by an\n    R function that evaluates the log unnormalized density.  Algorithms\n    are random walk Metropolis algorithm (function metrop), simulated\n    tempering (function temper), and morphometric random walk Metropolis\n    (Johnson and Geyer, 2012, <doi:10.1214/12-AOS1048>,\n    function morph.metrop),\n    which achieves geometric ergodicity by change of variable."}, "STFTS": {"categories": ["TimeSeries"], "description": "A collection of statistical hypothesis tests of functional time series. While it will include more tests when the related literature are enriched, this package contains the following key tests: functional stationarity test, functional trend stationarity test, functional unit root test, to name a few."}, "Lmoments": {"categories": ["Distributions"], "description": "Contains functions to estimate\n        L-moments and trimmed L-moments from the data. Also\n        contains functions to estimate the parameters of the normal\n        polynomial quantile mixture and the Cauchy polynomial quantile\n        mixture from L-moments and trimmed L-moments."}, "lisrelToR": {"categories": ["Psychometrics"], "description": "This is an unofficial package aimed at automating the import of \n            'LISREL' <https://ssicentral.com/index.php/products/lisrel/> output \n            in R. This package or its maintainer is not in any way affiliated with \n            the creators of 'LISREL' and 'SSI, Inc'."}, "EdSurvey": {"categories": ["OfficialStatistics"], "description": "Read in and analyze functions for education survey and assessment data from the National Center for Education Statistics (NCES) <https://nces.ed.gov/>, including National Assessment of Educational Progress (NAEP) data <https://nces.ed.gov/nationsreportcard/> and data from the International Assessment Database: Organisation for Economic Co-operation and Development (OECD) <https://www.oecd.org/>, including Programme for International Student Assessment (PISA), Teaching and Learning International Survey (TALIS), Programme for the International Assessment of Adult Competencies (PIAAC), and International Association for the Evaluation of Educational Achievement (IEA) <https://www.iea.nl/>, including Trends in International Mathematics and Science Study (TIMSS), TIMSS Advanced, Progress in International Reading Literacy Study (PIRLS), International Civic and Citizenship Study (ICCS), International Computer and Information Literacy Study (ICILS), and Civic Education Study (CivEd)."}, "MplusAutomation": {"categories": ["Psychometrics"], "description": "Leverages the R language to automate latent variable model estimation\n\tand interpretation using 'Mplus', a powerful latent variable modeling program\n\tdeveloped by Muthen and Muthen (<https://www.statmodel.com>). Specifically, this package\n    provides routines for creating related groups of models, running batches of\n    models, and extracting and tabulating model parameters and fit statistics."}, "smerc": {"categories": ["Spatial"], "description": "Implements statistical methods for analyzing the counts of areal data,\n    with a focus on the detection of spatial clusters and clustering.  The package\n    has a heavy emphasis on spatial scan methods, which were first introduced\n    by Kulldorff and Nagarwalla (1995) <doi:10.1002/sim.4780140809> and\n    Kulldorff (1997) <doi:10.1080/03610929708831995>."}, "covid19italy": {"categories": ["Epidemiology"], "description": "Provides a daily summary of the Coronavirus (COVID-19) cases in Italy by country, region and province level. Data source: Presidenza del Consiglio dei Ministri - Dipartimento della Protezione Civile <https://www.protezionecivile.it/>."}, "jomo": {"categories": ["MissingData"], "description": "Similarly to Schafer's package 'pan', 'jomo' is a package for multilevel joint modelling multiple imputation (Carpenter and Kenward, 2013) <doi:10.1002/9781119942283>.\n Novel aspects of 'jomo' are the possibility of handling binary and categorical data through latent normal variables, the option to use cluster-specific covariance matrices and to impute compatibly with the substantive model. "}, "Rblpapi": {"categories": ["Finance"], "description": "An R Interface to 'Bloomberg' is provided via the 'Blp API'."}, "stringdist": {"categories": ["NaturalLanguageProcessing", "OfficialStatistics"], "description": "Implements an approximate string matching version of R's native\n    'match' function. Also offers fuzzy text search based on various string\n     distance measures. Can calculate various string distances based on edits\n    (Damerau-Levenshtein, Hamming, Levenshtein, optimal sting alignment), qgrams (q-\n    gram, cosine, jaccard distance) or heuristic metrics (Jaro, Jaro-Winkler). An\n    implementation of soundex is provided as well. Distances can be computed between\n    character vectors while taking proper care of encoding or between integer\n    vectors representing generic sequences. This package is built for speed and\n    runs in parallel by using 'openMP'. An API for C or C++ is exposed as well.\n    Reference: MPJ van der Loo (2014) <doi:10.32614/RJ-2014-011>."}, "stochQN": {"categories": ["Optimization"], "description": "Implementations of stochastic, limited-memory quasi-Newton optimizers,\n\tsimilar in spirit to the LBFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) algorithm,\n\tfor smooth stochastic optimization. Implements the following methods:\n\toLBFGS (online LBFGS) (Schraudolph, N.N., Yu, J. and Guenter, S., 2007 <http://proceedings.mlr.press/v2/schraudolph07a.html>),\n\tSQN (stochastic quasi-Newton) (Byrd, R.H., Hansen, S.L., Nocedal, J. and Singer, Y., 2016 <arXiv:1401.7020>),\n\tadaQN (adaptive quasi-Newton) (Keskar, N.S., Berahas, A.S., 2016, <arXiv:1511.01169>).\n\tProvides functions for easily creating R objects\n\twith partial_fit/predict methods from some given objective/gradient/predict functions.\n\tIncludes an example stochastic logistic regression using these optimizers.\n\tProvides header files and registered C routines for using it directly from C/C++."}, "tseriesEntropy": {"categories": ["TimeSeries"], "description": "Implements an Entropy measure of dependence based on the Bhattacharya-Hellinger-Matusita distance. Can be used as a (nonlinear) autocorrelation/crosscorrelation function for continuous and categorical time series. The package includes tests for serial dependence and nonlinearity based on it. Some routines have a parallel version that can be used in a multicore/cluster environment. The package makes use of S4 classes."}, "MittagLeffleR": {"categories": ["Distributions"], "description": "Implements the Mittag-Leffler function, distribution,\n  random variate generation, and estimation. Based on the Laplace-Inversion\n  algorithm by Garrappa, R. (2015) <doi:10.1137/140971191>."}, "diagmeta": {"categories": ["MetaAnalysis"], "description": "Provides methods by Steinhauser et al. (2016) <doi:10.1186/s12874-016-0196-1> for meta-analysis of diagnostic accuracy studies with several cutpoints."}, "genalg": {"categories": ["Optimization"], "description": "R based genetic algorithm for binary and floating point\n        chromosomes."}, "ZINARp": {"categories": ["TimeSeries"], "description": "Simulation, exploratory data analysis and Bayesian analysis of the p-order Integer-valued Autoregressive (INAR(p)) and Zero-inflated p-order Integer-valued Autoregressive (ZINAR(p)) processes, as described in Garay et al. (2020) <doi:10.1080/00949655.2020.1754819>. "}, "gustave": {"categories": ["OfficialStatistics"], "description": "Provides a toolkit for analytical variance estimation in survey sampling. Apart from the implementation of standard variance estimators, its main feature is to help the sampling expert produce easy-to-use variance estimation \"wrappers\", where systematic operations (linearization, domain estimation) are handled in a consistent and transparent way."}, "mAr": {"categories": ["TimeSeries"], "description": "R functions for the estimation and eigen-decomposition of multivariate autoregressive models."}, "visualize": {"categories": ["Distributions", "TeachingStatistics"], "description": "Graphs the pdf or pmf and highlights what area or probability is\n    present in user defined locations. Visualize is able to provide lower tail,\n    bounded, upper tail, and two tail calculations. Supports strict and equal\n    to inequalities.  Also provided on the graph is the mean and variance of\n    the distribution. "}, "hypergeo": {"categories": ["NumericalMathematics"], "description": "The Gaussian hypergeometric function for complex numbers."}, "sdpt3r": {"categories": ["Optimization"], "description": "Solves the general Semi-Definite Linear Programming formulation using an R implementation of SDPT3 (K.C. Toh, M.J. Todd, and R.H. Tutuncu (1999) <doi:10.1080/10556789908805762>). This includes problems such as the nearest correlation matrix problem (Higham (2002) <doi:10.1093/imanum/22.3.329>), D-optimal experimental design (Smith (1918) <doi:10.2307/2331929>), Distance Weighted Discrimination (Marron and Todd (2012) <doi:10.1198/016214507000001120>), as well as graph theory problems including the maximum cut problem. Technical details surrounding SDPT3 can be found in R.H Tutuncu, K.C. Toh, and M.J. Todd (2003) <doi:10.1007/s10107-002-0347-5>. "}, "dexter": {"categories": ["Psychometrics"], "description": "A system for the management, assessment, and psychometric analysis of data from educational and psychological tests. "}, "tsfeatures": {"categories": ["TimeSeries"], "description": "Methods for extracting various features from time series data. The features provided are those from Hyndman, Wang and Laptev (2013) <doi:10.1109/ICDMW.2015.104>, Kang, Hyndman and Smith-Miles (2017) <doi:10.1016/j.ijforecast.2016.09.004> and from Fulcher, Little and Jones (2013) <doi:10.1098/rsif.2013.0048>. Features include spectral entropy, autocorrelations, measures of the strength of seasonality and trend, and so on. Users can also define their own feature functions."}, "miceadds": {"categories": ["MissingData"], "description": "\n    Contains functions for multiple imputation which\n    complements existing functionality in R.\n    In particular, several imputation methods for the\n    mice package (van Buuren & Groothuis-Oudshoorn, 2011,\n    <doi:10.18637/jss.v045.i03>) are included.\n    Main features of the miceadds package include\n    plausible value imputation (Mislevy, 1991,\n    <doi:10.1007/BF02294457>), multilevel imputation for\n    variables at any level or with any number of hierarchical\n    and non-hierarchical levels (Grund, Luedtke & Robitzsch,\n    2018, <doi:10.1177/1094428117703686>; van Buuren, 2018, \n    Ch.7, <doi:10.1201/9780429492259>), imputation using \n    partial least squares (PLS) for high dimensional \n    predictors (Robitzsch, Pham & Yanagida, 2016), \n    nested multiple imputation (Rubin, 2003, \n    <doi:10.1111/1467-9574.00217>), substantive model\n    compatible imputation (Bartlett et al., 2015,\n    <doi:10.1177/0962280214521348>), and features\n    for the generation of synthetic datasets\n    (Reiter, 2005, <doi:10.1111/j.1467-985X.2004.00343.x>;\n    Nowok, Raab, & Dibben, 2016, <doi:10.18637/jss.v074.i11>)."}, "MixAll": {"categories": ["Cluster"], "description": "Algorithms and methods for model-based clustering and classification.\n It supports various types of data: continuous, categorical and counting\n and can handle mixed data of these types. It can fit Gaussian (with diagonal\n covariance structure), gamma, categorical and Poisson models. The algorithms\n also support missing values. This package can be used as an independent\n alternative to the (not free) 'mixtcomp' software available at\n <https://massiccc.lille.inria.fr/>."}, "factorstochvol": {"categories": ["Finance", "TimeSeries"], "description": "Markov chain Monte Carlo (MCMC) sampler for fully Bayesian estimation of latent factor stochastic volatility models with interweaving <doi:10.1080/10618600.2017.1322091>. Sparsity can be achieved through the usage of Normal-Gamma priors on the factor loading matrix <doi:10.1016/j.jeconom.2018.11.007>."}, "frailtyHL": {"categories": ["Survival"], "description": "Implements the h-likelihood estimation procedures for general frailty models including competing-risk models and joint models."}, "rdmulti": {"categories": ["Econometrics"], "description": "The regression discontinuity (RD) design is a popular quasi-experimental design for causal inference and policy evaluation. The 'rdmulti' package provides tools to analyze RD designs with multiple cutoffs or scores: rdmc() estimates pooled and cutoff specific effects for multi-cutoff designs, rdmcplot() draws RD plots for multi-cutoff designs and rdms() estimates effects in cumulative cutoffs or multi-score designs. See Cattaneo, Titiunik and Vazquez-Bare (2020) <https://rdpackages.github.io/references/Cattaneo-Titiunik-VazquezBare_2020_Stata.pdf> for further methodological details. "}, "fixedincome": {"categories": ["Finance"], "description": "Fixed income mathematics made easy. A rich set of functions \n    that helps with calculations of interest rates and fixed income.\n    It has objects that abstract interest rates, compounding factors, day count rules,\n    forward rates and term structure of interest rates.\n    Many interpolation methods and parametric curve models commonly used by practitioners\n    are implemented."}, "longurl": {"categories": ["WebTechnologies"], "description": "Tools are provided to expand vectors of short URLs into long 'URLs'. \n    No 'API' services are used, which may mean that this operates more slowly than \n    'API' services do (since they usually cache results of expansions that every \n    user of the service requests). You can setup your own caching layer with the \n    'memoise' package if you wish to have a speedup during single sessions or add \n    larger dependencies, such as 'Redis', to gain a longer-term performance boost \n    at the expense of added complexity."}, "DSAIDE": {"categories": ["Epidemiology"], "description": "Exploration of simulation models (apps) of various infectious disease transmission dynamics scenarios.\n    The purpose of the package is to help individuals learn \n    about infectious disease epidemiology (ecology/evolution) from a dynamical systems perspective.\n    All apps include explanations of the underlying models and instructions on what to do with the models. "}, "Spectrum": {"categories": ["Cluster"], "description": "A self-tuning spectral clustering method for single or multi-view data. 'Spectrum' uses a new type of adaptive density aware kernel that strengthens connections in the graph based on common nearest neighbours. It uses a tensor product graph data integration and diffusion procedure to integrate different data sources and reduce noise. 'Spectrum' uses either the eigengap or multimodality gap heuristics to determine the number of clusters. The method is sufficiently flexible so that a wide range of Gaussian and non-Gaussian structures can be clustered with automatic selection of K."}, "BayesMassBal": {"categories": ["Bayesian"], "description": "Bayesian tools that can be used to reconcile, or mass balance, mass flow rate data collected from chemical or particulate separation processes aided by constraints governed by the conservation of mass.\n    Functions included in the package aid the user in organizing and constraining data, using Markov chain Monte Carlo methods to obtain samples from Bayesian models, and in computation of the marginal likelihood of the data, given a particular model, for model selection.  Marginal likelihood is approximated by methods in Chib S (1995) <doi:10.2307/2291521>."}, "FrF2": {"categories": ["ClinicalTrials", "ExperimentalDesign"], "description": "Regular and non-regular Fractional Factorial 2-level designs \n        can be created. Furthermore, analysis tools for Fractional\n        Factorial designs with 2-level factors are offered (main\n        effects and interaction plots for all factors simultaneously,\n        cube plot for looking at the simultaneous effects of three\n        factors, full or half normal plot, alias structure in a more\n        readable format than with the built-in function alias). "}, "tailDepFun": {"categories": ["ExtremeValue"], "description": "Provides functions implementing minimal distance estimation methods for parametric tail dependence models, as proposed in  \n\t Einmahl, J.H.J., Kiriliouk, A., Krajina, A., and Segers, J. (2016) <doi:10.1111/rssb.12114> and \n\t Einmahl, J.H.J., Kiriliouk, A., and Segers, J. (2018) <doi:10.1007/s10687-017-0303-7>."}, "censusGeography": {"categories": ["OfficialStatistics"], "description": "Converts the United States Census geographic code for city, state (FIP and ICP),\n    region, and birthplace, into the name of the region. e.g. takes an input of\n    Census city code 5330 to it's actual city, Philadelphia. Will return NA for code\n    that doesn't correspond to real location."}, "qMRI": {"categories": ["MedicalImaging"], "description": "Implementation of methods for estimation of quantitative maps\n    from Multi-Parameter Mapping (MPM) acquisitions (Weiskopf et al. (2013)\n    <doi:10.3389/fnins.2013.00095>) including adaptive\n    smoothing methods in the framework of the ESTATICS model\n    (Estimating the apparent transverse relaxation time (R2*) from images with\n    different contrasts, Weiskopf et al. (2014) <doi:10.3389/fnins.2014.00278>).\n    The smoothing method is described in Mohammadi et al. (2017).\n    <doi:10.20347/WIAS.PREPRINT.2432>. Usage of the package is also described in\n    Polzehl and Tabelow (2019),\n    Magnetic Resonance Brain Imaging, Chapter 6, Springer, Use R! Series.\n    <doi:10.1007/978-3-030-29184-6_6>."}, "nse": {"categories": ["Econometrics"], "description": "Collection of functions designed to calculate numerical standard error (NSE) of univariate time series as described in Ardia et al. (2018) <doi:10.1515/jtse-2017-0011> and Ardia and Bluteau (2017) <doi:10.21105/joss.00172>."}, "CPBayes": {"categories": ["Bayesian", "MetaAnalysis"], "description": "A Bayesian meta-analysis method for studying cross-phenotype\n    genetic associations. It uses summary-level data across multiple phenotypes to\n    simultaneously measure the evidence of aggregate-level pleiotropic association\n    and estimate an optimal subset of traits associated with the risk locus. CPBayes\n    is based on a spike and slab prior. The methodology is available from: A Majumdar, T Haldar, S Bhattacharya, JS Witte (2018) <doi:10.1371/journal.pgen.1007139>."}, "bayesGAM": {"categories": ["Bayesian"], "description": "The 'bayesGAM' package is designed to provide a user friendly option to fit univariate and multivariate response Generalized Additive Models (GAM) using Hamiltonian Monte Carlo (HMC) with few technical burdens.  The functions in this package use 'rstan' (Stan Development Team 2020)  to call 'Stan' routines that run the HMC simulations. The 'Stan' code for these models is already pre-compiled for the user. The programming formulation for models in 'bayesGAM' is designed to be familiar to analysts who fit statistical models in 'R'.\n    Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., ... & Riddell, A. (2017). Stan: A probabilistic programming language. Journal of statistical software, 76(1). \n    Stan Development Team. 2018. RStan: the R interface to Stan. R package version 2.17.3.   <https://mc-stan.org/>\n    Neal, Radford (2011) \"Handbook of Markov Chain Monte Carlo\" ISBN: 978-1420079418.\n    Betancourt, Michael, and Mark Girolami. \"Hamiltonian Monte Carlo for hierarchical models.\" Current trends in Bayesian methodology with applications 79.30 (2015): 2-4.\n    Thomas, S., Tu, W. (2020) \"Learning Hamiltonian Monte Carlo in R\" <arXiv:2006.16194>,\n    Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013) \"Bayesian Data Analysis\" ISBN: 978-1439840955, \n    Agresti, Alan (2015) \"Foundations of Linear and Generalized Linear Models ISBN: 978-1118730034, \n    Pinheiro, J., Bates, D. (2006) \"Mixed-effects Models in S and S-Plus\" ISBN: 978-1441903174.\n    Ruppert, D., Wand, M. P., & Carroll, R. J. (2003). Semiparametric regression (No. 12). Cambridge university press. ISBN: 978-0521785167."}, "dLagM": {"categories": ["TimeSeries"], "description": "Provides time series regression models with one predictor using finite distributed lag models, polynomial (Almon) distributed lag models, geometric distributed lag models with Koyck transformation, and autoregressive distributed lag models. It also consists of functions for computation of h-step ahead forecasts from these models. See Demirhan (2020)(<doi:10.1371/journal.pone.0228812>) and Baltagi (2011)(<doi:10.1007/978-3-642-20059-5>) for more information."}, "varSelRF": {"categories": ["ChemPhys", "HighPerformanceComputing", "MachineLearning"], "description": "Variable selection from random forests using both\n        backwards variable elimination (for the selection of small sets\n        of non-redundant variables) and selection based on the\n        importance spectrum (somewhat similar to scree plots; for the\n        selection of large, potentially highly-correlated variables).\n        Main applications in high-dimensional data (e.g., microarray\n        data, and other genomics and proteomics applications). "}, "meboot": {"categories": ["Econometrics", "TimeSeries"], "description": "Maximum entropy density based dependent data bootstrap. \n  An algorithm is provided to create a population of time series (ensemble) \n  without assuming stationarity. The reference paper (Vinod, H.D., 2004 <doi:10.1016/j.jempfin.2003.06.002>) explains\n  how the algorithm satisfies the ergodic theorem and the central limit theorem."}, "tmvmixnorm": {"categories": ["Distributions"], "description": "Efficient sampling of truncated multivariate (scale) mixtures of normals under linear inequality constraints is nontrivial due to the analytically intractable normalizing constant. Meanwhile, traditional methods may subject to numerical issues, especially when the dimension is high and dependence is strong.    Algorithms proposed by Li and Ghosh (2015) <doi:10.1080/15598608.2014.996690> are adopted for overcoming difficulties in simulating truncated distributions. Efficient rejection sampling for simulating truncated univariate normal distribution is included in the package, which shows superiority in terms of acceptance rate and numerical stability compared to existing methods and R packages. An efficient function for sampling from truncated multivariate normal distribution subject to convex polytope restriction regions based on Gibbs sampler for conditional truncated univariate distribution is provided. By extending the sampling method, a function for sampling truncated multivariate Student's t distribution is also developed.     Moreover, the proposed method and computation remain valid for high dimensional and strong dependence scenarios. Empirical results in Li and Ghosh (2015) <doi:10.1080/15598608.2014.996690> illustrated the superior performance in terms of various criteria (e.g. mixing and integrated auto-correlation time)."}, "FCPS": {"categories": ["Cluster"], "description": "Over sixty clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the mirrored density plot (MD-plot) of clusterability are implemented. The packages is published in Thrun, M.C., Stier Q.: \"Fundamental Clustering Algorithms Suite\" (2021), SoftwareX, <doi:10.1016/j.softx.2020.100642>. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: \"Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems\" (2020), Data in Brief, <doi:10.1016/j.dib.2020.105501>."}, "POT": {"categories": ["ExtremeValue"], "description": "Some functions useful to perform a Peak Over Threshold\n        analysis in univariate and bivariate cases, see Beirlant et al. (2004)\n        <doi:10.1002/0470012382>. A user guide is available in the vignette."}, "weights": {"categories": ["OfficialStatistics"], "description": "Provides a variety of functions for producing simple weighted statistics, such as weighted Pearson's correlations, partial correlations, Chi-Squared statistics, histograms, and t-tests.  Also now includes some software for quickly recoding survey data and plotting estimates from interaction terms in regressions (and multiply imputed regressions) both with and without weights. NOTE: Weighted partial correlation calculations pulled to address a bug."}, "CausalMBSTS": {"categories": ["CausalInference"], "description": "Infers the causal effect of an intervention on a multivariate response through the use of Multivariate \n    Bayesian Structural Time Series models (MBSTS) as described in Menchetti & Bojinov (2020) <arXiv:2006.12269>. \n    The package also includes functions for model building and forecasting.  "}, "laeken": {"categories": ["OfficialStatistics"], "description": "Estimation of indicators on social exclusion and poverty, as well\n    as Pareto tail modeling for empirical income distributions."}, "fastpseudo": {"categories": ["Survival"], "description": "Computes pseudo-observations for survival analysis on right-censored data based on restricted mean survival time."}, "kml": {"categories": ["Cluster"], "description": "An implementation of k-means specifically design\n        to cluster longitudinal data. It provides facilities to deal with missing\n        value, compute several quality criterion (Calinski and Harabatz,\n        Ray and Turie, Davies and Bouldin, BIC, ...) and propose a graphical\n        interface for choosing the 'best' number of clusters."}, "xplain": {"categories": ["TeachingStatistics"], "description": "Allows to provide live interpretations and explanations of statistical \n    functions in R. These interpretations and explanations are shown when the explained function \n    is called by the user. They can interact with the values of the explained function's actual \n    results to offer relevant, meaningful insights. The 'xplain' interpretations and explanations \n    are based on an easy-to-use XML format that allows to include R code to interact with the \n    returns of the explained function."}, "bayesianETAS": {"categories": ["Bayesian"], "description": "The Epidemic Type Aftershock Sequence  (ETAS) model is one of the best-performing methods for modeling and forecasting earthquake occurrences. This package implements Bayesian estimation routines to draw samples from the full posterior distribution of the model parameters, given an earthquake catalog. The paper on which this package is based is Gordon J. Ross - Bayesian Estimation of the ETAS Model for Earthquake Occurrences (2016), available from the below URL."}, "AzureGraph": {"categories": ["WebTechnologies"], "description": "A simple interface to the 'Microsoft Graph' API <https://docs.microsoft.com/en-us/graph/overview>. 'Graph' is a comprehensive framework for accessing data in various online Microsoft services. This package was originally intended to provide an R interface only to the 'Azure Active Directory' part, with a view to supporting interoperability of R and 'Azure': users, groups, registered apps and service principals. However it has since been expanded into a more general tool for interacting with Graph. Part of the 'AzureR' family of packages."}, "BayesARIMAX": {"categories": ["Bayesian", "TimeSeries"], "description": "The Autoregressive Integrated Moving Average (ARIMA) model is very popular univariate time series model. Its application has been widened by the incorporation of exogenous variable(s) (X) in the model and modified as ARIMAX by Bierens (1987) <doi:10.1016/0304-4076(87)90086-8>. In this package we estimate the ARIMAX model using Bayesian framework. "}, "opencage": {"categories": ["WebTechnologies"], "description": "Geocode with the OpenCage API, either from place name to longitude \n    and latitude (forward geocoding) or from longitude and latitude to the name \n    and address of a location (reverse geocoding), see \n    <https://opencagedata.com/>."}, "AzureTableStor": {"categories": ["WebTechnologies"], "description": "An interface to the table storage service in 'Azure': <https://azure.microsoft.com/en-us/services/storage/tables/>. Supplies functionality for reading and writing data stored in tables, both as part of a storage account and from a 'CosmosDB' database with the table service API. Part of the 'AzureR' family of packages."}, "miCoPTCM": {"categories": ["Survival"], "description": "Fits Semiparametric Promotion Time Cure Models, taking into account (using a \n\t\t\t corrected score approach or the SIMEX algorithm) or not the measurement error\n\t\t\t in the covariates, using a backfitting approach to maximize the likelihood."}, "trawl": {"categories": ["Distributions"], "description": "Contains R functions for simulating and estimating integer-valued trawl processes as \n    described in the article Veraart (2019),\"Modeling, simulation and inference for\n    multivariate time series of counts using trawl processes\", Journal of Multivariate Analysis,\n    169, pages 110-129,\n    <doi:10.1016/j.jmva.2018.08.012> and for \n    simulating random vectors from the bivariate negative binomial and the bi- and trivariate \n    logarithmic series distributions."}, "synthpop": {"categories": ["OfficialStatistics"], "description": "A tool for producing synthetic versions of microdata containing confidential information so that they are safe to be released to users for exploratory analysis. The key objective of generating synthetic data is to replace sensitive original values with synthetic ones causing minimal distortion of the statistical information contained in the data set. Variables, which can be categorical or continuous, are synthesised one-by-one using sequential modelling. Replacements are generated by drawing from conditional distributions fitted to the original data using parametric or classification and regression trees models. Data are synthesised via the function syn() which can be largely automated, if default settings are used, or with methods defined by the user. Optional parameters can be used to influence the disclosure risk and the analytical quality of the synthesised data. For a description of the implemented method see Nowok, Raab and Dibben (2016) <doi:10.18637/jss.v074.i11>."}, "earlyR": {"categories": ["Epidemiology"], "description": "Implements a simple, likelihood-based estimation of the reproduction number (R0) using a branching process with a Poisson likelihood. This model requires knowledge of the serial interval distribution, and dates of symptom onsets. Infectiousness is determined by weighting R0 by the probability mass function of the serial interval on the corresponding day. It is a simplified version of the model introduced by Cori et al. (2013) <doi:10.1093/aje/kwt133>."}, "nmw": {"categories": ["Pharmacokinetics"], "description": "This shows how NONMEM(R) <http://www.iconplc.com/innovation/nonmem/> software works. NONMEM's classical estimation methods like 'First Order(FO) approximation', 'First Order Conditional Estimation(FOCE)', and 'Laplacian approximation' are explained."}, "psychotree": {"categories": ["Psychometrics"], "description": "Recursive partitioning based on psychometric models,\n  employing the general MOB algorithm (from package partykit) to obtain\n  Bradley-Terry trees, Rasch trees, rating scale and partial credit trees, and\n  MPT trees, trees for 1PL, 2PL, 3PL and 4PL models and generalized partial \n  credit models."}, "ra4bayesmeta": {"categories": ["MetaAnalysis"], "description": "Functionality for performing a principled reference analysis in the Bayesian normal-normal hierarchical model used for Bayesian meta-analysis, as described in Ott, Plummer and Roos (2021, \"How vague is vague? How informative is informative? Reference analysis for Bayesian meta-analysis\", under minor revision for Statistics in Medicine). Computes a reference posterior, induced by a minimally informative improper reference prior for the between-study (heterogeneity) standard deviation. Determines additional proper anti-conservative (and conservative) prior benchmarks. Includes functions for reference analyses at both the posterior and the prior level, which, given the data, quantify the informativeness of a heterogeneity prior of interest relative to the minimally informative reference prior and the proper prior benchmarks. The functions operate on data sets which are compatible with the 'bayesmeta' package."}, "cops": {"categories": ["Psychometrics"], "description": "Cluster optimized proximity scaling (COPS) refers to multidimensional scaling (MDS) methods that aim at pronouncing the clustered appearance of the configuration (Rusch, Mair & Hornik, 2021, <doi:10.1080/10618600.2020.1869027> ). They achieve this by transforming proximities/distances with power functions and augment the fitting criterion with a clusteredness index, the OPTICS Cordillera (Rusch, Hornik & Mair, 2018, <doi:10.1080/10618600.2017.1349664> ). There are two variants: One for finding the configuration directly (COPS-C) for ratio, power, interval and non-metric MDS (Borg & Groenen, 2005, ISBN:978-0-387-28981-6), and one for using the augmented fitting criterion to find optimal parameters (P-COPS). The package contains various functions, wrappers, methods and classes for fitting, plotting and displaying different MDS models in a COPS framework like ratio, interval and non-metric MDS for COPS-C and P-COPS with Torgerson scaling (Torgerson, 1958, ISBN:978-0471879459), scaling by majorizing a complex function (SMACOF; de Leeuw, 1977, <https://escholarship.org/uc/item/4ps3b5mj> ), Sammon mapping (Sammon, 1969, <doi:10.1109/T-C.1969.222678> ), elastic scaling (McGee, 1966, <doi:10.1111/j.2044-8317.1966.tb00367.x> ), s-stress (Takane, Young & de Leeuw, 1977, <doi:10.1007/BF02293745> ), r-stress (de Leeuw, Groenen & Mair, 2016, <https://rpubs.com/deleeuw/142619>), power-stress (Buja & Swayne, 2002 <doi:10.1007/s00357-001-0031-0>), restricted power stress, approximated power stress, power elastic scaling, power Sammon mapping (Rusch, Mair & Hornik, 2021, <doi:10.1080/10618600.2020.1869027> ). All of these models can also solely be fit as MDS with power transformations. The package further contains a function for pattern search optimization, the \u201cAdaptive Luus-Jakola Algorithm\u201d (Rusch, Mair & Hornik, 2021, <doi:10.1080/10618600.2020.1869027> )."}, "BAwiR": {"categories": ["SportsAnalytics"], "description": "Collection of tools to work with basketball data. Functions available are related to friendly \n\tweb scraping and visualization. Data were obtained from <https://www.euroleague.net/>, \n\t<https://www.eurocupbasketball.com/> and <https://www.acb.com/>, following the instructions \n        of their respectives robots.txt files, when available. Tools for visualization include a population pyramid, 2D plots, \n        circular plots of players' percentiles, plots of players' monthly/yearly stats,\n\tteam heatmaps, team shooting plots, team four factors plots, cross-tables with the results of regular season games\n\tand maps of nationalities. Please see Vinue (2020) <doi:10.1089/big.2018.0124>. "}, "FHDI": {"categories": ["MissingData"], "description": "Impute general multivariate missing data with the fractional hot deck imputation based on Jaekwang Kim (2011) <doi:10.1093/biomet/asq073>."}, "ForecastComb": {"categories": ["TimeSeries"], "description": "Provides geometric- and regression-based forecast\n    combination methods under a unified user interface for the packages 'ForecastCombinations'\n    and 'GeomComb'. Additionally, updated tools and convenience functions for data pre-processing are available in order to deal with \n    common problems in forecast combination (missingness, collinearity). For method details see Hsiao C, Wan SK (2014). <doi:10.1016/j.jeconom.2013.11.003>, Hansen BE (2007). <doi:10.1111/j.1468-0262.2007.00785.x>,\n    Elliott G, Gargano A, Timmermann A (2013). <doi:10.1016/j.jeconom.2013.04.017>, \n    and Clemen RT (1989). <doi:10.1016/0169-2070(89)90012-5>."}, "icsw": {"categories": ["CausalInference"], "description": "Provides the necessary tools to estimate average treatment effects with an instrumental variable by re-weighting observations using a model of compliance. "}, "lava": {"categories": ["Psychometrics"], "description": "A general implementation of Structural Equation Models\n\twith latent variables (MLE, 2SLS, and composite likelihood\n\testimators) with both continuous, censored, and ordinal\n\toutcomes (Holst and Budtz-Joergensen (2013) <doi:10.1007/s00180-012-0344-y>).\n\tMixture latent variable models and non-linear latent variable models\n\t(Holst and Budtz-Joergensen (2019) <doi:10.1093/biostatistics/kxy082>).\n\tThe package also provides methods for graph exploration (d-separation,\n\tback-door criterion), simulation of general non-linear latent variable\n\tmodels, and estimation of influence functions for a broad range of\n\tstatistical models."}, "nlts": {"categories": ["TimeSeries"], "description": "R functions for (non)linear time series analysis with an emphasis on nonparametric autoregression and order estimation, and tests for linearity / additivity."}, "growfunctions": {"categories": ["FunctionalData"], "description": "Estimates a collection of time-indexed functions under\n    either of Gaussian process (GP) or intrinsic Gaussian Markov\n    random field (iGMRF) prior formulations where a Dirichlet process\n    mixture allows sub-groupings of the functions to share the same\n    covariance or precision parameters.  The GP and iGMRF formulations\n    both support any number of additive covariance or precision terms,\n    respectively, expressing either or both of multiple trend and\n    seasonality."}, "fechner": {"categories": ["Psychometrics"], "description": "Functions and example datasets for Fechnerian scaling of discrete\n  object sets.  User can compute Fechnerian distances among objects representing\n  subjective dissimilarities, and other related information.  See\n  package?fechner for an overview."}, "zen4R": {"categories": ["WebTechnologies"], "description": "Provides an Interface to 'Zenodo' (<https://zenodo.org>) REST API, \n  including management of depositions, attribution of DOIs by 'Zenodo' and \n  upload and download of files."}, "QTOCen": {"categories": ["CausalInference"], "description": "Provides methods for estimation of mean- and quantile-optimal treatment regimes from censored data. \n    Specifically, we have developed distinct functions for three types of right censoring for static treatment using quantile criterion: (1) independent/random censoring, (2) treatment-dependent random censoring, and (3) covariates-dependent random censoring. It also includes a function to estimate quantile-optimal dynamic treatment regimes for independent censored data. Finally, this package also includes a simulation data generative model of a dynamic treatment experiment proposed in literature."}, "RCAL": {"categories": ["MissingData"], "description": "Regularized calibrated estimation for causal inference and missing-data problems with high-dimensional data, based on Tan (2020a) <doi:10.1093/biomet/asz059>, Tan (2020b) <doi:10.1214/19-AOS1824> and Sun and Tan (2020) <arXiv:2009.09286>."}, "muhaz": {"categories": ["Survival"], "description": "Produces a smooth estimate of the hazard\n  function for censored data."}, "packrat": {"categories": ["ReproducibleResearch"], "description": "Manage the R packages your project depends\n    on in an isolated, portable, and reproducible way."}, "DoubleML": {"categories": ["CausalInference", "Econometrics", "MachineLearning"], "description": "Implementation of the double/debiased machine learning framework of\n    Chernozhukov et al. (2018) <doi:10.1111/ectj.12097> for partially linear\n    regression models, partially linear instrumental variable regression models,\n    interactive regression models and interactive instrumental variable\n    regression models. 'DoubleML' allows estimation of the nuisance parts in\n    these models by machine learning methods and computation of the Neyman\n    orthogonal score functions. 'DoubleML' is built on top of 'mlr3' and the\n    'mlr3' ecosystem. The object-oriented implementation of 'DoubleML' based on\n    the 'R6' package is very flexible. "}, "rgenoud": {"categories": ["HighPerformanceComputing", "MachineLearning", "Optimization"], "description": "A genetic algorithm plus derivative optimizer."}, "arules": {"categories": ["MachineLearning", "ModelDeployment"], "description": "Provides the infrastructure for representing, manipulating and analyzing \n  transaction data and patterns (frequent itemsets and association rules). \n  Also provides C implementations of the association mining algorithms Apriori and Eclat. \n  Hahsler, Gruen and Hornik (2005) <doi:10.18637/jss.v014.i15>."}, "archivist": {"categories": ["ReproducibleResearch"], "description": "Data exploration and modelling is a process in which a lot of data\n    artifacts are produced. Artifacts like: subsets, data aggregates, plots,\n    statistical models, different versions of data sets and different versions\n    of results. The more projects we work with the more artifacts are produced\n    and the harder it is to manage these artifacts. Archivist helps to store\n    and manage artifacts created in R. Archivist allows you to store selected\n    artifacts as a binary files together with their metadata and relations.\n    Archivist allows to share artifacts with others, either through shared\n    folder or github. Archivist allows to look for already created artifacts by\n    using it's class, name, date of the creation or other properties. Makes it\n    easy to restore such artifacts. Archivist allows to check if new artifact\n    is the exact copy that was produced some time ago. That might be useful\n    either for testing or caching."}, "constants": {"categories": ["ChemPhys"], "description": "CODATA internationally recommended values of the fundamental\n    physical constants, provided as symbols for direct use within the R language.\n    Optionally, the values with uncertainties and/or units are also provided if\n    the 'errors', 'units' and/or 'quantities' packages are installed.\n    The Committee on Data for Science and Technology (CODATA) is an\n    interdisciplinary committee of the International Council for Science which\n    periodically provides the internationally accepted set of values of the\n    fundamental physical constants. This package contains the \"2018 CODATA\"\n    version, published on May 2019:\n    Eite Tiesinga, Peter J. Mohr, David B. Newell, and Barry N. Taylor (2020)\n    <https://physics.nist.gov/cuu/Constants/>."}, "tsallisqexp": {"categories": ["Distributions"], "description": "Tsallis distribution also known as the q-exponential family distribution. Provide distribution d, p, q, r functions, fitting and testing functions. Project initiated by Paul Higbie and based on Cosma Shalizi's code."}, "invGauss": {"categories": ["Survival"], "description": "Fits the (randomized drift) inverse Gaussian distribution to survival data. The model is described in Aalen OO, Borgan O, Gjessing HK. Survival and Event History Analysis. A Process Point of View. Springer, 2008. It is based on describing time to event as the barrier hitting time of a Wiener process, where drift towards the barrier has been randomized with a Gaussian distribution. The model allows covariates to influence starting values of the Wiener process and/or average drift towards a barrier, with a user-defined choice of link functions. "}, "CausalGPS": {"categories": ["CausalInference"], "description": "Provides a framework for estimating causal effects of a continuous \n    exposure using observational data, and implementing matching and weighting \n    on the generalized propensity score.\n    Wu, X., Mealli, F., Kioumourtzoglou, M.A., Dominici, F. and Braun, D., \n    2018. Matching on generalized propensity scores with continuous exposures. \n    arXiv preprint <arXiv:1812.06575>."}, "asaur": {"categories": ["Survival"], "description": "Data sets are referred to in the text \"Applied Survival Analysis Using R\"\n by Dirk F. Moore, Springer, 2016, ISBN: 978-3-319-31243-9, <doi:10.1007/978-3-319-31245-3>."}, "PracTools": {"categories": ["OfficialStatistics"], "description": "Functions and datasets to support Valliant, Dever, and Kreuter, Practical Tools for Designing and Weighting Survey Samples (2nd edition, 2018). Contains functions for sample size calculation for survey samples using stratified or clustered one-, two-, and three-stage sample designs. Other functions compute variance components for multistage designs and sample sizes in two-phase designs. A number of example data sets are included."}, "formatR": {"categories": ["ReproducibleResearch"], "description": "Provides a function tidy_source() to format R source code. Spaces\n    and indent will be added to the code automatically, and comments will be\n    preserved under certain conditions, so that R code will be more\n    human-readable and tidy. There is also a Shiny app as a user interface in\n    this package (see tidy_app())."}, "tableone": {"categories": ["CausalInference"], "description": "Creates 'Table 1', i.e., description of baseline patient\n    characteristics, which is essential in every medical research.\n    Supports both continuous and categorical variables, as well as\n    p-values and standardized mean differences. Weighted data are\n    supported via the 'survey' package."}, "regspec": {"categories": ["TimeSeries"], "description": "Computes linear Bayesian spectral estimates from multirate\tdata for second-order stationary time series. Provides credible intervals and methods for plotting various spectral estimates. Please see the paper \u2018Should we sample a time series more frequently?\u2019 (doi below) for a full description of and motivation for the methodology."}, "lodi": {"categories": ["MissingData"], "description": "Impute observed values below the limit of detection (LOD) via\n    censored likelihood multiple imputation (CLMI) in single-pollutant\n    models, developed by Boss et al (2019) <doi:10.1097/EDE.0000000000001052>.\n    CLMI handles exposure detection limits that may change throughout the course\n    of exposure assessment. 'lodi' provides functions for imputing and\n    pooling for this method."}, "rlecuyer": {"categories": ["Distributions", "HighPerformanceComputing"], "description": "Provides an interface to the C implementation of the\n        random number generator with multiple independent streams\n        developed by L'Ecuyer et al (2002). The main purpose of this\n        package is to enable the use of this random number generator in\n        parallel R applications."}, "iqLearn": {"categories": ["CausalInference"], "description": "Estimate an optimal dynamic treatment regime using Interactive Q-learning."}, "metafuse": {"categories": ["MetaAnalysis"], "description": "Fused lasso method to cluster and estimate regression coefficients\n    of the same covariate across different data sets when a large number of\n    independent data sets are combined. Package supports Gaussian, binomial,\n    Poisson and Cox PH models."}, "DTAplots": {"categories": ["MetaAnalysis"], "description": "Function to create forest plots. Functions to use posterior samples from Bayesian bivariate meta-analysis model, Bayesian hierarchical summary receiver operating characteristic (HSROC) meta-analysis model or Bayesian latent class (LC) meta-analysis model to create Summary Receiver Operating Characteristic (SROC) plots using methods described by Harbord et al (2007)<doi:10.1093/biostatistics/kxl004>."}, "radiant": {"categories": ["WebTechnologies"], "description": "A platform-independent browser-based interface for business\n    analytics in R, based on the shiny package. The application combines the\n    functionality of 'radiant.data', 'radiant.design', 'radiant.basics',\n    'radiant.model', and 'radiant.multivariate'."}, "BNPTSclust": {"categories": ["TimeSeries"], "description": "Performs the algorithm for time series clustering described in Nieto-Barajas and Contreras-Cristan (2014)."}, "aqp": {"categories": ["Environmetrics"], "description": "The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice."}, "nflreadr": {"categories": ["SportsAnalytics"], "description": "A minimal package for downloading data from 'GitHub'\n    repositories of the 'nflverse' project."}, "rsample": {"categories": ["Spatial"], "description": "Classes and functions to create and summarize different types\n    of resampling objects (e.g. bootstrap, cross-validation)."}, "coga": {"categories": ["Distributions"], "description": "Evaluation for density and distribution function of convolution of gamma\n    distributions in R. Two related exact methods and one approximate method are\n    implemented with efficient algorithm and C++ code. A quick guide for choosing\n    correct method and usage of this package is given in package vignette. For the\n    detail of methods used in this package, we refer the user to\n    Mathai(1982)<doi:10.1007/BF02481056>,\n    Moschopoulos(1984)<doi:10.1007/BF02481123>,\n    Barnabani(2017)<doi:10.1080/03610918.2014.963612>,\n    Hu et al.(2020)<doi:10.1007/s00180-019-00924-9>."}, "ore": {"categories": ["NaturalLanguageProcessing"], "description": "Provides an alternative to R's built-in functionality for handling\n    regular expressions, based on the Onigmo library. Offers first-class\n    compiled regex objects, partial matching and function-based substitutions,\n    amongst other features."}, "ruimtehol": {"categories": ["NaturalLanguageProcessing"], "description": "Wraps the 'StarSpace' library <https://github.com/facebookresearch/StarSpace> \n    allowing users to calculate word, sentence, article, document, webpage, link and entity 'embeddings'. \n    By using the 'embeddings', you can perform text based multi-label classification, \n    find similarities between texts and categories, do collaborative-filtering based recommendation \n    as well as content-based recommendation, find out relations between entities, calculate \n    graph 'embeddings' as well as perform semi-supervised learning and multi-task learning on plain text. \n    The techniques are explained in detail in the paper: 'StarSpace: Embed All The Things!' by Wu et al. (2017), available at <arXiv:1709.03856>."}, "astsa": {"categories": ["TimeSeries"], "description": "Data sets and scripts to accompany Time Series Analysis and Its Applications: With R Examples (4th ed), by R.H. Shumway and D.S. Stoffer. Springer Texts in Statistics, 2017, <doi:10.1007/978-3-319-52452-8>, and Time Series: A Data Analysis Approach Using R. Chapman-Hall, 2019, <doi:10.1201/9780429273285>."}, "tidyr": {"categories": ["MissingData"], "description": "Tools to help to create tidy data, where each column is a\n    variable, each row is an observation, and each cell contains a single\n    value.  'tidyr' contains tools for changing the shape (pivoting) and\n    hierarchy (nesting and 'unnesting') of a dataset, turning deeply\n    nested lists into rectangular data frames ('rectangling'), and\n    extracting values out of string columns. It also includes tools for\n    working with missing values (both implicit and explicit)."}, "ROAuth": {"categories": ["WebTechnologies"], "description": "Provides an interface to the OAuth 1.0 specification\n        allowing users to authenticate via OAuth to the\n        server of their choice."}, "ismev": {"categories": ["Environmetrics", "ExtremeValue"], "description": "Functions to support the computations carried out in\n  \u2018An Introduction to Statistical Modeling of Extreme Values\u2019 by\n  Stuart Coles. The functions may be divided into the following \n  groups; maxima/minima, order statistics, peaks over thresholds\n  and point processes.  "}, "rpf": {"categories": ["Psychometrics"], "description": "The purpose of this package is to factor out logic\n    and math common to Item Factor Analysis fitting, diagnostics, and\n    analysis. It is envisioned as core support code suitable for more\n    specialized IRT packages to build upon. Complete access to optimized C\n    functions are made available with R_RegisterCCallable().\n    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>."}, "rpact": {"categories": ["ClinicalTrials"], "description": "Design and analysis of confirmatory adaptive clinical trials with continuous, binary, and survival endpoints according to the methods described in the monograph by Wassmer and Brannath (2016) <doi:10.1007/978-3-319-32562-0>. This includes classical group sequential as well as multi-stage adaptive hypotheses tests that are based on the combination testing principle."}, "streamDepletr": {"categories": ["Hydrology"], "description": "Implementation of analytical models for estimating streamflow \n    depletion due to groundwater pumping, and other related tools. Functions\n    are broadly split into two groups: (1) analytical streamflow depletion\n    models, which estimate streamflow depletion for a single stream reach\n    resulting from groundwater pumping; and (2) depletion apportionment \n    equations, which distribute estimated streamflow depletion among multiple\n    stream reaches within a stream network. See Zipper et al. (2018) <doi:10.1029/2018WR022707>\n    for more information on depletion apportionment equations and Zipper et\n    al. (2019) <doi:10.1029/2018WR024403> for more information on analytical\n    depletion functions, which combine analytical models and depletion apportionment\n    equations."}, "Rvmmin": {"categories": ["Optimization"], "description": "Variable metric nonlinear function minimization with bounds constraints."}, "trajr": {"categories": ["SpatioTemporal", "Tracking"], "description": "A toolbox to assist with statistical analysis of 2-dimensional animal trajectories.\n    It provides simple access to algorithms for calculating and assessing a variety of \n    characteristics such as speed and acceleration, as well as multiple measures of \n    straightness or tortuosity. McLean & Skowron Volponi (2018) <doi:10.1111/eth.12739>."}, "FITSio": {"categories": ["ChemPhys"], "description": "Utilities to read and write files in the FITS (Flexible\n  Image Transport System) format, a standard format in astronomy (see\n  e.g. <https://en.wikipedia.org/wiki/FITS> for more information).\n  Present low-level routines allow: reading, parsing, and modifying\n  FITS headers; reading FITS images (multi-dimensional arrays);\n  reading FITS binary and ASCII tables; and writing FITS images\n  (multi-dimensional arrays).  Higher-level functions allow: reading\n  files composed of one or more headers and a single (perhaps\n  multidimensional) image or single table; reading tables into\n  data frames; generating vectors for image array axes; scaling and\n  writing images as 16-bit integers.  Known incompletenesses are\n  reading random group extensions, as well as\n  complex and array descriptor data types in binary tables."}, "DOSPortfolio": {"categories": ["Finance"], "description": "\n  Constructs dynamic optimal shrinkage estimators for the weights of the global \n  minimum variance portfolio which are reconstructed at given reallocation \n  points as derived in Bodnar, Parolya, and Thors\u00e9n (2021) (<arXiv:2106.02131>).\n  Two dynamic shrinkage estimators are available in this package. One using \n  overlapping samples while the other use nonoverlapping samples."}, "qap": {"categories": ["Optimization"], "description": "Implements heuristics for the Quadratic Assignment Problem (QAP). Although, the QAP was introduced as a combinatorial optimization problem for the facility location problem in operations research, it also has many applications in data analysis. The problem is NP-hard and the package implements a simulated annealing heuristic."}, "rospca": {"categories": ["Robust"], "description": "Implementation of robust sparse PCA using the ROSPCA algorithm \n             of Hubert et al. (2016) <doi:10.1080/00401706.2015.1093962>."}, "survival": {"categories": ["ClinicalTrials", "Econometrics", "Survival"], "description": "Contains the core survival analysis routines, including\n\t     definition of Surv objects, \n\t     Kaplan-Meier and Aalen-Johansen (multi-state) curves, Cox models,\n\t     and parametric accelerated failure time models."}, "arulesCBA": {"categories": ["ModelDeployment"], "description": "Provides the infrastructure for association rule-based classification including the algorithms \n  CBA, CMAR, CPAR, C4.5, FOIL, PART, PRM, RCAR, and RIPPER to build associative classifiers."}, "MALDIquant": {"categories": ["ChemPhys"], "description": "A complete analysis pipeline for matrix-assisted laser\n        desorption/ionization-time-of-flight (MALDI-TOF) and other\n        two-dimensional mass spectrometry data. In addition to commonly\n        used plotting and processing methods it includes distinctive\n        features, namely baseline subtraction methods such as\n        morphological filters (TopHat) or the statistics-sensitive\n        non-linear iterative peak-clipping algorithm (SNIP), peak\n        alignment using warping functions, handling of replicated\n        measurements as well as allowing spectra with different\n        resolutions."}, "RKEA": {"categories": ["NaturalLanguageProcessing"], "description": "An R interface to KEA (Version 5.0).\n  KEA (for Keyphrase Extraction Algorithm) allows for extracting\n  keyphrases from text documents. It can be either used for free\n  indexing or for indexing with a controlled vocabulary. For more\n  information see <http://www.nzdl.org/Kea/>."}, "PBSmodelling": {"categories": ["DifferentialEquations", "Spatial"], "description": "Provides software to facilitate the design, testing, and operation\n   of computer models. It focuses particularly on tools that make it easy to\n   construct and edit a customized graphical user interface ('GUI'). Although our\n   simplified 'GUI' language depends heavily on the R interface to the 'Tcl/Tk'\n   package, a user does not need to know 'Tcl/Tk'. Examples illustrate models\n   built with other R packages, including 'PBSmapping', 'PBSddesolve', and 'BRugs'. \n   A complete user's guide 'PBSmodelling-UG.pdf' shows how to use this package\n   effectively."}, "CoTiMA": {"categories": ["MetaAnalysis"], "description": "The 'CoTiMA' package performs meta-analyses of correlation matrices of repeatedly measured variables taken from \n   studies that used different time intervals. Different time intervals between measurement occasions impose problems for \n   meta-analyses because the effects (e.g. cross-lagged effects) cannot be simply aggregated, for example, by means of common \n   fixed or random effects analysis. However, continuous time math, which is applied in 'CoTiMA', can be used to extrapolate or \n   intrapolate the results from all studies to any desired time lag. By this, effects obtained in studies that used different \n   time intervals can be meta-analyzed. 'CoTiMA' fits models to empirical data using the structural equation model (SEM) package \n   'ctsem', the effects specified in a SEM are related to parameters that are not directly included in the model (i.e., \n   continuous time parameters; together, they represent the continuous time structural equation model, CTSEM). Statistical \n   model comparisons and significance tests are then performed on the continuous time parameter estimates. 'CoTiMA' also allows \n   analysis of publication bias (Egger's test, PET-PEESE estimates, zcurve analysis etc.) and analysis of statistical power \n   (post hoc power, required sample sizes). See Dormann, C., Guthier, C., & Cortina, J. M. (2019) <doi:10.1177/1094428119847277>.\n   and Guthier, C., Dormann, C., & Voelkle, M. C. (2020) <doi:10.1037/bul0000304>."}, "sparr": {"categories": ["Spatial"], "description": "Provides functions to estimate kernel-smoothed spatial and spatio-temporal densities and relative risk functions, and perform subsequent inference. Methodological details can be found in the accompanying tutorial: Davies et al. (2018) <doi:10.1002/sim.7577>."}, "Boptbd": {"categories": ["CausalInference"], "description": "Computes Bayesian A- and D-optimal block designs under the linear mixed effects model settings \n\tusing block/array exchange algorithm of Debusho, Gemechu and Haines (2018) <doi:10.1080/03610918.2018.1429617> where the interest is in a \n\tcomparison of all possible elementary treatment contrasts. The package also provides an optional \n\tmethod of using the graphical user interface (GUI) R package 'tcltk' to ensure that it is user friendly."}, "SMPracticals": {"categories": ["Survival", "TeachingStatistics"], "description": "Contains the datasets and a few functions for use with \n        the practicals outlined in Appendix A of the book\n        Statistical Models (Davison, 2003, Cambridge University Press).\n        The practicals themselves can be found at\n        <http://statwww.epfl.ch/davison/SM/>."}, "GSODR": {"categories": ["Hydrology"], "description": "Provides automated downloading, parsing, cleaning, unit conversion\n    and formatting of Global Surface Summary of the Day ('GSOD') weather data\n    from the from the USA National Centers for Environmental Information\n    ('NCEI').  Units are converted from from United States Customary System\n    ('USCS') units to International System of Units ('SI').  Stations may be\n    individually checked for number of missing days defined by the user, where\n    stations with too many missing observations are omitted.  Only stations with\n    valid reported latitude and longitude values are permitted in the final\n    data.  Additional useful elements, saturation vapour pressure ('es'), actual\n    vapour pressure ('ea') and relative humidity ('RH') are calculated from the\n    original data using the improved August-Roche-Magnus approximation (Alduchov\n    & Eskridge 1996) and included in the final data set.  The resulting metadata\n    include station identification information, country, state, latitude,\n    longitude, elevation, weather observations and associated flags.  For\n    information on the 'GSOD' data from 'NCEI', please see the 'GSOD'\n    'readme.txt' file available from,\n    <https://www1.ncdc.noaa.gov/pub/data/gsod/readme.txt>."}, "BayesCACE": {"categories": ["Bayesian"], "description": "Performs CACE (Complier Average Causal Effect analysis) on either a single study or meta-analysis of datasets with binary outcomes, using either complete or incomplete noncompliance information. Our package implements the Bayesian methods proposed in Zhou et al. (2019) <doi:10.1111/biom.13028>, which introduces a Bayesian hierarchical model for estimating CACE in meta-analysis of clinical trials with noncompliance, and Zhou et al. (2021) <doi:10.1080/01621459.2021.1900859>, with an application example on Epidural Analgesia."}, "boot": {"categories": ["Econometrics", "Optimization", "Survival", "TimeSeries"], "description": "Functions and datasets for bootstrapping from the\n  book \"Bootstrap Methods and Their Application\" by A. C. Davison and \n  D. V. Hinkley (1997, CUP), originally written by Angelo Canty for S."}, "phonics": {"categories": ["NaturalLanguageProcessing"], "description": "Provides a collection of phonetic algorithms including\n    Soundex, Metaphone, NYSIIS, Caverphone, and others.  The package is\n    documented in <doi:10.18637/jss.v095.i08>."}, "gems": {"categories": ["Survival"], "description": "Simulate and analyze multistate models with general hazard\n    functions. gems provides functionality for the preparation of hazard functions\n    and parameters, simulation from a general multistate model and predicting future\n    events. The multistate model is not required to be a Markov model and may take\n    the history of previous events into account. In the basic version, it allows\n    to simulate from transition-specific hazard function, whose parameters are\n    multivariable normally distributed."}, "InspectChangepoint": {"categories": ["TimeSeries"], "description": "Provides a data-driven projection-based method for estimating changepoints in high-dimensional time series. Multiple changepoints are estimated using a (wild) binary segmentation scheme."}, "GMMBoost": {"categories": ["MachineLearning"], "description": "Likelihood-based boosting approaches for generalized mixed models are provided."}, "R2OpenBUGS": {"categories": ["GraphicalModels"], "description": "Using this package,\n it is possible to call a BUGS model, summarize inferences and\n convergence in a table and graph, and save the simulations in arrays for easy access\n in R. "}, "covid19swiss": {"categories": ["Epidemiology"], "description": "Provides a daily summary of the Coronavirus (COVID-19) cases in Switzerland cantons and Principality of Liechtenstein. Data source: Specialist Unit for Open Government Data Canton of Zurich <https://www.zh.ch/de/politik-staat/opendata.html>."}, "ibmdbR": {"categories": ["ModelDeployment"], "description": "\n        Functionality required to efficiently use R with IBM(R) Db2(R) \n        Warehouse offerings (formerly IBM dashDB(R)) and IBM Db2 for z/OS(R) in \n        conjunction with IBM Db2 Analytics Accelerator for z/OS. \n        Many basic and complex R operations are pushed down into the database, \n        which removes the main memory boundary of R and allows to make full \n        use of parallel processing in the underlying database."}, "condGEE": {"categories": ["Survival"], "description": "Solves for the mean parameters, the variance parameter, and their asymptotic variance in a conditional GEE for recurrent event gap times, as described by Clement and Strawderman (2009) in the journal Biostatistics. Makes a parametric assumption for the length of the censored gap time."}, "PlayerRatings": {"categories": ["SportsAnalytics"], "description": "Implements schemes for estimating player or \n  team skill based on dynamic updating. Implemented methods include \n  Elo, Glicko, Glicko-2 and Stephenson. Contains pdf documentation \n  of a reproducible analysis using approximately two million chess \n  matches. Also contains an Elo based method for multi-player games\n  where the result is a placing or a score. This includes zero-sum\n  games such as poker and mahjong."}, "igraph": {"categories": ["GraphicalModels", "Optimization"], "description": "Routines for simple graphs and network analysis. It can\n  handle large graphs very well and provides functions for generating random\n  and regular graphs, graph visualization, centrality methods and much more."}, "metaLik": {"categories": ["ClinicalTrials", "MetaAnalysis"], "description": "First- and higher-order likelihood inference in\n        meta-analysis and meta-regression models."}, "Rtnmin": {"categories": ["Optimization"], "description": "Truncated Newton function minimization with bounds constraints\n\tbased on the 'Matlab'/'Octave' codes of Stephen Nash."}, "lda": {"categories": ["NaturalLanguageProcessing"], "description": "Implements latent Dirichlet allocation (LDA)\n\t     and related models.  This includes (but is not limited\n\t     to) sLDA, corrLDA, and the mixed-membership stochastic\n\t     blockmodel.  Inference for all of these models is\n\t     implemented via a fast collapsed Gibbs sampler written\n\t     in C.  Utility functions for reading/writing data\n\t     typically used in topic models, as well as tools for\n\t     examining posterior distributions are also included."}, "spectralAnalysis": {"categories": ["ChemPhys"], "description": "\n    Infrared, near-infrared and Raman spectroscopic data measured during chemical reactions, provide structural fingerprints by which molecules can be identified and quantified. The application of these spectroscopic techniques as inline process analytical tools (PAT), provides the (pharma-)chemical industry with novel tools, allowing to monitor their chemical processes, resulting in a better process understanding through insight in reaction rates, mechanistics, stability, etc.\n    Data can be read into R via the generic spc-format, which is generally supported by spectrometer vendor software. Versatile pre-processing functions are available to perform baseline correction by linking to the 'baseline' package; noise reduction via the 'signal' package; as well as time alignment, normalization, differentiation, integration and interpolation. Implementation based on the S4 object system allows storing a pre-processing pipeline as part of a spectral data object, and easily transferring it to other datasets. Interactive plotting tools are provided based on the 'plotly' package. \n    Non-negative matrix factorization (NMF) has been implemented to perform multivariate analyses on individual spectral datasets or on multiple datasets at once. NMF provides a parts-based representation of the spectral data in terms of spectral signatures of the chemical compounds and their relative proportions.\n    The functionality to read in spc-files was adapted from the 'hyperSpec' package."}, "TSrepr": {"categories": ["TimeSeries"], "description": "Methods for representations (i.e. dimensionality reduction, preprocessing, feature extraction) of time series to help more accurate and effective time series data mining.\n    Non-data adaptive, data adaptive, model-based and data dictated (clipped) representation methods are implemented. Also various normalisation methods (min-max, z-score, Box-Cox, Yeo-Johnson),\n    and forecasting accuracy measures are implemented."}, "bcROCsurface": {"categories": ["MissingData"], "description": "The bias-corrected estimation methods for the receiver operating characteristics\n  ROC surface and the volume under ROC surfaces (VUS) under missing at random (MAR)\n  assumption."}, "bartMachine": {"categories": ["Bayesian", "MachineLearning"], "description": "An advanced implementation of Bayesian Additive Regression Trees with expanded features for data analysis and visualization."}, "PopED": {"categories": ["ExperimentalDesign"], "description": "Optimal experimental designs for both population and individual\n    studies based on nonlinear mixed-effect models. Often this is based on a\n    computation of the Fisher Information Matrix. This package was developed\n    for pharmacometric problems, and examples and predefined models are available\n    for these types of systems. The methods are described in Nyberg et al. \n    (2012) <doi:10.1016/j.cmpb.2012.05.005>, and Foracchia et al. (2004) \n    <doi:10.1016/S0169-2607(03)00073-7>."}, "mapdata": {"categories": ["Spatial"], "description": "Supplement to maps package, providing the larger and/or\n\thigher-resolution databases."}, "TestScorer": {"categories": ["Psychometrics"], "description": "GUI for entering test items and obtaining raw\n   and transformed scores. The results are shown on the\n   console and can be saved to a tabular text file for further\n   statistical analysis. The user can define his own tests and\n   scoring procedures through a GUI."}, "poilog": {"categories": ["Distributions"], "description": "Functions for obtaining the density, random deviates \n             and maximum likelihood estimates of the Poisson lognormal \n             distribution and the bivariate Poisson lognormal distribution."}, "Evapotranspiration": {"categories": ["Hydrology"], "description": "Uses data and constants to calculate potential evapotranspiration (PET) and actual evapotranspiration (AET) from 21 different formulations including Penman, Penman-Monteith FAO 56, Priestley-Taylor and Morton formulations."}, "amt": {"categories": ["Tracking"], "description": "Manage and analyze animal movement data. The functionality of 'amt' includes methods to calculate home ranges, track statistics (e.g. step lengths, speed, or turning angles), prepare data for fitting habitat selection analyses, and simulation of space-use from fitted step-selection functions."}, "ncdf4": {"categories": ["Spatial", "SpatioTemporal"], "description": "Provides a high-level R interface to data files written using Unidata's netCDF library (version 4 or earlier), which are binary data files that are portable across platforms and include metadata information in addition to the data sets.  Using this package, netCDF files (either version 4 or \"classic\" version 3) can be opened and data sets read in easily.  It is also easy to create new netCDF dimensions, variables, and files, in either version 3 or 4 format, and manipulate existing netCDF files.  This package replaces the former ncdf package, which only worked with netcdf version 3 files.  For various reasons the names of the functions have had to be changed from the names in the ncdf package.  The old ncdf package is still available at the URL given below, if you need to have backward compatibility.  It should be possible to have both the ncdf and ncdf4 packages installed simultaneously without a problem.  However, the ncdf package does not provide an interface for netcdf version 4 files."}, "JMbayes": {"categories": ["Survival"], "description": "Shared parameter models for the joint modeling of longitudinal and time-to-event data using MCMC; Dimitris Rizopoulos (2016) <doi:10.18637/jss.v072.i07>. "}, "dfmta": {"categories": ["ExperimentalDesign"], "description": "Phase I/II adaptive dose-finding design for single-agent\n   Molecularly Targeted Agent (MTA), according to the paper \"Phase\n   I/II Dose-Finding Design for Molecularly Targeted Agent: Plateau\n   Determination using Adaptive Randomization\", Riviere Marie-Karelle et\n   al. (2016) <doi:10.1177/0962280216631763>."}, "mice": {"categories": ["MissingData"], "description": "Multiple imputation using Fully Conditional Specification (FCS)\n    implemented by the MICE algorithm as described in Van Buuren and\n    Groothuis-Oudshoorn (2011) <doi:10.18637/jss.v045.i03>. Each variable has\n    its own imputation model. Built-in imputation models are provided for\n    continuous data (predictive mean matching, normal), binary data (logistic\n    regression), unordered categorical data (polytomous logistic regression)\n    and ordered categorical data (proportional odds). MICE can also impute\n    continuous two-level data (normal model, pan, second-level variables).\n    Passive imputation can be used to maintain consistency between variables.\n    Various diagnostic plots are available to inspect the quality of the\n    imputations."}, "MM": {"categories": ["Distributions"], "description": "Various utilities for the Multiplicative Multinomial distribution."}, "TP.idm": {"categories": ["Survival"], "description": "Estimation of transition probabilities for the illness-death model. Both the Aalen-Johansen estimator for a Markov model and a novel non-Markovian estimator by de Una-Alvarez and Meira-Machado (2015) <doi:10.1111/biom.12288>, see also Balboa and de Una-Alvarez (2018) <doi:10.18637/jss.v083.i10>, are included."}, "rdatacite": {"categories": ["WebTechnologies"], "description": "Client for the web service methods provided\n    by 'DataCite' (<https://www.datacite.org/>), including functions to interface with\n    their 'RESTful' search API. The API is backed by 'Elasticsearch', allowing\n    expressive queries, including faceting."}, "presize": {"categories": ["ClinicalTrials"], "description": "Bland (2009) <doi:10.1136/bmj.b3985> recommended to\n    base study sizes on the width of the confidence interval rather the power of \n    a statistical test. The goal of 'presize' is to provide functions for such \n    precision based sample size calculations. For a given sample size, the \n    functions will return the precision (width of the confidence interval), and \n    vice versa. "}, "GJRM": {"categories": ["Survival"], "description": "Routines for fitting various joint (and univariate) regression models, with several types of covariate effects, in the presence of equations' errors association, endogeneity, non-random sample selection or partial observability."}, "OptGS": {"categories": ["ExperimentalDesign"], "description": "Functions to find near-optimal multi-stage designs for continuous outcomes."}, "GeneralizedHyperbolic": {"categories": ["Distributions"], "description": "Functions for the hyperbolic and related distributions.\n  Density, distribution and quantile functions and random number generation\n  are provided for the hyperbolic distribution, the generalized hyperbolic\n        distribution, the generalized inverse Gaussian distribution and\n        the skew-Laplace distribution. Additional functionality is\n        provided for the hyperbolic distribution, normal inverse\n\tGaussian distribution and generalized inverse Gaussian distribution,\n\tincluding fitting of these distributions to data. Linear models with\n        hyperbolic errors may be fitted using hyperblmFit."}, "RBtest": {"categories": ["MissingData"], "description": "The regression-based (RB) approach is a method to test the missing data mechanism.\n\t\t\tThis package contains two functions that test the type of missing data (Missing Completely \n\t\t\tAt Random vs Missing At Random) on the basis of the RB approach. The first function applies \n\t\t\tthe RB approach independently on each variable with missing data, using the completely \n\t\t\tobserved variables only. The second function tests the missing data mechanism globally \n\t\t\t(on all variables with missing data) with the use of all available information. The \n\t\t\talgorithm is adapted both to continuous and categorical data. "}, "LaF": {"categories": ["HighPerformanceComputing"], "description": "Methods for fast access to large ASCII files.  Currently the\n    following file formats are supported: comma separated format (CSV) and fixed\n    width format. It is assumed that the files are too large to fit into memory,\n    although the package can also be used to efficiently access files that do\n    fit into memory. Methods are provided to access and process files blockwise.\n    Furthermore, an opened file can be accessed as one would an ordinary\n    data.frame. The LaF vignette gives an overview of the functionality\n    provided."}, "text2vec": {"categories": ["NaturalLanguageProcessing"], "description": "Fast and memory-friendly tools for text vectorization, topic\n    modeling (LDA, LSA), word embeddings (GloVe), similarities. This package\n    provides a source-agnostic streaming API, which allows researchers to perform\n    analysis of collections of documents which are larger than available RAM. All\n    core functions are parallelized to benefit from multicore machines."}, "numDeriv": {"categories": ["NumericalMathematics"], "description": "Methods for calculating (usually) accurate\n\tnumerical first and second order derivatives. Accurate calculations \n\tare done using 'Richardson\u201ds' extrapolation or, when applicable, a\n\tcomplex step derivative is available. A simple difference \n\tmethod is also provided. Simple difference is (usually) less accurate\n\tbut is much quicker than 'Richardson\u201ds' extrapolation and provides a \n\tuseful cross-check. \n\tMethods are provided for real scalar and vector valued functions. "}, "HDShOP": {"categories": ["Finance"], "description": "Constructs shrinkage estimators of high-dimensional mean-variance portfolios and performs \n    high-dimensional tests on optimality of a given portfolio. The techniques developed in \n    Bodnar et al. (2018) <doi:10.1016/j.ejor.2017.09.028>, Bodnar et al. (2019) \n    <doi:10.1109/TSP.2019.2929964>, Bodnar et al. (2020) <doi:10.1109/TSP.2020.3037369> \n    are central to the package. They provide simple and feasible estimators and tests for optimal \n    portfolio weights, which are applicable for 'large p and large n' situations where p is the \n    portfolio dimension (number of stocks) and n is the sample size. The package also includes tools\n    for constructing portfolios based on shrinkage estimators of the mean vector and covariance matrix\n    as well as a new Bayesian estimator for the Markowitz efficient frontier recently developed by \n    Bauder et al. (2021) <doi:10.1080/14697688.2020.1748214>."}, "bayesforecast": {"categories": ["Bayesian", "TimeSeries"], "description": "Fit Bayesian time series models using 'Stan' for full Bayesian inference. A wide range \n  of distributions and models are supported, allowing users to fit Seasonal ARIMA, ARIMAX, Dynamic \n  Harmonic Regression, GARCH, t-student innovation GARCH models, asymmetric GARCH, Random Walks, stochastic \n  volatility models for univariate time series.  Prior specifications are flexible and explicitly encourage \n  users to apply prior distributions that actually reflect their beliefs. Model fit can easily be assessed \n  and compared with typical visualization methods, information criteria such as loglik, AIC, BIC WAIC, Bayes \n  factor and leave-one-out cross-validation methods. References: Hyndman (2017)\n    <doi:10.18637/jss.v027.i03>; Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>."}, "rdrop2": {"categories": ["WebTechnologies"], "description": "Provides full programmatic access to the 'Dropbox' file hosting platform <https://dropbox.com>, including support for all standard file operations."}, "JM": {"categories": ["Survival"], "description": "Shared parameter models for the joint modeling of longitudinal and time-to-event data. "}, "rmatio": {"categories": ["NumericalMathematics", "SpatioTemporal"], "description": "Read and write 'Matlab' MAT files from R. The 'rmatio'\n    package supports reading MAT version 4, MAT version 5 and MAT\n    compressed version 5. The 'rmatio' package can write version 5 MAT\n    files and version 5 files with variable compression."}, "RNetCDF": {"categories": ["Spatial", "SpatioTemporal"], "description": "An interface to the 'NetCDF' file formats designed by Unidata\n  for efficient storage of array-oriented scientific data and descriptions.\n  Most capabilities of 'NetCDF' version 4 are supported. Optional conversions\n  of time units are enabled by 'UDUNITS' version 2, also from Unidata."}, "ewoc": {"categories": ["ClinicalTrials"], "description": "An implementation of a variety of escalation with overdose control designs introduced by Babb, Rogatko and Zacks (1998) <doi:10.1002/(SICI)1097-0258(19980530)17:10%3C1103::AID-SIM793%3E3.0.CO;2-9>. It calculates the next dose as a clinical trial proceeds and performs simulations to obtain operating characteristics."}, "psy": {"categories": ["ChemPhys", "Psychometrics"], "description": "Kappa, ICC, reliability coefficient, parallel analysis, \n     multi-traits multi-methods, spherical representation of a correlation matrix."}, "BetaBit": {"categories": ["TeachingStatistics"], "description": "Three games: proton, frequon and regression. Each one is a console-based data-crunching game for younger and older data scientists.\n  Act as a data-hacker and find Slawomir Pietraszko's credentials to the Proton server.\n  In proton you have to solve four data-based puzzles to find the login and password.\n  There are many ways to solve these puzzles. You may use loops, data filtering, ordering, aggregation or other tools.\n  Only basics knowledge of R is required to play the game, yet the more functions you know, the more approaches you can try.\n  In frequon you will help to perform statistical cryptanalytic attack on a corpus of ciphered messages.\n  This time seven sub-tasks are pushing the bar much higher. Do you accept the challenge?\n  In regression you will test your modeling skills in a series of eight sub-tasks.\n  Try only if ANOVA is your close friend.\n  It's a part of Beta and Bit project.\n  You will find more about the Beta and Bit project at <https://betabit.wiki>."}, "ProjectTemplate": {"categories": ["ReproducibleResearch"], "description": "Provides functions to\n    automatically build a directory structure for a new R\n    project. Using this structure, 'ProjectTemplate'\n    automates data loading, preprocessing, library\n    importing and unit testing."}, "repo": {"categories": ["ReproducibleResearch"], "description": "A data manager meant to avoid manual storage/retrieval of\n    data to/from the file system. It builds one (or more) centralized\n    repository where R objects are stored with rich annotations,\n    including corresponding code chunks, and easily searched and\n    retrieved. See Napolitano (2017) <doi:10.1037/a0028240> for further\n    information."}, "reda": {"categories": ["Survival"], "description": "Contains implementations of recurrent event data analysis routines\n    including (1) survival and recurrent event data simulation from\n    stochastic process point of view by the thinning method\n    proposed by Lewis and Shedler (1979) <doi:10.1002/nav.3800260304>\n    and the inversion method introduced in Cinlar (1975, ISBN:978-0486497976),\n    (2) the mean cumulative function (MCF) estimation by the\n    Nelson-Aalen estimator of the cumulative hazard rate function,\n    (3) two-sample recurrent event responses comparison with the pseudo-score\n    tests proposed by Lawless and Nadeau (1995) <doi:10.2307/1269617>,\n    (4) gamma frailty model with spline rate function following\n    Fu, et al. (2016) <doi:10.1080/10543406.2014.992524>."}, "mpt": {"categories": ["Psychometrics"], "description": "Fitting and testing multinomial processing tree (MPT) models, a\n  class of nonlinear models for categorical data.  The parameters are the\n  link probabilities of a tree-like graph and represent the latent cognitive\n  processing steps executed to arrive at observable response categories\n  (Batchelder & Riefer, 1999 <doi:10.3758/bf03210812>; Erdfelder et al., 2009\n  <doi:10.1027/0044-3409.217.3.108>; Riefer & Batchelder, 1988\n  <doi:10.1037/0033-295x.95.3.318>)."}, "servr": {"categories": ["WebTechnologies"], "description": "Start an HTTP server in R to serve static files, or dynamic\n    documents that can be converted to HTML files (e.g., R Markdown) under a\n    given directory."}, "profvis": {"categories": ["HighPerformanceComputing"], "description": "Interactive visualizations for profiling R code."}, "SDLfilter": {"categories": ["Tracking"], "description": "Functions to filter GPS/Argos locations, as well as assessing the sample size for the analysis of animal distributions. The filters remove temporal and spatial duplicates, fixes located at a given height from estimated high tide line, and locations with high error as described in Shimada et al. (2012) <doi:10.3354/meps09747> and Shimada et al. (2016) <doi:10.1007/s00227-015-2771-0>. Sample size for the analysis of animal distributions can be assessed by the conventional area-based approach or the alternative probability-based approach as described in Shimada et al. (2021) <doi:10.1111/2041-210X.13506>. "}, "SmallCountRounding": {"categories": ["OfficialStatistics"], "description": "A statistical disclosure control tool to protect frequency tables in cases where small values are sensitive. The function PLSrounding() performs small count rounding of necessary inner cells so that all small frequencies of cross-classifications to be published (publishable cells) are rounded. This is equivalent to changing micro data since frequencies of unique combinations are changed. Thus, additivity and consistency are guaranteed. The methodology is described in Langsrud and Heldal (2018) <https://www.researchgate.net/publication/327768398_An_Algorithm_for_Small_Count_Rounding_of_Tabular_Data>."}, "elfDistr": {"categories": ["Distributions"], "description": "Density, distribution function, quantile function\n    and random generation for the Kumaraswamy Complementary Weibull\n    Geometric (Kw-CWG) lifetime probability distribution proposed\n    in Afify, A.Z. et al (2017) <doi:10.1214/16-BJPS322>."}, "ExtremeRisks": {"categories": ["ExtremeValue"], "description": "A set of procedures for estimating risks related to extreme events via risk measures such as Expectile, Value-at-Risk, etc. is provided. Estimation methods for univariate independent observations and temporal dependent observations are available. The methodology is extended to the case of independent multidimensional observations.  The statistical inference is performed through parametric and non-parametric estimators. Inferential procedures such as confidence intervals, confidence regions and hypothesis testing are obtained by exploiting the asymptotic theory. Adapts the methodologies derived in Padoan and Stupfler (2020) <arXiv:2004.04078>, Padoan and Stupfler (2020) <arXiv:2007.08944>, Daouia et al. (2018) <doi:10.1111/rssb.12254>, Drees (2000) <doi:10.1214/aoap/1019487617>, Drees (2003) <doi:10.3150/bj/1066223272>, de Haan and Ferreira (2006) <doi:10.1007/0-387-34471-3>, de Haan et al. (2016) <doi:10.1007/s00780-015-0287-6>."}, "BayesianTools": {"categories": ["Bayesian"], "description": "General-purpose MCMC and SMC samplers, as well as plot and\n    diagnostic functions for Bayesian statistics, with a particular focus on\n    calibrating complex system models. Implemented samplers include various\n    Metropolis MCMC variants (including adaptive and/or delayed rejection MH), the\n    T-walk, two differential evolution MCMCs, two DREAM MCMCs, and a sequential\n    Monte Carlo (SMC) particle filter."}, "peperr": {"categories": ["HighPerformanceComputing", "Survival"], "description": "Designed for prediction error estimation\n        through resampling techniques, possibly accelerated by parallel\n        execution on a compute cluster. Newly developed model fitting\n        routines can be easily incorporated. Methods used in the package are detailed in\n        Porzelius Ch., Binder H. and Schumacher M. (2009) <doi:10.1093/bioinformatics/btp062>\n        and were used, for instance, in\n        Porzelius Ch., Schumacher M.and  Binder H. (2011) <doi:10.1007/s00180-011-0236-6>."}, "smovie": {"categories": ["TeachingStatistics"], "description": "Provides movies to help students to understand statistical \n  concepts.  The 'rpanel' package  <https://cran.r-project.org/package=rpanel> \n  is used to create interactive plots that move to illustrate key statistical \n  ideas and methods.  There are movies to: visualise probability distributions\n  (including user-supplied ones); illustrate sampling distributions of the\n  sample mean (central limit theorem), the median, the sample maximum \n  (extremal types theorem) and (the Fisher transformation of the) \n  product moment correlation coefficient; examine the influence of an \n  individual observation in simple linear regression; illustrate key concepts \n  in statistical hypothesis testing. Also provided are dpqr functions for the \n  distribution of the Fisher transformation of the correlation coefficient \n  under sampling from a bivariate normal distribution."}, "spatial": {"categories": ["Spatial"], "description": "Functions for kriging and point pattern analysis."}, "BayesLogit": {"categories": ["Bayesian"], "description": "Tools for sampling from the PolyaGamma distribution based on Polson, Scott, and Windle (2013) <doi:10.1080/01621459.2013.829001>.  Useful for logistic regression."}, "behaviorchange": {"categories": ["CausalInference"], "description": "Contains specialised analyses and\n    visualisation tools for behavior change science.\n    These facilitate conducting determinant studies\n    (for example, using confidence interval-based\n    estimation of relevance, CIBER, or CIBERlite\n    plots, see Crutzen, Noijen & Peters (2017)\n    <doi:10.3389/fpubh.2017.00165>),\n    systematically developing, reporting,\n    and analysing interventions (for example, using\n    Acyclic Behavior Change Diagrams), and reporting\n    about intervention effectiveness (for example, using\n    the Numbers Needed for Change, see Gruijters & Peters\n    (2017) <doi:10.31234/osf.io/2bau7>), and computing the\n    required sample size (using the Meaningful Change\n    Definition, see Gruijters & Peters (2020)\n    <doi:10.1080/08870446.2020.1841762>).\n    This package is especially useful for\n    researchers in the field of behavior change or\n    health psychology and to behavior change\n    professionals such as intervention developers and\n    prevention workers."}, "nimble": {"categories": ["Bayesian"], "description": "A system for writing hierarchical statistical models largely\n    compatible with 'BUGS' and 'JAGS', writing nimbleFunctions to operate models\n    and do basic R-style math, and compiling both models and nimbleFunctions via\n    custom-generated C++. 'NIMBLE' includes default methods for MCMC, Monte Carlo\n    Expectation Maximization, and some other tools. The nimbleFunction system makes\n    it easy to do things like implement new MCMC samplers from R, customize the\n    assignment of samplers to different parts of a model from R, and compile the\n    new samplers automatically via C++ alongside the samplers 'NIMBLE' provides.\n    'NIMBLE' extends the 'BUGS'/'JAGS' language by making it extensible: New\n    distributions and functions can be added, including as calls to external\n    compiled code. Although most people think of MCMC as the main goal of the\n    'BUGS'/'JAGS' language for writing models, one can use 'NIMBLE' for writing\n    arbitrary other kinds of model-generic algorithms as well. A full User Manual is\n    available at <https://r-nimble.org>."}, "RSurvey": {"categories": ["Spatial"], "description": "A geographic information system (GIS) graphical user interface (GUI) that\n  provides data viewing, management, and analysis tools."}, "snowFT": {"categories": ["HighPerformanceComputing"], "description": "Extension of the snow package supporting fault tolerant and reproducible applications, as well as supporting easy-to-use parallel programming - only one function is needed. Dynamic cluster size is also available."}, "acebayes": {"categories": ["Bayesian", "ExperimentalDesign"], "description": "Optimal Bayesian experimental design using the approximate coordinate exchange (ACE) algorithm. See <doi:10.18637/jss.v095.i13>."}, "move": {"categories": ["SpatioTemporal", "Tracking"], "description": "Contains functions to access movement data stored in 'movebank.org'\n    as well as tools to visualize and statistically analyze animal movement data,\n    among others functions to calculate dynamic Brownian Bridge Movement Models.\n    Move helps addressing movement ecology questions."}, "DistatisR": {"categories": ["Psychometrics"], "description": "Implement DiSTATIS and CovSTATIS (three-way multidimensional scaling). For the  analysis of  multiple distance/covariance matrices collected on the same set of observations."}, "ISwR": {"categories": ["TeachingStatistics"], "description": "Data sets and scripts for text examples and exercises in \n  P. Dalgaard (2008), \u2018Introductory Statistics with R\u2019, 2nd ed., Springer Verlag, ISBN 978-0387790534.  "}, "AzureAuth": {"categories": ["WebTechnologies"], "description": "Provides Azure Active Directory (AAD) authentication functionality for R users of Microsoft's 'Azure' cloud <https://azure.microsoft.com/>. Use this package to obtain 'OAuth' 2.0 tokens for services including Azure Resource Manager, Azure Storage and others. It supports both AAD v1.0 and v2.0, as well as multiple authentication methods, including device code and resource owner grant. Tokens are cached in a user-specific directory obtained using the 'rappdirs' package. The interface is based on the 'OAuth' framework in the 'httr' package, but customised and streamlined for Azure. Part of the 'AzureR' family of packages."}, "mixPHM": {"categories": ["Cluster", "Survival"], "description": "Fits multiple variable mixtures of various parametric proportional hazard models using the EM-Algorithm. Proportionality restrictions can be imposed on the latent groups and/or on the variables. Several survival distributions can be specified. Missing values and censored values are allowed. Independence is assumed over the single variables."}, "pa": {"categories": ["Finance"], "description": "It provides tools for conducting performance attribution for equity portfolios. The package uses two methods: the Brinson method and a regression-based analysis."}, "SharpeR": {"categories": ["Finance"], "description": "A collection of tools for analyzing significance of assets,\n    funds, and trading strategies, based on the Sharpe ratio and overfit \n    of the same. Provides density, distribution, quantile and random generation \n    of the Sharpe ratio distribution based on normal returns, as well\n    as the optimal Sharpe ratio over multiple assets. Computes confidence intervals\n    on the Sharpe and provides a test of equality of Sharpe ratios based on \n    the Delta method. The statistical foundations of the Sharpe can be found in\n    the author's Short Sharpe Course  <doi:10.2139/ssrn.3036276>."}, "popbio": {"categories": ["Environmetrics"], "description": "Construct and analyze projection matrix models from a demography study of marked individuals classified by age or stage. The package covers methods described in Matrix Population Models by Caswell (2001) and Quantitative Conservation Biology by Morris and Doak (2002)."}, "rdwd": {"categories": ["Hydrology"], "description": "Handle climate data from the 'DWD' ('Deutscher Wetterdienst', see \n             <https://www.dwd.de/EN/climate_environment/cdc/cdc_node_en.html> for more information).\n             Choose observational time series from meteorological stations with 'selectDWD()'.\n             Find raster data from radar and interpolation according to <https://bookdown.org/brry/rdwd/raster-data.html>.\n             Download (multiple) data sets with progress bars and no re-downloads through 'dataDWD()'.\n             Read both tabular observational data and binary gridded datasets with 'readDWD()'."}, "imputeMulti": {"categories": ["MissingData"], "description": "Implements imputation methods using EM and Data Augmentation for\n    multinomial data following the work of Schafer 1997 <ISBN: 978-0-412-04061-0>."}, "MMDai": {"categories": ["MissingData"], "description": "A method to impute the missingness in categorical data. Details see the paper <doi:10.4310/SII.2020.v13.n1.a2>."}, "Epi": {"categories": ["Epidemiology", "Survival"], "description": "Functions for demographic and epidemiological analysis in\n  the Lexis diagram, i.e. register and cohort follow-up data. In\n  particular representation, manipulation, rate estimation and\n  simulation for multistate data - the Lexis suite of functions, which\n  includes interfaces to 'mstate', 'etm' and 'cmprsk' packages.\n  Contains functions for Age-Period-Cohort and Lee-Carter modeling and\n  a function for interval censored data and some useful functions for\n  tabulation and plotting, as well as a number of epidemiological data\n  sets."}, "epicontacts": {"categories": ["Epidemiology"], "description": "A collection of tools for representing epidemiological contact data, composed of case line lists and contacts between cases. Also contains procedures for data handling, interactive graphics, and statistics."}, "EBMAforecast": {"categories": ["TimeSeries"], "description": "Create forecasts from multiple predictions using ensemble Bayesian model averaging (EBMA). EBMA models can be estimated using an expectation maximization (EM) algorithm or as fully Bayesian models via Gibbs sampling. The methods in this package are Montgomery, Hollenbach, and Ward (2015) <doi:10.1016/j.ijforecast.2014.08.001> and Montgomery, Hollenbach, and Ward (2012) <doi:10.1093/pan/mps002>."}, "ddalpha": {"categories": ["FunctionalData"], "description": "Contains procedures for depth-based supervised learning, which are entirely non-parametric, in particular the DDalpha-procedure (Lange, Mosler and Mozharovskyi, 2014 <doi:10.1007/s00362-012-0488-4>). The training data sample is transformed by a statistical depth function to a compact low-dimensional space, where the final classification is done. It also offers an extension to functional data and routines for calculating certain notions of statistical depth functions. 50 multivariate and 5 functional classification problems are included. (Pokotylo, Mozharovskyi and Dyckerhoff, 2019 <doi:10.18637/jss.v091.i05>)."}, "ShinyItemAnalysis": {"categories": ["Psychometrics"], "description": "Package including functions and interactive shiny application\n    for the psychometric analysis of educational tests, psychological\n    assessments, health-related and other types of multi-item\n    measurements, or ratings from multiple raters."}, "spatialprobit": {"categories": ["Econometrics", "Spatial"], "description": "Bayesian Estimation of Spatial Probit and Tobit Models."}, "extremevalues": {"categories": ["OfficialStatistics"], "description": "Detect outliers in one-dimensional data."}, "RColorBrewer": {"categories": ["Spatial"], "description": "Provides color schemes for maps (and other graphics)\n        designed by Cynthia Brewer as described at http://colorbrewer2.org."}, "EstimateGroupNetwork": {"categories": ["Psychometrics"], "description": "Can be used to simultaneously estimate networks (Gaussian Graphical Models) in data from different groups or classes via Joint Graphical Lasso. Tuning parameters are selected via information criteria (AIC / BIC / extended BIC) or cross validation."}, "rnrfa": {"categories": ["Hydrology"], "description": "Utility functions to retrieve data from the UK National River Flow\n  Archive (<https://nrfa.ceh.ac.uk/>, terms and conditions:\n  <https://nrfa.ceh.ac.uk/costs-terms-and-conditions>).\n  The package contains R wrappers to the UK NRFA data temporary-API. There are\n  functions to retrieve stations falling in a bounding box, to generate a map\n  and extracting time series and general information. The package is fully\n  described in Vitolo et al (2016) \"rnrfa: An R package to Retrieve, Filter and\n  Visualize Data from the UK National River Flow Archive\"\n  <https://journal.r-project.org/archive/2016/RJ-2016-036/RJ-2016-036.pdf>."}, "XRJulia": {"categories": ["NumericalMathematics"], "description": "A Julia interface structured according to the general\n\t     form described in package 'XR' and in the book \"Extending R\"."}, "GAS": {"categories": ["TimeSeries"], "description": "Simulate, estimate and forecast using univariate and multivariate GAS models \n  as described in Ardia et al. (2019) <doi:10.18637/jss.v088.i06>."}, "paths": {"categories": ["CausalInference"], "description": "In causal mediation analysis with multiple causally ordered mediators, a set of path-specific\n  effects are identified under standard ignorability assumptions. This package implements an imputation\n  approach to estimating these effects along with a set of bias formulas for conducting sensitivity analysis\n  (Zhou and Yamamoto <doi:10.31235/osf.io/2rx6p>). It contains two main functions: paths() for estimating \n  path-specific effects and sens() for conducting sensitivity analysis. Estimation uncertainty is quantified \n  using the nonparametric bootstrap."}, "ctmcmove": {"categories": ["SpatioTemporal", "Tracking"], "description": "Software to facilitates taking movement data in xyt format and pairing it with raster covariates within a continuous time Markov chain (CTMC) framework.  As described in Hanks et al. (2015) <doi:10.1214/14-AOAS803> , this allows flexible modeling of movement in response to covariates (or covariate gradients) with model fitting possible within a Poisson GLM framework. "}, "replicateBE": {"categories": ["ClinicalTrials"], "description": "Performs comparative bioavailability calculations for Average\n    Bioequivalence with Expanding Limits (ABEL). Implemented are 'Method A' /\n    'Method B' and the detection of outliers. If the design allows, assessment\n    of the empiric Type I Error and iteratively adjusting alpha to control the\n    consumer risk. Average Bioequivalence - optionally with a tighter (narrow\n    therapeutic index drugs) or wider acceptance range (South Africa: Cmax) -\n    is implemented as well."}, "poLCA": {"categories": ["Cluster", "Psychometrics"], "description": "Latent class analysis and latent class regression models \n    for polytomous outcome variables.  Also known as latent structure analysis."}, "nmaINLA": {"categories": ["MetaAnalysis"], "description": "Performs network meta-analysis using integrated nested Laplace approximations ('INLA') \n             which is described in Guenhan, Held, and Friede (2018) <doi:10.1002/jrsm.1285>. \n             Includes methods to assess the heterogeneity and inconsistency in the network. \n             Contains more than ten different network meta-analysis dataset. \n             'INLA' package can be obtained from <https://www.r-inla.org>. "}, "rpart": {"categories": ["Environmetrics", "MachineLearning", "Survival"], "description": "Recursive partitioning for classification, \n  regression and survival trees.  An implementation of most of the \n  functionality of the 1984 book by Breiman, Friedman, Olshen and Stone."}, "vapour": {"categories": ["Spatial"], "description": "Provides low-level access to 'GDAL' functionality for R packages.  \n  'GDAL' is the 'Geospatial Data Abstraction Library' a translator for raster and vector geospatial data formats \n  that presents a single raster abstract data model and single vector abstract data model to the calling application \n  for all supported formats <https://gdal.org/>. "}, "huge": {"categories": ["GraphicalModels"], "description": "Provides a general framework for\n        high-dimensional undirected graph estimation. It integrates\n        data preprocessing, neighborhood screening, graph estimation,\n        and model selection techniques into a pipeline. In\n        preprocessing stage, the nonparanormal(npn) transformation is\n        applied to help relax the normality assumption. In the graph\n        estimation stage, the graph structure is estimated by\n        Meinshausen-Buhlmann graph estimation or the graphical lasso,\n        and both methods can be further accelerated by the lossy\n        screening rule preselecting the neighborhood of each variable\n        by correlation thresholding. We target on high-dimensional data\n        analysis usually d >> n, and the computation is\n        memory-optimized using the sparse matrix output. We also\n        provide a computationally efficient approach, correlation\n        thresholding graph estimation. Three\n        regularization/thresholding parameter selection methods are\n        included in this package: (1)stability approach for\n        regularization selection (2) rotation information criterion (3)\n        extended Bayesian information criterion which is only available\n        for the graphical lasso."}, "trapezoid": {"categories": ["Distributions"], "description": "The trapezoid package provides 'dtrapezoid', 'ptrapezoid', 'qtrapezoid',\n    and 'rtrapezoid' functions for the trapezoidal distribution."}, "REDCapR": {"categories": ["WebTechnologies"], "description": "Encapsulates functions to streamline calls from R to the REDCap\n    API.  REDCap (Research Electronic Data CAPture) is a web application for\n    building and managing online surveys and databases developed at Vanderbilt\n    University.  The Application Programming Interface (API) offers an avenue\n    to access and modify data programmatically, improving the capacity for\n    literate and reproducible programming."}, "ArDec": {"categories": ["TimeSeries"], "description": "Autoregressive-based decomposition of a time series based on the approach in West (1997). Particular cases include the extraction of trend and seasonal components."}, "nsRFA": {"categories": ["Environmetrics", "Hydrology"], "description": "A collection of statistical tools for objective (non-supervised) applications \n             of the Regional Frequency Analysis methods in hydrology. \n             The package refers to the index-value method and, more precisely, helps the\n             hydrologist to: (1) regionalize the index-value; (2) form homogeneous regions \n             with similar growth curves; (3) fit distribution functions to the \n             empirical regional growth curves.\n             Most of the methods are those described in the Flood Estimation Handbook \n            (Centre for Ecology & Hydrology, 1999, ISBN:9781906698003).\n             Homogeneity tests from Hosking and Wallis (1993) <doi:10.1029/92WR01980> \n             and Viglione et al. (2007) <doi:10.1029/2006WR005095> are available."}, "DFIT": {"categories": ["Psychometrics"], "description": "A set of functions to perform Raju, van der Linden and Fleer's\n    (1995, <doi:10.1177/014662169501900405>) Differential Functioning of Items\n    and Tests (DFIT) analyses. It includes functions to use the Monte Carlo Item\n    Parameter Replication approach (Oshima, Raju, & Nanda, 2006, <doi:10.1111/j.1745-3984.2006.00001.x>)\n    for obtaining the associated statistical significance\n    tests cut-off points. They may also be used for a priori and post-hoc power\n    calculations (Cervantes, 2017, <doi:10.18637/jss.v076.i05>)."}, "brglm": {"categories": ["Econometrics"], "description": "Fit generalized linear models with binomial responses using either an adjusted-score approach to bias reduction or maximum penalized likelihood where penalization is by Jeffreys invariant prior. These procedures return estimates with improved frequentist properties (bias, mean squared error) that are always finite even in cases where the maximum likelihood estimates are infinite (data separation). Fitting takes place by fitting generalized linear models on iteratively updated pseudo-data. The interface is essentially the same as 'glm'.  More flexibility is provided by the fact that custom pseudo-data representations can be specified and used for model fitting. Functions are provided for the construction of confidence intervals for the reduced-bias estimates."}, "future.apply": {"categories": ["HighPerformanceComputing"], "description": "Implementations of apply(), by(), eapply(), lapply(), Map(), .mapply(), mapply(), replicate(), sapply(), tapply(), and vapply() that can be resolved using any future-supported backend, e.g. parallel on the local machine or distributed on a compute cluster.  These future_*apply() functions come with the same pros and cons as the corresponding base-R *apply() functions but with the additional feature of being able to be processed via the future framework."}, "paran": {"categories": ["Psychometrics"], "description": "An implementation of Horn's technique for numerically and graphically evaluating the components or factors retained in a principle components analysis (PCA) or common factor analysis (FA). Horn's method contrasts eigenvalues produced through a PCA or FA on a number of random data sets of uncorrelated variables with the same number of variables and observations as the experimental or observational data set to produce eigenvalues for components or factors that are adjusted for the sample error-induced inflation. Components with adjusted eigenvalues greater than one are retained. paran may also be used to conduct parallel analysis following Glorfeld's (1995) suggestions to reduce the likelihood of over-retention."}, "lslx": {"categories": ["Psychometrics"], "description": "Fits semi-confirmatory structural equation modeling (SEM) via penalized likelihood (PL) or penalized least squares (PLS). For details, please see Huang (2020) <doi:10.18637/jss.v093.i07>."}, "tigris": {"categories": ["Spatial"], "description": "Download TIGER/Line shapefiles from the United States Census Bureau \n    (<https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html>) \n    and load into R as 'sf' objects."}, "mdmb": {"categories": ["MissingData"], "description": "\n    Contains model-based treatment of missing data for regression \n    models with missing values in covariates or the dependent \n    variable using maximum likelihood or Bayesian estimation \n    (Ibrahim et al., 2005; <doi:10.1198/016214504000001844>;\n    Luedtke, Robitzsch, & West, 2020a, 2020b;\n    <doi:10.1080/00273171.2019.1640104><doi:10.1037/met0000233>).\n    The regression model can be nonlinear (e.g., interaction \n    effects, quadratic effects or B-spline functions). \n    Multilevel models with missing data in predictors are\n    available for Bayesian estimation. Substantive-model compatible \n    multiple imputation can be also conducted."}, "bootnet": {"categories": ["Psychometrics"], "description": "Bootstrap methods to assess accuracy and stability of estimated network structures\n              and centrality indices <doi:10.3758/s13428-017-0862-1>. Allows for flexible \n              specification of any undirected network estimation procedure in R, and offers \n              default sets for various estimation routines."}, "kst": {"categories": ["Psychometrics"], "description": "Knowledge space theory by Doignon and Falmagne (1999) \n   <doi:10.1007/978-3-642-58625-5> is a set- and order-theoretical\n   framework, which proposes mathematical formalisms to operationalize \n   knowledge structures in a particular domain. The 'kst' package provides \n   basic functionalities to generate, handle, and manipulate knowledge \n   structures and knowledge spaces."}, "evtree": {"categories": ["MachineLearning"], "description": "Commonly used classification and regression tree methods like the CART algorithm\n             are recursive partitioning methods that build the model in a forward stepwise search.\n\t     Although this approach is known to be an efficient heuristic, the results of recursive\n\t     tree methods are only locally optimal, as splits are chosen to maximize homogeneity at\n\t     the next step only. An alternative way to search over the parameter space of trees is\n\t     to use global optimization methods like evolutionary algorithms. The 'evtree' package\n\t     implements an evolutionary algorithm for learning globally optimal classification and\n\t     regression trees in R. CPU and memory-intensive tasks are fully computed in C++ while\n\t     the 'partykit' package is leveraged to represent the resulting trees in R, providing\n\t     unified infrastructure for summaries, visualizations, and predictions."}, "EUfootball": {"categories": ["SportsAnalytics"], "description": "Contains match results from seven European men's football leagues, namely Premier League (England), Ligue 1 (France), \n\t     Bundesliga (Germany), Serie A (Italy), Primera Division (Spain), Eredivisie (The Netherlands), Super Lig (Turkey).\n\t     Includes Seasons 2010/2011 until 2019/2020 and a set of interesting covariates. Can be used all purposes."}, "Rssa": {"categories": ["TimeSeries"], "description": "Methods and tools for Singular Spectrum Analysis including decomposition,\n             forecasting and gap-filling for univariate and multivariate time series.\n             General description of the methods with many examples can be found in the book\n             Golyandina (2018, <doi:10.1007/978-3-662-57380-8>).\n             See 'citation(\"Rssa\")' for details."}, "momentuHMM": {"categories": ["MissingData", "SpatioTemporal", "Tracking"], "description": "Extended tools for analyzing telemetry data using generalized hidden Markov models. Features of momentuHMM (pronounced \u201cmomentum\u201d) include data pre-processing and visualization, fitting HMMs to location and auxiliary biotelemetry or environmental data, biased and correlated random walk movement models, hierarchical HMMs, multiple imputation for incorporating location measurement error and missing data, user-specified design matrices and constraints for covariate modelling of parameters, random effects, decoding of the state process, visualization of fitted models, model checking and selection, and simulation. See McClintock and Michelot (2018) <doi:10.1111/2041-210X.12995>."}, "PRISMAstatement": {"categories": ["MetaAnalysis"], "description": "Plot a PRISMA <http://prisma-statement.org/> flow\n    chart describing the identification, screening, eligibility and\n    inclusion or studies in systematic reviews. The PRISMA statement\n    defines an evidence-based, minimal set of items for reporting in\n    systematic reviews and meta-analyses. PRISMA should be used for the\n    reporting of studies evaluating randomized clinical trials (RCT), and\n    is also for reporting on systematic reviews of other types of\n    research. There is also a function to generate flow charts describing\n    exclusions and inclusions for any kind of study."}, "statmod": {"categories": ["Distributions", "NumericalMathematics"], "description": "A collection of algorithms and functions to aid statistical modeling. Includes limiting dilution analysis (aka ELDA), growth curve comparisons, mixed linear models, heteroscedastic regression, inverse-Gaussian probability calculations, Gauss quadrature and a secure convergence algorithm for nonlinear models. Also includes advanced generalized linear model functions including Tweedie and Digamma distributional families and a secure convergence algorithm."}, "speff2trial": {"categories": ["ClinicalTrials"], "description": "Performs estimation and testing of the treatment effect in a 2-group randomized clinical trial with a quantitative, dichotomous, or right-censored time-to-event endpoint. The method improves efficiency by leveraging baseline predictors of the endpoint. The inverse probability weighting technique of Robins, Rotnitzky, and Zhao (JASA, 1994) is used to provide unbiased estimation when the endpoint is missing at random."}, "gsl": {"categories": ["NumericalMathematics", "Optimization"], "description": "\n An R wrapper for some of the functionality of the\n Gnu Scientific Library."}, "xslt": {"categories": ["WebTechnologies"], "description": "An extension for the 'xml2' package to transform XML documents\n    by applying an 'xslt' style-sheet."}, "R2BEAT": {"categories": ["OfficialStatistics"], "description": "Multivariate optimal allocation for different domains in one and two stages stratified sample design. R2BEAT extends the Neyman (1934) \u2013 Tschuprow (1923) allocation method to the case of several variables, adopting a generalization of the Bethel\u2019s proposal (1989).R2BEAT develops this methodology but, moreover, it allows to determine the sample allocation in the multivariate and multi-domains case of estimates for two-stage stratified samples. It also allows to perform both Primary Stage Units and Secondary Stage Units selection. This package requires the availability of ReGenesees, that can be installed from <https://github.com/DiegoZardetto/ReGenesees>."}, "aws.signature": {"categories": ["WebTechnologies"], "description": "Generates version 2 and version 4 request signatures for Amazon Web Services ('AWS') <https://aws.amazon.com/> Application Programming Interfaces ('APIs') and provides a mechanism for retrieving credentials from environment variables, 'AWS' credentials files, and 'EC2' instance metadata. For use on 'EC2' instances, users will need to install the suggested package 'aws.ec2metadata' <https://cran.r-project.org/package=aws.ec2metadata>."}, "ADPclust": {"categories": ["Cluster"], "description": "An implementation of ADPclust clustering procedures (Fast\n    Clustering Using Adaptive Density Peak Detection). The work is built and\n    improved upon the idea of Rodriguez and Laio (2014)<doi:10.1126/science.1242072>. \n    ADPclust clusters data by finding density peaks in a density-distance plot \n    generated from local multivariate Gaussian density estimation. It includes \n    an automatic centroids selection and parameter optimization algorithm, which \n    finds the number of clusters and cluster centroids by comparing average \n    silhouettes on a grid of testing clustering results; It also includes a user \n    interactive algorithm that allows the user to manually selects cluster \n    centroids from a two dimensional \"density-distance plot\". Here is the \n    research article associated with this package: \"Wang, Xiao-Feng, and \n    Yifan Xu (2015)<doi:10.1177/0962280215609948> Fast clustering using adaptive \n    density peak detection.\" Statistical methods in medical research\". url:\n    http://smm.sagepub.com/content/early/2015/10/15/0962280215609948.abstract. "}, "Rlibeemd": {"categories": ["TimeSeries"], "description": "An R interface for libeemd (Luukko, Helske, R\u00e4s\u00e4nen, 2016) <doi:10.1007/s00180-015-0603-9>, \n    a C library of highly efficient parallelizable functions  for performing the ensemble empirical mode decomposition (EEMD), \n    its complete variant (CEEMDAN), the regular empirical mode decomposition (EMD), and bivariate EMD (BEMD). \n    Due to the possible portability issues CRAN version no longer supports OpenMP, you can install OpenMP-supported version \n    from GitHub: <https://github.com/helske/Rlibeemd/>."}, "SimilarityMeasures": {"categories": ["SpatioTemporal", "Tracking"], "description": "Functions to run and assist four\n  different similarity measures. The similarity\n  measures included are: longest common\n  subsequence (LCSS), Frechet distance, edit distance\n  and dynamic time warping (DTW). Each of these\n  similarity measures can be calculated from two\n  n-dimensional trajectories, both in matrix form."}, "ranger": {"categories": ["MachineLearning", "Survival"], "description": "A fast implementation of Random Forests, particularly suited for high\n          dimensional data. Ensembles of classification, regression, survival and\n          probability prediction trees are supported. Data from genome-wide association\n          studies can be analyzed efficiently. In addition to data frames, datasets of\n          class 'gwaa.data' (R package 'GenABEL') and 'dgCMatrix' (R package 'Matrix') \n          can be directly analyzed."}, "MODISTools": {"categories": ["Hydrology"], "description": "Programmatic interface to the Oak Ridge National Laboratories\n    'MODIS Land Products Subsets' web services \n    (<https://modis.ornl.gov/data/modis_webservice.html>). Allows for easy\n    downloads of 'MODIS' time series directly to your R workspace or\n    your computer."}, "rnoaa": {"categories": ["Hydrology"], "description": "Client for many 'NOAA' data sources including the 'NCDC' climate\n    'API' at <https://www.ncdc.noaa.gov/cdo-web/webservices/v2>, with functions for\n    each of the 'API' 'endpoints': data, data categories, data sets, data types,\n    locations, location categories, and stations. In addition, we have an interface\n    for 'NOAA' sea ice data, the 'NOAA' severe weather inventory, 'NOAA' Historical\n    Observing 'Metadata' Repository ('HOMR') data, 'NOAA' storm data via 'IBTrACS',\n    tornado data via the 'NOAA' storm prediction center, and more."}, "TSANN": {"categories": ["TimeSeries"], "description": "The best ANN structure for time series data analysis is a demanding need in the present era.\n    This package will find the best-fitted ANN model based on forecasting accuracy.\n    The optimum size of the hidden layers was also determined after determining the number of lags to be included.\n    This package has been developed using the algorithm of Paul and Garai (2021) <doi:10.1007/s00500-021-06087-4>."}, "RcppParallel": {"categories": ["HighPerformanceComputing"], "description": "High level functions for parallel programming with 'Rcpp'.\n    For example, the 'parallelFor()' function can be used to convert the work of\n    a standard serial \"for\" loop into a parallel one and the 'parallelReduce()'\n    function can be used for accumulating aggregate or other values."}, "roadoi": {"categories": ["WebTechnologies"], "description": "This web client interfaces Unpaywall <https://unpaywall.org/products/api>, formerly\n    oaDOI, a service finding free full-texts of academic papers by linking DOIs with \n    open access journals and repositories. It provides unified access to various data sources \n    for open access full-text links including Crossref and the Directory of Open Access \n    Journals (DOAJ). API usage is free and no registration is required."}, "geouy": {"categories": ["Spatial"], "description": "The toolbox have functions to load and process geographic information for Uruguay. \n        And extra-function to get address coordinates and orthophotos through the uruguayan 'IDE' API <https://www.gub.uy/infraestructura-datos-espaciales/tramites-y-servicios/servicios/servicio-direcciones-geograficas>."}, "NISTunits": {"categories": ["ChemPhys"], "description": "Fundamental physical constants (Quantity, Value, Uncertainty, Unit) for \n    SI (International System of Units) and non-SI units, plus unit conversions\n    Based on the data from NIST (National Institute of Standards and Technology, USA)"}, "BurStMisc": {"categories": ["Finance"], "description": "Script search, corner, genetic optimization, permutation tests, write expect test."}, "RPostgres": {"categories": ["Databases"], "description": "Fully DBI-compliant Rcpp-backed interface to\n    PostgreSQL <https://www.postgresql.org/>, an open-source relational\n    database."}, "feasts": {"categories": ["TimeSeries"], "description": "Provides a collection of features, decomposition methods, \n    statistical summaries and graphics functions for the analysing tidy time\n    series data. The package name 'feasts' is an acronym comprising of its key\n    features: Feature Extraction And Statistics for Time Series."}, "msmtools": {"categories": ["Survival"], "description": "A fast and general method for restructuring classical longitudinal data into\n    augmented ones. The reason for this is to facilitate the modeling of longitudinal data under\n    a multi-state framework using the 'msm' package."}, "metasens": {"categories": ["ClinicalTrials", "MetaAnalysis", "MissingData"], "description": "The following methods are implemented to evaluate how sensitive the results of a meta-analysis are to potential bias in meta-analysis and to support Schwarzer et al. (2015) <doi:10.1007/978-3-319-21416-0>, Chapter 5 'Small-Study Effects in Meta-Analysis':\n - Copas selection model described in Copas & Shi (2001) <doi:10.1177/096228020101000402>;\n - limit meta-analysis by R\u00fccker et al. (2011) <doi:10.1093/biostatistics/kxq046>;\n - upper bound for outcome reporting bias by Copas & Jackson (2004) <doi:10.1111/j.0006-341X.2004.00161.x>;\n - imputation methods for missing binary data by Gamble & Hollis (2005) <doi:10.1016/j.jclinepi.2004.09.013> and Higgins et al. (2008) <doi:10.1177/1740774508091600>;\n - LFK index test and Doi plot by Furuya-Kanamori et al. (2018) <doi:10.1097/XEB.0000000000000141>."}, "splancs": {"categories": ["Spatial", "SpatioTemporal"], "description": "The Splancs package was written as an enhancement to S-Plus for display and analysis of spatial point pattern data; it has been ported to R and is in \"maintenance mode\". "}, "fTrading": {"categories": ["Finance"], "description": "A collection of functions for trading and rebalancing financial\n\tinstruments. It implements various technical indicators to analyse time series such\n\tas moving averages or stochastic oscillators."}, "filling": {"categories": ["MissingData"], "description": "Filling in the missing entries of a partially observed data is one of fundamental problems in various disciplines of mathematical science. For many cases, data at our interests have canonical form of matrix in that the problem is posed upon a matrix with missing values to fill in the entries under preset assumptions and models. We provide a collection of methods from multiple disciplines under Matrix Completion, Imputation, and Inpainting. See Davenport and Romberg (2016) <doi:10.1109/JSTSP.2016.2539100> for an overview of the topic."}, "micEconSNQP": {"categories": ["Econometrics"], "description": "Tools for econometric production analysis\n   with the Symmetric Normalized Quadratic (SNQ) profit function,\n   e.g. estimation, imposing convexity in prices,\n   and calculating elasticities and shadow prices."}, "oro.nifti": {"categories": ["MedicalImaging"], "description": "Functions for the input/output and visualization of\n    medical imaging data that follow either the 'ANALYZE', 'NIfTI' or 'AFNI'\n    formats.  This package is part of the Rigorous Analytics bundle."}, "anipaths": {"categories": ["SpatioTemporal", "Tracking"], "description": "Animation of observed trajectories using spline-based interpolation (see for example, Buderman, F. E., Hooten, M. B., Ivan, J. S. and Shenk, T. M. (2016), <doi:10.1111/2041-210X.12465> \"A functional model for characterizing long-distance movement behaviour\". Methods Ecol Evol). Intended to be used exploratory data analysis, and perhaps for preparation of presentations."}, "gap": {"categories": ["MetaAnalysis"], "description": "As first reported [Zhao, J. H. 2007. \"gap: Genetic Analysis Package\". J Stat Soft 23(8):1-18.\n        <doi:10.18637/jss.v023.i08>], it is designed as an integrated package for genetic data\n        analysis of both population and family data. Currently, it contains functions for\n        sample size calculations of both population-based and family-based designs, probability\n        of familial disease aggregation, kinship calculation, statistics in linkage analysis,\n        and association analysis involving genetic markers including haplotype analysis with or\n        without environmental covariates. Over years, the package has been developed in-between\n        many projects hence also in line with the name (gap)."}, "basicMCMCplots": {"categories": ["Bayesian"], "description": "Provides methods for examining posterior MCMC samples\n    from a single chain using trace plots and density plots, and from\n    multiple chains by comparing posterior medians and credible intervals\n    from each chain.  These plotting functions have a variety of options,\n    such as figure sizes, legends, parameters to plot, and saving plots to file.\n    Functions interface with the NIMBLE software package, see\n    de Valpine, Turek, Paciorek, Anderson-Bergman, Temple Lang and Bodik (2017)\n    <doi:10.1080/10618600.2016.1172487>."}, "mvmeta": {"categories": ["MetaAnalysis"], "description": "Collection of functions to perform fixed and random-effects multivariate and univariate meta-analysis and meta-regression."}, "apt": {"categories": ["Econometrics"], "description": "Asymmetric price transmission between two time series is assessed. Several functions are available for linear and nonlinear threshold cointegration, and furthermore, symmetric and asymmetric error correction model."}, "SensoMineR": {"categories": ["ExperimentalDesign", "Psychometrics"], "description": "Statistical Methods to Analyse Sensory Data. SensoMineR: A package for sensory data analysis. S. Le and F. Husson (2008) <doi:10.1111/j.1745-459X.2007.00137.x>."}, "tm": {"categories": ["HighPerformanceComputing", "NaturalLanguageProcessing"], "description": "A framework for text mining applications within R."}, "nsprcomp": {"categories": ["Psychometrics"], "description": "Two methods for performing a constrained principal\n        component analysis (PCA), where non-negativity and/or sparsity\n        constraints are enforced on the principal axes (PAs). The\n        function 'nsprcomp' computes one principal component (PC) after\n        the other. Each PA is optimized such that the corresponding PC\n        has maximum additional variance not explained by the previous\n        components. In contrast, the function 'nscumcomp' jointly\n        computes all PCs such that the cumulative variance is maximal.\n        Both functions have the same interface as the 'prcomp' function\n        from the 'stats' package (plus some extra parameters), and both\n        return the result of the analysis as an object of class\n        'nsprcomp', which inherits from 'prcomp'. See\n        <https://sigg-iten.ch/learningbits/2013/05/27/nsprcomp-is-on-cran/>\n        and Sigg et al. (2008) <doi:10.1145/1390156.1390277> for more\n        details."}, "quantoptr": {"categories": ["CausalInference"], "description": "Estimation methods for optimal treatment regimes under three different criteria, namely marginal quantile, marginal mean, and mean absolute difference. For the first two criteria, both one-stage and two-stage estimation method are implemented. A doubly robust estimator for estimating the quantile-optimal treatment regime is also included. "}, "misaem": {"categories": ["MissingData"], "description": "Estimate parameters of linear regression and logistic regression with missing covariates with missing data, perform model selection and prediction, using EM-type algorithms. Jiang W., Josse J., Lavielle M., TraumaBase Group (2020) <doi:10.1016/j.csda.2019.106907>."}, "sweep": {"categories": ["TimeSeries"], "description": "\n    Tidies up the forecasting modeling and prediction work flow, \n    extends the 'broom' package \n    with 'sw_tidy', 'sw_glance', 'sw_augment', and 'sw_tidy_decomp' functions \n    for various forecasting models,\n    and enables converting 'forecast' objects to \n    \"tidy\" data frames with 'sw_sweep'."}, "threshr": {"categories": ["ExtremeValue"], "description": "Provides functions for the selection of thresholds for use in \n    extreme value models, based mainly on the methodology in \n    Northrop, Attalides and Jonathan (2017) <doi:10.1111/rssc.12159>.\n    It also performs predictive inferences about future extreme values, \n    based either on a single threshold or on a weighted average of inferences \n    from multiple thresholds, using the 'revdbayes' package \n    <https://cran.r-project.org/package=revdbayes>.   \n    At the moment only the case where the data can be treated as \n    independent identically distributed observations is considered."}, "osmdata": {"categories": ["Spatial"], "description": "Download and import of 'OpenStreetMap' ('OSM') data as 'sf'\n    or 'sp' objects.  'OSM' data are extracted from the 'Overpass' web\n    server (<https://overpass-api.de/>) and processed with very fast 'C++'\n    routines for return to 'R'."}, "ebal": {"categories": ["CausalInference"], "description": "Package implements entropy balancing, a data preprocessing procedure described in Hainmueller (2008, <doi:10.1093/pan/mpr025>) that allows users to reweight a dataset such that the covariate distributions in the reweighted data satisfy a set of user specified moment conditions. This can be useful to create balanced samples in observational studies with a binary treatment where the control group data can be reweighted to match the covariate moments in the treatment group. Entropy balancing can also be used to reweight a survey sample to known characteristics from a target population."}, "clValid": {"categories": ["Cluster"], "description": "Statistical and biological validation of clustering results. This package implements Dunn Index, Silhouette, Connectivity, Stability, BHI and BSI. Further information can be found in Brock, G et al. (2008) <doi:10.18637/jss.v025.i04>."}, "RcppDL": {"categories": ["MachineLearning"], "description": "This package is based on the C++ code from Yusuke Sugomori,\n             which implements basic machine learning methods with \n             many layers (deep learning), including dA (Denoising Autoencoder), \n             SdA (Stacked Denoising Autoencoder), RBM (Restricted Boltzmann machine) and \n             DBN (Deep Belief Nets)."}, "multipol": {"categories": ["NumericalMathematics"], "description": "Various utilities to manipulate multivariate polynomials."}, "survC1": {"categories": ["Survival"], "description": "Performs inference for C of risk prediction models with censored survival data, using the method proposed by Uno et al. (2011) <doi:10.1002/sim.4154>. Inference for the difference in C between two competing prediction models is also implemented."}, "hwwntest": {"categories": ["TimeSeries"], "description": "Provides methods to test whether time series is consistent\n\twith white noise."}, "maxLik": {"categories": ["Optimization"], "description": "Functions for Maximum Likelihood (ML) estimation, non-linear\n   optimization, and related tools.  It includes a unified way to call\n   different optimizers, and classes and methods to handle the results from\n   the Maximum Likelihood viewpoint.  It also includes a number of convenience\n   tools for testing and developing your own models."}, "mlbstats": {"categories": ["SportsAnalytics"], "description": "Computational functions for player metrics in major league baseball including batting, pitching, fielding, base-running, and overall player statistics. This package is actively maintained with new metrics being added as they are developed."}, "Iso": {"categories": ["ChemPhys"], "description": "Linear order and unimodal order (univariate)\n\t     isotonic regression; bivariate isotonic regression\n\t     with linear order on both variables."}, "ipcwswitch": {"categories": ["CausalInference"], "description": "Contains functions for formatting clinical trials data and implementing inverse probability of censoring weights to handle treatment switches when estimating causal treatment effect in randomized clinical trials."}, "rtrim": {"categories": ["OfficialStatistics"], "description": "The TRIM model is widely used for estimating growth and decline of\n    animal populations based on (possibly sparsely available) count data. The\n    current package is a reimplementation of the original TRIM software developed\n    at Statistics Netherlands by Jeroen Pannekoek. See\n    <https://www.cbs.nl/en-gb/society/nature-and-environment/indices-and-trends%2d%2dtrim%2d%2d>\n    for more information about TRIM."}, "rLiDAR": {"categories": ["Spatial"], "description": "Set of tools for reading, processing and visualizing small set of\n    LiDAR (Light Detection and Ranging) data for forest inventory applications.\n    More details were published in Silva et al. (2016) <doi:10.1080/07038992.2016.1196582>."}, "NetworkChange": {"categories": ["Bayesian"], "description": "Network changepoint analysis for undirected network data. The package implements a hidden Markov network change point model (Park and Sohn (2020)). Functions for break number detection using the approximate marginal likelihood and WAIC are also provided."}, "metaMA": {"categories": ["MetaAnalysis"], "description": "Combination of either p-values or modified effect sizes from different\n    studies to find differentially expressed genes."}, "matchingMarkets": {"categories": ["Bayesian", "Econometrics", "Finance", "Optimization"], "description": "Implements structural estimators to correct for\n    the sample selection bias from observed outcomes in matching\n    markets. This includes one-sided matching of agents into\n    groups as well as two-sided matching of students to schools.\n    The package also contains algorithms to find stable matchings\n    in the three most common matching problems: the stable roommates\n    problem, the college admissions problem, and the house\n    allocation problem."}, "redux": {"categories": ["Databases"], "description": "A 'hiredis' wrapper that includes support for\n    transactions, pipelining, blocking subscription, serialisation of\n    all keys and values, 'Redis' error handling with R errors.\n    Includes an automatically generated 'R6' interface to the full\n    'hiredis' API.  Generated functions are faithful to the\n    'hiredis' documentation while attempting to match R's argument\n    semantics.  Serialisation must be explicitly done by the user, but\n    both binary and text-mode serialisation is supported."}, "SuppDists": {"categories": ["Distributions"], "description": "Ten distributions supplementing those built into R.\n  Inverse Gauss, Kruskal-Wallis, Kendall's Tau, Friedman's chi\n  squared, Spearman's rho, maximum F ratio, the Pearson product\n  moment correlation coefficient, Johnson distributions, normal\n  scores and generalized hypergeometric distributions."}, "speaq": {"categories": ["ChemPhys"], "description": "Makes Nuclear Magnetic Resonance spectroscopy (NMR spectroscopy) data analysis as easy as possible by only requiring a small set of functions to perform an entire analysis. 'speaq' offers the possibility of raw spectra alignment and quantitation but also an analysis based on features whereby the spectra are converted to peaks which are then grouped and turned into features. These features can be processed with any number of statistical tools either included in 'speaq' or available elsewhere on CRAN. More details can be found in Vu et al. (2011) <doi:10.1186/1471-2105-12-405> and Beirnaert et al. (2018) <doi:10.1371/journal.pcbi.1006018>. "}, "effsize": {"categories": ["MetaAnalysis"], "description": "A collection of functions to compute the standardized \n  effect sizes for experiments (Cohen d, Hedges g, Cliff delta, Vargha-Delaney A). \n  The computation algorithms have been optimized to allow efficient computation even \n  with very large data sets."}, "LexisPlotR": {"categories": ["Survival"], "description": "Plots empty Lexis grids, adds lifelines and highlights certain areas of the grid, like cohorts and age groups."}, "GAD": {"categories": ["ExperimentalDesign"], "description": "This package analyses complex ANOVA models with any\n        combination of orthogonal/nested and fixed/random factors, as\n        described by Underwood (1997). There are two restrictions: (i)\n        data must be balanced; (ii) fixed nested factors are not\n        allowed. Homogeneity of variances is checked using Cochran's C\n        test and 'a posteriori' comparisons of means are done using\n        Student-Newman-Keuls (SNK) procedure."}, "fitdistrplus": {"categories": ["Distributions", "Survival"], "description": "Extends the fitdistr() function (of the MASS package) with several functions \n  to help the fit of a parametric distribution to non-censored or censored data. \n  Censored data may contain left censored, right censored and interval censored values, \n  with several lower and upper bounds. In addition to maximum likelihood estimation (MLE), \n  the package provides moment matching (MME), quantile matching (QME), maximum goodness-of-fit \n  estimation (MGE) and maximum spacing estimation (MSE) methods (available only for \n  non-censored data). Weighted versions of MLE, MME, QME and MSE are available. See e.g. \n  Casella & Berger (2002), Statistical inference, Pacific Grove, for a general introduction \n  to parametric estimation."}, "styler": {"categories": ["ReproducibleResearch"], "description": "Pretty-prints R code without changing the user's formatting\n    intent."}, "airGRteaching": {"categories": ["Hydrology"], "description": "Add-on package to the 'airGR' package that simplifies its use and is aimed at being used for teaching hydrology. The package provides 1) three functions that allow to complete very simply a hydrological modelling exercise 2) plotting functions to help students to explore observed data and to interpret the results of calibration and simulation of the GR ('G\u00e9nie rural') models 3) a 'Shiny' graphical interface that allows for displaying the impact of model parameters on hydrographs and models internal variables."}, "estimatr": {"categories": ["CausalInference", "Econometrics"], "description": "Fast procedures for small set of commonly-used, design-appropriate estimators with robust standard errors and confidence intervals. Includes estimators for linear regression, instrumental variables regression, difference-in-means, Horvitz-Thompson estimation, and regression improving precision of experimental estimates by interacting treatment with centered pre-treatment covariates introduced by Lin (2013) <doi:10.1214/12-AOAS583>."}, "metavcov": {"categories": ["MetaAnalysis", "MissingData"], "description": "Collection of functions to compute covariances for different effect sizes, data visualization, and single and multiple imputations for missing data. Effect sizes include correlation (r), mean difference (MD), standardized mean difference (SMD), log odds ratio (logOR), log risk ratio (logRR), and risk difference (RD)."}, "tidyhydat": {"categories": ["Hydrology"], "description": "Provides functions to access historical and real-time national 'hydrometric'\n    data from Water Survey of Canada data sources (<https://dd.weather.gc.ca/hydrometric/csv/> and\n    <https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/>) and then applies tidy data principles."}, "h2o": {"categories": ["HighPerformanceComputing", "MachineLearning", "ModelDeployment"], "description": "R interface for 'H2O', the scalable open source machine learning\n    platform that offers parallelized implementations of many supervised and\n    unsupervised machine learning algorithms such as Generalized Linear\n    Models (GLM), Gradient Boosting Machines (including XGBoost), Random Forests,\n    Deep Neural Networks (Deep Learning), Stacked Ensembles, Naive Bayes,\n    Generalized Additive Models (GAM), ANOVA GLM, Cox Proportional Hazards, K-Means, PCA, ModelSelection,\n    Word2Vec, as well as a fully automatic machine learning algorithm (H2O AutoML)."}, "RSurveillance": {"categories": ["Epidemiology"], "description": "A range of functions for the design and\n    analysis of disease surveillance activities. These functions were\n    originally developed for animal health surveillance activities but can be\n    equally applied to aquatic animal, wildlife, plant and human health\n    surveillance activities. Utilities are included for sample size calculation\n    and analysis of representative surveys for disease freedom, risk-based\n    studies for disease freedom and for prevalence estimation.\n    This package is based on Cameron A., Conraths F., Frohlich A., Schauer B.,\n    Schulz K., Sergeant E., Sonnenburg J., Staubach C. (2015). R package of \n    functions for risk-based surveillance. Deliverable 6.24, WP 6 - Decision \n    making tools for implementing risk-based surveillance, Grant Number \n    no. 310806, RISKSUR (<https://www.fp7-risksur.eu/sites/default/files/documents/Deliverables/RISKSUR_%28310806%29_D6.24.pdf>). \n    Many of the 'RSurveillance' functions are incorporated into the 'epitools'\n    website: Sergeant, ESG, 2019. Epitools epidemiological calculators. \n    Ausvet Pty Ltd. Available at: <http://epitools.ausvet.com.au>."}, "dyn": {"categories": ["Environmetrics", "Finance", "TimeSeries"], "description": "Time series regression.  The dyn class interfaces ts,\n        irts(), zoo() and zooreg() time series classes to lm(), glm(),\n        loess(), quantreg::rq(), MASS::rlm(), MCMCpack::MCMCregress(),\n        quantreg::rq(), randomForest::randomForest() and other regression\n        functions allowing those functions to be used with time series\n        including specifications that may contain lags, diffs and\n        missing values."}, "ercv": {"categories": ["ExtremeValue"], "description": "Provides a methodology simple and trustworthy for the analysis of extreme values and multiple threshold tests for a generalized Pareto distribution, together\n    with an automatic threshold selection algorithm. See del Castillo, J, Daoudi, J and Lockhart, R (2014) <doi:10.1111/sjos.12037>."}, "rootSolve": {"categories": ["DifferentialEquations", "NumericalMathematics"], "description": "Routines to find the root of nonlinear functions, and to perform steady-state and equilibrium analysis of ordinary differential equations (ODE). \n  Includes routines that: (1) generate gradient and jacobian matrices (full and banded),\n  (2) find roots of non-linear equations by the 'Newton-Raphson' method, \n  (3) estimate steady-state conditions of a system of (differential) equations in full, banded or sparse form, using the 'Newton-Raphson' method, or by dynamically running,\n  (4) solve the steady-state conditions for uni-and multicomponent 1-D, 2-D, and 3-D partial differential equations, that have been converted to ordinary differential equations\n    by numerical differencing (using the method-of-lines approach).\n  Includes fortran code."}, "bayesmove": {"categories": ["Tracking"], "description": "Methods for assessing animal movement from telemetry and biologging\n    data using non-parametric Bayesian methods. This includes features for pre-\n    processing and analysis of data, as well as the visualization of results\n    from the models. This framework does not rely on standard parametric density\n    functions, which provides flexibility during model fitting. Further details \n    regarding part of this framework can be found in Cullen et al. (2021) <doi:10.1101/2020.11.05.369702>."}, "torch": {"categories": ["MachineLearning"], "description": "Provides functionality to define and train neural networks similar to\n    'PyTorch' by Paszke et al (2019) <arXiv:1912.01703> but written entirely in R\n    using the 'libtorch' library. Also supports low-level tensor operations and\n    'GPU' acceleration."}, "SimSurvey": {"categories": ["OfficialStatistics"], "description": "Simulate age-structured populations that vary in space and time and \n    explore the efficacy of a range of built-in or user-defined sampling \n    protocols to reproduce the population parameters of the known population. \n    (See Regular et al. (2020) <doi:10.1371/journal.pone.0232822> for more\n    details)."}, "rorcid": {"categories": ["WebTechnologies"], "description": "Client for the 'Orcid.org' API (<https://orcid.org/>).\n    Functions included for searching for people, searching by 'DOI',\n    and searching by 'Orcid' 'ID'."}, "ads": {"categories": ["Spatial"], "description": "Perform first- and second-order multi-scale analyses derived from Ripley K-function (Ripley B. D. (1977) <doi:10.1111/j.2517-6161.1977.tb01615.x>), for univariate,\n multivariate and marked mapped data in rectangular, circular or irregular shaped sampling windows, with tests of \n statistical significance based on Monte Carlo simulations."}, "GIGrvg": {"categories": ["Distributions"], "description": "\n  Generator and density function for the\n  Generalized Inverse Gaussian (GIG) distribution."}, "cifti": {"categories": ["MedicalImaging"], "description": "Functions for the input/output and visualization of\n    medical imaging data in the form of 'CIFTI' files \n    <https://www.nitrc.org/projects/cifti/>."}, "pinnacle.data": {"categories": ["SportsAnalytics"], "description": "Market odds from from Pinnacle, an online sports betting bookmaker (see <https://www.pinnacle.com> for more information). Included are datasets for the Major League Baseball (MLB) 2016 season and the USA election 2016. These datasets can be used to build models and compare statistical information with the information from prediction markets.The Major League Baseball (MLB) 2016 dataset can be used for sabermetrics analysis and also can be used in conjunction with other popular Major League Baseball (MLB) datasets such as Retrosheets or the Lahman package by merging by GameID."}, "Rserve": {"categories": ["ModelDeployment", "NumericalMathematics", "WebTechnologies"], "description": "Rserve acts as a socket server (TCP/IP or local sockets) \n\t     which allows binary requests to be sent to R. Every\n\t     connection has a separate workspace and working\n\t     directory. Client-side implementations are available\n\t     for popular languages such as C/C++ and Java, allowing\n\t     any application to use facilities of R without the need of\n\t     linking to R code. Rserve supports remote connection,\n\t     user authentication and file transfer. A simple R client\n\t     is included in this package as well."}, "survJamda": {"categories": ["Survival"], "description": "Microarray gene expression data can be analyzed individually or jointly using merging methods or meta-analysis to predict patients' survival and risk assessment. "}, "MST": {"categories": ["Survival"], "description": "Constructs trees for multivariate survival data using marginal and frailty models.\n    Grows, prunes, and selects the best-sized tree."}, "JointModel": {"categories": ["Survival"], "description": "Joint fit of a semiparametric regression model for longitudinal responses and a semiparametric transformation model for time-to-event data. "}, "in2extRemes": {"categories": ["ExtremeValue"], "description": "Graphical User Interface (GUI) to some of the functions in the package extRemes version >= 2.0 are included."}, "gh": {"categories": ["WebTechnologies"], "description": "Minimal client to access the 'GitHub' 'API'."}, "DiceKriging": {"categories": ["ExperimentalDesign"], "description": "Estimation, validation and prediction of kriging models. Important functions : km, print.km, plot.km, predict.km."}, "ChemoSpec2D": {"categories": ["ChemPhys"], "description": "A collection of functions for exploratory chemometrics of 2D spectroscopic data sets such as COSY (correlated spectroscopy) and HSQC (heteronuclear single quantum coherence) 2D NMR (nuclear magnetic resonance) spectra. 'ChemoSpec2D' deploys methods aimed primarily at classification of samples and the identification of spectral features which are important in distinguishing samples from each other. Each 2D spectrum (a matrix) is treated as the unit of observation, and thus the physical sample in the spectrometer corresponds to the  sample from a statistical perspective.  In addition to chemometric tools, a few tools are provided for plotting 2D spectra, but these are not intended to replace the functionality typically available on the spectrometer. 'ChemoSpec2D' takes many of its cues from 'ChemoSpec' and tries to create consistent graphical output and to be very user friendly."}, "socialmixr": {"categories": ["Epidemiology"], "description": "Provides methods for sampling contact matrices from diary data for use in infectious disease modelling, as discussed in Mossong et al. (2008) <doi:10.1371/journal.pmed.0050074>."}, "bshazard": {"categories": ["Survival"], "description": "The function estimates the hazard function non parametrically \n\t     from a survival object (possibly adjusted for covariates). \n\t     The smoothed estimate is based on B-splines from the perspective \n\t     of generalized linear mixed models. Left truncated \n\t     and right censoring data are allowed."}, "seasonalview": {"categories": ["TimeSeries"], "description": "A graphical user interface to the 'seasonal' package and\n  'X-13ARIMA-SEATS', the U.S. Census Bureau's seasonal adjustment software. \n  Unifies the code base of <http://www.seasonal.website> and the GUI in the\n  'seasonal' package."}, "TouRnament": {"categories": ["SportsAnalytics"], "description": "Contains two functions related to sports competitions. One to create league tables and one to create a match schedule."}, "ivreg": {"categories": ["CausalInference", "Econometrics"], "description": "Instrumental variable estimation for linear models by two-stage least-squares (2SLS) regression or by robust-regression via M-estimation (2SM) or MM-estimation (2SMM). The main ivreg() model-fitting function is designed to provide a workflow as similar as possible to standard lm() regression. A wide range of methods is provided for fitted ivreg model objects, including extensive functionality for computing and graphing regression diagnostics in addition to other standard model tools."}, "rmeta": {"categories": ["ClinicalTrials", "MetaAnalysis"], "description": "Functions for simple fixed and random effects\n        meta-analysis for two-sample comparisons and cumulative\n        meta-analyses. Draws standard summary plots, funnel plots, and\n        computes summaries and tests for association and heterogeneity."}, "RPostgreSQL": {"categories": ["Databases", "Spatial"], "description": "Database interface and 'PostgreSQL' driver for 'R'.\n This package provides a Database Interface 'DBI' compliant \n driver for 'R' to access 'PostgreSQL' database systems.  \n In order to build and install this package from source, 'PostgreSQL' \n itself must be present your system to provide 'PostgreSQL' functionality \n via its libraries and header files. These files are provided as\n 'postgresql-devel' package under some Linux distributions.\n On 'macOS' and 'Microsoft Windows' system the attached 'libpq' library source will be used."}, "HDclassif": {"categories": ["Cluster"], "description": "Discriminant analysis and data clustering methods for high\n    dimensional data, based on the assumption that high-dimensional data live in\n    different subspaces with low dimensionality proposing a new parametrization of\n    the Gaussian mixture model which combines the ideas of dimension reduction and\n    constraints on the model."}, "pbdMPI": {"categories": ["HighPerformanceComputing"], "description": "An efficient interface to MPI by utilizing S4\n        classes and methods with a focus on Single Program/Multiple Data\n        ('SPMD')\n        parallel programming style, which is intended for batch parallel\n        execution."}, "eba": {"categories": ["Psychometrics"], "description": "Fitting and testing multi-attribute probabilistic choice\n  models, especially the Bradley-Terry-Luce (BTL) model (Bradley &\n  Terry, 1952 <doi:10.1093/biomet/39.3-4.324>; Luce, 1959),\n  elimination-by-aspects (EBA) models (Tversky, 1972 <doi:10.1037/h0032955>),\n  and preference tree (Pretree) models (Tversky & Sattath, 1979\n  <doi:10.1037/0033-295X.86.6.542>)."}, "DPQ": {"categories": ["Distributions"], "description": "Computations for approximations and alternatives for the 'DPQ'\n  (Density (pdf), Probability (cdf) and Quantile) functions for probability\n  distributions in R.\n  Primary focus is on (central and non-central) beta, gamma and related\n  distributions such as the chi-squared, F, and t.\n  \u2013\n  This is for the use of researchers in these numerical approximation\n  implementations, notably for my own use in order to improve standard\n  R pbeta(), qgamma(), ..., etc: {'\"dpq\"'-functions}."}, "cluster": {"categories": ["Cluster", "Environmetrics", "Robust"], "description": "Methods for Cluster analysis.  Much extended the original from\n\tPeter Rousseeuw, Anja Struyf and Mia Hubert,\n\tbased on Kaufman and Rousseeuw (1990) \"Finding Groups in Data\"."}, "ExtremalDep": {"categories": ["ExtremeValue"], "description": "A set of procedures for modelling parametrically and non-parametrically the dependence structure of multivariate extreme-values is provided. The statistical inference is performed with non-parametric estimators, likelihood-based estimators and Bayesian techniques. Adapts the methodologies derived in Beranger et al. (2019) <arXiv:1904.08251>, Beranger et al. (2017) <doi:10.1111/sjos.12240>, Beranger and Padoan (2015) <arXiv:1508.05561>, Marcon et al. (2017) <doi:10.1002/sta4.145>, Marcon et al. (2017) <doi:10.1016/j.jspi.2016.10.004> and Marcon et al. (2016) <doi:10.1214/16-EJS1162>. It also refers to the works of Bortot (2010) <https://www.semanticscholar.org/paper/Tail-dependence-in-bivariate-skew-Normal-and-skew-t-Bortot/b0dc1cb608d35bf515c76e39aacc14b4de82e281?p2df>, Padoan (2011) <doi:10.1016/j.jmva.2011.01.014>, Cooley et al. (2010) <doi:10.1016/j.jmva.2010.04.007>, Husler and Reiss (1989) <doi:10.1016/0167-7152(89)90106-5>, Engelke et al. (2015) <doi:10.1111/rssb.12074>, Coles and Tawn (1991) <doi:10.1111/j.2517-6161.1991.tb01830.x>, Nikoloulopoulos et al. (2011) <doi:10.1007/s10687-008-0072-4>, Opitz (2013) <doi:10.1016/j.jmva.2013.08.008>, Tawn (1990) <doi:10.2307/2336802>, Azzalini and Capitanio (2014) <doi:10.1017/CBO9781139248891>, Azzalini (2003) <doi:10.1111/1467-9469.00322>, Azzalini and Capitanio (1999) <doi:10.1111/1467-9868.00194>, Azzalini and Dalla Valle (1996) <doi:10.1093/biomet/83.4.715>, Einmahl et al. (2013) <doi:10.1007/s10687-012-0156-z>, Naveau et al (2009) <doi:10.1093/biomet/asp001> and Heffernan and Tawn (2004) <doi:10.1111/j.1467-9868.2004.02050.x>."}, "soma": {"categories": ["Optimization"], "description": "An R implementation of the Self-Organising Migrating Algorithm, a general-purpose, stochastic optimisation algorithm. The approach is similar to that of genetic algorithms, although it is based on the idea of a series of \u201cmigrations\u201d by a fixed set of individuals, rather than the development of successive generations. It can be applied to any cost-minimisation problem with a bounded parameter space, and is robust to local minima."}, "landest": {"categories": ["Survival"], "description": "Provides functions to estimate survival and a treatment effect using a landmark estimation approach."}, "Deriv": {"categories": ["NumericalMathematics"], "description": "R-based solution for symbolic differentiation. It admits\n    user-defined function as well as function substitution\n    in arguments of functions to be differentiated. Some symbolic\n    simplification is part of the work."}, "lsmeans": {"categories": ["Survival"], "description": "Obtain least-squares means for linear, generalized linear, \n    and mixed models. Compute contrasts or linear functions of \n    least-squares means, and comparisons of slopes. \n    Plots and compact letter displays. Least-squares means were proposed in\n    Harvey, W (1960) \"Least-squares analysis of data with unequal subclass numbers\",\n    Tech Report ARS-20-8, USDA National Agricultural Library, and discussed\n    further in Searle, Speed, and Milliken (1980) \"Population marginal means \n    in the linear model: An alternative to least squares means\", \n    The American Statistician 34(4), 216-221 <doi:10.1080/00031305.1980.10483031>.\n    NOTE: lsmeans now relies primarily on code in the 'emmeans' package.\n    'lsmeans' will be archived in the near future."}, "RMAWGEN": {"categories": ["Environmetrics", "Hydrology", "TimeSeries"], "description": "S3 and S4 functions are implemented for spatial multi-site\n    stochastic generation of daily time series of temperature and\n    precipitation. These tools make use of Vector AutoRegressive models (VARs).\n    The weather generator model is then saved as an object and is calibrated by\n    daily instrumental \"Gaussianized\" time series through the 'vars' package\n    tools. Once obtained this model, it can it can be used for weather\n    generations and be adapted to work with several climatic monthly time\n    series."}, "caffsim": {"categories": ["Pharmacokinetics"], "description": "Simulate plasma caffeine concentrations using population pharmacokinetic model described in Lee, Kim, Perera, McLachlan and Bae (2015) <doi:10.1007/s00431-015-2581-x>."}, "np": {"categories": ["Econometrics"], "description": "Nonparametric (and semiparametric) kernel methods that seamlessly handle a mix of continuous, unordered, and ordered factor data types. We would like to gratefully acknowledge support from the Natural Sciences and Engineering Research Council of Canada (NSERC, <https://www.nserc-crsng.gc.ca/>), the Social Sciences and Humanities Research Council of Canada (SSHRC, <https://www.sshrc-crsh.gc.ca/>), and the Shared Hierarchical Academic Research Computing Network (SHARCNET, <https://sharcnet.ca/>)."}, "mosaic": {"categories": ["TeachingStatistics"], "description": "Data sets and utilities from Project MOSAIC (<http://www.mosaic-web.org>) used\n    to teach mathematics, statistics, computation and modeling.  Funded by the\n    NSF, Project MOSAIC is a community of educators working to tie together\n    aspects of quantitative work that students in science, technology,\n    engineering and mathematics will need in their professional lives, but\n    which are usually taught in isolation, if at all."}, "BACCT": {"categories": ["Bayesian"], "description": "Implements the Bayesian Augmented Control (BAC, a.k.a. Bayesian historical data borrowing) method under clinical trial setting by calling 'Just Another Gibbs Sampler' ('JAGS') software. In addition, the 'BACCT' package evaluates user-specified decision rules by computing the type-I error/power, or probability of correct go/no-go decision at interim look. The evaluation can be presented numerically or graphically. Users need to have 'JAGS' 4.0.0 or newer installed due to a compatibility issue with 'rjags' package. Currently, the package implements the BAC method for binary outcome only. Support for continuous and survival endpoints will be added in future releases. We would like to thank AbbVie's Statistical Innovation group and Clinical Statistics group for their support in developing the 'BACCT' package."}, "nlsem": {"categories": ["Psychometrics"], "description": "Estimation of structural equation models with nonlinear effects\n  and underlying nonnormal distributions."}, "mbbefd": {"categories": ["Distributions"], "description": "Distributions that are typically used for exposure rating in\n             general insurance, in particular to price reinsurance contracts.\n             The vignette shows code snippets to fit the distribution to\n             empirical data. See, e.g., Bernegger (1997) <doi:10.2143/AST.27.1.563208>\n             freely available on-line."}, "ecoreg": {"categories": ["MetaAnalysis"], "description": "Estimating individual-level covariate-outcome associations \n using aggregate data (\"ecological inference\") or a combination of \n aggregate and individual-level data (\"hierarchical related regression\")."}, "LARF": {"categories": ["CausalInference", "Econometrics"], "description": "Provides instrumental variable estimation of treatment effects when both the endogenous treatment and its instrument are binary. Applicable to both binary and continuous outcomes."}, "km.ci": {"categories": ["Survival"], "description": "Computes various confidence intervals for the Kaplan-Meier\n        estimator, namely: Peto's CI, Rothman CI, CI's based on\n        Greenwood's variance, Thomas and Grunkemeier CI and the\n        simultaneous confidence bands by Nair and Hall and Wellner."}, "rasterVis": {"categories": ["Spatial", "SpatioTemporal"], "description": "Methods for enhanced visualization and interaction with raster data. It implements visualization methods for quantitative data and categorical data, both for univariate and multivariate rasters. It also provides methods to display spatiotemporal rasters, and vector fields. See the website for examples."}, "googleCloudStorageR": {"categories": ["WebTechnologies"], "description": "Interact with Google Cloud Storage <https://cloud.google.com/storage/> \n  API in R. Part of the 'cloudyr' <https://cloudyr.github.io/> project."}, "Qtools": {"categories": ["MissingData"], "description": "Functions for unconditional and conditional quantiles. These\n    include methods for transformation-based quantile regression,\n    quantile-based measures of location, scale and shape, methods for quantiles\n    of discrete variables, quantile-based multiple imputation, restricted\n    quantile regression, and directional quantile classification. A vignette\n\tis given in Geraci (2016, The R Journal) <doi:10.32614/RJ-2016-037> and\n\tincluded in the package."}, "modeest": {"categories": ["Distributions"], "description": "Provides estimators of the mode of univariate\n    data or univariate distributions. "}, "legion": {"categories": ["TimeSeries"], "description": "Functions implementing multivariate state space models for purposes of time series analysis and forecasting.\n             The focus of the package is on multivariate models, such as Vector Exponential Smoothing,\n             Vector ETS (Error-Trend-Seasonal model) etc. It currently includes Vector Exponential\n             Smoothing (VES, de Silva et al., 2010, <doi:10.1177/1471082X0901000401>), Vector ETS and\n             simulation function for VES."}, "ecoval": {"categories": ["Hydrology"], "description": "Functions for evaluating and visualizing\n  ecological assessment procedures for surface waters\n  containing physical, chemical and biological assessments\n  in the form of value functions."}, "matrixsampling": {"categories": ["Distributions"], "description": "Provides samplers for various matrix variate distributions: Wishart, inverse-Wishart, normal, t, inverted-t, Beta type I, Beta type II, Gamma, confluent hypergeometric. Allows to simulate the noncentral Wishart distribution without the integer restriction on the degrees of freedom."}, "RPyGeo": {"categories": ["Spatial"], "description": "Provides access to ArcGIS geoprocessing tools by building an \n             interface between R and the ArcPy Python side-package via the \n             reticulate package. "}, "tmvtnorm": {"categories": ["Distributions"], "description": "Random number generation for the truncated multivariate normal and Student t distribution. \n  Computes probabilities, quantiles and densities, \n  including one-dimensional and bivariate marginal densities. Computes first and second moments (i.e. mean and covariance matrix) for the double-truncated multinormal case."}, "Tinflex": {"categories": ["Distributions"], "description": "A universal non-uniform random number generator\n  for quite arbitrary distributions with piecewise twice\n  differentiable densities."}, "kernelboot": {"categories": ["Distributions"], "description": "Smoothed bootstrap and functions for random generation from\n             univariate and multivariate kernel densities. It does not\n             estimate kernel densities."}, "dlsem": {"categories": ["Econometrics"], "description": "Inference functionalities for distributed-lag linear structural equation models (DLSEMs). DLSEMs are Markovian structural causal models where each factor of the joint probability distribution is a distributed-lag linear regression with constrained lag shapes (Magrini, 2018 <doi:10.2478/bile-2018-0012>; Magrini et al., 2019 <doi:10.1007/s11135-019-00855-z>). DLSEMs account for temporal delays in the dependence relationships among the variables through a single parameter per covariate, thus allowing to perform dynamic causal inference in a feasible fashion. Endpoint-constrained quadratic, quadratic decreasing, linearly decreasing and gamma lag shapes are available."}, "ReIns": {"categories": ["Distributions", "ExtremeValue"], "description": "Functions from the book \"Reinsurance: Actuarial and Statistical Aspects\" (2017) by Hansjoerg Albrecher, Jan Beirlant and Jef Teugels <http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470772689.html>."}, "icarus": {"categories": ["OfficialStatistics"], "description": "Provides user-friendly tools for calibration in survey sampling.\n    The package is production-oriented, and its interface is inspired by the famous\n    popular macro 'Calmar' for SAS, so that 'Calmar' users can quickly get used to\n    'icarus'. In addition to calibration (with linear, raking and logit methods),\n    'icarus' features functions for calibration on tight bounds and penalized\n    calibration."}, "DIFboost": {"categories": ["Psychometrics"], "description": "Performs detection of Differential Item Functioning using the method DIFboost as proposed by Schauberger and Tutz (2016) <doi:10.1111/bmsp.12060>. "}, "tufterhandout": {"categories": ["ReproducibleResearch"], "description": "Custom template and output formats for use with rmarkdown. Produce\n    Edward Tufte-style handouts in html formats with full support for rmarkdown\n    features"}, "Quandl": {"categories": ["TimeSeries"], "description": "Functions for interacting directly with the Quandl API to offer data in a number of formats usable in R, downloading a zip with all data from a Quandl database, and the ability to search. This R package uses the Quandl API. For more information go to <https://docs.quandl.com>. For more help on the package itself go to <https://www.quandl.com/tools/r>."}, "Tides": {"categories": ["TimeSeries"], "description": "Calculate Characteristics of Quasi-Periodic Time Series, e.g. Estuarine Water Levels."}, "nparACT": {"categories": ["Tracking"], "description": "Computes interdaily stability (IS), intradaily variability (IV) & the relative amplitude (RA) from actigraphy data as described in Blume et al. (2016) <doi:10.1016/j.mex.2016.05.006> and van Someren et al. (1999) <doi:10.3109/07420529908998724>. Additionally, it also computes L5 (i.e. the 5 hours with lowest average actigraphy amplitude) and M10 (the 10 hours with highest average amplitude) as well as the respective start times. The flex versions will also compute the L-value for a user-defined number of minutes. IS describes the strength of coupling of a rhythm to supposedly stable zeitgebers. It varies between 0 (Gaussian Noise) and 1 for perfect IS. IV describes the fragmentation of a rhythm, i.e. the frequency and extent of transitions between rest and activity. It is near 0 for a perfect sine wave, about 2 for Gaussian noise and may be even higher when a definite ultradian period of about 2 hrs is present. RA is the relative amplitude of a rhythm. Note that to obtain reliable results, actigraphy data should cover a reasonable number of days."}, "pch": {"categories": ["Survival"], "description": "Piecewise constant hazard models for survival data. \n    The package allows for right-censored, left-truncated, and interval-censored data."}, "rsurface": {"categories": ["ExperimentalDesign"], "description": "Produces tables with the level of replication (number of replicates) and the\n    experimental uncoded values of the quantitative factors to be used for rotatable Central\n    Composite Design (CCD) experimentation and a 2-D contour plot of the corresponding\n    variance of the predicted response according to\n    Mead et al. (2012) <doi:10.1017/CBO9781139020879> design_ccd(), and analyzes\n    CCD data with response surface methodology ccd_analysis(). A rotatable CCD\n    provides values of the variance of the predicted response that are concentrically\n    distributed around the average treatment combination used in the experimentation, which\n    with uniform precision (implied by the use of several replicates at the average\n    treatment combination) improves greatly the search and finding of an optimum response.\n    These properties of a rotatable CCD represent undeniable advantages over the classical\n    factorial design, as discussed by Panneton et al. (1999) <doi:10.13031/2013.13267> and\n    Mead et al. (2012) <doi:10.1017/CBO9781139020879.018> among others."}, "minimax": {"categories": ["Distributions"], "description": "The minimax family of distributions is a two-parameter\n        family like the beta family, but computationally a lot more\n        tractible."}, "flowr": {"categories": ["HighPerformanceComputing", "ReproducibleResearch"], "description": "This framework allows you to design and implement complex\n    pipelines, and deploy them on your institution's computing cluster. This has\n    been built keeping in mind the needs of bioinformatics workflows. However, it is\n    easily extendable to any field where a series of steps (shell commands) are to\n    be executed in a (work)flow."}, "OptHedging": {"categories": ["Finance"], "description": "Estimation of value and hedging strategy of call and put options, based on optimal hedging and Monte Carlo method, from Chapter 3 of 'Statistical Methods for Financial Engineering', by Bruno Remillard, CRC Press, (2013)."}, "nifti.io": {"categories": ["MedicalImaging"], "description": "Tools for reading and writing NIfTI-1.1 (NII) files, including optimized voxelwise read/write operations and a simplified method to write dataframes to NII.\n    Specification of the NIfTI-1.1 format can be found here <https://nifti.nimh.nih.gov/nifti-1>.\n    Scientific publication first using these tools\n        Koscik TR, Man V, Jahn A, Lee CH, Cunningham WA (2020) <doi:10.1016/j.neuroimage.2020.116764> \"Decomposing the neural pathways in a simple, value-based choice.\" Neuroimage, 214, 116764. "}, "FFD": {"categories": ["OfficialStatistics"], "description": "Functions, S4 classes/methods and a graphical user interface (GUI) to design surveys to substantiate freedom from disease using a modified hypergeometric function (see Cameron and Baldock, 1997, <doi:10.1016/s0167-5877(97)00081-0>). Herd sensitivities are computed according to sampling strategies \"individual sampling\" or \"limited sampling\" (see M. Ziller, T. Selhorst, J. Teuffert, M. Kramer and H. Schlueter, 2002, <doi:10.1016/S0167-5877(01)00245-8>). Methods to compute the a-posteriori alpha-error are implemented. Risk-based targeted sampling is supported."}, "FMStable": {"categories": ["Distributions"], "description": "Some basic procedures for dealing\n        with log maximally skew stable distributions, which are also\n        called finite moment log stable distributions."}, "acs": {"categories": ["OfficialStatistics"], "description": "Provides a general toolkit for downloading, managing,\n  analyzing, and presenting data from the U.S. Census\n  (<https://www.census.gov/data/developers/data-sets.html>), including\n  SF1 (Decennial short-form), SF3 (Decennial long-form), and the\n  American Community Survey (ACS).  Confidence intervals provided with\n  ACS data are converted to standard errors to be bundled with\n  estimates in complex acs objects.  Package provides new methods to\n  conduct standard operations on acs objects and present/plot data in\n  statistically appropriate ways."}, "FoReco": {"categories": ["TimeSeries"], "description": "Classical (bottom-up and top-down), optimal and heuristic combination forecast \n    reconciliation procedures for cross-sectional, temporal, and cross-temporal linearly \n    constrained time series (Di Fonzo and Girolimetto, 2021) <doi:10.1016/j.ijforecast.2021.08.004>."}, "pander": {"categories": ["ReproducibleResearch"], "description": "Contains some functions catching all messages, 'stdout' and other\n    useful information while evaluating R code and other helpers to return user\n    specified text elements (like: header, paragraph, table, image, lists etc.)\n    in 'pandoc' markdown or several type of R objects similarly automatically\n    transformed to markdown format. Also capable of exporting/converting (the\n    resulting) complex 'pandoc' documents to e.g. HTML, 'PDF', 'docx' or 'odt'. This\n    latter reporting feature is supported in brew syntax or with a custom reference\n    class with a smarty caching 'backend'."}, "TPmsm": {"categories": ["Survival"], "description": "Estimation of transition probabilities for the\n\tillness-death model and or the three-state progressive model."}, "bcf": {"categories": ["CausalInference"], "description": "Causal inference for a binary treatment and continuous outcome using Bayesian Causal Forests. See Hahn, Murray and Carvalho (2017) <arXiv:1706.09523> for additional information. This implementation relies on code originally accompanying Pratola et. al. (2013) <arXiv:1309.1906>."}, "penalizedLDA": {"categories": ["MachineLearning"], "description": "Implements the penalized LDA proposal of \"Witten and Tibshirani (2011), Penalized classification using Fisher's linear discriminant, to appear in Journal of the Royal Statistical Society, Series B\"."}, "chessR": {"categories": ["SportsAnalytics"], "description": "A set of functions to enable users to extract chess game data from \n    popular chess sites, including 'Lichess'<https://lichess.org/> and \n    'Chess.com' <https://www.chess.com/> and then perform analysis on that game data."}, "support.CEs": {"categories": ["ExperimentalDesign"], "description": "Provides basic functions that support an implementation of (discrete) choice experiments (CEs). CEs is a question-based survey method measuring people's preferences for goods/services and their characteristics. Refer to Louviere et al. (2000) <doi:10.1017/CBO9780511753831> for details on CEs, and Aizaki (2012) <doi:10.18637/jss.v050.c02> for the package."}, "tables": {"categories": ["ReproducibleResearch"], "description": "Computes and displays complex tables of summary statistics.\n  Output may be in LaTeX, HTML, plain text, or an R\n  matrix for further processing."}, "fBonds": {"categories": ["Finance"], "description": "It implements the Nelson-Siegel and the Nelson-Siegel-Svensson\n\tterm structures."}, "MIIVsem": {"categories": ["Psychometrics"], "description": "Functions for estimating structural equation models using \n    instrumental variables."}, "frailtyEM": {"categories": ["Survival"], "description": "Contains functions for fitting shared frailty models with a semi-parametric\n    baseline hazard with the Expectation-Maximization algorithm. Supported data formats \n    include clustered failures with left truncation and recurrent events in gap-time\n    or Andersen-Gill format. Several frailty distributions, such as the the gamma, positive stable\n    and the Power Variance Family are supported. "}, "Bivariate.Pareto": {"categories": ["Distributions"], "description": "Perform competing risks analysis under bivariate Pareto models. See Shih et al. (2019) <doi:10.1080/03610926.2018.1425450> for details."}, "mailR": {"categories": ["WebTechnologies"], "description": "Interface to Apache Commons Email to send emails\n    from R."}, "distributions3": {"categories": ["Distributions"], "description": "Tools to create and manipulate probability distributions\n    using S3.  Generics random(), pdf(), cdf() and quantile() provide\n    replacements for base R's r/d/p/q style functions.  Functions and\n    arguments have been named carefully to minimize confusion for students\n    in intro stats courses. The documentation for each distribution\n    contains detailed mathematical notes."}, "mapsf": {"categories": ["Spatial"], "description": "Create and integrate thematic maps in your workflow. This package\n    helps to design various cartographic representations such as proportional\n    symbols, choropleth or typology maps. It also offers several functions to\n    display layout elements that improve the graphic presentation of maps\n    (e.g. scale bar, north arrow, title, labels). 'mapsf' maps 'sf' objects on\n    'base' graphics."}, "tidytext": {"categories": ["NaturalLanguageProcessing"], "description": "Using tidy data principles can make many text mining tasks\n    easier, more effective, and consistent with tools already in wide use.\n    Much of the infrastructure needed for text mining with tidy data\n    frames already exists in packages like 'dplyr', 'broom', 'tidyr', and\n    'ggplot2'. In this package, we provide functions and supporting data\n    sets to allow conversion of text to and from tidy formats, and to\n    switch seamlessly between tidy tools and existing text mining\n    packages."}, "primes": {"categories": ["NumericalMathematics"], "description": "Fast functions for dealing with prime numbers, such as testing\n    whether a number is prime and generating a sequence prime numbers.\n    Additional functions include finding prime factors and Ruth-Aaron pairs,\n    finding next and previous prime numbers in the series, finding or estimating\n    the nth prime, estimating the number of primes less than or equal to an\n    arbitrary number, computing primorials, prime k-tuples (e.g., twin primes),\n    finding the greatest common divisor and smallest (least) common multiple,\n    testing whether two numbers are coprime, and computing Euler's totient\n    function. Most functions are vectorized for speed and convenience."}, "pema": {"categories": ["MetaAnalysis"], "description": "Conduct penalized meta-analysis, see Van Lissa & Van Erp (2021).\n    <doi:10.31234/osf.io/6phs5>. In meta-analysis, there are\n    often between-study differences. These can be coded as moderator variables,\n    and controlled for using meta-regression. However, if the number of\n    moderators is large relative to the number of studies, such an analysis may\n    be overfit. Penalized meta-regression is useful in these cases, because\n    it shrinks the regression slopes of irrelevant moderators towards zero."}, "rcoreoa": {"categories": ["WebTechnologies"], "description": "Client for the CORE API (<https://core.ac.uk/docs/>).\n    CORE (<https://core.ac.uk>) aggregates open access research\n    outputs from repositories and journals worldwide and make them\n    available to the public."}, "easypower": {"categories": ["ExperimentalDesign"], "description": "Power analysis is used in the estimation of sample sizes for\n    experimental designs. Most programs and R packages will only output the highest\n    recommended sample size to the user. Often the user input can be complicated\n    and computing multiple power analyses for different treatment comparisons can\n    be time consuming. This package simplifies the user input and allows the user\n    to view all of the sample size recommendations or just the ones they want to see.\n    The calculations used to calculate the recommended sample sizes are from the\n    'pwr' package."}, "BVAR": {"categories": ["Bayesian", "TimeSeries"], "description": "Estimation of hierarchical Bayesian vector autoregressive models\n    following Kuschnig & Vashold (2021) <doi:10.18637/jss.v100.i14>.\n    Implements hierarchical prior selection for conjugate priors in the fashion\n    of Giannone, Lenza & Primiceri (2015) <doi:10.1162/REST_a_00483>.\n    Functions to compute and identify impulse responses, calculate forecasts,\n    forecast error variance decompositions and scenarios are available.\n    Several methods to print, plot and summarise results facilitate analysis."}, "rMIDAS": {"categories": ["MissingData"], "description": "A tool for multiply imputing missing data using 'MIDAS', a deep learning method based on denoising autoencoder neural networks. This algorithm offers significant accuracy and efficiency advantages over other multiple imputation strategies, particularly when applied to large datasets with complex features. Alongside interfacing with 'Python' to run the core algorithm, this package contains functions for processing data before and after model training, running imputation model diagnostics, generating multiple completed datasets, and estimating regression models on these datasets."}, "netchain": {"categories": ["CausalInference"], "description": "In networks, treatments may spill over from the treated individual to his or her social contacts and outcomes may be contagious over time. Under this setting, causal inference on the collective outcome observed over all network is often of interest. We use chain graph models approximating the projection of the full longitudinal data onto the observed data to identify the causal effect of the intervention on the whole outcome. Justification of such approximation is demonstrated in Ogburn et al. (2018) <arXiv:1812.04990>."}, "blockrand": {"categories": ["ClinicalTrials"], "description": "Create randomizations for block random clinical trials.  Can also produce a pdf file of randomization cards."}, "OrdNor": {"categories": ["Distributions"], "description": "Implementation of a procedure for generating samples from a mixed distribution of ordinal and normal random variables with a pre-specified correlation matrix and marginal distributions.\n             The details of the method are explained in Demirtas et al. (2015) <doi:10.1080/10543406.2014.920868>."}, "hydroTSM": {"categories": ["Environmetrics", "Hydrology"], "description": "S3 functions for management, analysis, interpolation and plotting of time series used in hydrology and related environmental sciences. In particular, this package is highly oriented to hydrological modelling tasks. The focus of this package has been put in providing a collection of tools useful for the daily work of hydrologists (although an effort was made to optimise each function as much as possible, functionality has had priority over speed). Bugs / comments / questions / collaboration of any kind are very welcomed, and in particular, datasets that can be included in this package for academic purposes."}, "HypergeoMat": {"categories": ["NumericalMathematics"], "description": "Evaluates the hypergeometric functions of a matrix argument, which appear in random matrix theory. This is an implementation of Koev & Edelman's algorithm (2006) <doi:10.1090/S0025-5718-06-01824-2>. "}, "ald": {"categories": ["Distributions"], "description": "It provides the density, distribution function, quantile function, \n             random number generator, likelihood function, moments and Maximum Likelihood estimators for a given sample, all this for\n             the three parameter Asymmetric Laplace Distribution defined \n             in Koenker and Machado (1999). This is a special case of the skewed family of distributions\n             available in Galarza et.al. (2017) <doi:10.1002/sta4.140> useful for quantile regression. "}, "pomp": {"categories": ["DifferentialEquations", "Epidemiology", "TimeSeries"], "description": "Tools for data analysis with partially observed Markov process (POMP) models (also known as stochastic dynamical systems, hidden Markov models, and nonlinear, non-Gaussian, state-space models).  The package provides facilities for implementing POMP models, simulating them, and fitting them to time series data by a variety of frequentist and Bayesian methods.  It is also a versatile platform for implementation of inference methods for general POMP models."}, "survivalMPL": {"categories": ["Survival"], "description": "Estimate the regression coefficients and the baseline hazard \n      of proportional hazard Cox models with left, right or interval censored survival data \n      using maximum penalised likelihood. A 'non-parametric' smooth estimate of the baseline hazard\n      function is provided."}, "trackdf": {"categories": ["SpatioTemporal"], "description": "Data frame class for storing collective movement data (e.g. fish\n    schools, ungulate herds, baboon troops) collected from GPS trackers or\n    computer vision tracking software. "}, "changepoint.geo": {"categories": ["TimeSeries"], "description": "Implements the high-dimensional changepoint detection method GeomCP and the related mappings used for changepoint detection. These methods view the changepoint problem from a geometrical viewpoint and aim to extract relevant geometrical features in order to detect changepoints. The geomcp() function should be your first point of call. References: Grundy et al. (2020) <doi:10.1007/s11222-020-09940-y>. "}, "robustrank": {"categories": ["MissingData"], "description": "Implements two-sample tests for paired data with missing values (Fong, Huang, Lemos and McElrath 2018, Biostatics, <doi:10.1093/biostatistics/kxx039>) and modified Wilcoxon-Mann-Whitney two sample location test, also known as the Fligner-Policello test. "}, "Ecohydmod": {"categories": ["Hydrology"], "description": "Simulates the soil water balance (soil moisture, evapotranspiration, leakage and runoff), rainfall series by using the marked Poisson process and the vegetation growth through the normalized difference vegetation index (NDVI). Please see Souza et al. (2016) <doi:10.1002/hyp.10953>."}, "psych": {"categories": ["Psychometrics"], "description": "A general purpose toolbox for personality, psychometric theory and experimental psychology.   Functions are primarily for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis, although others provide basic descriptive statistics. Item Response Theory is done using  factor analysis of tetrachoric and polychoric correlations. Functions for analyzing data at multiple levels include within and between group statistics, including correlations and factor analysis.   Functions for simulating and testing particular item and test structures are included. Several functions  serve as a useful front end for structural equation modeling.  Graphical displays of path diagrams, factor analysis and structural equation models are created using basic graphics. Some of the functions are written to support a book on psychometric theory as well as publications in personality research. For more information, see the <https://personality-project.org/r/> web page."}, "mapdeck": {"categories": ["Spatial"], "description": "Provides a mechanism to plot an interactive map using 'Mapbox GL' \n\t\t(<https://docs.mapbox.com/mapbox-gl-js/api/>), a javascript library for interactive maps,\n\t\tand 'Deck.gl' (<https://deck.gl/>), a javascript library which uses 'WebGL' for \n\t\tvisualising large data sets."}, "ALTopt": {"categories": ["ExperimentalDesign"], "description": "Creates the optimal (D, U and I) designs for the accelerated life\n    testing with right censoring or interval censoring. It uses generalized \n    linear model (GLM) approach to derive the asymptotic variance-covariance \n    matrix of regression coefficients. The failure time distribution is assumed \n    to follow Weibull distribution with a known shape parameter and log-linear \n    link functions are used to model the relationship between failure time \n    parameters and stress variables. The acceleration model may have multiple \n    stress factors, although most ALTs involve only two or less stress factors. \n    ALTopt package also provides several plotting functions including contour plot,\n    Fraction of Use Space (FUS) plot and Variance Dispersion graphs of Use Space\n    (VDUS) plot. For more details, see Seo and Pan (2015) <doi:10.32614/RJ-2015-029>."}, "SimHaz": {"categories": ["Survival"], "description": "Generate power for the Cox proportional hazards model by simulating survival events data with time dependent exposure status for subjects. A dichotomous exposure variable is considered with a single transition from unexposed to exposed status during the subject's time on study."}, "distrTEst": {"categories": ["Distributions"], "description": "Evaluation (S4-)classes based on package distr for evaluating procedures\n        (estimators/tests) at data/simulation in a unified way."}, "NlcOptim": {"categories": ["Optimization"], "description": "Optimization for nonlinear objective and constraint functions.  Linear or nonlinear equality and inequality constraints are allowed.  It accepts the input parameters as a constrained matrix."}, "RSQLite": {"categories": ["Databases"], "description": "Embeds the SQLite database engine in R and\n    provides an interface compliant with the DBI package. The source for\n    the SQLite engine and for various extensions in a recent version is\n    included. System libraries will never be consulted because\n    this package relies on static linking for the plugins it includes;\n    this also ensures a consistent experience across all installations."}, "dlnm": {"categories": ["TimeSeries"], "description": "Collection of functions for distributed lag linear and non-linear models."}, "trackdem": {"categories": ["SpatioTemporal", "Tracking"], "description": "Obtain population density and body size structure, using video material or image sequences as input. Functions assist in the creation of image sequences from videos, background detection and subtraction, particle identification and tracking. An artificial neural network can be trained for noise filtering. The goal is to supply accurate estimates of population size, structure and/or individual behavior, for use in  evolutionary and ecological studies."}, "CIEE": {"categories": ["CausalInference"], "description": "In many studies across different disciplines, detailed measures of the variables of interest are available. If assumptions can be made regarding the direction of effects between the assessed variables, this has to be considered in the analysis. The functions in this package implement the novel approach CIEE (causal inference using estimating equations; Konigorski et al., 2018, <doi:10.1002/gepi.22107>) for estimating and testing the direct effect of an exposure variable on a primary outcome, while adjusting for indirect effects of the exposure on the primary outcome through a secondary intermediate outcome and potential factors influencing the secondary outcome. The underlying directed acyclic graph (DAG) of this considered model is described in the vignette. CIEE can be applied to studies in many different fields, and it is implemented here for the analysis of a continuous primary outcome and a time-to-event primary outcome subject to censoring. CIEE uses estimating equations to obtain estimates of the direct effect and robust sandwich standard error estimates. Then, a large-sample Wald-type test statistic is computed for testing the absence of the direct effect. Additionally, standard multiple regression, regression of residuals, and the structural equation modeling approach are implemented for comparison. "}, "instaR": {"categories": ["WebTechnologies"], "description": "Provides an interface to the Instagram API <https://instagram.com/\n    developer/>, which allows R users to download public pictures filtered by\n    hashtag, popularity, user or location, and to access public users' profile data."}, "samon": {"categories": ["MissingData"], "description": "In a clinical trial with repeated measures designs, outcomes are often taken from subjects at fixed time-points.  The focus of the trial may be to compare the mean outcome in two or more groups at some pre-specified time after enrollment. In the presence of missing data auxiliary assumptions are necessary to perform such comparisons.  One commonly employed assumption is the missing at random assumption (MAR).   The 'samon' package allows the user to perform a (parameterized) sensitivity analysis of this assumption.  In particular it can be used to examine the sensitivity of tests in the difference in outcomes to violations of the MAR assumption.  The sensitivity analysis can be performed under two scenarios, a) where the data exhibit a monotone missing data pattern (see the samon() function), and, b) where in addition to a monotone missing data pattern the data exhibit intermittent missing values (see the samonIM() function)."}, "BAYSTAR": {"categories": ["Bayesian", "TimeSeries"], "description": "Fit two-regime threshold autoregressive (TAR) models by Markov chain Monte Carlo methods. "}, "TSstudio": {"categories": ["TimeSeries"], "description": "Provides a set of tools for descriptive and predictive analysis of time series data. That includes functions for interactive visualization of time series objects and as well utility functions for automation time series forecasting."}, "linelist": {"categories": ["Epidemiology"], "description": "Provides tools to help storing and handling case line list data. The 'linelist' class adds a tagging system to classical 'data.frame' objects to identify key epidemiological data such as dates of symptom onset, epidemiological case definition, age, gender or disease outcome. Once tagged, these variables can be seamlessly used in downstream analyses, making data pipelines more robust and reliable. "}, "faoutlier": {"categories": ["Psychometrics"], "description": "Tools for detecting and summarize influential cases that\n    can affect exploratory and confirmatory factor analysis models as well as\n    structural equation models more generally (Chalmers, 2015, <doi:10.1177/0146621615597894>; \n    Flora, D. B., LaBrish, C. & Chalmers, R. P., 2012, <doi:10.3389/fpsyg.2012.00055>)."}, "backtest": {"categories": ["Finance"], "description": "The backtest package provides facilities for exploring\n        portfolio-based conjectures about financial instruments\n        (stocks, bonds, swaps, options, et cetera)."}, "boot.heterogeneity": {"categories": ["MetaAnalysis"], "description": "Implements a bootstrap-based heterogeneity test for standardized mean differences (d), Fisher-transformed Pearson's correlations (r), and natural-logarithm-transformed odds ratio (or) in meta-analysis studies. Depending on the presence of moderators, this Monte Carlo based test can be implemented in the random- or mixed-effects model. This package uses rma() function from the R package 'metafor' to obtain parameter estimates and likelihoods, so installation of R package 'metafor' is required. This approach refers to the studies of Anscombe (1956) <doi:10.2307/2332926>, Haldane (1940) <doi:10.2307/2332614>, Hedges (1981) <doi:10.3102/10769986006002107>, Hedges & Olkin (1985, ISBN:978-0123363800), Silagy, Lancaster, Stead, Mant, & Fowler (2004) <doi:10.1002/14651858.CD000146.pub2>, Viechtbauer (2010) <doi:10.18637/jss.v036.i03>, and Zuckerman (1994, ISBN:978-0521432009). "}, "COVID19": {"categories": ["Epidemiology"], "description": "Provides a daily summary of COVID-19 cases, deaths, recovered, tests,\n  vaccinations, and hospitalizations for 230+ countries, 760+ regions, \n  and 12000+ administrative divisions of lower level. \n  Includes policy measures, mobility data, and geospatial identifiers.\n  Data source: COVID-19 Data Hub <https://covid19datahub.io>."}, "NetworkRiskMeasures": {"categories": ["Finance"], "description": "Implements some risk measures for (financial) networks, such as DebtRank, Impact Susceptibility, Impact Diffusion and Impact Fluidity. "}, "gsisdecoder": {"categories": ["SportsAnalytics"], "description": "A set of high efficient functions to decode identifiers of National \n  Football League players."}, "StratifiedRF": {"categories": ["MissingData"], "description": "Random Forest-like tree ensemble that works with groups of predictor variables. When building a tree, a number of variables is taken randomly from each group separately, thus ensuring that it considers variables from each group for the splits. Useful when rows contain information about different things (e.g. user information and product information) and it's not sensible to make a prediction with information from only one group of variables, or when there are far more variables from one group than the other and it's desired to have groups appear evenly on trees.\n    Trees are grown using the C5.0 algorithm rather than the usual CART algorithm. Supports parallelization (multithreaded), missing values in predictors, and categorical variables (without doing One-Hot encoding in the processing). Can also be used to create a regular (non-stratified) Random Forest-like model, but made up of C5.0 trees and with some additional control options.\n    As it's built with C5.0 trees, it works only for classification (not for regression)."}, "m2r": {"categories": ["NumericalMathematics"], "description": "Persistent interface to 'Macaulay2' <http://www.math.uiuc.edu/Macaulay2/>\n    and front-end tools facilitating its use in the 'R' ecosystem. For details see \n    Kahle et. al. (2020) <doi:10.18637/jss.v093.i09>."}, "plotly": {"categories": ["WebTechnologies"], "description": "Create interactive web graphics from 'ggplot2' graphs and/or a custom interface to the (MIT-licensed) JavaScript library 'plotly.js' inspired by the grammar of graphics."}, "TSEntropies": {"categories": ["TimeSeries"], "description": "Computes various entropies of given time series. This is the initial version that includes ApEn() and SampEn() functions for calculating approximate entropy and sample entropy. Approximate entropy was proposed by S.M. Pincus in \"Approximate entropy as a measure of system complexity\", Proceedings of the National Academy of Sciences of the United States of America, 88, 2297-2301 (March 1991). Sample entropy was proposed by J. S. Richman and J. R. Moorman in \"Physiological time-series analysis using approximate entropy and sample entropy\", American Journal of Physiology, Heart and Circulatory Physiology, 278, 2039-2049 (June 2000). This package also contains FastApEn() and FastSampEn() functions for calculating fast approximate entropy and fast sample entropy. These are newly designed very fast algorithms, resulting from the modification of the original algorithms. The calculated values of these entropies are not the same as the original ones, but the entropy trend of the analyzed time series determines equally reliably. Their main advantage is their speed, which is up to a thousand times higher. A scientific article describing their properties has been submitted to The Journal of Supercomputing and in present time it is waiting for the acceptance."}, "accelerometry": {"categories": ["Tracking"], "description": "A collection of functions that perform operations on time-series accelerometer data, such as identify non-wear time, flag minutes that are part of an activity bout, and find the maximum 10-minute average count value. The functions are generally very flexible, allowing for a variety of algorithms to be implemented. Most of the functions are written in C++ for efficiency."}, "plm": {"categories": ["CausalInference", "Econometrics", "SpatioTemporal"], "description": "A set of estimators for models and (robust) covariance matrices, and tests for panel data\n             econometrics, including within/fixed effects, random effects, between, first-difference, \n             nested random effects as well as instrumental-variable (IV) and Hausman-Taylor-style models,\n             panel generalized method of moments (GMM) and general FGLS models,\n             mean groups (MG), demeaned MG, and common correlated effects (CCEMG) and pooled (CCEP) estimators\n             with common factors, variable coefficients and limited dependent variables models.\n             Test functions include model specification, serial correlation, cross-sectional dependence,\n             panel unit root and panel Granger (non-)causality. Typical references are general econometrics \n             text books such as Baltagi (2021), Econometric Analysis of Panel Data, ISBN-13:978-3-030-53952-8, \n             Hsiao (2014), Analysis of Panel Data <doi:10.1017/CBO9781139839327>, and Croissant and Millo (2018), \n             Panel Data Econometrics with R, ISBN-13:978-1-118-94918-4."}, "RKelly": {"categories": ["SportsAnalytics"], "description": "Calculates the Kelly criterion (Kelly, J.L. (1956) <doi:10.1002/j.1538-7305.1956.tb03809.x>) for bets given quoted prices, model predictions and commissions.\n    Additionally it contains helper functions to calculate the probabilities for wins and draws in multi-leg games."}, "estudy2": {"categories": ["CausalInference", "Finance"], "description": "An implementation of a most commonly used event study methodology,\n    including both parametric and nonparametric tests. It contains variety\n    aspects of the rate of return estimation (the core calculation is done in\n    C++), as well as three classical for event study market models: mean\n    adjusted returns, market adjusted returns and single-index market models.\n    There are 6 parametric and 6 nonparametric tests provided, which examine\n    cross-sectional daily abnormal return (see the documentation of the\n    functions for more information). Parametric tests include tests proposed by \n    Brown and Warner (1980) <doi:10.1016/0304-405X(80)90002-1>, Brown and Warner\n    (1985) <doi:10.1016/0304-405X(85)90042-X>, Boehmer et al. (1991)\n    <doi:10.1016/0304-405X(91)90032-F>, Patell (1976) <doi:10.2307/2490543>, and\n    Lamb (1995) <doi:10.2307/253695>. Nonparametric tests covered in estudy2 are\n    tests described in Corrado and Zivney (1992) <doi:10.2307/2331331>,\n    McConnell and Muscarella (1985) <doi:10.1016/0304-405X(85)90006-6>,\n    Boehmer et al. (1991) <doi:10.1016/0304-405X(91)90032-F>, Cowan (1992)\n    <doi:10.1007/BF00939016>, Corrado (1989) <doi:10.1016/0304-405X(89)90064-0>,\n    Campbell and Wasley (1993) <doi:10.1016/0304-405X(93)90025-7>, Savickas (2003)\n    <doi:10.1111/1475-6803.00052>, Kolari and Pynnonen (2010)\n    <doi:10.1093/rfs/hhq072>. Furthermore, tests for the cumulative\n    abnormal returns proposed by Brown and Warner (1985)\n    <doi:10.1016/0304-405X(85)90042-X> and Lamb (1995) <doi:10.2307/253695>\n    are included."}, "twitteR": {"categories": ["WebTechnologies"], "description": "Provides an interface to the Twitter web API."}, "hydroGOF": {"categories": ["Environmetrics", "Hydrology"], "description": "S3 functions implementing both statistical and graphical goodness-of-fit measures between observed and simulated values, mainly oriented to be used during the calibration, validation, and application of hydrological models. Missing values in observed and/or simulated values can be removed before computations. Comments / questions / collaboration of any kind are very welcomed."}, "JointAI": {"categories": ["MissingData"], "description": "Joint analysis and imputation of incomplete data in the Bayesian\n    framework, using (generalized) linear (mixed) models and extensions there of,\n    survival models, or joint models for longitudinal and survival data, as\n    described in Erler, Rizopoulos and Lesaffre (2021) <doi:10.18637/jss.v100.i20>.\n    Incomplete covariates, if present, are automatically imputed.\n    The package performs some preprocessing of the data and creates a 'JAGS'\n    model, which will then automatically be passed to 'JAGS' \n    <https://mcmc-jags.sourceforge.io/> with the help of \n    the package 'rjags'."}, "lmomco": {"categories": ["Distributions", "ExtremeValue"], "description": "Extensive functions for L-moments (LMs) and probability-weighted moments\n  (PWMs), parameter estimation for distributions, LM computation for distributions, and\n  L-moment ratio diagrams. Maximum likelihood and maximum product of spacings estimation\n  are also available. LMs for right-tail and left-tail censoring by known or unknown\n  threshold and by indicator variable are available. Asymmetric (asy) trimmed LMs\n  (TL-moments, TLMs) are supported. LMs of residual (resid) and reversed (rev) resid life\n  are implemented along with 13 quantile function operators for reliability and survival\n  analyses. Exact analytical bootstrap estimates of order statistics, LMs, and variances-\n  covariances of LMs are provided. The Harri-Coble Tau34-squared Normality Test is available.\n  Distribution support with \"L\" (LMs), \"TL\" (TLMs) and added (+) support for right-tail\n  censoring (RC) encompasses: Asy Exponential (Exp) Power [L], Asy Triangular [L],\n  Cauchy [TL], Eta-Mu [L], Exp. [L], Gamma [L], Generalized (Gen) Exp Poisson [L],\n  Gen Extreme Value [L], Gen Lambda [L,TL], Gen Logistic [L), Gen Normal [L],\n  Gen Pareto [L+RC, TL], Govindarajulu [L], Gumbel [L], Kappa [L], Kappa-Mu [L],\n  Kumaraswamy [L], Laplace [L], Linear Mean Resid. Quantile Function [L], Normal [L],\n  3-p log-Normal [L], Pearson Type III [L], Rayleigh [L], Rev-Gumbel [L+RC], Rice/Rician [L],\n  Slash [TL], 3-p Student t [L], Truncated Exponential [L], Wakeby [L], and Weibull [L].\n  Multivariate sample L-comoments (LCMs) are implemented to measure asymmetric associations."}, "stampr": {"categories": ["SpatioTemporal"], "description": "Perform spatial temporal analysis of moving polygons; a\n    longstanding analysis problem in Geographic Information Systems. Facilitates\n    directional analysis, shape analysis, and some other simple functionality for\n    examining spatial-temporal patterns of moving polygons."}, "CMF": {"categories": ["MissingData"], "description": "Collective matrix factorization (CMF) finds joint low-rank\n\trepresentations for a collection of matrices with shared row or column\n\tentities. This code learns a variational Bayesian approximation for CMF,\n\tsupporting multiple likelihood potentials and missing data, while\n\tidentifying both factors shared by multiple matrices and factors private\n\tfor each matrix. For further details on the method see\n\tKlami et al. (2014) <arXiv:1312.5921>.\n\tThe package can also be used to learn Bayesian canonical correlation\n\tanalysis (CCA) and group factor analysis (GFA) models, both of which are\n\tspecial cases of CMF. This is likely to be useful for people looking for\n\tCCA and GFA solutions supporting missing data and non-Gaussian likelihoods.\n\tSee Klami et al. (2013) <http://www.jmlr.org/papers/v14/klami13a.html> and\n\tVirtanen et al. (2012) <http://proceedings.mlr.press/v22/virtanen12.html>\n\tfor details on Bayesian CCA and GFA, respectively."}, "robustbase": {"categories": ["Robust"], "description": "\"Essential\" Robust Statistics.\n Tools allowing to analyze data with robust methods.  This includes\n regression methodology including model selections and multivariate\n statistics where we strive to cover the book \"Robust Statistics,\n Theory and Methods\" by 'Maronna, Martin and Yohai'; Wiley 2006."}, "mvtsplot": {"categories": ["SpatioTemporal"], "description": "A function for plotting multivariate time series data."}, "Iscores": {"categories": ["MissingData"], "description": "Implementation of a KL-based scoring rule to assess the quality of different missing value imputations in the broad sense as introduced in Michel et al. (2021)  <arXiv:2106.03742>."}, "strucchange": {"categories": ["Econometrics", "Environmetrics", "Finance", "TimeSeries"], "description": "Testing, monitoring and dating structural changes in (linear)\n             regression models. strucchange features tests/methods from\n\t     the generalized fluctuation test framework as well as from\n\t     the F test (Chow test) framework. This includes methods to\n\t     fit, plot and test fluctuation processes (e.g., CUSUM, MOSUM,\n\t     recursive/moving estimates) and F statistics, respectively.\n             It is possible to monitor incoming data online using\n             fluctuation processes.\n             Finally, the breakpoints in regression models with structural\n             changes can be estimated together with confidence intervals.\n             Emphasis is always given to methods for visualizing the data."}, "DiagrammeR": {"categories": ["GraphicalModels"], "description": "\n    Build graph/network structures using functions for stepwise addition and\n    deletion of nodes and edges. Work with data available in tables for bulk\n    addition of nodes, edges, and associated metadata. Use graph selections\n    and traversals to apply changes to specific nodes or edges. A wide\n    selection of graph algorithms allow for the analysis of graphs. Visualize\n    the graphs and take advantage of any aesthetic properties assigned to\n    nodes and edges."}, "HH": {"categories": ["ClinicalTrials", "TeachingStatistics"], "description": "Support software for Statistical Analysis and Data Display (Second Edition, Springer, ISBN 978-1-4939-2121-8, 2015) and (First Edition, Springer, ISBN 0-387-40270-5, 2004) by Richard M. Heiberger and Burt Holland.  This contemporary presentation of statistical methods features extensive use of graphical displays for exploring data and for displaying the analysis.  The second edition includes redesigned graphics and additional chapters. The authors emphasize how to construct and interpret graphs, discuss principles of graphical design, and show how accompanying traditional tabular results are used to confirm the visual impressions derived directly from the graphs. Many of the graphical formats are novel and appear here for the first time in print.  All chapters have exercises.  All functions introduced in the book are in the package.  R code for all examples, both graphs and tables, in the book is included in the scripts directory of the package."}, "sparklyr": {"categories": ["Databases", "ModelDeployment"], "description": "R interface to Apache Spark, a fast and general\n    engine for big data processing, see <https://spark.apache.org/>. This\n    package supports connecting to local and remote Apache Spark clusters,\n    provides a 'dplyr' compatible back-end, and provides an interface to\n    Spark's built-in machine learning algorithms."}, "gaussquad": {"categories": ["NumericalMathematics"], "description": "A collection of functions to perform Gaussian quadrature\n        with different weight functions corresponding to the orthogonal\n        polynomials in package orthopolynom.  Examples verify the\n        orthogonality and inner products of the polynomials."}, "nmaplateplot": {"categories": ["MetaAnalysis"], "description": "A graphical display of results from network meta-analysis (NMA). \n  It is suitable for outcomes like odds ratio (OR), risk ratio (RR), \n  risk difference (RD) and standardized mean difference (SMD). \n  It also has an option to visually display and compare \n  the surface under the cumulative ranking (SUCRA) of different treatments."}, "tmap": {"categories": ["OfficialStatistics", "Spatial"], "description": "Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps."}, "stdReg": {"categories": ["CausalInference"], "description": "Contains functionality for regression standardization. Four general classes of models are allowed; generalized linear models, conditional generalized estimating equation models, Cox proportional hazards models and shared frailty gamma-Weibull models. Sjolander, A. (2016) <doi:10.1007/s10654-016-0157-3>."}, "htmltidy": {"categories": ["WebTechnologies"], "description": "HTML documents can be beautiful and pristine. They can also be\n    wretched, evil, malformed demon-spawn. Now, you can tidy up that HTML and XHTML\n    before processing it with your favorite angle-bracket crunching tools, going beyond\n    the limited tidying that 'libxml2' affords in the 'XML' and 'xml2' packages and\n    taming even the ugliest HTML code generated by the likes of Google Docs and Microsoft\n    Word. It's also possible to use the functions provided to format or \"pretty print\"\n    HTML content as it is being tidied. Utilities are also included that make it  \n    possible to view formatted and \"pretty printed\" HTML/XML\n    content from HTML/XML document objects, nodes, node sets and plain character HTML/XML\n    using 'vkbeautify' (by Vadim Kiryukhin) and 'highlight.js' (by Ivan Sagalaev).\n    Also (optionally) enables filtering of nodes via XPath or viewing an HTML/XML document\n    in \"tree\" view using 'XMLDisplay' (by Lev Muchnik). See\n    <https://github.com/vkiryukhin/vkBeautify> and \n    <http://www.levmuchnik.net/Content/ProgrammingTips/WEB/XMLDisplay/DisplayXMLFileWithJavascript.html> \n    for more information about 'vkbeautify' and 'XMLDisplay', respectively."}, "FindIt": {"categories": ["CausalInference"], "description": "The heterogeneous treatment effect estimation procedure \n        proposed by Imai and Ratkovic (2013)<doi:10.1214/12-AOAS593>.  \n        The proposed method is applicable, for\n        example, when selecting a small number of most (or least)\n        efficacious treatments from a large number of alternative\n        treatments as well as when identifying subsets of the\n        population who benefit (or are harmed by) a treatment of\n        interest. The method adapts the Support Vector Machine\n        classifier by placing separate LASSO constraints over the\n        pre-treatment parameters and causal heterogeneity parameters of\n        interest. This allows for the qualitative distinction between\n        causal and other parameters, thereby making the variable\n        selection suitable for the exploration of causal heterogeneity. \t\n        The package also contains a class of functions, CausalANOVA, \n        which estimates the average marginal interaction effects (AMIEs)\n        by a regularized ANOVA as proposed by Egami and Imai (2019)<doi:10.1080/01621459.2018.1476246>. \n        It contains a variety of regularization techniques to facilitate \n\tanalysis of large factorial experiments. "}, "tm.plugin.europresse": {"categories": ["NaturalLanguageProcessing"], "description": "Provides a 'tm' Source to create corpora from\n  articles exported from the 'Europresse' content provider as\n  HTML files. It is able to read both text content and meta-data\n  information (including source, date, title, author and pages)."}, "ArchaeoChron": {"categories": ["Bayesian"], "description": "Provides a list of functions for the Bayesian modeling of archaeological chronologies. The Bayesian models are implemented in 'JAGS' ('JAGS' stands for Just Another Gibbs Sampler. It is a program for the analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation. See <http://mcmc-jags.sourceforge.net/> and \"JAGS Version 4.3.0 user manual\", Martin Plummer (2017) <https://sourceforge.net/projects/mcmc-jags/files/Manuals/>.). The inputs are measurements with their associated standard deviations and the study period. The output is the MCMC sample of the posterior distribution of the event date with or without radiocarbon calibration. "}, "CAvariants": {"categories": ["Psychometrics"], "description": "Provides six variants of two-way correspondence analysis (ca):\n   simple ca, singly ordered ca, doubly ordered ca, non symmetrical ca,\n   singly ordered non symmetrical ca, and doubly ordered non symmetrical\n   ca."}, "denseFLMM": {"categories": ["FunctionalData"], "description": "Estimation of functional linear mixed models for densely sampled data based on functional principal component analysis."}, "TestDataImputation": {"categories": ["MissingData", "Psychometrics"], "description": "Functions for imputing missing item responses for dichotomous and\n    polytomous test and assessment data. This package enables missing imputation\n    methods that are suitable for test and assessment data, including: \n    listwise (LW) deletion (see De Ayala et al. 2001 <doi:10.1111/j.1745-3984.2001.tb01124.x>), \n    treating as incorrect (IN, see Lord, 1974 <doi:10.1111/j.1745-3984.1974.tb00996.x>; \n    Mislevy & Wu, 1996 <doi:10.1002/j.2333-8504.1996.tb01708.x>; \n    Pohl et al., 2014 <doi:10.1177/0013164413504926>), person mean imputation (PM), \n    item mean imputation (IM), two-way (TW) and response function (RF) imputation,\n    (see Sijtsma & van der Ark, 2003 <doi:10.1207/s15327906mbr3804_4>),\n    logistic regression (LR) imputation, predictive mean matching (PMM), and expectation\u2013maximization (EM) \n    imputation (see Finch, 2008 <doi:10.1111/j.1745-3984.2008.00062.x>)."}, "tnet": {"categories": ["CausalInference"], "description": "Binary ties limit the richness of network analyses as relations are unique. The two-mode structure contains a number of features lost when projection it to a one-mode network. Longitudinal datasets allow for an understanding of the causal relationship among ties, which is not the case in cross-sectional datasets as ties are dependent upon each other."}, "OrdFacReg": {"categories": ["Survival"], "description": "In biomedical studies, researchers are often interested in assessing the association between one or more ordinal explanatory variables and an outcome variable, at the same time adjusting for covariates of any type. The outcome variable may be continuous, binary, or represent censored survival times. In the absence of a precise knowledge of the response function, using monotonicity constraints on the ordinal variables improves efficiency in estimating parameters, especially when sample sizes are small. This package implements an active set algorithm that efficiently computes such estimators."}, "tranSurv": {"categories": ["Survival"], "description": "A latent, quasi-independent truncation time is assumed to be linked with the observed dependent truncation time, the event time, and an unknown transformation parameter via a structural transformation model. The transformation parameter is chosen to minimize the conditional Kendall's tau (Martin and Betensky, 2005) <doi:10.1198/016214504000001538> or the regression coefficient estimates (Jones and Crowley, 1992) <doi:10.2307/2336782>. The marginal distribution for the truncation time and the event time are completely left unspecified. The methodology is applied to survival curve estimation and regression analysis."}, "RPushbullet": {"categories": ["WebTechnologies"], "description": "An R interface to the Pushbullet messaging service which\n provides fast and efficient notifications (and file transfer) between\n computers, phones and tablets.  An account has to be registered at the \n site <https://www.pushbullet.com> site to obtain a (free) API key."}, "metaforest": {"categories": ["MetaAnalysis"], "description": "Conduct random forests-based meta-analysis, obtain partial dependence plots for metaforest and classic meta-analyses, and cross-validate and tune metaforest- and classic meta-analyses in conjunction with the caret package. A requirement of classic meta-analysis is that the studies being aggregated are conceptually similar, and ideally, close replications. However, in many fields, there is substantial heterogeneity between studies on the same topic. Classic meta-analysis lacks the power to assess more than a handful of univariate moderators. MetaForest, by contrast, has substantial power to explore heterogeneity in meta-analysis. It can identify important moderators from a larger set of potential candidates, even with as little as 20 studies (Van Lissa, in preparation). This is an appealing quality, because many meta-analyses have small sample sizes. Moreover, MetaForest yields a measure of variable importance which can be used to identify important moderators, and offers partial prediction plots to explore the shape of the marginal relationship between moderators and effect size."}, "FME": {"categories": ["Bayesian", "DifferentialEquations"], "description": "Provides functions to help in fitting models to data, to\n  perform Monte Carlo, sensitivity and identifiability analysis. It is\n  intended to work with models be written as a set of differential\n  equations that are solved either by an integration routine from\n  package 'deSolve', or a steady-state solver from package\n  'rootSolve'. However, the methods can also be used with other types of\n  functions."}, "ACSWR": {"categories": ["TeachingStatistics"], "description": "A book designed to meet the requirements of masters students. Tattar, P.N., Suresh, R., and Manjunath, B.G. \"A Course in Statistics with R\", J. Wiley, ISBN 978-1-119-15272-9. "}, "wktmo": {"categories": ["TimeSeries"], "description": "Converts weekly data to monthly data.\n     Users can use three types of week formats: ISO week, epidemiology week (epi week) and calendar date. "}, "AmericanCallOpt": {"categories": ["Finance"], "description": "This package includes a set of pricing functions for\n        American call options. The following cases are covered: Pricing\n        of an American call using the standard binomial approximation;\n        Hedge parameters for an American call with a standard binomial\n        tree; Binomial pricing of an American call with continuous\n        payout from the underlying asset; Binomial pricing of an\n        American call with an underlying stock that pays proportional\n        dividends in discrete time; Pricing of an American call on\n        futures using a binomial approximation; Pricing of a currency\n        futures American call using a binomial approximation; Pricing\n        of a perpetual American call. The user should kindly notice\n        that this material is for educational purposes only. The codes\n        are not optimized for computational efficiency as they are\n        meant to represent standard cases of analytical and numerical\n        solution."}, "imputeYn": {"categories": ["Survival"], "description": "Method brings less bias and more efficient estimates for AFT models."}, "bayesloglin": {"categories": ["Bayesian"], "description": "The function MC3() searches for log-linear models with the highest posterior probability. The function gibbsSampler() is a blocked Gibbs sampler for sampling from the posterior distribution of the log-linear parameters. The functions findPostMean() and findPostCov() compute the posterior mean and covariance matrix for decomposable models which, for these models, is available in closed form."}, "car": {"categories": ["Econometrics", "Finance", "TeachingStatistics"], "description": "\n  Functions to Accompany J. Fox and S. Weisberg, \n  An R Companion to Applied Regression, Third Edition, Sage, 2019."}, "mitml": {"categories": ["MissingData"], "description": "Provides tools for multiple imputation of missing data in multilevel\n modeling. Includes a user-friendly interface to the packages 'pan' and 'jomo',\n and several functions for visualization, data management and the analysis \n of multiply imputed data sets."}, "Rborist": {"categories": ["HighPerformanceComputing", "MachineLearning"], "description": "Scalable implementation of classification and regression forests, as described by Breiman (2001), <doi:10.1023/A:1010933404324>."}, "treemap": {"categories": ["OfficialStatistics"], "description": "A treemap is a space-filling visualization of hierarchical\n    structures. This package offers great flexibility to draw treemaps."}, "epinet": {"categories": ["Epidemiology"], "description": "A collection of epidemic/network-related tools. Simulates transmission of diseases through contact networks. Performs Bayesian inference on network and epidemic parameters, given epidemic data."}, "jarbes": {"categories": ["MetaAnalysis"], "description": "Provides a new class of Bayesian meta-analysis models that incorporates a model for internal and external validity bias. In this way, it is possible to combine studies of diverse quality and different types. For example, we can combine the results of randomized control trials (RCTs) with the results of observational studies (OS). "}, "PTSR": {"categories": ["TimeSeries"], "description": "A collection of functions to simulate, estimate and forecast a wide range of regression based dynamic models for positive time series. \n\tThis package implements the results presented in Prass, T.S.; Carlos, J.H.; Taufemback, C.G. and Pumi, G. (2022). \"Positive Time Series Regression\" <arXiv:2201.03667>."}, "mvQuad": {"categories": ["NumericalMathematics"], "description": "Provides methods to construct multivariate grids, which can be used\n    for multivariate quadrature. This grids can be based on different quadrature\n    rules like Newton-Cotes formulas (trapezoidal-, Simpson's- rule, ...) or Gauss\n    quadrature (Gauss-Hermite, Gauss-Legendre, ...). For the construction of the\n    multidimensional grid the product-rule or the combination- technique can be\n    applied."}, "BEST": {"categories": ["Bayesian"], "description": "An alternative to t-tests, producing posterior estimates for group means and standard deviations and their differences and effect sizes. It implements the method of Kruschke (2013) Bayesian estimation supersedes the t test. Journal of Experimental Psychology: General, 142(2):573-603 <doi:10.1037/a0029146>."}, "RobustBayesianCopas": {"categories": ["MetaAnalysis"], "description": "Fits the robust Bayesian Copas (RBC) selection model of Bai et al. (2020) <arXiv:2005.02930> for correcting and quantifying publication bias in univariate meta-analysis. Also fits standard random effects meta-analysis and the Copas-like selection model of Ning et al. (2017) <doi:10.1093/biostatistics/kxx004>. "}, "AzureQstor": {"categories": ["WebTechnologies"], "description": "An interface to 'Azure Queue Storage'. This is a cloud service for storing large numbers of messages, for example from automated sensors, that can be accessed remotely via authenticated calls using HTTP or HTTPS. Queue storage is often used to create a backlog of work to process asynchronously. Part of the 'AzureR' family of packages."}, "RSiteCatalyst": {"categories": ["WebTechnologies"], "description": "Functions for interacting with the Adobe Analytics API V1.4\n    (<https://api.omniture.com/admin/1.4/rest/>)."}, "RSAGA": {"categories": ["Spatial"], "description": "Provides access to geocomputing and terrain analysis functions\n    of the geographical information system (GIS) 'SAGA' (System for Automated\n    Geoscientific Analyses) from within R by running the command line version of\n    SAGA. This package furthermore provides several R functions for handling ASCII\n    grids, including a flexible framework for applying local functions (including\n    predict methods of fitted models) and focal functions to multiple grids. SAGA\n    GIS is available under GPLv2 / LGPLv2 licence from \n    <http://sourceforge.net/projects/saga-gis/>."}, "Rlgt": {"categories": ["TimeSeries"], "description": "An implementation of a number of Global Trend models for time series forecasting \n    that are Bayesian generalizations and extensions of some Exponential Smoothing models. \n    The main differences/additions include 1) nonlinear global trend, 2) Student-t error \n    distribution, and 3) a function for the error size, so heteroscedasticity. The methods \n    are particularly useful for short time series. When tested on the well-known M3 dataset,\n    they are able to outperform all classical time series algorithms. The models are fitted \n    with MCMC using the 'rstan' package."}, "mniw": {"categories": ["Distributions"], "description": "Density evaluation and random number generation for the Matrix-Normal Inverse-Wishart (MNIW) distribution, as well as the the Matrix-Normal, Matrix-T, Wishart, and Inverse-Wishart distributions.  Core calculations are implemented in a portable (header-only) C++ library, with matrix manipulations using the 'Eigen' library for linear algebra.  Also provided is a Gibbs sampler for Bayesian inference on a random-effects model with multivariate normal observations."}, "circular": {"categories": ["Distributions", "Environmetrics"], "description": "Circular Statistics, from \"Topics in circular Statistics\" (2001) S. Rao Jammalamadaka and A. SenGupta, World Scientific."}, "mem": {"categories": ["Epidemiology"], "description": "The Moving Epidemic Method, created by T Vega and JE Lozano (2012, 2015) <doi:10.1111/j.1750-2659.2012.00422.x>, <doi:10.1111/irv.12330>, allows the weekly assessment of the epidemic and intensity status to help in routine respiratory infections surveillance in health systems. Allows the comparison of different epidemic indicators, timing and shape with past epidemics and across different regions or countries with different surveillance systems. Also, it gives a measure of the performance of the method in terms of sensitivity and specificity of the alert week."}, "revss": {"categories": ["Robust"], "description": "Implements the estimation techniques described in Rousseeuw &\n    Verboven (2002) <doi:10.1016/S0167-9473(02)00078-6> for the location and\n    scale of very small samples."}, "Rcmdr": {"categories": ["Finance", "TeachingStatistics"], "description": "\n  A platform-independent basic-statistics GUI (graphical user interface) for R, based on the tcltk package."}, "abtest": {"categories": ["Bayesian"], "description": "Provides functions for Bayesian A/B testing including prior elicitation\n    options based on Kass and Vaidyanathan (1992) <doi:10.1111/j.2517-6161.1992.tb01868.x>. \n    Gronau, Raj K. N., & Wagenmakers (2021) <doi:10.18637/jss.v100.i17>."}, "arulesSequences": {"categories": ["ModelDeployment"], "description": "Add-on for arules to handle and mine frequent sequences.\n    Provides interfaces to the C++ implementation of cSPADE by  \n    Mohammed J. Zaki."}, "bartBMA": {"categories": ["Bayesian"], "description": "\"BART-BMA Bayesian Additive Regression Trees using Bayesian Model Averaging\" (Hernandez B, Raftery A.E., Parnell A.C. (2018) <doi:10.1007/s11222-017-9767-1>) is an extension to the original BART sum-of-trees model (Chipman et al 2010). BART-BMA differs to the original BART model in two main aspects in order to implement a greedy model which \n  will be computationally feasible for high dimensional data. Firstly BART-BMA uses a greedy search for the best split points and variables when growing decision trees within each sum-of-trees \n  model. This means trees are only grown based on the most predictive set of split rules. Also rather than using Markov chain Monte Carlo (MCMC), BART-BMA uses a greedy implementation of Bayesian Model Averaging called Occam's Window \n  which take a weighted average over multiple sum-of-trees models to form its overall prediction. This means that only the set of sum-of-trees for which there is high support from the data\n  are saved to memory and used in the final model."}, "soptdmaeA": {"categories": ["ExperimentalDesign"], "description": "Computes sequential A-, MV-, D- and E-optimal or near-optimal block and row-column designs for two-colour cDNA microarray experiments using the linear fixed effects and mixed effects models where the interest is in a comparison of all possible elementary treatment contrasts. The package also provides an optional method of using the graphical user interface (GUI) R package 'tcltk' to ensure that it is user friendly."}, "credule": {"categories": ["Finance"], "description": "It provides functions to bootstrap Credit Curves from market quotes (Credit Default Swap - CDS - spreads) and price Credit Default Swaps - CDS."}, "DEoptim": {"categories": ["Optimization"], "description": "Implements the Differential Evolution algorithm for global optimization of a real-valued function \n      of a real-valued parameter vector as described in Mullen et al. (2011) <doi:10.18637/jss.v040.i06>."}, "rmumps": {"categories": ["NumericalMathematics"], "description": "Some basic features of 'MUMPS' (Multifrontal Massively Parallel\n         sparse direct Solver) are wrapped in a class whose methods can be used\n         for sequentially solving a sparse linear system (symmetric or not)\n         with one or many right hand sides (dense or sparse).\n         There is a possibility to do separately symbolic analysis,\n         LU (or LDL^t) factorization and system solving.\n         Third part ordering libraries are included and can be used: 'PORD', 'METIS', 'SCOTCH'.\n         'MUMPS' method was first described in Amestoy et al. (2001) <doi:10.1137/S0895479899358194>\n         and Amestoy et al. (2006) <doi:10.1016/j.parco.2005.07.004>."}, "crrSC": {"categories": ["Survival"], "description": "Extension of 'cmprsk' to Stratified and Clustered data.    \n      A goodness of fit test for Fine-Gray model is also provided.        \n      Methods are detailed in the following articles: Zhou et al. (2011) <doi:10.1111/j.1541-0420.2010.01493.x>,\n      Zhou et al. (2012) <doi:10.1093/biostatistics/kxr032>, \n      Zhou et al. (2013) <doi:10.1002/sim.5815>."}, "NonCompart": {"categories": ["Pharmacokinetics"], "description": "Conduct a noncompartmental analysis with industrial strength.\n             Some features are\n             1) Use of CDISC SDTM terms\n             2) Automatic or manual slope selection\n             3) Supporting both 'linear-up linear-down' and 'linear-up log-down' method\n             4) Interval(partial) AUCs with 'linear' or 'log' interpolation method\n             * Reference: Gabrielsson J, Weiner D. Pharmacokinetic and Pharmacodynamic Data Analysis - Concepts and Applications. 5th ed. 2016. (ISBN:9198299107)."}, "rio": {"categories": ["WebTechnologies"], "description": "Streamlined data import and export by making assumptions that\n    the user is probably willing to make: 'import()' and 'export()' determine\n    the data structure from the file extension, reasonable defaults are used for\n    data import and export (e.g., 'stringsAsFactors=FALSE'), web-based import is\n    natively supported (including from SSL/HTTPS), compressed files can be read\n    directly without explicit decompression, and fast import packages are used where\n    appropriate. An additional convenience function, 'convert()', provides a simple\n    method for converting between file types."}, "graphicalExtremes": {"categories": ["ExtremeValue"], "description": "Statistical methodology for sparse multivariate extreme value models. Methods are\n             provided for exact simulation and statistical inference for multivariate Pareto distributions\n             on graphical structures as described in the paper 'Graphical Models for Extremes' by\n             Engelke and Hitz (2018) <arXiv:1812.01734>. "}, "GenSA": {"categories": ["Optimization"], "description": "Performs search for global minimum of a very complex non-linear\n    objective function with a very large number of optima."}, "penalized": {"categories": ["MachineLearning", "Survival"], "description": "Fitting possibly high dimensional penalized\n        regression models. The penalty structure can be any combination\n        of an L1 penalty (lasso and fused lasso), an L2 penalty (ridge) and a\n        positivity constraint on the regression coefficients. The\n        supported regression models are linear, logistic and Poisson\n        regression and the Cox Proportional Hazards model.\n        Cross-validation routines allow optimization of the tuning\n        parameters."}, "lamW": {"categories": ["NumericalMathematics"], "description": "Implements both real-valued branches of the Lambert-W function\n    (Corless et al, 1996) <doi:10.1007/BF02124750> without the need for\n    installing the entire GSL."}, "Rlinsolve": {"categories": ["NumericalMathematics"], "description": "Solving a system of linear equations is one of the most fundamental\n    computational problems for many fields of mathematical studies, such as\n    regression problems from statistics or numerical partial differential equations.\n    We provide basic stationary iterative solvers such as Jacobi, Gauss-Seidel,\n    Successive Over-Relaxation and SSOR methods. Nonstationary, also known as\n\tKrylov subspace methods are also provided. Sparse matrix computation is also supported\n    in that solving large and sparse linear systems can be manageable using 'Matrix' package\n    along with 'RcppArmadillo'. For a more detailed description, see a book by Saad (2003)\n    <doi:10.1137/1.9780898718003>."}, "rms": {"categories": ["Econometrics", "ReproducibleResearch", "Survival"], "description": "Regression modeling, testing, estimation, validation,\n\tgraphics, prediction, and typesetting by storing enhanced model design\n\tattributes in the fit.  'rms' is a collection of functions that\n\tassist with and streamline modeling.  It also contains functions for\n\tbinary and ordinal logistic regression models, ordinal models for\n  continuous Y with a variety of distribution families, and the Buckley-James\n\tmultiple regression model for right-censored responses, and implements\n\tpenalized maximum likelihood estimation for logistic and ordinary\n\tlinear models.  'rms' works with almost any regression model, but it\n\twas especially written to work with binary or ordinal regression\n\tmodels, Cox regression, accelerated failure time models,\n\tordinary linear models,\tthe Buckley-James model, generalized least\n\tsquares for serially or spatially correlated observations, generalized\n\tlinear models, and quantile regression."}, "mixexp": {"categories": ["ExperimentalDesign"], "description": "Functions for creating designs for mixture experiments, making ternary contour plots, and making mixture effect plots."}, "causaloptim": {"categories": ["CausalInference"], "description": "When causal quantities are not identifiable from the observed data, it still may be possible \n            to bound these quantities using the observed data. We outline a class of problems for which the \n            derivation of tight bounds is always a linear programming problem and can therefore, at least \n            theoretically, be solved using a symbolic linear optimizer. We extend and generalize the \n            approach of Balke and Pearl (1994) <doi:10.1016/B978-1-55860-332-5.50011-0> and we provide \n            a user friendly graphical interface for setting up such problems via directed acyclic \n            graphs (DAG), which only allow for problems within this class to be depicted. The user can \n            then define linear constraints to further refine their assumptions to meet their specific \n            problem, and then specify a causal query using a text interface. The program converts this \n            user defined DAG, query, and constraints, and returns tight bounds. The bounds can be \n            converted to R functions to evaluate them for specific datasets, and to latex code for \n            publication. The methods and proofs of tightness and validity of the bounds are described in\n            a preprint by Sachs, Gabriel, and Sj\u00f6lander (2021) \n            <https://sachsmc.github.io/causaloptim/articles/CausalBoundsMethods.pdf>."}, "coxrobust": {"categories": ["Robust", "Survival"], "description": "An implementation of robust estimation in Cox model. Functionality includes fitting efficiently and robustly Cox proportional hazards regression model in its basic form, where explanatory variables are time independent with one event per subject.  Method is based on a smooth modification of the partial likelihood."}, "rdd": {"categories": ["Econometrics"], "description": "Provides the tools to undertake estimation in\n    Regression Discontinuity Designs. Both sharp and fuzzy designs are\n    supported. Estimation is accomplished using local linear regression.\n    A provided function will utilize Imbens-Kalyanaraman optimal\n    bandwidth calculation. A function is also included to test the\n    assumption of no-sorting effects."}, "AzureKusto": {"categories": ["WebTechnologies"], "description": "An interface to 'Azure Data Explorer', also known as 'Kusto', a fast, highly scalable data exploration service from Microsoft: <https://azure.microsoft.com/en-us/services/data-explorer/>. Includes 'DBI' and 'dplyr' interfaces, with the latter modelled after the 'dbplyr' package, whereby queries are translated from R into the native 'KQL' query language and executed lazily. On the admin side, the package extends the object framework provided by 'AzureRMR' to support creation and deletion of databases, and management of database principals. Part of the 'AzureR' family of packages."}, "bnstruct": {"categories": ["GraphicalModels", "MissingData"], "description": "Bayesian Network Structure Learning from Data with Missing Values.\n    The package implements the Silander-Myllymaki complete search,\n    the Max-Min Parents-and-Children, the Hill-Climbing, the\n    Max-Min Hill-climbing heuristic searches, and the Structural\n    Expectation-Maximization algorithm. Available scoring functions are\n    BDeu, AIC, BIC. The package also implements methods for generating and using\n    bootstrap samples, imputed data, inference."}, "polyMatrix": {"categories": ["NumericalMathematics"], "description": "\n  Implementation of class \"polyMatrix\" for storing a matrix of polynomials and implements \n  basic matrix operations; including a determinant and characteristic polynomial.\n  It is based on the package 'polynom' and uses a lot of its methods to implement matrix operations.\n  This package includes 3 methods of triangularization of polynomial matrices:\n  Extended Euclidean algorithm which is most classical but numerically unstable;\n  Sylvester algorithm based on LQ decomposition;\n  Interpolation algorithm is based on LQ decomposition and Newton interpolation.\n  Both methods are described in \n  D. Henrion & M. Sebek, Reliable numerical methods for polynomial matrix triangularization,\n  IEEE Transactions on Automatic Control (Volume 44, Issue 3, Mar 1999, Pages 497-508) <doi:10.1109/9.751344>\n  and in \n  Salah Labhalla, Henri Lombardi & Roger Marlin, \n  Algorithmes de calcule de la reduction de Hermite d'une matrice a coefficients polynomeaux,\n  Theoretical Computer Science (Volume 161, Issue 1-2, July 1996, Pages 69-92) <doi:10.1016/0304-3975(95)00090-9>."}, "loglognorm": {"categories": ["Distributions"], "description": "Functions to sample from the double log normal distribution and calculate the density, distribution and quantile functions."}, "AzureContainers": {"categories": ["WebTechnologies"], "description": "An interface to container functionality in Microsoft's 'Azure' cloud: <https://azure.microsoft.com/en-us/product-categories/containers/>. Manage 'Azure Container Instance' (ACI), 'Azure Container Registry' (ACR) and 'Azure Kubernetes Service' (AKS) resources, push and pull images, and deploy services. On the client side, lightweight shells to the 'docker', 'docker-compose', 'kubectl' and 'helm' commandline tools are provided. Part of the 'AzureR' family of packages."}, "dbparser": {"categories": ["Epidemiology"], "description": "This tool is for parsing the 'DrugBank' XML database <https://www.drugbank.ca/>. The parsed \n    data are then returned in a proper 'R' dataframe with the ability to save \n    them in a given database."}, "ncar": {"categories": ["Pharmacokinetics"], "description": "Conduct a noncompartmental analysis with industrial strength.\n             Some features are\n             1) CDISC SDTM terms\n             2) Automatic or manual slope selection\n             3) Supporting both 'linear-up linear-down' and 'linear-up log-down' method\n             4) Interval(partial) AUCs with 'linear' or 'log' interpolation method\n             5) Produce pdf, rtf, text report files.\n             * Reference: Gabrielsson J, Weiner D. Pharmacokinetic and Pharmacodynamic Data Analysis - Concepts and Applications. 5th ed. 2016. (ISBN:9198299107)."}, "climate": {"categories": ["Hydrology"], "description": "Automatize downloading of meteorological and hydrological data from publicly available repositories:\n    OGIMET (<http://ogimet.com/index.phtml.en>), \n    University of Wyoming - atmospheric vertical profiling data (<http://weather.uwyo.edu/upperair/>),\n    Polish Institute of Meterology and Water Management - National Research Institute (<https://danepubliczne.imgw.pl>),\n    and National Oceanic & Atmospheric Administration (NOAA).\n    This package also allows for adding geographical coordinates for each observation."}, "npsurv": {"categories": ["Survival"], "description": "Non-parametric survival analysis of exact and\n\t     interval-censored observations. The methods implemented\n\t     are developed by Wang (2007)\n\t     <doi:10.1111/j.1467-9868.2007.00583.x>, Wang (2008)\n\t     <doi:10.1016/j.csda.2007.10.018>, Wang and Taylor (2013)\n\t     <doi:10.1007/s11222-012-9341-9> and Wang and Fani (2018)\n\t     <doi:10.1007/s11222-017-9724-z>."}, "factoextra": {"categories": ["Cluster"], "description": "Provides some easy-to-use functions to extract and visualize the\n    output of multivariate data analyses, including 'PCA' (Principal Component\n    Analysis), 'CA' (Correspondence Analysis), 'MCA' (Multiple Correspondence\n    Analysis), 'FAMD' (Factor Analysis of Mixed Data), 'MFA' (Multiple Factor Analysis) and 'HMFA' (Hierarchical Multiple\n    Factor Analysis) functions from different R packages. It contains also functions\n    for simplifying some clustering analysis steps and provides 'ggplot2' - based\n    elegant data visualization."}, "spam": {"categories": ["Distributions"], "description": "Set of functions for sparse matrix algebra.\n    Differences with other sparse matrix packages are:\n    (1) we only support (essentially) one sparse matrix format,\n    (2) based on transparent and simple structure(s),\n    (3) tailored for MCMC calculations within G(M)RF.\n    (4) and it is fast and scalable (with the extension package spam64).\n    Documentation about 'spam' is provided by vignettes included in this package, see also Furrer and Sain (2010) <doi:10.18637/jss.v036.i10>; see 'citation(\"spam\")' for details."}, "alleHap": {"categories": ["MissingData"], "description": "Tools to simulate alphanumeric alleles, impute genetic missing data and reconstruct non-recombinant haplotypes from pedigree databases in a deterministic way. Allelic simulations can be implemented taking into account many factors (such as number of families, markers, alleles per marker,\n    probability and proportion of missing genotypes, recombination rate, etc).\n    Genotype imputation can be used with simulated datasets or real databases (previously loaded in .ped format). Haplotype reconstruction can be carried\n    out even with missing data, since the program firstly imputes each family genotype (without a reference panel), to later reconstruct the corresponding\n    haplotypes for each family member. All this considering that each individual (due to meiosis) should unequivocally have two alleles per marker (one inherited\n    from each parent) and thus imputation and reconstruction results can be deterministically calculated."}, "FAOSTAT": {"categories": ["OfficialStatistics"], "description": "Download Data from the FAOSTAT Database of the Food and Agricultural Organization (FAO) of the United Nations.\n    A list of functions to download statistics from FAOSTAT (database of the FAO <https://www.fao.org/faostat/>) \n    and WDI (database of the World Bank <https://data.worldbank.org/>), and to perform some harmonization operations."}, "europepmc": {"categories": ["WebTechnologies"], "description": "An R Client for the Europe PubMed Central RESTful Web Service\n    (see <https://europepmc.org/RestfulWebService> for more information). It\n    gives access to both metadata on life science literature and open access\n    full texts. Europe PMC indexes all PubMed content and other literature\n    sources including Agricola, a bibliographic database of citations to the\n    agricultural literature, or Biological Patents. In addition to bibliographic\n    metadata, the client allows users to fetch citations and reference lists.\n    Links between life-science literature and other EBI databases, including\n    ENA, PDB or ChEMBL are also accessible. No registration or API key is\n    required. See the vignettes for usage examples."}, "faraway": {"categories": ["TeachingStatistics"], "description": "Books are \"Practical Regression and ANOVA in R\" on CRAN, \"Linear Models with R\" published 1st Ed. August 2004, 2nd Ed. July 2014 by CRC press, ISBN 9781439887332, and \"Extending the Linear Model with R\" published by CRC press in 1st Ed. December 2005 and 2nd Ed. March 2016, ISBN 9781584884248."}, "ecm": {"categories": ["TimeSeries"], "description": "Functions for easy building of error correction models (ECM) for time series regression. "}, "clarifai": {"categories": ["WebTechnologies"], "description": "Get description of images from Clarifai API. For more information,\n    see <http://clarifai.com>. Clarifai uses a large deep learning cloud to come\n    up with descriptive labels of the things in an image. It also provides how\n    confident it is about each of the labels."}, "BoomSpikeSlab": {"categories": ["Bayesian"], "description": "Spike and slab regression with a variety of residual error\n  distributions corresponding to Gaussian, Student T, probit, logit, SVM, and a\n  few others.  Spike and slab regression is Bayesian regression with prior\n  distributions containing a point mass at zero.  The posterior updates the\n  amount of mass on this point, leading to a posterior distribution that is\n  actually sparse, in the sense that if you sample from it many coefficients are\n  actually zeros.  Sampling from this posterior distribution is an elegant way\n  to handle Bayesian variable selection and model averaging.  See\n  <doi:10.1504/IJMMNO.2014.059942> for an explanation of the Gaussian case."}, "NADIA": {"categories": ["MissingData"], "description": "Creates a uniform interface for several advanced imputations missing data methods. Every available method can be used as a part of 'mlr3' pipelines which allows easy tuning and performance evaluation. Most of the used functions work separately on the training and test sets (imputation is trained on the training set and impute training data. After that imputation is again trained on the test set and impute test data)."}, "selectMeta": {"categories": ["MetaAnalysis"], "description": "Publication bias, the fact that studies identified for inclusion in a meta analysis do not represent all studies on the topic of interest, is commonly recognized as a threat to the validity of the results of a meta analysis. One way to explicitly model publication bias is via selection models or weighted probability distributions. In this package we provide implementations of several parametric and nonparametric weight functions. The novelty in Rufibach (2011) is the proposal of a non-increasing variant of the nonparametric weight function of Dear & Begg (1992). The new approach potentially offers more insight in the selection process than other methods, but is more flexible than parametric approaches. To maximize the log-likelihood function proposed by Dear & Begg (1992) under a monotonicity constraint we use a differential evolution algorithm proposed by Ardia et al (2010a, b) and implemented in Mullen et al (2009). In addition, we offer a method to compute a confidence interval for the overall effect size theta, adjusted for selection bias as well as a function that computes the simulation-based p-value to assess the null hypothesis of no selection as described in Rufibach (2011, Section 6)."}, "compHclust": {"categories": ["Cluster"], "description": "Performs the complementary hierarchical clustering procedure and returns X' (the expected residual matrix) and a vector of the relative gene importances."}, "GenBinomApps": {"categories": ["Distributions"], "description": "Density, distribution function, quantile function and random generation for the Generalized Binomial Distribution. Functions to compute the Clopper-Pearson Confidence Interval and the required sample size. Enhanced model for burn-in studies, where failures are tackled by countermeasures."}, "RLT": {"categories": ["MachineLearning"], "description": "Random forest with a variety of additional features for regression, classification and survival analysis. The features include: parallel computing with OpenMP, embedded model for selecting the splitting variable (based on Zhu, Zeng & Kosorok, 2015), subject weight, variable weight, tracking subjects used in each tree, etc."}, "sda": {"categories": ["MachineLearning"], "description": "Provides an efficient framework for \n   high-dimensional linear and diagonal discriminant analysis with \n   variable selection.  The classifier is trained using James-Stein-type \n   shrinkage estimators and predictor variables are ranked using \n   correlation-adjusted t-scores (CAT scores).  Variable selection error \n   is controlled using false non-discovery rates or higher criticism."}, "mscstexta4r": {"categories": ["NaturalLanguageProcessing", "WebTechnologies"], "description": "R Client for the Microsoft Cognitive Services Text Analytics\n    REST API, including Sentiment Analysis, Topic Detection, Language Detection,\n    and Key Phrase Extraction. An account MUST be registered at the Microsoft\n    Cognitive Services website <https://www.microsoft.com/cognitive-services/>\n    in order to obtain a (free) API key. Without an API key, this package will\n    not work properly."}, "frailtySurv": {"categories": ["Survival"], "description": "Simulates and fits semiparametric shared frailty models under a\n    wide range of frailty distributions using a consistent and\n    asymptotically-normal estimator. Currently supports: gamma, power variance\n    function, log-normal, and inverse Gaussian frailty models."}, "cacIRT": {"categories": ["Psychometrics"], "description": "Computes classification accuracy and consistency indices under Item Response Theory. Implements the total score IRT-based methods in Lee, Hanson & Brennen (2002) and Lee (2010), the IRT-based methods in Rudner (2001, 2005), and the total score nonparametric methods in Lathrop & Cheng (2014). For dichotomous and polytomous tests."}, "missForest": {"categories": ["MissingData"], "description": "The function 'missForest' in this package is used to\n        impute missing values particularly in the case of mixed-type\n        data. It uses a random forest trained on the observed values of\n        a data matrix to predict the missing values. It can be used to\n        impute continuous and/or categorical data including complex\n        interactions and non-linear relations. It yields an out-of-bag\n        (OOB) imputation error estimate without the need of a test set\n        or elaborate cross-validation. It can be run in parallel to \n        save computation time."}, "rdbnomics": {"categories": ["OfficialStatistics", "TimeSeries"], "description": "R access to hundreds of millions data series from DBnomics API\n    (<https://db.nomics.world/>)."}, "spatialsample": {"categories": ["Spatial"], "description": "Functions and classes for spatial resampling to use with the\n    'rsample' package, such as spatial cross-validation (Brenning, 2012)\n    <doi:10.1109/IGARSS.2012.6352393>. The scope of 'rsample' and\n    'spatialsample' is to provide the basic building blocks for creating\n    and analyzing resamples of a spatial data set, but neither package\n    includes functions for modeling or computing statistics. The resampled\n    spatial data sets created by 'spatialsample' do not contain much\n    overhead in memory."}, "smoothSurv": {"categories": ["Survival"], "description": "Contains, as a main contribution, a function to fit\n             a regression model with possibly right, left or interval\n             censored observations and with the error distribution\n             expressed as a mixture of G-splines. Core part\n             of the computation is done in compiled C++ written\n             using the Scythe Statistical Library Version 0.3."}, "bayes4psy": {"categories": ["Bayesian"], "description": "Contains several Bayesian models for data analysis of psychological tests. A user friendly interface for these models should enable students and researchers to perform professional level Bayesian data analysis without advanced knowledge in programming and Bayesian statistics. This package is based on the Stan platform (Carpenter et el. 2017 <doi:10.18637/jss.v076.i01>)."}, "MarkowitzR": {"categories": ["Finance"], "description": "A collection of tools for analyzing significance of \n    Markowitz portfolios."}, "worcs": {"categories": ["ReproducibleResearch"], "description": "Create reproducible and transparent research projects in 'R'.\n    This package is based on the Workflow for Open\n    Reproducible Code in Science (WORCS), a step-by-step procedure based on best\n    practices for\n    Open Science. It includes an 'RStudio' project template, several\n    convenience functions, and all dependencies required to make your project\n    reproducible and transparent. WORCS is explained in the tutorial paper\n    by Van Lissa, Brandmaier, Brinkman, Lamprecht, Struiksma, & Vreede (2021).\n    <doi:10.3233/DS-210031>."}, "gcbd": {"categories": ["HighPerformanceComputing"], "description": "'GPU'/CPU Benchmarking on Debian-package based systems\n This package benchmarks performance of a few standard linear algebra\n operations (such as a matrix product and QR, SVD and LU decompositions)\n across a number of different 'BLAS' libraries as well as a 'GPU' implementation.\n To do so, it takes advantage of the ability to 'plug and play' different\n 'BLAS' implementations easily on a Debian and/or Ubuntu system.  The current\n version supports\n  - 'Reference BLAS' ('refblas') which are un-accelerated as a baseline\n  - Atlas which are tuned but typically configure single-threaded\n  - Atlas39 which are tuned and configured for multi-threaded mode\n  - 'Goto Blas' which are accelerated and multi-threaded\n  - 'Intel MKL' which is a commercial accelerated and multithreaded version.\n As for 'GPU' computing, we use the CRAN package\n  - 'gputools'\n For 'Goto Blas', the 'gotoblas2-helper' script from the ISM in Tokyo can be\n used. For 'Intel MKL' we use the Revolution R packages from Ubuntu 9.10."}, "arrow": {"categories": ["HighPerformanceComputing"], "description": "'Apache' 'Arrow' <https://arrow.apache.org/> is a cross-language\n    development platform for in-memory data. It specifies a standardized\n    language-independent columnar memory format for flat and hierarchical data,\n    organized for efficient analytic operations on modern hardware. This\n    package provides an interface to the 'Arrow C++' library."}, "mixtools": {"categories": ["Cluster", "Distributions"], "description": "Analyzes finite mixture models for various parametric and semiparametric settings.  This includes mixtures of parametric distributions (normal, multivariate normal, multinomial, gamma), various Reliability Mixture Models (RMMs), mixtures-of-regressions settings (linear regression, logistic regression, Poisson regression, linear regression with changepoints, predictor-dependent mixing proportions, random effects regressions, hierarchical mixtures-of-experts), and tools for selecting the number of components (bootstrapping the likelihood ratio test statistic, mixturegrams, and model selection criteria).  Bayesian estimation of mixtures-of-linear-regressions models is available as well as a novel data depth method for obtaining credible bands.  This package is based upon work supported by the National Science Foundation under Grant No. SES-0518772."}, "CANSIM2R": {"categories": ["OfficialStatistics"], "description": "Extract CANSIM (Statistics Canada) tables and transform them into readily usable data in panel (wide) format. It can also extract more than one table at a time and produce the resulting merge by time period and geographical region."}, "spatialreg": {"categories": ["Econometrics", "Spatial"], "description": "A collection of all the estimation functions for spatial cross-sectional models (on lattice/areal data using spatial weights matrices) contained up to now in 'spdep', 'sphet' and 'spse'. These model fitting functions include maximum likelihood methods for cross-sectional models proposed by 'Cliff' and 'Ord' (1973, ISBN:0850860369) and (1981, ISBN:0850860814), fitting methods initially described by 'Ord' (1975) <doi:10.1080/01621459.1975.10480272>. The models are further described by 'Anselin' (1988) <doi:10.1007/978-94-015-7799-1>. Spatial two stage least squares and spatial general method of moment models initially proposed by 'Kelejian' and 'Prucha' (1998) <doi:10.1023/A:1007707430416> and (1999) <doi:10.1111/1468-2354.00027> are provided. Impact methods and MCMC fitting methods proposed by 'LeSage' and 'Pace' (2009) <doi:10.1201/9781420064254> are implemented for the family of cross-sectional spatial regression models. Methods for fitting the log determinant term in maximum likelihood and MCMC fitting are compared by 'Bivand et al.' (2013) <doi:10.1111/gean.12008>, and model fitting methods by 'Bivand' and 'Piras' (2015) <doi:10.18637/jss.v063.i18>; both of these articles include extensive lists of references. 'spatialreg' >= 1.1-* corresponded to 'spdep' >= 1.1-1, in which the model fitting functions were deprecated and passed through to 'spatialreg', but masked those in 'spatialreg'. From versions 1.2-*, the functions have been made defunct in 'spdep'."}, "scoringRules": {"categories": ["TimeSeries"], "description": "Dictionary-like reference for computing scoring rules in a wide\n    range of situations. Covers both parametric forecast distributions (such as\n    mixtures of Gaussians) and distributions generated via simulation."}, "incidence2": {"categories": ["Epidemiology"], "description": "Provides functions and classes to compute, handle and visualise \n  incidence from dated events for a defined time interval. Dates can be \n  provided in various standard formats. The class 'incidence2' is used to store\n  computed incidence and can be easily manipulated, subsetted, and plotted.\n  This package is part of the RECON (<https://www.repidemicsconsortium.org/>) \n  toolkit for outbreak analysis (<https://www.reconverse.org>)."}, "scoringutils": {"categories": ["TimeSeries"], "description": "\n    Provides a collection of metrics and proper scoring rules \n    (Tilmann Gneiting & Adrian E Raftery (2007) \n    <doi:10.1198/016214506000001437>, Jordan, A., Kr\u00fcger, F., & Lerch, S. (2019)\n    <doi:10.18637/jss.v090.i12>) within a consistent framework for \n    evaluation, comparison and visualisation of forecasts. \n    In addition to proper scoring rules, functions are provided to assess \n    bias, sharpness and calibration \n    (Sebastian Funk, Anton Camacho, Adam J. Kucharski, Rachel Lowe, Rosalind\n    M. Eggo, W. John Edmunds (2019) <doi:10.1371/journal.pcbi.1006785>) of \n    forecasts. \n    Several types of predictions (e.g. binary, discrete, continuous) which may \n    come in different formats (e.g. forecasts represented by predictive samples \n    or by quantiles of the predictive distribution) can be evaluated. \n    Scoring metrics can be used either through a convenient data.frame format, \n    or can be applied as individual functions in a vector / matrix format. \n    All functionality has been implemented with a focus on performance and is \n    robustly tested. "}, "eicm": {"categories": ["MissingData"], "description": "Model fitting and species biotic interaction network topology selection for explicit\n  interaction community models. Explicit interaction community models are an extension of binomial\n  linear models for joint modelling of species communities, that incorporate both the effects of\n  species biotic interactions and the effects of missing covariates. Species interactions are modelled\n  as direct effects of each species on each of the others, and are estimated alongside the effects of\n  missing covariates, modelled as latent factors. The package includes a penalized maximum likelihood\n  fitting function, and a genetic algorithm for selecting the most parsimonious species interaction\n  network topology."}, "MultisiteMediation": {"categories": ["CausalInference"], "description": "Multisite causal mediation analysis using the methods proposed by Qin and Hong (2017) <doi:10.3102/1076998617694879>, Qin, Hong, Deutsch, and Bein (2019) <doi:10.1111/rssa.12446>, and Qin, Deutsch, and Hong (2021) <doi:10.1002/pam.22268>. It enables causal mediation analysis in multisite trials, in which individuals are assigned to a treatment or a control group at each site. It allows for estimation and hypothesis testing for not only the population average but also the between-site variance of direct and indirect effects transmitted through one single mediator or two concurrent (conditionally independent) mediators. This strategy conveniently relaxes the assumption of no treatment-by-mediator interaction while greatly simplifying the outcome model specification without invoking strong distributional assumptions. This package also provides a function that can further incorporate a sample weight and a nonresponse weight for multisite causal mediation analysis in the presence of complex sample and survey designs and non-random nonresponse, to enhance both the internal validity and external validity. The package also provides a weighting-based balance checking function for assessing the remaining overt bias."}, "spectrino": {"categories": ["ChemPhys"], "description": "Spectra viewer, organizer, data preparation and property blocks\n        from within R or stand-alone. Binary (application) part is \n        installed separately using spnInstallApp() from spectrino package."}, "pipe.design": {"categories": ["ExperimentalDesign"], "description": "Implements the Product of Independent beta Probabilities dose Escalation (PIPE) design for dual-agent Phase I trials as described in Mander AP, Sweeting MJ (2015) <doi:10.1002/sim.6434>."}, "contfrac": {"categories": ["NumericalMathematics"], "description": "Various utilities for evaluating continued fractions."}, "forecast": {"categories": ["Econometrics", "Environmetrics", "Finance", "MissingData", "TimeSeries"], "description": "Methods and tools for displaying and analysing\n             univariate time series forecasts including exponential smoothing\n             via state space models and automatic ARIMA modelling."}, "designmatch": {"categories": ["CausalInference", "ExperimentalDesign"], "description": "Includes functions for the construction of matched samples that are balanced and representative by design.  Among others, these functions can be used for matching in observational studies with treated and control units, with cases and controls, in related settings with instrumental variables, and in discontinuity designs.  Also, they can be used for the design of randomized experiments, for example, for matching before randomization.  By default, 'designmatch' uses the 'GLPK' optimization solver, but its performance is greatly enhanced by the 'Gurobi' optimization solver and its associated R interface.  For their installation, please follow the instructions at <http://user.gurobi.com/download/gurobi-optimizer> and <http://www.gurobi.com/documentation/7.0/refman/r_api_overview.html>.  We have also included directions in the gurobi_installation file in the inst folder."}, "V8": {"categories": ["WebTechnologies"], "description": "An R interface to V8: Google's open source JavaScript and WebAssembly \n    engine. This package can be compiled either with V8 version 6 and up or NodeJS\n    when built as a shared library."}, "EpiReport": {"categories": ["Epidemiology"], "description": "Drafting an epidemiological report in 'Microsoft Word' format for a given disease,\n  similar to the Annual Epidemiological Reports published by the European Centre \n  for Disease Prevention and Control. Through standalone functions, it is specifically \n  designed to generate each disease specific output presented in these reports and includes:\n  - Table with the distribution of cases by Member State over the last five years;\n  - Seasonality plot with the distribution of cases at the European Union / European Economic Area level, \n  by month, over the past five years;\n  - Trend plot with the trend and number of cases at the European Union / European Economic Area level, \n  by month, over the past five years;\n  - Age and gender bar graph with the distribution of cases at the European Union / European Economic Area level.\n  Two types of datasets can be used:\n  - The default dataset of dengue 2015-2019 data;\n  - Any dataset specified as described in the vignette."}, "gfoRmula": {"categories": ["CausalInference"], "description": "Implements the parametric g-formula algorithm of Robins (1986) \n    <doi:10.1016/0270-0255(86)90088-6>. The g-formula can be used to estimate \n    the causal effects of hypothetical time-varying treatment interventions on \n    the mean or risk of an outcome from longitudinal data with time-varying \n    confounding. This package allows: 1) binary or continuous/multi-level \n    time-varying treatments; 2) different types of outcomes (survival or \n    continuous/binary end of follow-up); 3) data with competing events or \n    truncation by death and loss to follow-up and other types of censoring \n    events; 4) different options for handling competing events in the case of \n    survival outcomes; 5) a random measurement/visit process; 6) joint \n    interventions on multiple treatments; and 7) general incorporation of a \n    priori knowledge of the data structure."}, "BaM": {"categories": ["Bayesian"], "description": "Functions and datasets for Jeff Gill: \"Bayesian Methods: A Social and Behavioral Sciences Approach\". First, Second, and Third Edition. Published by Chapman and Hall/CRC (2002, 2007, 2014) <doi:10.1201/b17888>."}, "diffusion": {"categories": ["TimeSeries"], "description": "Various diffusion models to forecast new product growth. Currently\n    the package contains Bass, Gompertz and Gamma/Shifted Gompertz curves. See\n    Meade and Islam (2006) <doi:10.1016/j.ijforecast.2006.01.005>."}, "RxODE": {"categories": ["DifferentialEquations"], "description": "Facilities for running simulations from ordinary\n    differential equation ('ODE') models, such as pharmacometrics and other\n    compartmental models.  A compilation manager translates the ODE model\n    into C, compiles it, and dynamically loads the object code into R for\n    improved computational efficiency.  An event table object facilitates\n    the specification of complex dosing regimens (optional) and sampling\n    schedules.  NB: The use of this package requires both C and\n    Fortran compilers, for details on their use with R please see\n    Section 6.3, Appendix A, and Appendix D in the \"R Administration and\n    Installation\" manual. Also the code is mostly released under GPL.  The\n    'VODE' and 'LSODA' are in the public domain.  The information is available\n    in the inst/COPYRIGHTS. "}, "FlowScreen": {"categories": ["Hydrology"], "description": "Screens daily streamflow time series for temporal trends and\n    change-points. This package has been primarily developed for assessing the\n    quality of daily streamflow time series. It also contains tools for plotting\n    and calculating many different streamflow metrics. The package can be used to\n    produce summary screening plots showing change-points and significant temporal\n    trends for high flow, low flow, and/or baseflow statistics, or it can be used\n    to perform more detailed hydrological time series analyses. The package was\n    designed for screening daily streamflow time series from Water Survey Canada\n    and the United States Geological Survey but will also work with streamflow time\n    series from many other agencies."}, "SOAs": {"categories": ["ExperimentalDesign"], "description": "Creates stratum orthogonal arrays (also known as strong orthogonal arrays). These are arrays with more levels per column than the typical orthogonal array, and whose low order projections behave like orthogonal arrays, when collapsing levels to coarser strata. Details are described in Groemping (2022) \"A unifying implementation of stratum (aka strong) orthogonal arrays\" <http://www1.bht-berlin.de/FB_II/reports/Report-2022-002.pdf>."}, "JMdesign": {"categories": ["ExperimentalDesign"], "description": "Performs power calculations for joint modeling of longitudinal \n    and survival data with k-th order trajectories when the variance-covariance\n    matrix, Sigma_theta, is unknown."}, "lmtest": {"categories": ["Econometrics", "Finance"], "description": "A collection of tests, data sets, and examples\n for diagnostic checking in linear regression models. Furthermore,\n some generic tools for inference in parametric models are provided."}, "maps": {"categories": ["Spatial"], "description": "Display of maps.  Projection code and larger maps are in\n             separate packages ('mapproj' and 'mapdata')."}, "spd": {"categories": ["Distributions"], "description": "The Semi Parametric Piecewise Distribution blends the Generalized Pareto Distribution for the tails with a kernel based interior."}, "garma": {"categories": ["TimeSeries"], "description": "Methods for estimating univariate long memory-seasonal/cyclical\n             Gegenbauer time series processes. See for example (2018) <doi:10.1214/18-STS649>.\n             Refer to the vignette for details of fitting these processes."}, "Rvcg": {"categories": ["MedicalImaging"], "description": "Operations on triangular meshes based on 'VCGLIB'. This package\n    integrates nicely with the R-package 'rgl' to render the meshes processed by\n    'Rvcg'. The Visualization and Computer Graphics Library (VCG for short) is\n    an open source portable C++ templated library for manipulation, processing\n    and displaying with OpenGL of triangle and tetrahedral meshes. The library,\n    composed by more than 100k lines of code, is released under the GPL license,\n    and it is the base of most of the software tools of the Visual Computing Lab of\n    the Italian National Research Council Institute ISTI <http://vcg.isti.cnr.it>,\n    like 'metro' and 'MeshLab'. The 'VCGLIB' source is pulled from trunk\n    <https://github.com/cnr-isti-vclab/vcglib> and patched to work with options\n    determined by the configure script as well as to work with the header files\n    included by 'RcppEigen'."}, "bayeslongitudinal": {"categories": ["Bayesian"], "description": "Adjusts longitudinal regression models using Bayesian methodology \n            for covariance structures of composite symmetry (SC), \n            autoregressive ones of order 1 AR (1) and \n            autoregressive moving average of order (1,1) ARMA (1,1)."}, "daarem": {"categories": ["NumericalMathematics"], "description": "Implements the DAAREM method for accelerating the convergence of slow, monotone sequences from smooth, fixed-point iterations such as the EM algorithm. For further details about the DAAREM method, see Henderson, N.C. and Varadhan, R. (2019) <doi:10.1080/10618600.2019.1594835>."}, "ugomquantreg": {"categories": ["Distributions"], "description": "Unit-Gompertz density, cumulative distribution, quantile functions and random deviate\n   \tgeneration of the unit-Gompertz distribution. In addition, there are a function for fitting the \n   \tGeneralized Additive Models for Location, Scale and Shape."}, "spatstat": {"categories": ["Spatial", "SpatioTemporal", "Survival"], "description": "Comprehensive open-source toolbox for analysing Spatial Point Patterns. Focused mainly on two-dimensional point patterns, including multitype/marked points, in any spatial region. Also supports three-dimensional point patterns, space-time point patterns in any number of dimensions, point patterns on a linear network, and patterns of other geometrical objects. Supports spatial covariate data such as pixel images. \n\tContains over 2000 functions for plotting spatial data, exploratory data analysis, model-fitting, simulation, spatial sampling, model diagnostics, and formal inference. \n\tData types include point patterns, line segment patterns, spatial windows, pixel images, tessellations, and linear networks. \n\tExploratory methods include quadrat counts, K-functions and their simulation envelopes, nearest neighbour distance and empty space statistics, Fry plots, pair correlation function, kernel smoothed intensity, relative risk estimation with cross-validated bandwidth selection, mark correlation functions, segregation indices, mark dependence diagnostics, and kernel estimates of covariate effects. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.\n\tParametric models can be fitted to point pattern data using the functions ppm(), kppm(), slrm(), dppm() similar to glm(). Types of models include Poisson, Gibbs and Cox point processes, Neyman-Scott cluster processes, and determinantal point processes. Models may involve dependence on covariates, inter-point interaction, cluster formation and dependence on marks. Models are fitted by maximum likelihood, logistic regression, minimum contrast, and composite likelihood methods. \n\tA model can be fitted to a list of point patterns (replicated point pattern data) using the function mppm(). The model can include random effects and fixed effects depending on the experimental design, in addition to all the features listed above.\n\tFitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots."}, "LSWPlib": {"categories": ["TimeSeries"], "description": "Library of functions for the statistical analysis and simulation of Locally Stationary Wavelet Packet (LSWP) processes.  The methods implemented by this library are described in Cardinali and Nason (2017) <doi:10.1111/jtsa.12230>."}, "NPRED": {"categories": ["Hydrology"], "description": "Partial informational correlation (PIC) is used to identify the meaningful predictors to the response from a large set of potential predictors. Details of methodologies used in the package can be found in Sharma, A., Mehrotra, R. (2014). <doi:10.1002/2013WR013845>, Sharma, A., Mehrotra, R., Li, J., & Jha, S. (2016). <doi:10.1016/j.envsoft.2016.05.021>, and Mehrotra, R., & Sharma, A. (2006). <doi:10.1016/j.advwatres.2005.08.007>."}, "sf": {"categories": ["Spatial", "SpatioTemporal"], "description": "Support for simple features, a standardized way to\n    encode spatial vector data. Binds to 'GDAL' for reading and writing\n    data, to 'GEOS' for geometrical operations, and to 'PROJ' for\n    projection conversions and datum transformations. Uses by default the 's2'\n    package for spherical geometry operations on ellipsoidal (long/lat) coordinates."}, "IDPSurvival": {"categories": ["Survival"], "description": "Functions to perform robust\n\t\t nonparametric survival analysis with right censored \n\t\t data using a prior near-ignorant Dirichlet Process.\n\t\t Mangili, F., Benavoli, A., de Campos, C.P., Zaffalon, M. (2015) <doi:10.1002/bimj.201500062>."}, "mize": {"categories": ["Optimization"], "description": "Optimization algorithms implemented in R, including\n    conjugate gradient (CG), Broyden-Fletcher-Goldfarb-Shanno (BFGS) and the\n    limited memory BFGS (L-BFGS) methods. Most internal parameters can be set \n    through the call interface. The solvers hold up quite well for \n    higher-dimensional problems."}, "rdlocrand": {"categories": ["Econometrics"], "description": "The regression discontinuity (RD) design is a popular quasi-experimental design for causal inference and policy evaluation. Under the local randomization approach, RD designs can be interpreted as randomized experiments inside a window around the cutoff. This package provides tools to perform randomization inference for RD designs under local randomization: rdrandinf() to perform hypothesis testing using randomization inference, rdwinselect() to select a window around the cutoff in which randomization is likely to hold, rdsensitivity() to assess the sensitivity of the results to different window lengths and null hypotheses and rdrbounds() to construct Rosenbaum bounds for sensitivity to unobserved confounders. See Cattaneo, Titiunik and Vazquez-Bare (2016) <https://rdpackages.github.io/references/Cattaneo-Titiunik-VazquezBare_2016_Stata.pdf> for further methodological details."}, "scs": {"categories": ["Optimization"], "description": "Solves convex cone programs via operator splitting. Can solve:\n    linear programs ('LPs'), second-order cone programs ('SOCPs'), semidefinite programs\n    ('SDPs'), exponential cone programs ('ECPs'), and power cone programs ('PCPs'), or\n    problems with any combination of those cones. 'SCS' uses 'AMD' (a set of routines for permuting sparse matrices prior to factorization) and 'LDL' (a sparse 'LDL' factorization and solve package) from 'SuiteSparse' (<https://people.engr.tamu.edu/davis/suitesparse.html>)."}, "LTRCtrees": {"categories": ["Survival"], "description": "Recursive partition algorithms designed for fitting survival tree with left-truncated and right censored (LTRC) data, as well as interval-censored data.\n    The LTRC trees can also be used to fit survival tree with time-varying covariates."}, "dataseries": {"categories": ["TimeSeries"], "description": "Download and import time series from <http://www.dataseries.org>, a comprehensive and up-to-date collection of open data from Switzerland."}, "metaSEM": {"categories": ["MetaAnalysis", "Psychometrics"], "description": "A collection of functions for conducting meta-analysis using a\n             structural equation modeling (SEM) approach via the 'OpenMx' and\n             'lavaan' packages. It also implements various procedures to\n\t\t\t perform meta-analytic structural equation modeling on the\n             correlation and covariance matrices,\n\t\t\t see Cheung (2015) <doi:10.3389/fpsyg.2014.01521>."}, "betafunctions": {"categories": ["Distributions", "Psychometrics"], "description": "Package providing a number of functions for working with Two- and \n    Four-parameter Beta and closely related distributions (i.e., the Gamma-\n    Binomial-, and Beta-Binomial distributions), including parameterization in\n    terms of moments, and fitting of Beta distributions to vectors of values.\n    Includes d/p/r- and a function and for calculating moments of Beta-Binomial\n    distributions. Also includes functions for estimating classification \n    accuracy, diagnostic performance, and consistency, making use of what is\n    generally known as the 'Livingston and Lewis approach' in the psychometric \n    literature which models observed-score distributions in terms of the Beta-\n    Binomial distribution. A shiny app is available, providing a GUI for the\n    Livingston and Lewis approach when used for binary classifications. For\n    url to the app, see documentation for the LL.CA() function. \n    Livingston and Lewis (1995) <doi:10.1111/j.1745-3984.1995.tb00462.x>.\n    Lord (1965) <doi:10.1007/BF02289490>.\n    Hanson (1991) <https://files.eric.ed.gov/fulltext/ED344945.pdf>.\n    Tharwat (2020) <doi:10.1016/j.aci.2018.08.003>."}, "EpiModel": {"categories": ["Epidemiology"], "description": "Tools for simulating mathematical models of infectious disease dynamics.\n    Epidemic model classes include deterministic compartmental models, stochastic\n    individual-contact models, and stochastic network models. Network models use the\n    robust statistical methods of exponential-family random graph models (ERGMs)\n    from the Statnet suite of software packages in R. Standard templates for epidemic\n    modeling include SI, SIR, and SIS disease types. EpiModel features an API for\n    extending these templates to address novel scientific research aims. Full\n    methods for EpiModel are detailed in Jenness et al. (2018, <doi:10.18637/jss.v084.i08>)."}, "DIRECT": {"categories": ["Bayesian"], "description": "A Bayesian clustering method for replicated time series or replicated measurements from multiple experimental conditions, e.g., time-course gene expression data.  It estimates the number of clusters directly from the data using a Dirichlet-process prior.  See Fu, A. Q., Russell, S., Bray, S. and Tavare, S. (2013) Bayesian clustering of replicated time-course gene expression data with weak signals. The Annals of Applied Statistics. 7(3) 1334-1361. <doi:10.1214/13-AOAS650>."}, "berryFunctions": {"categories": ["Hydrology"], "description": "Draw horizontal histograms, color scattered points by 3rd dimension,\n    enhance date- and log-axis plots, zoom in X11 graphics, trace errors and warnings, \n    use the unit hydrograph in a linear storage cascade, convert lists to data.frames and arrays, \n    fit multiple functions."}, "multitaper": {"categories": ["TimeSeries"], "description": "Implements multitaper spectral analysis using discrete prolate spheroidal sequences (Slepians) and sine tapers. It includes an adaptive weighted multitaper spectral estimate, a coherence estimate, Thomson's Harmonic F-test, and complex demodulation. The Slepians sequences are generated efficiently using a tridiagonal matrix solution, and jackknifed confidence intervals are available for most estimates. This package is an implementation of the method described in D.J. Thomson (1982) \"Spectrum estimation and harmonic analysis\" <doi:10.1109/PROC.1982.12433>."}, "bigchess": {"categories": ["SportsAnalytics"], "description": "Provides functions for reading *.PGN files with more than one game, including large files without copying it into RAM (using 'ff' package or 'RSQLite' package). Handle chess data and chess aggregated data, count figure moves statistics, create player profile, plot winning chances, browse openings. Set of functions of R API to communicate with UCI-protocol based chess engines."}, "compute.es": {"categories": ["MetaAnalysis"], "description": "Several functions are available for calculating the most\n    widely used effect sizes (ES), along with their variances, confidence\n    intervals and p-values.  The output includes ES's of d (mean difference), g\n    (unbiased estimate of d), r (correlation coefficient), z' (Fisher's z), and\n    OR (odds ratio and log odds ratio). In addition, NNT (number needed to\n    treat), U3, CLES (Common Language Effect Size) and Cliff's Delta are\n    computed. This package uses recommended formulas as described in The\n    Handbook of Research Synthesis and Meta-Analysis (Cooper, Hedges, &\n    Valentine, 2009)."}, "wooldridge": {"categories": ["Econometrics", "TeachingStatistics"], "description": "Students learning both econometrics and R may find the introduction \n    to both challenging. The wooldridge data package aims to lighten the task by efficiently \n    loading any data set found in the text with a single command. Data sets have been \n    compressed to a fraction of their original size. Documentation files contain page numbers, \n    the original source, time of publication, and notes from the author suggesting avenues for \n    further analysis and research. If one needs an introduction to R model syntax, a \n    vignette contains solutions to examples from chapters of the text. \n    Data sets are from the 7th edition (Wooldridge 2020, ISBN-13: 978-1-337-55886-0), \n    and are backwards compatible with all previous versions of the text."}, "clusterSEs": {"categories": ["Econometrics", "Robust"], "description": "Calculate p-values and confidence intervals using cluster-adjusted\n    t-statistics (based on Ibragimov and Muller (2010) <doi:10.1198/jbes.2009.08046>, pairs cluster bootstrapped t-statistics, and wild cluster bootstrapped t-statistics (the latter two techniques based on Cameron, Gelbach, and Miller (2008) <doi:10.1162/rest.90.3.414>. Procedures are included for use with GLM, ivreg, plm (pooling or fixed effects), and mlogit models."}, "clustermq": {"categories": ["HighPerformanceComputing"], "description": "Evaluate arbitrary function calls using workers on HPC schedulers\n    in single line of code. All processing is done on the network without\n    accessing the file system. Remote schedulers are supported via SSH."}, "quint": {"categories": ["MachineLearning"], "description": "Grows a qualitative interaction tree. Quint is a tool for subgroup analysis, suitable for data from a two-arm randomized controlled trial. More information in Dusseldorp, E., Doove, L., & Van Mechelen, I. (2016) <doi:10.3758/s13428-015-0594-z>."}, "statespacer": {"categories": ["TimeSeries"], "description": "A tool that makes estimating models in state space form \n    a breeze. See \"Time Series Analysis by State Space Methods\" by \n    Durbin and Koopman (2012, ISBN: 978-0-19-964117-8) for details \n    about the algorithms implemented."}, "pder": {"categories": ["Econometrics"], "description": "Data sets for the Panel Data Econometrics with R <doi:10.1002/9781119504641> book."}, "MultipleBubbles": {"categories": ["TimeSeries"], "description": "Provides the Augmented Dickey-Fuller test and its variations to check the existence of bubbles (explosive behavior) for time series, based on the article by Peter C. B. Phillips, Shuping Shi and Jun Yu (2015a) <doi:10.1111/iere.12131>. Some functions may take a while depending on the size of the data used, or the number of Monte Carlo replications applied."}, "ExtremeBounds": {"categories": ["Econometrics"], "description": "An implementation of Extreme Bounds Analysis (EBA), a global sensitivity analysis that examines the robustness of determinants in regression models. The package supports both Leamer's and Sala-i-Martin's versions of EBA, and allows users to customize all aspects of the analysis."}, "mcmcensemble": {"categories": ["Bayesian"], "description": "Provides ensemble samplers for\n    affine-invariant Monte Carlo Markov Chain, which allow a faster\n    convergence for badly scaled estimation problems. Two samplers are\n    proposed: the 'differential.evolution' sampler from ter Braak and\n    Vrugt (2008) <doi:10.1007/s11222-008-9104-9> and the 'stretch' sampler\n    from Goodman and Weare (2010) <doi:10.2140/camcos.2010.5.65>."}, "tidyqwi": {"categories": ["OfficialStatistics"], "description": "The purpose of this package is to access the \n    United States Census Bureau's Quarterly Workforce Indicator data. Additionally, \n    the data will be retrieved in a tidy format for further manipulation with full variable\n    descriptions added if desired. Information about the United States Census Bureau's \n    Quarterly Workforce Indicator is available at \n    <https://www.census.gov/data/developers/data-sets/qwi.html>."}, "DoseFinding": {"categories": ["ClinicalTrials", "ExperimentalDesign"], "description": "The DoseFinding package provides functions for the design and analysis\n\t     of dose-finding experiments (with focus on pharmaceutical Phase\n\t     II clinical trials). It provides functions for: multiple contrast\n\t     tests, fitting non-linear dose-response models (using Bayesian and\n\t     non-Bayesian estimation), calculating optimal designs and an\n\t     implementation of the MCPMod methodology (Pinheiro et al. (2014)\n\t     <doi:10.1002/sim.6052>)."}, "DBItest": {"categories": ["Databases"], "description": "A helper that tests DBI back ends for conformity\n    to the interface."}, "metaviz": {"categories": ["MetaAnalysis"], "description": "A compilation of functions to create visually appealing and information-rich \n    plots of meta-analytic data using 'ggplot2'. Currently allows to create forest plots, \n    funnel plots, and many of their variants, such as rainforest plots, thick forest plots, \n    additional evidence contour funnel plots, and sunset funnel plots. In addition, functionalities \n    for visual inference with the funnel plot in the context of meta-analysis are provided."}, "surveyoutliers": {"categories": ["OfficialStatistics"], "description": "At present, the only functionality is the calculation of optimal one-sided winsorizing cutoffs. The main function is optimal.onesided.cutoff.bygroup. It calculates the optimal tuning parameter for one-sided winsorisation, and so calculates winsorised values for a variable of interest. See the help file for this function for more details and an example."}, "cmprskQR": {"categories": ["Survival"], "description": "Estimation, testing and regression modeling of\n subdistribution functions in competing risks using quantile regressions,\n as described in Peng and Fine (2009) <doi:10.1198/jasa.2009.tm08228>."}, "dynamicTreeCut": {"categories": ["Cluster"], "description": "Contains methods for detection of clusters in hierarchical clustering dendrograms."}, "googlesheets4": {"categories": ["WebTechnologies"], "description": "Interact with Google Sheets through the Sheets API v4\n    <https://developers.google.com/sheets/api>. \"API\" is an acronym for\n    \"application programming interface\"; the Sheets API allows users to\n    interact with Google Sheets programmatically, instead of via a web\n    browser. The \"v4\" refers to the fact that the Sheets API is currently\n    at version 4. This package can read and write both the metadata and\n    the cell data in a Sheet."}, "uaparserjs": {"categories": ["WebTechnologies"], "description": "Despite there being a section in RFC 7231\n    <https://tools.ietf.org/html/rfc7231#section-5.5.3> defining a suggested\n    structure for 'User-Agent' headers this data is notoriously difficult\n    to parse consistently. Tools are provided that will take in user agent\n    strings and return structured R objects. This is a 'V8'-backed package\n    based on the 'ua-parser' project <https://github.com/ua-parser>."}, "mFilter": {"categories": ["TimeSeries"], "description": "The mFilter package implements several time series filters useful\n        for smoothing and extracting trend and cyclical components of a\n        time series. The routines are commonly used in economics and\n        finance, however they should also be interest to other areas.\n        Currently, Christiano-Fitzgerald, Baxter-King,\n        Hodrick-Prescott, Butterworth, and trigonometric regression\n        filters are included in the package."}, "rODE": {"categories": ["DifferentialEquations"], "description": "Show physics, math and engineering students how an ODE solver\n    is made and how effective R classes can be for the construction of\n    the equations that describe natural phenomena. Inspiration for this work \n    comes from the book on \"Computer Simulations in Physics\" \n    by Harvey Gould, Jan Tobochnik, and Wolfgang Christian. \n    Book link: <http://www.compadre.org/osp/items/detail.cfm?ID=7375>."}, "metamicrobiomeR": {"categories": ["MetaAnalysis"], "description": "Generalized Additive Model for Location, Scale and Shape (GAMLSS) \n    with zero inflated beta (BEZI) family for analysis of microbiome relative abundance data \n    (with various options for data transformation/normalization to address compositional effects) and \n    random effects meta-analysis models for meta-analysis pooling estimates across microbiome studies \n    are implemented. \n    Random Forest model to predict microbiome age based on relative abundances of  \n    shared bacterial genera with the Bangladesh data (Subramanian et al 2014), \n    comparison of multiple diversity indexes using linear/linear mixed effect models \n    and some data display/visualization are also implemented.\n    The reference paper is published by \n    Ho NT, Li F, Wang S, Kuhn L (2019) <doi:10.1186/s12859-019-2744-2> . "}, "mongolite": {"categories": ["Databases"], "description": "High-performance MongoDB client based on 'mongo-c-driver' and 'jsonlite'.\n    Includes support for aggregation, indexing, map-reduce, streaming, encryption,\n    enterprise authentication, and GridFS. The online user manual provides an overview \n    of the available methods in the package: <https://jeroen.github.io/mongolite/>."}, "aster": {"categories": ["Survival"], "description": "Aster models (Geyer, Wagenius, and Shaw, 2007,\n    <doi:10.1093/biomet/asm030>; Shaw, Geyer, Wagenius, Hangelbroek, and\n    Etterson, 2008, <doi:10.1086/588063>; Geyer, Ridley, Latta, Etterson,\n    and Shaw, 2013, <doi:10.1214/13-AOAS653>) are exponential family\n    regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, life table analysis,\n    zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    There are also random effects versions of these models."}, "orthopolynom": {"categories": ["NumericalMathematics"], "description": "A collection of functions to construct sets of orthogonal\n        polynomials and their recurrence relations. Additional\n        functions are provided to calculate the derivative, integral,\n        value and roots of lists of polynomial objects."}, "ROI": {"categories": ["Optimization"], "description": "The R Optimization Infrastructure ('ROI') <doi:10.18637/jss.v094.i15>\n\tis a sophisticated framework for handling optimization problems in R.\n\tAdditional information can be found on the 'ROI' homepage <http://roi.r-forge.r-project.org/>."}, "stratamatch": {"categories": ["CausalInference"], "description": "A pilot matching design to automatically \n    stratify and match large datasets.  The manual_stratify() function allows\n    users to manually stratify a dataset based on categorical variables of \n    interest, while the auto_stratify() function does automatically by\n    allocating a held-aside (pilot) data set, fitting a prognostic score  \n    (see Hansen (2008) <doi:10.1093/biomet/asn004>) on the pilot set, and stratifying the data set based\n    on prognostic score quantiles.  The strata_match() function then does optimal\n    matching of the data set in parallel within strata."}, "quadprogXT": {"categories": ["Optimization"], "description": "Extends the quadprog package to solve quadratic programs with\n    absolute value constraints and absolute values in the objective function."}, "pvclust": {"categories": ["Cluster", "Environmetrics", "HighPerformanceComputing"], "description": "An implementation of multiscale bootstrap resampling for\n             assessing the uncertainty in hierarchical cluster analysis.\n             It provides SI (selective inference) p-value, AU (approximately unbiased)\n             p-value and BP (bootstrap probability) value for each cluster in a dendrogram."}, "acp": {"categories": ["TimeSeries"], "description": "Analysis of count data exhibiting autoregressive properties, using the Autoregressive Conditional Poisson model (ACP(p,q)) proposed by Heinen (2003)."}, "httpcode": {"categories": ["WebTechnologies"], "description": "Find and explain the meaning of 'HTTP' status codes.\n    Functions included for searching for codes by full or partial number,\n    by message, and get appropriate dog and cat images for many\n    status codes."}, "WufooR": {"categories": ["WebTechnologies"], "description": "Allows form managers to download entries from their respondents\n    using Wufoo JSON API (<https://www.wufoo.com>). Additionally, the Wufoo reports - when public - can be\n    also acquired programmatically. Note that building new forms within this package\n    is not supported."}, "stargazer": {"categories": ["ReproducibleResearch"], "description": "Produces LaTeX code, HTML/CSS code and ASCII text for well-formatted tables that hold \n    regression analysis results from several models side-by-side, as well as summary\n    statistics."}, "scorecardModelUtils": {"categories": ["MissingData"], "description": "Provides infrastructure functionalities such as missing value treatment, information value calculation, GINI calculation etc. which are used for developing a traditional credit scorecard as well as a machine learning based model. The functionalities defined are standard steps for any credit underwriting scorecard development, extensively used in financial domain."}, "sjlabelled": {"categories": ["MissingData"], "description": "Collection of functions dealing with labelled data, like reading and \n    writing data between R and other statistical software packages like 'SPSS',\n    'SAS' or 'Stata', and working with labelled data. This includes easy ways \n    to get, set or change value and variable label attributes, to convert \n    labelled vectors into factors or numeric (and vice versa), or to deal with \n    multiple declared missing values."}, "fpc": {"categories": ["Cluster"], "description": "Various methods for clustering and cluster validation.\n  Fixed point clustering. Linear regression clustering. Clustering by \n  merging Gaussian mixture components. Symmetric \n  and asymmetric discriminant projections for visualisation of the \n  separation of groupings. Cluster validation statistics\n  for distance based clustering including corrected Rand index. \n  Standardisation of cluster validation statistics by random clusterings and \n  comparison between many clustering methods and numbers of clusters based on\n  this.  \n  Cluster-wise cluster stability assessment. Methods for estimation of \n  the number of clusters: Calinski-Harabasz, Tibshirani and Walther's \n  prediction strength, Fang and Wang's bootstrap stability. \n  Gaussian/multinomial mixture fitting for mixed \n  continuous/categorical variables. Variable-wise statistics for cluster\n  interpretation. DBSCAN clustering. Interface functions for many \n  clustering methods implemented in R, including estimating the number of\n  clusters with kmeans, pam and clara. Modality diagnosis for Gaussian\n  mixtures. For an overview see package?fpc."}, "clusterPower": {"categories": ["ClinicalTrials"], "description": "Calculate power for cluster randomized trials (CRTs) including multi-arm trials, individually randomized group treatment trials (IGRTTs), stepped wedge trials (SWTs) and others using closed-form (analytic) solutions, and estimates power using Monte Carlo methods."}, "smoothmest": {"categories": ["Distributions"], "description": "Some M-estimators for 1-dimensional location (Bisquare,\n             ML for the Cauchy distribution, and the estimators from \n             application of the smoothing principle introduced in Hampel, \n             Hennig and Ronchetti (2011) to the above, the Huber\n             M-estimator, and the median, main function is smoothm), \n             and Pitman estimator."}, "calculus": {"categories": ["NumericalMathematics"], "description": "Efficient C++ optimized functions for numerical and symbolic calculus as described in Guidotti (2020) <arXiv:2101.00086>. It includes basic arithmetic, tensor calculus, Einstein summing convention, fast computation of the Levi-Civita symbol and generalized Kronecker delta, Taylor series expansion, multivariate Hermite polynomials, high-order derivatives, ordinary differential equations, differential operators (Gradient, Jacobian, Hessian, Divergence, Curl, Laplacian) and numerical integration in arbitrary orthogonal coordinate systems: cartesian, polar, spherical, cylindrical, parabolic or user defined by custom scale factors. "}, "Dykstra": {"categories": ["Optimization"], "description": "Solves quadratic programming problems using Richard L. Dykstra's cyclic projection algorithm. Routine allows for a combination of equality and inequality constraints. See Dykstra (1983) <doi:10.1080/01621459.1983.10477029> for details."}, "survivalROC": {"categories": ["Survival"], "description": "Compute time-dependent ROC curve from censored survival\n        data using Kaplan-Meier (KM) or Nearest Neighbor Estimation\n        (NNE) method of Heagerty, Lumley & Pepe (Biometrics, Vol 56 No\n        2, 2000, PP 337-344)"}, "SPORTSCausal": {"categories": ["CausalInference"], "description": "A time series causal inference model for Randomized Controlled Trial (RCT) under spillover effect. 'SPORTSCausal' (Spillover Time Series Causal Inference) separates treatment effect and spillover effect from given responses of experiment group and control group by predicting the response without treatment. It reports both effects by fitting the Bayesian Structural Time Series (BSTS) model based on 'CausalImpact', as described in Brodersen et al. (2015) <doi:10.1214/14-AOAS788>. "}, "triangle": {"categories": ["Distributions"], "description": "Provides the \"r, q, p, and d\" distribution functions for the triangle distribution."}, "TSdbi": {"categories": ["Finance", "TimeSeries"], "description": "Provides a common interface to time series databases. The\n\tobjective is to define a standard interface so users can retrieve time \n\tseries data from various sources with a simple, common, set of \n\tcommands, and so programs can be written to be portable with respect \n\tto the data source. The SQL implementations also provide a database \n\ttable design, so users needing to set up a time series database \n\thave a reasonably complete way to do this easily. The interface \n\tprovides for a variety of options with respect to the representation \n\tof time series in R. The interface, and the SQL implementations, also\n\thandle vintages of time series data (sometime called editions or \n\treal-time data). There is also a (not yet well tested) mechanism to\n\thandle multilingual data documentation.\n\tComprehensive examples of all the 'TS*' packages is provided in the\n\tvignette Guide.pdf with the 'TSdata' package."}, "BCC1997": {"categories": ["Finance"], "description": "Calculates the prices of European options based on the universal solution provided by Bakshi, Cao and Chen (1997) <doi:10.1111/j.1540-6261.1997.tb02749.x>. This solution considers stochastic volatility, stochastic interest and random jumps. Please cite their work if this package is used. "}, "VarianceGamma": {"categories": ["Distributions"], "description": "Provides functions for the variance gamma\n        distribution. Density, distribution and quantile functions.\n        Functions for random number generation and fitting of the\n        variance gamma to data. Also, functions for computing moments\n        of the variance gamma distribution of any order about any\n        location. In addition, there are functions for checking the\n        validity of parameters and to interchange different sets of\n        parameterizations for the variance gamma distribution."}, "plsRglm": {"categories": ["MissingData"], "description": "Provides (weighted) Partial least squares Regression for generalized linear models and repeated k-fold cross-validation of such models using various criteria <arXiv:1810.01005>. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available."}, "MultiLCIRT": {"categories": ["Psychometrics"], "description": "Framework for the Item Response Theory analysis of dichotomous and ordinal polytomous outcomes under the assumption of multidimensionality and discreteness of the latent traits. The fitting algorithms allow for missing responses and for different item parameterizations and are based on the Expectation-Maximization paradigm. Individual covariates affecting the class weights may be included in the new version (since 2.1)."}, "preference": {"categories": ["CausalInference"], "description": "Design and analyze two-stage randomized trials with a continuous\n    outcome measure. The package contains functions to compute the required \n    sample size needed to detect a given preference, treatment, and selection \n    effect; alternatively, the package contains functions that can report the \n    study power given a fixed sample size. Finally, analysis functions are \n    provided to test each effect using either summary data (i.e. means, \n    variances) or raw study data <doi:10.18637/jss.v094.c02>."}, "surv2sampleComp": {"categories": ["Survival"], "description": "Performs inference of several model-free group contrast measures, which include difference/ratio of cumulative incidence rates at given time points, quantiles, and restricted mean survival times (RMST). Two kinds of covariate adjustment procedures (i.e., regression and augmentation) for inference of the metrics based on RMST are also included."}, "rgdal": {"categories": ["Spatial"], "description": "Provides bindings to the 'Geospatial' Data Abstraction Library ('GDAL') (>= 1.11.4) and access to projection/transformation operations from the 'PROJ' library. Please note that 'rgdal' will be retired by the end of 2023, plan transition to sf/stars/'terra' functions using 'GDAL' and 'PROJ' at your earliest convenience. Use is made of classes defined in the 'sp' package. Raster and vector map data can be imported into R, and raster and vector 'sp' objects exported. The 'GDAL' and 'PROJ' libraries are external to the package, and, when installing the package from source, must be correctly installed first; it is important that 'GDAL' < 3 be matched with 'PROJ' < 6. From 'rgdal' 1.5-8, installed with to 'GDAL' >=3, 'PROJ' >=6 and 'sp' >= 1.4, coordinate reference systems use 'WKT2_2019' strings, not 'PROJ' strings. 'Windows' and 'macOS' binaries (including 'GDAL', 'PROJ' and their dependencies) are provided on 'CRAN'. "}, "GMCM": {"categories": ["Cluster", "MetaAnalysis"], "description": "Unsupervised Clustering and Meta-analysis using Gaussian Mixture\n    Copula Models."}, "hwriter": {"categories": ["ReproducibleResearch"], "description": "Easy-to-use and versatile functions to output R objects in\n        HTML format."}, "GB2": {"categories": ["Distributions"], "description": "Package GB2 explores the Generalized Beta distribution of the second kind. Density, cumulative distribution function, quantiles and moments of the distributions are given. Functions for the full log-likelihood, the profile log-likelihood and the scores are provided. Formulas for various indicators of inequality and poverty under the GB2 are implemented. The GB2 is fitted by the methods of maximum pseudo-likelihood estimation using the full and profile log-likelihood, and non-linear least squares estimation of the model parameters. Various plots for the visualization and analysis of the results are provided. Variance estimation of the parameters is provided for the method of maximum pseudo-likelihood estimation. A mixture distribution based on the compounding property of the GB2 is presented (denoted as \"compound\" in the documentation). This mixture distribution is based on the discretization of the distribution of the underlying random scale parameter. The discretization can be left or right tail. Density, cumulative distribution function, moments and quantiles for the mixture distribution are provided. The compound mixture distribution is fitted using the method of maximum pseudo-likelihood estimation. The fit can also incorporate the use of auxiliary information. In this new version of the package, the mixture case is complemented with new functions for variance estimation by linearization and comparative density plots. "}, "pwt8": {"categories": ["Econometrics"], "description": "The Penn World Table 8.x provides information on relative levels of\n\tincome, output, inputs, and productivity for 167 countries\n\tbetween 1950 and 2011."}, "metaRNASeq": {"categories": ["MetaAnalysis"], "description": "Implementation of two p-value combination techniques (inverse normal and Fisher methods). A vignette is provided to explain how to perform a meta-analysis from two independent RNA-seq experiments."}, "swirl": {"categories": ["TeachingStatistics"], "description": "Use the R console as an interactive learning\n    environment. Users receive immediate feedback as they are guided through\n    self-paced lessons in data science and R programming."}, "bspmma": {"categories": ["Bayesian", "MetaAnalysis"], "description": "The main functions carry out Gibbs' sampler routines for nonparametric and semiparametric Bayesian models for random effects meta-analysis."}, "matchingR": {"categories": ["Optimization"], "description": "Computes matching algorithms quickly using Rcpp.\n    Implements the Gale-Shapley Algorithm to compute the stable\n    matching for two-sided markets, such as the stable marriage\n    problem and the college-admissions problem. Implements Irving's\n    Algorithm for the stable roommate problem. Implements the top\n    trading cycle algorithm for the indivisible goods trading problem."}, "seas": {"categories": ["Environmetrics", "TimeSeries"], "description": "Capable of deriving seasonal statistics, such as \"normals\", and\n  analysis of seasonal data, such as departures. This package also has\n  graphics capabilities for representing seasonal data, including boxplots for\n  seasonal parameters, and bars for summed normals. There are many specific\n  functions related to climatology, including precipitation normals,\n  temperature normals, cumulative precipitation departures and precipitation\n  interarrivals. However, this package is designed to represent any\n  time-varying parameter with a discernible seasonal signal, such as found\n  in hydrology and ecology."}, "SLHD": {"categories": ["ExperimentalDesign"], "description": "Generate the optimal Latin Hypercube Designs (LHDs) for computer experiments with quantitative factors and the optimal Sliced Latin Hypercube Designs (SLHDs) for computer experiments with both quantitative and qualitative factors. Details of the algorithm can be found in Ba, S., Brenneman, W. A. and Myers, W. R. (2015), \"Optimal Sliced Latin Hypercube Designs,\" Technometrics. Important function in this package is \"maximinSLHD\". "}, "nnls": {"categories": ["ChemPhys", "Optimization"], "description": "An R interface to the Lawson-Hanson implementation of an\n        algorithm for non-negative least squares (NNLS).  Also allows\n        the combination of non-negative and non-positive constraints."}, "plac": {"categories": ["Survival"], "description": "A semi-parametric estimation method for the Cox model\n    with left-truncated data using augmented information\n    from the marginal of truncation times."}, "surveillance": {"categories": ["Environmetrics", "Epidemiology", "SpatioTemporal", "TimeSeries"], "description": "Statistical methods for the modeling and monitoring of time series\n        of counts, proportions and categorical data, as well as for the modeling\n        of continuous-time point processes of epidemic phenomena.\n        The monitoring methods focus on aberration detection in count data time\n        series from public health surveillance of communicable diseases, but\n        applications could just as well originate from environmetrics,\n        reliability engineering, econometrics, or social sciences. The package\n        implements many typical outbreak detection procedures such as the\n        (improved) Farrington algorithm, or the negative binomial GLR-CUSUM\n        method of H\u00f6hle and Paul (2008) <doi:10.1016/j.csda.2008.02.015>.\n        A novel CUSUM approach combining logistic and multinomial logistic\n        modeling is also included. The package contains several real-world data\n        sets, the ability to simulate outbreak data, and to visualize the\n        results of the monitoring in a temporal, spatial or spatio-temporal\n        fashion. A recent overview of the available monitoring procedures is\n        given by Salmon et al. (2016) <doi:10.18637/jss.v070.i10>.\n        For the retrospective analysis of epidemic spread, the package provides\n        three endemic-epidemic modeling frameworks with tools for visualization,\n        likelihood inference, and simulation. hhh4() estimates models for\n        (multivariate) count time series following Paul and Held (2011)\n        <doi:10.1002/sim.4177> and Meyer and Held (2014) <doi:10.1214/14-AOAS743>.\n        twinSIR() models the susceptible-infectious-recovered (SIR) event\n        history of a fixed population, e.g, epidemics across farms or networks,\n        as a multivariate point process as proposed by H\u00f6hle (2009)\n        <doi:10.1002/bimj.200900050>. twinstim() estimates self-exciting point\n        process models for a spatio-temporal point pattern of infective events,\n        e.g., time-stamped geo-referenced surveillance data, as proposed by\n        Meyer et al. (2012) <doi:10.1111/j.1541-0420.2011.01684.x>.\n        A recent overview of the implemented space-time modeling frameworks\n        for epidemic phenomena is given by Meyer et al. (2017)\n        <doi:10.18637/jss.v077.i11>."}, "CompareCausalNetworks": {"categories": ["CausalInference"], "description": "Unified interface for the estimation of causal networks, including\n    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate\n    additive noise model), 'bivariateCAM' (bivariate causal additive model),\n    'CAM' (causal additive model) (from package 'CAM'; the package is \n    temporarily unavailable on the CRAN repository; formerly available versions \n    can be obtained from the archive), 'hiddenICP' (invariant\n    causal prediction with hidden variables), 'ICP' (invariant causal prediction)\n    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence\n    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC\n    Algorithm), 'FCI' (fast causal inference), \n    'RFCI' (really fast causal inference) (all from package 'pcalg') and\n    regression."}, "subdetect": {"categories": ["CausalInference"], "description": "A test for the existence of a subgroup with enhanced treatment \n    effect. And, a sample size calculation procedure for the subgroup \n    detection test."}, "spls": {"categories": ["ChemPhys"], "description": "Provides functions for fitting a sparse\n        partial least squares (SPLS) regression and classification\n        (Chun and Keles (2010) <doi:10.1111/j.1467-9868.2009.00723.x>)."}, "joint.Cox": {"categories": ["MetaAnalysis", "Survival"], "description": "Fit survival data and perform dynamic prediction under joint frailty-copula models for tumour progression and death.\n Likelihood-based methods are employed for estimating model parameters, where the baseline hazard functions are modeled by the cubic M-spline or the Weibull model.\n The methods are applicable for meta-analytic data containing individual-patient information from several studies.\n Survival outcomes need information on both terminal event time (e.g., time-to-death) and non-terminal event time (e.g., time-to-tumour progression).\n Methodologies were published in\n Emura et al. (2017) <doi:10.1177/0962280215604510>, Emura et al. (2018) <doi:10.1177/0962280216688032>,\n Emura et al. (2020) <doi:10.1177/0962280219892295>, Shinohara et al. (2020) <doi:10.1080/03610918.2020.1855449>,\n Wu et al. (2020) <doi:10.1007/s00180-020-00977-1>, and Emura et al. (2021) <doi:10.1177/09622802211046390>.\n See also the book of Emura et al. (2019) <doi:10.1007/978-981-13-3516-7>.\n Survival data from ovarian cancer patients are also available."}, "brolgar": {"categories": ["TimeSeries"], "description": "Provides a framework of tools to summarise, visualise, and explore \n  longitudinal data. It builds upon the tidy time series data frames used in the\n  'tsibble' package, and is designed to integrate within the 'tidyverse', and\n  'tidyverts' (for time series) ecosystems. The methods implemented include \n  calculating features for understanding longitudinal data, including \n  calculating summary statistics such as quantiles, medians, and numeric ranges,\n  sampling individual series, identifying individual series representative of a \n  group, and extending the facet system  in 'ggplot2' to facilitate exploration of samples of data. These methods are\n  fully described in the paper \"brolgar: An R package to Browse Over \n  Longitudinal Data Graphically and Analytically in R\", Nicholas Tierney, \n  Dianne Cook, Tania Prvan (2020) <arXiv:2012.01619>."}, "prettyunits": {"categories": ["ReproducibleResearch"], "description": "Pretty, human readable formatting of quantities.\n    Time intervals: '1337000' -> '15d 11h 23m 20s'.\n    Vague time intervals: '2674000' -> 'about a month ago'.\n    Bytes: '1337' -> '1.34 kB'."}, "aspect": {"categories": ["Psychometrics"], "description": "Contains various functions for optimal scaling. One function performs optimal scaling by maximizing an aspect (i.e. a target function such as the sum of eigenvalues, sum of squared correlations, squared multiple correlations, etc.) of the corresponding correlation matrix. Another function performs implements the LINEALS approach for optimal scaling by minimization of an aspect based on pairwise correlations and correlation ratios. The resulting correlation matrix and category scores can be used for further multivariate methods such as structural equation models. "}, "bmixture": {"categories": ["Bayesian", "Cluster", "Distributions"], "description": "Provides statistical tools for Bayesian estimation of mixture distributions, mainly a mixture of Gamma, Normal, and t-distributions. The package is implemented based on the Bayesian literature for the finite mixture of distributions, including Mohammadi and et al. (2013) <doi:10.1007/s00180-012-0323-3> and Mohammadi and Salehi-Rad (2012) <doi:10.1080/03610918.2011.588358>."}, "xaringan": {"categories": ["ReproducibleResearch"], "description": "Create HTML5 slides with R Markdown and the JavaScript library\n    'remark.js' (<https://remarkjs.com>)."}, "LaplacesDemon": {"categories": ["Bayesian", "Distributions"], "description": "Provides a complete environment for Bayesian inference using a variety of different samplers (see ?LaplacesDemon for an overview)."}, "mdgc": {"categories": ["MissingData"], "description": "Provides functions to impute missing values using Gaussian \n    copulas for mixed data types as described by Christoffersen et al. \n    (2021) <arXiv:2102.02642>. The method is related to Hoff (2007) \n    <doi:10.1214/07-AOAS107> and Zhao and Udell (2019) <arXiv:1910.12845> \n    but differs by making a direct approximation of the log marginal likelihood \n    using an extended version of the Fortran code created by Genz and Bretz \n    (2002) <doi:10.1198/106186002394> in addition to also support multinomial \n    variables."}, "csci": {"categories": ["Survival"], "description": "Calculates pointwise confidence intervals for the cumulative distribution function of the event time for current status data, data where each individual is assessed at one time to see if they had the event or not by the assessment time."}, "GPareto": {"categories": ["Optimization"], "description": "Gaussian process regression models, a.k.a. Kriging models, are\n    applied to global multi-objective optimization of black-box functions.\n    Multi-objective Expected Improvement and Step-wise Uncertainty Reduction\n    sequential infill criteria are available. A quantification of uncertainty\n    on Pareto fronts is provided using conditional simulations."}, "clustvarsel": {"categories": ["ChemPhys", "Cluster"], "description": "Variable selection for Gaussian model-based clustering as implemented in the 'mclust' package. The methodology allows to find the (locally) optimal subset of variables in a data set that have group/cluster information. A greedy or headlong search can be used, either in a forward-backward or backward-forward direction, with or without sub-sampling at the hierarchical clustering stage for starting 'mclust' models. By default the algorithm uses a sequential search, but parallelisation is also available."}, "urca": {"categories": ["Econometrics", "Finance", "TimeSeries"], "description": "Unit root and cointegration tests encountered in applied \n econometric analysis are implemented."}, "highfrequency": {"categories": ["Finance"], "description": "Provide functionality to manage, clean and match highfrequency\n    trades and quotes data, calculate various liquidity measures, estimate and\n    forecast volatility, detect price jumps and investigate microstructure noise and intraday\n    periodicity."}, "lazyWeave": {"categories": ["ReproducibleResearch"], "description": "Provides the functionality to write LaTeX code from within R\n    without having to learn LaTeX.  Functionality also exists to create HTML\n    and Markdown code.  While the functionality still exists to write\n    complete documents with lazyWeave, it is generally easier to do so with\n    with markdown and knitr.  lazyWeave's main strength now is the ability\n    to design custom and complex tables for reporting results."}, "timeDate": {"categories": ["Finance", "TimeSeries"], "description": "The 'timeDate' class fulfils the conventions of the ISO 8601 \n\tstandard as well as of the ANSI C and POSIX standards. Beyond\n\tthese standards it provides the \"Financial Center\" concept\n\twhich allows to handle data records collected in different time \n\tzones and mix them up to have always the proper time stamps with \n\trespect to your personal financial center, or alternatively to the GMT\n\treference time. It can thus also handle time stamps from historical \n\tdata records from the same time zone, even if the financial \n\tcenters changed day light saving times at different calendar\n\tdates."}, "CalibrateSSB": {"categories": ["OfficialStatistics"], "description": "Functions to calculate weights, estimates of changes and corresponding variance estimates for panel data with non-response. Partially overlapping samples are handled. Initially, weights are calculated by linear calibration. By default, the survey package is used for this purpose. It is also possible to use ReGenesees, which can be installed from <https://github.com/DiegoZardetto/ReGenesees>. Variances of linear combinations (changes and averages) and ratios are calculated from a covariance matrix based on residuals according to the calibration model. The methodology was presented at the conference, The Use of R in Official Statistics, and is described in Langsrud (2016) <http://www.revistadestatistica.ro/wp-content/uploads/2016/06/RRS2_2016_A021.pdf>.  "}, "rplos": {"categories": ["WebTechnologies"], "description": "A programmatic interface to the 'SOLR' based\n    search API (<http://api.plos.org/>) provided by the Public\n    Library of Science journals to search their articles.\n    Functions are included for searching for articles, retrieving\n    articles, making plots, doing 'faceted' searches,\n    'highlight' searches, and viewing results of 'highlighted'\n    searches in a browser."}, "treeClust": {"categories": ["Cluster"], "description": "Create a measure of inter-point dissimilarity useful \n for clustering mixed data, and, optionally, perform the clustering."}, "survsim": {"categories": ["Survival"], "description": "Simulation of simple and complex survival data including recurrent and multiple events and competing risks. See Mori\u00f1a D, Navarro A. (2014) <doi:10.18637/jss.v059.i02> and Mori\u00f1a D, Navarro A. (2017) <doi:10.1080/03610918.2016.1175621>."}, "rapport": {"categories": ["ReproducibleResearch", "WebTechnologies"], "description": "Facilitating the creation of reproducible statistical\n    report templates. Once created, rapport templates can be exported to\n    various external formats (HTML, LaTeX, PDF, ODT etc.) with pandoc as the\n    converter backend."}, "Rcapture": {"categories": ["Environmetrics"], "description": "Estimation of abundance and other demographic parameters for closed \n             populations, open populations and the robust design in capture-recapture  \n             experiments using loglinear models.   "}, "idefix": {"categories": ["ExperimentalDesign"], "description": "Generates efficient designs for discrete choice experiments based on the multinomial logit model, and individually adapted designs for the mixed multinomial logit model. The generated designs can be presented on screen and choice data can be gathered using a shiny application. Traets F, Sanchez G, and Vandebroek M (2020) <doi:10.18637/jss.v096.i03>."}, "rsem": {"categories": ["MissingData", "Psychometrics"], "description": "A robust procedure is implemented to estimate means and covariance matrix of multiple variables with missing data using Huber weight and then to estimate a structural equation model."}, "FAVAR": {"categories": ["TimeSeries"], "description": "Estimate a FAVAR model by a Bayesian method, based on Bernanke et al. (2005) <doi:10.1162/0033553053327452>."}, "dae": {"categories": ["ExperimentalDesign"], "description": "The content falls into the following groupings: (i) Data, (ii)\n    Factor manipulation functions, (iii) Design functions, (iv) ANOVA functions, (v)\n    Matrix functions, (vi) Projector and canonical efficiency functions, and (vii)\n    Miscellaneous functions. There is a vignette describing how to use the \n    design functions for randomizing and assessing designs available as a \n    vignette called 'DesignNotes'. The ANOVA functions facilitate the extraction of \n    information when the 'Error' function has been used in the call to 'aov'. \n    The package 'dae' can also be installed from \n  <http://chris.brien.name/rpackages/>."}, "MAPA": {"categories": ["TimeSeries"], "description": "Functions and wrappers for using the Multiple Aggregation Prediction Algorithm (MAPA) for time series forecasting. MAPA models and forecasts time series at multiple temporal aggregation levels, thus strengthening and attenuating the various time series components for better holistic estimation of its structure. For details see Kourentzes et al. (2014) <doi:10.1016/j.ijforecast.2013.09.006>."}, "openadds": {"categories": ["WebTechnologies"], "description": "'Openaddresses' (<https://openaddresses.io/>) client. Search,\n    fetch data, and combine 'datasets'. Outputs are easy to visualize\n    with base plots, 'ggplot2', or 'leaflet'."}, "TrajDataMining": {"categories": ["Tracking"], "description": "Contains a set of methods for trajectory data preparation, such as filtering, compressing and clustering, and for trajectory pattern discovery."}, "gsheet": {"categories": ["WebTechnologies"], "description": "Simple package to download Google Sheets using just the sharing\n    link. Spreadsheets can be downloaded as a data frame, or as plain text to parse\n    manually. Google Sheets is the new name for Google Docs Spreadsheets <https://www.google.com/sheets/about>."}, "OpenML": {"categories": ["WebTechnologies"], "description": "We provide an R interface to 'OpenML.org' which is an online machine learning platform where researchers can access open data, download and upload data sets, share their machine learning tasks and experiments and organize them online to work and collaborate with other researchers. \n    The R interface allows to query for data sets with specific properties, and allows the downloading and uploading of data sets, tasks, flows and runs. \n    See <https://www.openml.org/guide/api> for more information."}, "NPMLEcmprsk": {"categories": ["Survival"], "description": "Given a failure type, the function computes covariate-specific probability of failure over time and covariate-specific conditional hazard rate based on possibly right-censored competing risk data. Specifically, it computes the non-parametric maximum-likelihood estimates of these quantities and their asymptotic variances in a semi-parametric mixture model for competing-risks data, as described in Chang et al. (2007a)."}, "gaussDiff": {"categories": ["Distributions"], "description": "A collection difference measures for multivariate Gaussian\n        probability density functions, such as the Euclidea mean, the\n        Mahalanobis distance, the Kullback-Leibler divergence, the\n        J-Coefficient, the Minkowski L2-distance, the Chi-square\n        divergence and the Hellinger Coefficient."}, "reinsureR": {"categories": ["Finance"], "description": "Application of reinsurance treaties to claims portfolios. \n            The package creates a class Claims whose objective is to \n            store claims and premiums, on which different treaties can be applied.\n            A statistical analysis can then be applied to measure the impact of\n            reinsurance, producing a table or graphical output. This package can\n            be used for estimating the impact of reinsurance on several portfolios\n            or for pricing treaties through statistical analysis. Documentation\n            for the implemented methods can be found in \"Reinsurance: Actuarial\n            and Statistical Aspects\" by Hansj\u00f6erg Albrecher, Jan Beirlant,\n            Jozef L. Teugels (2017, ISBN: 978-0-470-77268-3) and \n            \"REINSURANCE: A Basic Guide to Facultative and Treaty Reinsurance\"\n            by Munich Re (2010) <https://www.munichre.com/site/mram/get/documents_E96160999/mram/assetpool.mr_america/PDFs/3_Publications/reinsurance_basic_guide.pdf>."}, "Bessel": {"categories": ["NumericalMathematics"], "description": "Computations for Bessel function for complex, real and partly\n  'mpfr' (arbitrary precision) numbers; notably interfacing TOMS 644;\n  approximations for large arguments, experiments, etc."}, "ica": {"categories": ["Psychometrics"], "description": "Independent Component Analysis (ICA) using various algorithms: FastICA, Information-Maximization (Infomax), and Joint Approximate Diagonalization of Eigenmatrices (JADE)."}, "GORCure": {"categories": ["Survival"], "description": "Generalized Odds Rate Mixture Cure (GORMC) model is a flexible model of fitting survival data with a cure fraction, including the Proportional Hazards Mixture Cure (PHMC) model and the Proportional Odds Mixture Cure Model as special cases. This package fit the GORMC model with interval censored data."}, "immer": {"categories": ["Psychometrics"], "description": "\n    Implements some item response models for multiple\n    ratings, including the hierarchical rater model, \n    conditional maximum likelihood estimation of linear \n    logistic partial credit model and a wrapper function\n    to the commercial FACETS program. See Robitzsch and\n    Steinfeld (2018) for a description of the functionality\n    of the package. \n    See Wang, Su and Qiu (2014; <doi:10.1111/jedm.12045>)\n    for an overview of modeling alternatives."}, "bridgedist": {"categories": ["Distributions"], "description": "An implementation of the bridge distribution with logit-link in\n    R. In Wang and Louis (2003) <doi:10.1093/biomet/90.4.765>, such a univariate\n    bridge distribution was derived as the distribution of the random intercept that\n    'bridged' a marginal logistic regression and a conditional logistic regression.\n    The conditional and marginal regression coefficients are a scalar multiple\n    of each other. Such is not the case if the random intercept distribution was\n    Gaussian."}, "chilemapas": {"categories": ["Spatial"], "description": "Mapas terrestres con topologias simplificadas. Estos mapas no \n  tienen precision geodesica, por lo que aplica el DFL-83 de 1979 de la Republica\n  de Chile y se consideran referenciales sin validez legal.\n  No se incluyen los territorios antarticos y bajo ningun evento estos mapas\n  significan que exista una cesion u ocupacion de territorios soberanos en\n  contra del Derecho Internacional por parte de Chile. Esta paquete esta \n  documentado intencionalmente en castellano asciificado para que funcione sin \n  problema en diferentes plataformas.\n  (Terrestrial maps with simplified toplogies. These maps lack geodesic\n  precision, therefore DFL-83 1979 of the Republic of Chile applies and are\n  considered to have no legal validity.\n  Antartic territories are excluded and under no event these maps mean\n  there is a cession or occupation of sovereign territories against International\n  Laws from Chile. This package was intentionally documented in asciified\n  spanish to make it work without problem on different platforms.)"}, "gmnl": {"categories": ["Econometrics"], "description": "An implementation of maximum simulated likelihood method for the\n    estimation of multinomial logit models with random coefficients as presented by Sarrias and Daziano (2017) <doi:10.18637/jss.v079.i02>.\n    Specifically, it allows estimating models with continuous heterogeneity\n    such as the mixed multinomial logit and the generalized multinomial logit.\n    It also allows estimating models with discrete heterogeneity such as the\n    latent class and the mixed-mixed multinomial logit model."}, "pkr": {"categories": ["Pharmacokinetics"], "description": "Conduct a noncompartmental analysis as closely as possible to the most widely used commercial software.\n             Some features are\n             1) CDISC SDTM terms\n             2) Automatic slope selection with the same criterion of WinNonlin(R)\n             3) Supporting both 'linear-up linear-down' and 'linear-up log-down' method\n             4) Interval(partial) AUCs with 'linear' or 'log' interpolation method\n             * Reference: Gabrielsson J, Weiner D. Pharmacokinetic and Pharmacodynamic Data Analysis - Concepts and Applications. 5th ed. 2016. (ISBN:9198299107)."}, "synthACS": {"categories": ["Spatial"], "description": "Provides access to curated American Community Survey (ACS) base tables via a wrapper\n  to library(acs). Builds synthetic micro-datasets at any user-specified geographic level with\n  ten default attributes; and, conducts spatial microsimulation modeling (SMSM) via simulated\n  annealing.  SMSM is conducted in parallel by default. Lastly, we provide functionality for\n  data-extensibility of micro-datasets."}, "rhub": {"categories": ["WebTechnologies"], "description": "Run 'R CMD check' on any of the 'R-hub' (<https://builder.r-hub.io/>)\n    architectures, from the command line. The current architectures include\n    'Windows', 'macOS', 'Solaris' and various 'Linux' distributions."}, "gtsummary": {"categories": ["ReproducibleResearch"], "description": "Creates presentation-ready tables summarizing data\n    sets, regression models, and more. The code to create the tables is\n    concise and highly customizable. Data frames can be summarized with\n    any function, e.g. mean(), median(), even user-written functions.\n    Regression models are summarized and include the reference rows for\n    categorical variables. Common regression models, such as logistic\n    regression and Cox proportional hazards regression, are automatically\n    identified and the tables are pre-filled with appropriate column\n    headers."}, "clusterCrit": {"categories": ["Cluster"], "description": "Compute clustering validation indices."}, "GARCHSK": {"categories": ["Finance"], "description": "Functions for estimating a GARCHSK model and GJRSK model based on a publication by Leon et,al (2005)<doi:10.1016/j.qref.2004.12.020> and Nakagawa and Uchiyama (2020)<doi:10.3390/math8111990>. These are a GARCH-type model allowing for time-varying volatility, skewness and kurtosis."}, "ca": {"categories": ["Psychometrics"], "description": "Computation and visualization of simple, multiple and joint correspondence analysis."}, "moderndive": {"categories": ["TeachingStatistics"], "description": "Datasets and wrapper functions for tidyverse-friendly introductory linear regression, used in \"Statistical Inference via Data Science: A ModernDive into R and the Tidyverse\" available at <https://moderndive.com/>."}, "ascii": {"categories": ["ReproducibleResearch"], "description": "Coerce R object to 'asciidoc', 'txt2tags',\n    'restructuredText', 'org', 'textile' or 'pandoc' syntax.\n    Package comes with a set of drivers for 'Sweave'."}, "BayesGPfit": {"categories": ["Bayesian"], "description": "Bayesian inferences on nonparametric regression via Gaussian Processes with a modified exponential square kernel using a basis expansion approach."}, "hot.deck": {"categories": ["MissingData"], "description": "Performs multiple hot-deck imputation of categorical and continuous variables in a data frame."}, "simrel": {"categories": ["ExperimentalDesign"], "description": "Researchers have been using simulated data from a multivariate linear model to compare and evaluate different methods, ideas and models. Additionally, teachers and educators have been using a simulation tool to demonstrate and teach various statistical and machine learning concepts.\n    This package helps users to simulate linear model data with a wide range of properties by tuning few parameters such as relevant latent components. In addition, a shiny app as an 'RStudio' gadget gives users a simple interface for using the simulation function. See more on: S\u00e6b\u00f8, S., Alm\u00f8y, T., Helland, I.S. (2015) <doi:10.1016/j.chemolab.2015.05.012> and Rimal, R., Alm\u00f8y, T., S\u00e6b\u00f8, S. (2018) <doi:10.1016/j.chemolab.2018.02.009>."}, "pdR": {"categories": ["Econometrics"], "description": "Threshold model, panel version of Hylleberg et al. (1990) <doi:10.1016/0304-4076(90)90080-D> seasonal unit root tests, and panel unit root test of Chang (2002) <doi:10.1016/S0304-4076(02)00095-7>."}, "fanplot": {"categories": ["TimeSeries"], "description": "Visualise sequential distributions using a range of plotting\n    styles. Sequential distribution data can be input as either simulations or\n    values corresponding to percentiles over time. Plots are added to\n    existing graphic devices using the fan function. Users can choose from four\n    different styles, including fan chart type plots, where a set of coloured\n    polygon, with shadings corresponding to the percentile values are layered\n    to represent different uncertainty levels. Full details in R Journal article; Abel (2015) <doi:10.32614/RJ-2015-002>."}, "bignum": {"categories": ["NumericalMathematics"], "description": "Classes for storing and manipulating arbitrary-precision\n    integer vectors and high-precision floating-point vectors. These\n    extend the range and precision of the 'integer' and 'double' data\n    types found in R. This package utilizes the 'Boost.Multiprecision' C++\n    library. It is specifically designed to work well with the 'tidyverse'\n    collection of R packages."}, "ph2bayes": {"categories": ["ExperimentalDesign"], "description": "An implementation of Bayesian single-arm phase II design methods for binary outcome based on posterior\n  probability (Thall and Simon (1994) <doi:10.2307/2533377>) and predictive probability (Lee and Liu (2008) <doi:10.1177/1740774508089279>)."}, "rjson": {"categories": ["WebTechnologies"], "description": "Converts R object into JSON objects and vice-versa."}, "micEcon": {"categories": ["Econometrics"], "description": "Various tools for microeconomic analysis and microeconomic modelling,\n   e.g. estimating quadratic, Cobb-Douglas and Translog functions,\n   calculating partial derivatives and elasticities of these functions,\n   and calculating Hessian matrices, checking curvature\n   and preparing restrictions for imposing monotonicity of Translog functions."}, "odeintr": {"categories": ["DifferentialEquations"], "description": "Wraps the Boost odeint library for integration of differential\n    equations."}, "FAdist": {"categories": ["Distributions", "Hydrology"], "description": "Probability distributions that are sometimes useful in hydrology."}, "copula": {"categories": ["Distributions", "ExtremeValue", "Finance"], "description": "Classes (S4) of commonly used elliptical, Archimedean,\n extreme-value and other copula families, as well as their rotations,\n mixtures and asymmetrizations. Nested Archimedean copulas, related\n tools and special functions. Methods for density, distribution, random\n number generation, bivariate dependence measures, Rosenblatt transform,\n Kendall distribution function, perspective and contour plots. Fitting of\n copula models with potentially partly fixed parameters, including\n standard errors. Serial independence tests, copula specification tests\n (independence, exchangeability, radial symmetry, extreme-value\n dependence, goodness-of-fit) and model selection based on\n cross-validation. Empirical copula, smoothed versions, and\n non-parametric estimators of the Pickands dependence function."}, "PxWebApiData": {"categories": ["OfficialStatistics"], "description": "Function to read PX-Web data into R via API. The example code reads data from the three national statistical institutes, Statistics Norway, Statistics Sweden and Statistics Finland."}, "irtoys": {"categories": ["Psychometrics"], "description": "A collection of functions useful in learning and practicing IRT,\n    which can be combined into larger programs. Provides basic CTT analysis,\n    a simple common interface to the estimation of item\n    parameters in IRT models for binary responses with three different programs\n    (ICL, BILOG-MG, and ltm), ability estimation (MLE, BME, EAP, WLE, plausible \n    values), item and person fit statistics, scaling methods (MM, MS, Stocking-Lord,\n    and the complete Hebaera method), and a rich array of parametric and \n    non-parametric (kernel) plots. Estimates and plots Haberman's interaction model\n    when all items are dichotomously scored."}, "pastecs": {"categories": ["Environmetrics", "Spatial", "SpatioTemporal", "TimeSeries"], "description": "Regularisation, decomposition and analysis of space-time series.\n  The pastecs R package is a PNEC-Art4 and IFREMER (Benoit Beliaeff\n  <Benoit.Beliaeff@ifremer.fr>) initiative to bring PASSTEC 2000 functionalities to R."}, "FKF": {"categories": ["TimeSeries"], "description": "This is a fast and flexible implementation of the Kalman\n        filter and smoother, which can deal with NAs. It is entirely written in C and relies fully on linear algebra subroutines contained in\n        BLAS and LAPACK. Due to the speed of the filter, the fitting of\n        high-dimensional linear state space models to large datasets\n        becomes possible. This package also contains a plot function\n        for the visualization of the state vector and graphical\n        diagnostics of the residuals."}, "googleAnalyticsR": {"categories": ["WebTechnologies"], "description": "Interact with the Google Analytics \n  APIs <https://developers.google.com/analytics/>, including \n  the Core Reporting API (v3 and v4), Management API, User Activity API\n  GA4's Data API and Admin API and Multi-Channel Funnel API."}, "PanelMatch": {"categories": ["CausalInference"], "description": "Implements a set of methodological tools\n\t     that enable researchers to apply matching methods to\n\t     time-series cross-sectional data. Imai, Kim, and Wang\n\t     (2021) <http://web.mit.edu/insong/www/pdf/tscs.pdf> \n\t     proposes a nonparametric generalization of the\n\t     difference-in-differences estimator, which does not rely\n\t     on the linearity assumption as often done in\n\t     practice. Researchers first select a method of matching\n\t     each treated observation for a given unit in a\n\t     particular time period with control observations from\n\t     other units in the same time period that have a similar\n\t     treatment and covariate history. These methods include\n\t     standard matching methods based on propensity score and\n\t     Mahalanobis distance, as well as weighting methods. Once \n\t     matching is done, both short-term and long-term average \n\t     treatment effects for the treated can be estimated with \n\t     standard errors. The package also offers a visualization \n\t     technique that allows researchers to assess the quality \n\t     of matches by examining the resulting covariate balance."}, "SimCop": {"categories": ["ExtremeValue"], "description": "Provides a framework to generating random variates from\n             arbitrary multivariate copulae, while concentrating on\n             (bivariate) extreme value copulae.  Particularly useful if\n             the multivariate copulae are not available in closed\n             form.   "}, "latentnet": {"categories": ["Cluster", "HighPerformanceComputing"], "description": "Fit and simulate latent position and cluster models for statistical networks. See Krivitsky and Handcock (2008) <doi:10.18637/jss.v024.i05> and Krivitsky, Handcock, Raftery, and Hoff (2009) <doi:10.1016/j.socnet.2009.04.001>."}, "fourierin": {"categories": ["NumericalMathematics"], "description": "Computes Fourier integrals of functions of one and two variables using the Fast Fourier transform. The Fourier transforms must be evaluated on a regular grid for fast evaluation."}, "catR": {"categories": ["Psychometrics"], "description": "Provides routines for the generation of response patterns under unidimensional dichotomous and polytomous computerized adaptive testing (CAT) framework. It holds many standard functions to estimate ability, select the first item(s) to administer and optimally select the next item, as well as several stopping rules. Options to control for item exposure and content balancing are also available (Magis and Barrada (2017) <doi:10.18637/jss.v076.c01>)."}, "wavelets": {"categories": ["Finance", "TimeSeries"], "description": "Contains functions for computing and plotting\n        discrete wavelet transforms (DWT) and maximal overlap discrete\n        wavelet transforms (MODWT), as well as their inverses.\n        Additionally, it contains functionality for computing and\n        plotting wavelet transform filters that are used in the above\n        decompositions as well as multiresolution analyses."}, "gsubfn": {"categories": ["NaturalLanguageProcessing"], "description": "The gsubfn function is like gsub but can take a replacement \n   function or certain other objects instead of the replacement string.\n   Matches and back references are input to the replacement function and \n   replaced by the function output.   gsubfn can be used to split strings \n   based on content rather than delimiters and for quasi-perl-style string \n   interpolation. The package also has facilities for translating formulas \n   to functions and allowing such formulas in function calls instead of \n   functions.  This can be used with R functions such as apply, sapply,\n   lapply, optim, integrate, xyplot, Filter and any other function that \n   expects another function as an input argument or functions like cat\n   or sql calls that may involve strings where substitution is desirable.\n   There is also a facility for returning multiple objects from functions\n   and a version of transform that allows the RHS to refer to LHS used in\n   the same transform."}, "madrat": {"categories": ["ReproducibleResearch"], "description": "Provides a framework which should improve reproducibility and transparency in data processing. It provides functionality such as automatic meta data creation and management, rudimentary quality management, data caching, work-flow management and data aggregation.\n    * The title is a wish not a promise. By no means we expect this package to deliver everything what is needed to achieve full reproducibility and transparency, but we believe that it supports efforts in this direction."}, "odds.converter": {"categories": ["SportsAnalytics"], "description": "Conversion between the most common odds types for sports betting.\n    Hong Kong odds, US odds, Decimal odds, Indonesian odds, Malaysian odds, and raw\n    Probability are covered in this package."}, "EGRET": {"categories": ["Hydrology"], "description": "Statistics and graphics for streamflow history,\n    water quality trends, and the statistical modeling algorithm: Weighted\n    Regressions on Time, Discharge, and Season (WRTDS). The modeling\n    method is introduced and discussed in Hirsch et al. (2010) <doi:10.1111/j.1752-1688.2010.00482.x>,\n    and expanded in Hirsch and De Cicco (2015) <doi:10.3133/tm4A10>."}, "GMDH": {"categories": ["TimeSeries"], "description": "Group method of data handling (GMDH) - type neural network algorithm is the heuristic self-organization method for modelling the complex systems. In this package, GMDH-type neural network algorithms are applied to make short term forecasting for a univariate time series. "}, "rworldmap": {"categories": ["OfficialStatistics"], "description": "Enables mapping of country level and gridded user datasets."}, "sbgcop": {"categories": ["Bayesian"], "description": "Estimation and inference for parameters in a Gaussian copula model,\n        treating the univariate marginal distributions as nuisance\n        parameters as described in Hoff (2007) <doi:10.1214/07-AOAS107>. \n        This package also provides a\n        semiparametric imputation procedure for missing multivariate\n        data."}, "SimJoint": {"categories": ["Distributions"], "description": "Simulate multivariate correlated data given nonparametric marginals and their joint structure characterized by a Pearson or Spearman correlation matrix. The simulator engages the problem from a purely computational perspective. It assumes no statistical models such as copulas or parametric distributions, and can approximate the target correlations regardless of theoretical feasibility. The algorithm integrates and advances the Iman-Conover (1982) approach <doi:10.1080/03610918208812265> and the Ruscio-Kaczetow iteration (2008) <doi:10.1080/00273170802285693>. Package functions are carefully implemented in C++ for squeezing computing speed, suitable for large input in a manycore environment. Precision of the approximation and computing speed both substantially outperform various CRAN packages to date. Benchmarks are detailed in function examples. A simple heuristic algorithm is additionally designed to optimize the joint distribution in the post-simulation stage. The heuristic demonstrated good potential of achieving the same level of precision of approximation without the enhanced Iman-Conover-Ruscio-Kaczetow. The package contains a copy of Permuted Congruential Generator from <https://www.pcg-random.org>."}, "idbr": {"categories": ["OfficialStatistics"], "description": "Use R to make requests to the US Census Bureau's International Data Base API.\n             Results are returned as R data frames.  For more information about the IDB API, visit\n             <https://www.census.gov/data/developers/data-sets/international-database.html>."}, "prevR": {"categories": ["OfficialStatistics"], "description": "Spatial estimation of a prevalence surface\n    or a relative risks surface, using data from a Demographic and Health\n    Survey (DHS) or an analog survey, see Larmarange et al. (2011)\n    <doi:10.4000/cybergeo.24606>."}, "VARshrink": {"categories": ["TimeSeries"], "description": "\n    Vector autoregressive (VAR) model is a fundamental and effective approach\n    for multivariate time series analysis. Shrinkage estimation methods can be\n    applied to high-dimensional VAR models with dimensionality greater than\n    the number of observations, contrary to the standard ordinary least squares\n    method. This package is an integrative package delivering nonparametric,\n    parametric, and semiparametric methods in a unified and consistent manner,\n    such as the multivariate ridge regression in Golub, Heath, and Wahba (1979)\n    <doi:10.2307/1268518>, a James-Stein type nonparametric shrinkage method in\n    Opgen-Rhein and Strimmer (2007) <doi:10.1186/1471-2105-8-S2-S3>, and\n    Bayesian estimation methods using noninformative and informative priors\n    in Lee, Choi, and S.-H. Kim (2016) <doi:10.1016/j.csda.2016.03.007> and\n    Ni and Sun (2005) <doi:10.1198/073500104000000622>."}, "bootImpute": {"categories": ["MissingData"], "description": "Bootstraps and imputes incomplete datasets. Then performs inference on estimates obtained from analysing the imputed datasets as proposed by von Hippel and Bartlett (2019) <arXiv:1210.0870v10>."}, "meteoland": {"categories": ["Hydrology"], "description": "Functions to estimate weather variables at any position of a landscape [De Caceres et al. (2018) <doi:10.1016/j.envsoft.2018.08.003>]."}, "skewlmm": {"categories": ["Robust"], "description": "It fits scale mixture of skew-normal linear mixed models using an expectation\u2013maximization (EM) type algorithm, including some possibilities for modeling the within-subject dependence. Details can be found in Schumacher, Lachos and Matos (2021) <doi:10.1002/sim.8870>."}, "nets": {"categories": ["TimeSeries"], "description": "Sparse VAR estimation based on LASSO."}, "weightr": {"categories": ["MetaAnalysis"], "description": "Estimates the Vevea and Hedges (1995)\n    weight-function model. By specifying arguments, users can\n    also estimate the modified model described in Vevea and Woods (2005), \n    which may be more practical with small datasets. Users \n    can also specify moderators to estimate a linear model. \n    The package functionality allows users to easily extract the \n    results of these analyses as R objects for other uses. In addition, \n    the package includes a function to launch both models as \n    a Shiny application. Although the Shiny application is also available online, \n    this function allows users to launch it locally if they choose."}, "pool": {"categories": ["Databases"], "description": "Enables the creation of object pools, which make it\n    less computationally expensive to fetch a new object. Currently the\n    only supported pooled objects are 'DBI' connections."}, "evmix": {"categories": ["Distributions", "ExtremeValue"], "description": "The usual distribution functions, maximum likelihood inference and\n    model diagnostics for univariate stationary extreme value mixture models\n    are provided. Kernel density estimation including various boundary\n    corrected kernel density estimation methods and a wide choice of kernels,\n    with cross-validation likelihood based bandwidth estimator.\n    Reasonable consistency with the base functions in the 'evd' package is\n    provided, so that users can safely interchange most code."}, "brainR": {"categories": ["MedicalImaging"], "description": "This includes functions for creating 3D and 4D images using \n    'WebGL', 'rgl', and 'JavaScript' commands.  \n    This package relies on the X toolkit ('XTK',\n    <https://github.com/xtk/X#readme>)."}, "gafit": {"categories": ["Optimization"], "description": "A group of sample points are evaluated against a\n        user-defined expression, the sample points are lists of\n        parameters with values that may be substituted into that\n        expression. The genetic algorithm attempts to make the result\n        of the expression as low as possible (usually this would be the\n        sum of residuals squared)."}, "subplex": {"categories": ["Optimization"], "description": "The subplex algorithm for unconstrained optimization, developed by Tom Rowan <http://www.netlib.org/opt/subplex.tgz>."}, "TUWmodel": {"categories": ["Hydrology"], "description": "The model, developed at the Vienna University of Technology, is a lumped conceptual rainfall-runoff model, following the structure of the HBV model. \n             The model can also be run in a semi-distributed fashion and with dual representation of soil layer.\n             The model runs on a daily or shorter time step and consists of a snow routine, a soil moisture routine and a flow routing routine. \n             See Parajka, J., R. Merz, G. Bloeschl (2007) <doi:10.1002/hyp.6253> Uncertainty and multiple objective calibration in regional water balance modelling: case study in 320 Austrian catchments, Hydrological Processes, 21, 435-446. "}, "sentencepiece": {"categories": ["NaturalLanguageProcessing"], "description": "Unsupervised text tokenizer allowing to perform byte pair encoding and unigram modelling. \n    Wraps the 'sentencepiece' library <https://github.com/google/sentencepiece> which provides a language independent tokenizer to split text in words and smaller subword units. \n    The techniques are explained in the paper \"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\" by Taku Kudo and John Richardson (2018) <doi:10.18653/v1/D18-2012>.\n    Provides as well straightforward access to pretrained byte pair encoding models and subword embeddings trained on Wikipedia using 'word2vec', \n    as described in \"BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages\" by Benjamin Heinzerling and Michael Strube (2018) <http://www.lrec-conf.org/proceedings/lrec2018/pdf/1049.pdf>."}, "picasso": {"categories": ["MachineLearning"], "description": "Computationally efficient tools for fitting generalized linear model with convex or non-convex penalty. Users can enjoy the superior statistical property of non-convex penalty such as SCAD and MCP which has significantly less estimation error and overfitting compared to convex penalty such as lasso and ridge. Computation is handled by multi-stage convex relaxation and the PathwIse CAlibrated Sparse Shooting algOrithm (PICASSO) which exploits warm start initialization, active set updating, and strong rule for coordinate preselection to boost computation, and attains a linear convergence to a unique sparse local optimum with optimal statistical properties. The computation is memory-optimized using the sparse matrix output."}, "tsModel": {"categories": ["TimeSeries"], "description": "Tools for specifying time series regression models."}, "JoSAE": {"categories": ["OfficialStatistics"], "description": "Implementation of some unit and area level EBLUP estimators as well as the estimators of their MSE also under heteroscedasticity. The package further documents the publications Breidenbach and Astrup (2012) <doi:10.1007/s10342-012-0596-7>, Breidenbach et al. (2016) <doi:10.1016/j.rse.2015.07.026> and Breidenbach et al. (2018 in press). The vignette further explains the use of the implemented functions."}, "TransPhylo": {"categories": ["Epidemiology"], "description": "Inference of transmission tree from a dated phylogeny. \n    Includes methods to simulate and analyse outbreaks.\n    The methodology is described in\n    Didelot et al. (2014) <doi:10.1093/molbev/msu121>,\n    Didelot et al. (2017) <doi:10.1093/molbev/msw275>."}, "fdaACF": {"categories": ["FunctionalData", "TimeSeries"], "description": "Quantify the serial correlation across lags of a given functional \n    time series using the autocorrelation function and a partial autocorrelation\n    function for functional time series proposed in \n    Mestre et al. (2021) <doi:10.1016/j.csda.2020.107108>.\n    The autocorrelation functions are based on the L2 norm of the lagged covariance \n    operators of the series. Functions are available for estimating the \n    distribution of the autocorrelation functions under the assumption \n    of strong functional white noise."}, "reporttools": {"categories": ["ReproducibleResearch"], "description": "These functions are especially helpful when writing reports of data analysis using \"Sweave\"."}, "EloOptimized": {"categories": ["SportsAnalytics"], "description": "Provides an implementation of the maximum likelihood methods for deriving\n    Elo scores as published in Foerster, Franz et al. (2016) <doi:10.1038/srep35404>."}, "rbedrock": {"categories": ["SportsAnalytics"], "description": "Implements an interface to Minecraft (Bedrock Edition) worlds. Supports the analysis and management of these worlds and game saves."}, "robfilter": {"categories": ["Robust", "TimeSeries"], "description": "Implementations for several robust procedures that allow for (online)\n        extraction of the signal of univariate or multivariate time series by\n        applying robust regression techniques to a moving time window are provided.\n        Included are univariate filtering procedures based on repeated-median \n        regression as well as hybrid and trimmed filters derived from it; \n        see Schettlinger et al. (2006) <doi:10.1515/BMT.2006.010>. The adaptive \n        online repeated median by Schettlinger et al. (2010) <doi:10.1002/acs.1105> \n        and the slope comparing adaptive repeated median by Borowski and Fried (2013) \n        <doi:10.1007/s11222-013-9391-7> choose the width of the moving time \n        window adaptively. Multivariate versions are also provided; see  \n        Borowski et al. (2009) <doi:10.1080/03610910802514972> for a multivariate \n        online adaptive repeated median and Borowski (2012) <doi:10.17877/DE290R-14393>  \n        for a multivariate slope comparing adaptive repeated median. Furthermore, \n        a repeated-median based filter with automatic outlier replacement and \n        shift detection is provided; see Fried (2004) <doi:10.1080/10485250410001656444>."}, "dtts": {"categories": ["TimeSeries"], "description": "High-frequency time-series support via 'nanotime' and 'data.table'."}, "nardl": {"categories": ["TimeSeries"], "description": "Computes the nonlinear cointegrating autoregressive distributed lag model with automatic bases aic and bic lags selection of independent variables proposed by (Shin, Yu & Greenwood-Nimmo, 2014 <doi:10.1007/978-1-4899-8008-3_9>)."}, "GUIProfiler": {"categories": ["HighPerformanceComputing"], "description": "Show graphically the results of profiling R functions by tracking their execution time."}, "bife": {"categories": ["Econometrics"], "description": "Estimates fixed effects binary choice models (logit and probit) with potentially many\n  individual fixed effects and computes average partial effects. Incidental parameter bias can be\n  reduced with an asymptotic bias correction proposed by Fernandez-Val (2009) \n  <doi:10.1016/j.jeconom.2009.02.007>."}, "truncnorm": {"categories": ["Distributions"], "description": "Density, probability, quantile and random number\n        generation functions for the truncated normal distribution."}, "ggmap": {"categories": ["Spatial", "WebTechnologies"], "description": "A collection of functions to visualize spatial data and models\n    on top of static maps from various online sources (e.g Google Maps and Stamen\n    Maps). It includes tools common to those tasks, including functions for\n    geolocation and routing."}, "raster": {"categories": ["Spatial", "SpatioTemporal"], "description": "Reading, writing, manipulating, analyzing and modeling of spatial data. The package implements basic and high-level functions for raster data and for vector data operations such as intersections. See the manual and tutorials on <https://rspatial.org/> to get started."}, "qrng": {"categories": ["Distributions"], "description": "Functionality for generating (randomized) quasi-random numbers in\n  high dimensions."}, "tabuSearch": {"categories": ["Optimization"], "description": "Tabu search algorithm for binary configurations. A basic version of the algorithm as described by Fouskakis and Draper (2007) <doi:10.1111/j.1751-5823.2002.tb00174.x>."}, "EffectTreat": {"categories": ["CausalInference"], "description": "In personalized medicine, one wants to know, for a given patient and his or her outcome for a predictor (pre-treatment variable), how likely it is that a treatment will be more beneficial than an alternative treatment. This package allows for the quantification of the predictive causal association (i.e., the association between the predictor variable and the individual causal effect of the treatment) and related metrics. Part of this software has been developed using funding provided from the European Union's 7th Framework Programme for research, technological development and demonstration under Grant Agreement no 602552."}, "chemCal": {"categories": ["ChemPhys"], "description": "Simple functions for plotting linear\n\tcalibration functions and estimating standard errors for measurements\n\taccording to the Handbook of Chemometrics and Qualimetrics: Part A\n\tby Massart et al. (1997) There are also functions estimating the limit\n\tof detection (LOD) and limit of quantification (LOQ).\n\tThe functions work on model objects from - optionally weighted - linear\n\tregression (lm) or robust linear regression ('rlm' from the 'MASS' package)."}, "domino": {"categories": ["ModelDeployment"], "description": "A wrapper on top of the 'Domino Command-Line Client'. It lets you\n    run 'Domino' commands (e.g., \"run\", \"upload\", \"download\") directly from your\n    R environment. Under the hood, it uses R's system function to run the 'Domino'\n    executable, which must be installed as a prerequisite. 'Domino' is a service\n    that makes it easy to run your code on scalable hardware, with integrated\n    version control and collaboration features designed for analytical workflows\n    (see <http://www.dominodatalab.com> for more information)."}, "wCorr": {"categories": ["Psychometrics"], "description": "Calculates Pearson, Spearman, polychoric, and polyserial correlation coefficients, in weighted or unweighted form. The package implements tetrachoric correlation as a special case of the polychoric and biserial correlation as a specific case of the polyserial."}, "genie": {"categories": ["Robust"], "description": "Includes the reference implementation of Genie - a hierarchical\n    clustering algorithm that links two point groups in such a way that\n    an inequity measure (namely, the Gini index) of the cluster sizes\n    does not significantly increase above a given threshold.\n    This method most often outperforms many other data segmentation approaches\n    in terms of clustering quality as tested on a wide range of benchmark\n    datasets. At the same time, Genie retains the high speed of the single\n    linkage approach, therefore it is also suitable for analysing larger data sets.\n    For more details see (Gagolewski et al. 2016 <doi:10.1016/j.ins.2016.05.003>).\n    For an even faster and more feature-rich implementation, including,\n    amongst others, noise point detection, see the 'genieclust' package."}, "lmom": {"categories": ["Distributions", "ExtremeValue"], "description": "Functions related to L-moments: computation of L-moments\n  and trimmed L-moments of distributions and data samples; parameter\n  estimation; L-moment ratio diagram; plot vs. quantiles of an\n  extreme-value distribution."}, "snowfall": {"categories": ["HighPerformanceComputing"], "description": "Usability wrapper around snow for easier development of\n        parallel R programs. This package offers e.g. extended error\n        checks, and additional functions. All functions work in\n        sequential mode, too, if no cluster is present or wished.\n        Package is also designed as connector to the cluster management\n        tool sfCluster, but can also used without it."}, "qgtools": {"categories": ["MissingData"], "description": "Two linear mixed model approaches: REML(restricted maximum likelihood) and MINQUE (minimum norm quadratic unbiased estimation) approaches and several resampling techniques are integrated for various quantitative genetics analyses. With these two types of approaches, various unbalanced data structures, missing data, and any irregular genetic  mating designs can be analyzed and statistically tested. This package also offers fast computations for many large data sets. "}, "statcanR": {"categories": ["OfficialStatistics"], "description": "An easy connection with R to Statistics Canada's Web Data Service. Open economic data (formerly known as CANSIM tables, now identified by Product IDs (PID)) are accessible as a data frame, directly in the user's R environment.\n    Warin, Le Duc (2019) <doi:10.6084/m9.figshare.10544735>."}, "dCovTS": {"categories": ["TimeSeries"], "description": "Computing and plotting the distance covariance and correlation function of a univariate or a multivariate time series. Both versions of biased and unbiased estimators of distance covariance and correlation are provided. Test statistics for testing pairwise independence are also implemented. Some data sets are also included. References include: \n\t\t\t       a) Edelmann Dominic, Fokianos Konstantinos and Pitsillou Maria (2019). An Updated Literature Review of Distance Correlation and Its Applications to Time Series. International Statistical Review, 87(2): 237\u2013262. <doi:10.1111/insr.12294>.\n             b) Fokianos Konstantinos and Pitsillou Maria (2018). Testing independence for multivariate time series via the auto-distance correlation matrix. Biometrika, 105(2): 337\u2013352. <doi:10.1093/biomet/asx082>.\n             c) Fokianos Konstantinos and Pitsillou Maria (2017). Consistent testing for pairwise dependence in time series. Technometrics, 59(2): 262\u2013270. <doi:10.1080/00401706.2016.1156024>.\n             d) Pitsillou Maria and Fokianos Konstantinos (2016). dCovTS: Distance Covariance/Correlation for Time Series. R Journal, 8(2):324-340. <doi:10.32614/RJ-2016-049>."}, "edfReader": {"categories": ["MedicalImaging"], "description": "Reads European Data Format files EDF and EDF+, see <http://www.edfplus.info>,\n    BioSemi Data Format files BDF, see <http://www.biosemi.com/faq/file_format.htm>,\n    and BDF+ files, see <http://www.teuniz.net/edfbrowser/bdfplus%20format%20description.html>.\n    The files are read in two steps: first the header is read\n    and then the signals (using the header object as a parameter)."}, "MASS": {"categories": ["Distributions", "Econometrics", "Environmetrics", "NumericalMathematics", "Psychometrics", "Robust", "TeachingStatistics"], "description": "Functions and datasets to support Venables and Ripley,\n  \"Modern Applied Statistics with S\" (4th edition, 2002)."}, "BTLLasso": {"categories": ["Psychometrics"], "description": "Performs 'BTLLasso' as described by Schauberger and Tutz (2019) <doi:10.18637/jss.v088.i09> and Schauberger and Tutz (2017) <doi:10.1177/1471082X17693086>. BTLLasso is a method to include different types of variables in paired comparison models and, therefore, to allow for heterogeneity between subjects. Variables can be subject-specific, object-specific and subject-object-specific and can have an influence on the attractiveness/strength of the objects. Suitable L1 penalty terms are used to cluster certain effects and to reduce the complexity of the models."}, "TSdisaggregation": {"categories": ["TimeSeries"], "description": "First - Generates (potentially high-dimensional) high-frequency and low-frequency series for simulation studies in temporal disaggregation; Second - a toolkit utilizing temporal disaggregation and benchmarking techniques with a low-dimensional matrix of indicator series previously proposed in Dagum and Cholette (2006, ISBN:978-0-387-35439-2) ; and Third - novel techniques proposed by Mosley, Gibberd and Eckley (2021) <arXiv:2108.05783> for disaggregating low-frequency series in the presence of high-dimensional indicator matrices."}, "forecTheta": {"categories": ["TimeSeries"], "description": "Routines for forecasting univariate time series using Theta Models. Contains several cross-validation routines. "}, "MNB": {"categories": ["Distributions"], "description": "Diagnostic tools as residual analysis, global, \n    local and total-local influence for the multivariate model \n    from the random intercept Poisson generalized log gamma model \n    are available in this package. Including also, the estimation \n    process by maximum likelihood method, for details see \n    Fabio, L. F; Villegas, C. L.; Carrasco, J.M.F and de Castro, M. (2021) \n    <doi:10.1080/03610926.2021.1939380>."}, "bayesbr": {"categories": ["Bayesian"], "description": "Applies the Beta regression model in the Bayesian statistical view with the possibility of adding a spatial effect in the parameters, the Beta regression is used when the response variable is a proportion variable, that is, it only accepts values between 0 and 1.\n    The package 'bayesbr' uses 'rstan' package to build the Bayesian statistical models. The main function of the package receives as a parameter a form informing the independent variable and the co-variables of the model to be made, as output it returns a list with the results of the model. For more details see Ferrari and Cribari-Neto (2004) <doi:10.1080/0266476042000214501> and Hoffman and Gelman (2014) <arXiv:1111.4246>."}, "minimaxdesign": {"categories": ["ExperimentalDesign"], "description": "Provides two main functions, minimax() and miniMaxPro(), for computing minimax \n    and minimax projection designs using the minimax clustering algorithm in Mak and \n    Joseph (2018) <doi:10.1080/10618600.2017.1302881>. Current design region options \n    include the unit hypercube (\"hypercube\"), the unit simplex (\"simplex\"), the unit ball\n    (\"ball\"), as well as user-defined constraints on the unit hypercube (\"custom\"). Minimax\n    designs can also be computed on user-provided images using the function minimax.map(). \n    Design quality can be assessed using the function mMdist(), which computes the minimax\n    (fill) distance of a design."}, "asd": {"categories": ["ClinicalTrials", "ExperimentalDesign"], "description": "Package runs simulations for adaptive seamless designs with and without early outcomes \n                           for treatment selection and subpopulation type designs."}, "pan": {"categories": ["MissingData"], "description": "It provides functions and examples for maximum likelihood estimation for\n              generalized linear mixed models and Gibbs sampler for multivariate linear\n              mixed models with incomplete data, as described in Schafer JL (1997)\n              \"Imputation of missing covariates under a multivariate linear mixed model\".\n              Technical report 97-04, Dept. of Statistics, The Pennsylvania State University."}, "clinfun": {"categories": ["ClinicalTrials", "Survival"], "description": "Utilities to make your clinical collaborations easier if not\n          fun. It contains functions for designing studies such as Simon\n          2-stage and group sequential designs and for data analysis such\n          as Jonckheere-Terpstra test and estimating survival quantiles."}, "rstan": {"categories": ["Bayesian"], "description": "User-facing R functions are provided to parse, compile, test,\n    estimate, and analyze Stan models by accessing the header-only Stan library\n    provided by the 'StanHeaders' package. The Stan project develops a probabilistic\n    programming language that implements full Bayesian statistical inference\n    via Markov Chain Monte Carlo, rough Bayesian inference via 'variational'\n    approximation, and (optionally penalized) maximum likelihood estimation via\n    optimization. In all three cases, automatic differentiation is used to quickly\n    and accurately evaluate gradients without burdening the user with the need to\n    derive the partial derivatives."}, "LSTS": {"categories": ["TimeSeries"], "description": "A set of functions that allow stationary analysis and locally stationary time series analysis."}, "RDieHarder": {"categories": ["Distributions"], "description": "The 'RDieHarder' package provides an R interface to \n the 'DieHarder' suite of random number generators and tests that \n was developed by Robert G. Brown and David Bauer, extending \n earlier work by George Marsaglia and others. The 'DieHarder'\n library code is included."}, "vistributions": {"categories": ["Distributions"], "description": "Visualize and compute percentiles/probabilities of normal, t, f, chi square \n    and binomial distributions."}, "miWQS": {"categories": ["MissingData"], "description": "The miWQS package handles the uncertainty due to below the detection limit in a correlated component mixture problem.  Researchers want to determine if a set/mixture of continuous and correlated components/chemicals is associated with an outcome and if so, which components are important in that mixture. These components share a common outcome but are interval-censored between zero and low thresholds, or detection limits, that may be different across the components. This package applies the multiple imputation (MI) procedure to the weighted quantile sum regression (WQS) methodology for continuous, binary, or count outcomes (Hargarten & Wheeler (2020) <doi:10.1016/j.envres.2020.109466>). The imputation models are: bootstrapping imputation (Lubin et.al (2004) <doi:10.1289/ehp.7199>), univariate Bayesian imputation (Hargarten & Wheeler (2020) <doi:10.1016/j.envres.2020.109466>), and multivariate Bayesian regression imputation.  "}, "dextergui": {"categories": ["Psychometrics"], "description": "Classical Test and Item analysis, \n  Item Response analysis and data management for educational and psychological tests."}, "gganimate": {"categories": ["TeachingStatistics"], "description": "The grammar of graphics as implemented in the 'ggplot2' package has\n    been successful in providing a powerful API for creating static \n    visualisation. In order to extend the API for animated graphics this package\n    provides a completely new set of grammar, fully compatible with 'ggplot2' \n    for specifying transitions and animations in a flexible and extensible way."}, "shapefiles": {"categories": ["Spatial"], "description": "Functions to read and write ESRI shapefiles"}, "autostsm": {"categories": ["TimeSeries"], "description": "Automatic model selection for structural time series decomposition into trend, cycle, and seasonal components, plus optionality for structural interpolation, using the Kalman filter. \n  Koopman, Siem Jan and Marius Ooms (2012) \"Forecasting Economic Time Series Using Unobserved Components Time Series Models\" <doi:10.1093/oxfordhb/9780195398649.013.0006>.\n  Kim, Chang-Jin and Charles R. Nelson (1999) \"State-Space Models with Regime Switching: Classical and Gibbs-Sampling Approaches with Applications\" <doi:10.7551/mitpress/6444.001.0001><http://econ.korea.ac.kr/~cjkim/>. "}, "feather": {"categories": ["NumericalMathematics"], "description": "Read and write feather files, a lightweight binary columnar\n    data store designed for maximum speed."}, "APFr": {"categories": ["Bayesian"], "description": "Implements a multiple testing approach to the\n    choice of a threshold gamma on the p-values using the\n    Average Power Function (APF) and Bayes False Discovery\n    Rate (FDR) robust estimation. Function apf_fdr() \n    estimates both quantities from either raw data or\n    p-values. Function apf_plot() produces smooth graphs \n    and tables of the relevant results. Details of the methods\n    can be found in Quatto P, Margaritella N, et al. (2019) \n    <doi:10.1177/0962280219844288>."}, "wrProteo": {"categories": ["MissingData"], "description": "Data analysis of proteomics experiments by mass spectrometry is supported by this collection of functions mostly dedicated to the analysis of (bottom-up) quantitative (XIC) data. \n    Fasta-formatted proteomes (eg from UniProt Consortium <doi:10.1093/nar/gky1049>) can be read with automatic parsing and multiple annotation types (like species origin, abbreviated gene names, etc) extracted. \n    Quantitative proteomics measurements frequently contain multiple NA values, due to physical absence of given peptides in some samples, limitations in sensitivity or other reasons. \n    The functions provided here help to inspect graphically the data to investigate the nature of NA-values via their respective replicate measurements and to help/confirm the choice of NA-replacement by low random values. \n    Dedicated filtering and statistical testing using the framework of package 'limma' <doi:10.18129/B9.bioc.limma> can be run, enhanced by multiple rounds of NA-replacements to provide robustness towards rare stochastic events. \n    Multi-species samples, as frequently used in benchmark-tests (eg Navarro et al 2016 <doi:10.1038/nbt.3685>, Ramus et al 2016 <doi:10.1016/j.jprot.2015.11.011>), can be run with special options separating \n    the data into sub-groups during normalization and testing. \n    As example the data-set from Ramus et al 2016 <doi:10.1016/j.jprot.2015.11.011>) is provided quantified by MaxQuant (Tyanova et al 2016 <doi:10.1038/nprot.2016.136>), ProteomeDiscoverer,\n    OpenMS (<doi:10.1038/nmeth.3959>) and Proline (Bouyssie et al 2020 <doi:10.1093/bioinformatics/btaa118>). Meta-data provided in sdrf format can be integrated to the analysis.\n    Subsequently, ROC curves (Hand and Till 2001 <doi:10.1023/A:1010920819831>) can be constructed to compare multiple analysis approaches."}, "projects": {"categories": ["ReproducibleResearch"], "description": "Provides a project infrastructure with a focus on\n    manuscript creation. Creates a project folder with a single command,\n    containing subdirectories for specific components, templates for\n    manuscripts, and so on."}, "GNAR": {"categories": ["TimeSeries"], "description": "Simulation of, and fitting models for, Generalised Network Autoregressive (GNAR) time series models which take account of network structure.  Such models are described in Knight et al. (2020) <doi:10.18637/jss.v096.i05>."}, "mboost": {"categories": ["MachineLearning", "Survival"], "description": "Functional gradient descent algorithm\n  (boosting) for optimizing general risk functions utilizing\n  component-wise (penalised) least squares estimates or regression\n  trees as base-learners for fitting generalized linear, additive\n  and interaction models to potentially high-dimensional data.\n  Models and algorithms are described in <doi:10.1214/07-STS242>,\n  a hands-on tutorial is available from <doi:10.1007/s00180-012-0382-5>.\n  The package allows user-specified loss functions and base-learners."}, "SPOT": {"categories": ["Optimization"], "description": "A set of tools for model-based optimization and tuning of\n    algorithms (hyperparameter tuning respectively hyperparameter optimization). It includes surrogate models, optimizers, and design of experiment\n    approaches. The main interface is spot, which uses sequentially updated\n    surrogate models for the purpose of efficient optimization. The main goal is\n    to ease the burden of objective function evaluations, when a single evaluation\n    requires a significant amount of resources."}, "SSN": {"categories": ["Spatial"], "description": "Spatial statistical modeling and prediction for data on stream networks, including models based on in-stream distance (Ver Hoef, J.M. and Peterson, E.E., 2010. <doi:10.1198/jasa.2009.ap08248>.) Models are created using moving average constructions. Spatial linear models, including explanatory variables, can be fit with (restricted) maximum likelihood.  Mapping and other graphical functions are included. "}, "BETS": {"categories": ["TimeSeries"], "description": "It provides access to and information about the most important\n    Brazilian economic time series - from the Getulio Vargas Foundation <http://portal.fgv.br/en>,\n    the Central Bank of Brazil <http://www.bcb.gov.br> and the Brazilian Institute of Geography\n    and Statistics <http://www.ibge.gov.br>. It also presents tools for managing, analysing (e.g.\n    generating dynamic reports with a complete analysis of a series) and exporting\n    these time series."}, "gmm": {"categories": ["Econometrics", "Finance"], "description": "It is a complete suite to estimate models based on moment conditions. It includes the two step Generalized method of moments (Hansen 1982; <doi:10.2307/1912775>), the iterated GMM and continuous updated estimator (Hansen, Eaton and Yaron 1996; <doi:10.2307/1392442>) and several methods that belong to the Generalized Empirical Likelihood family of estimators (Smith 1997; <doi:10.1111/j.0013-0133.1997.174.x>, Kitamura 1997; <doi:10.1214/aos/1069362388>, Newey and Smith 2004; <doi:10.1111/j.1468-0262.2004.00482.x>, and Anatolyev 2005 <doi:10.1111/j.1468-0262.2005.00601.x>).\t"}, "texreg": {"categories": ["ReproducibleResearch"], "description": "Converts coefficients, standard errors, significance stars, and goodness-of-fit statistics of statistical models into LaTeX tables or HTML tables/MS Word documents or to nicely formatted screen output for the R console for easy model comparison. A list of several models can be combined in a single table. The output is highly customizable. New model types can be easily implemented. Details can be found in Leifeld (2013), JStatSoft <doi:10.18637/jss.v055.i08>. (If the Zelig package, which this package enhances, cannot be found on CRAN, you can find it at <https://github.com/IQSS/Zelig>. If the mnlogit package, which this package enhances, cannot be found on CRAN, you can find an old version in the CRAN Archive at <https://cran.r-project.org/src/contrib/Archive/mnlogit/>.)"}, "MetaIntegration": {"categories": ["MetaAnalysis"], "description": "An ensemble meta-prediction framework to integrate multiple regression \n    models into a current study. Gu, T., Taylor, J.M.G. and Mukherjee, B. (2020) \n    <arXiv:2010.09971>.\n    A meta-analysis framework along with two weighted estimators as the ensemble \n    of empirical Bayes estimators, which combines the estimates from the different \n    external models. The proposed framework is flexible and robust in the ways \n    that (i) it is capable of incorporating external models that use a slightly \n    different set of covariates; (ii) it is able to identify the most relevant \n    external information and diminish the influence of information that is less \n    compatible with the internal data; and (iii) it nicely balances the bias-variance \n    trade-off while preserving the most efficiency gain. The proposed estimators \n    are more efficient than the naive analysis of the internal data and other \n    naive combinations of external estimators."}, "cshapes": {"categories": ["Spatial", "SpatioTemporal"], "description": "Package for CShapes 2.0, a GIS dataset of country borders (1886-today). Includes functions for data extraction and the computation of distance matrices and -lists."}, "docopulae": {"categories": ["ExperimentalDesign"], "description": "A direct approach to optimal designs for copula models based on\n    the Fisher information. Provides flexible functions for building joint PDFs,\n    evaluating the Fisher information and finding optimal designs. It includes an\n    extensible solution to summation and integration called 'nint', functions for\n    transforming, plotting and comparing designs, as well as a set of tools for\n    common low-level tasks."}, "gmt": {"categories": ["Spatial"], "description": "Interface between the GMT map-making software and R, enabling the\n  user to manipulate geographic data within R and call GMT commands to draw and\n  annotate maps in postscript format. The gmt package is about interactive data\n  analysis, rapidly visualizing subsets and summaries of geographic data, while\n  performing statistical analysis in the R console."}, "simpleboot": {"categories": ["Econometrics"], "description": "Simple bootstrap routines."}, "Ryacas": {"categories": ["NumericalMathematics"], "description": "Interface to the 'yacas' computer algebra system (<http://www.yacas.org/>)."}, "freqdom.fda": {"categories": ["FunctionalData", "TimeSeries"], "description": "Implementations of functional dynamic principle components analysis. Related graphic tools and frequency domain methods.\n  These methods directly use multivariate dynamic principal components implementation,\n  following the guidelines from Hormann, Kidzinski and Hallin (2016), Dynamic Functional Principal Component <doi:10.1111/rssb.12076>."}, "addhazard": {"categories": ["Survival"], "description": "Contains tools to fit the additive hazards model to data from a cohort,\n    random sampling, two-phase Bernoulli sampling and two-phase finite population sampling,\n    as well as calibration tool to incorporate phase I auxiliary information into the\n    two-phase data model fitting.  This package provides regression parameter estimates and\n    their model-based and robust standard errors. It also offers tools to make prediction of\n    individual specific hazards."}, "fauxpas": {"categories": ["WebTechnologies"], "description": "HTTP error helpers. Methods included for general purpose HTTP \n    error handling, as well as individual methods for every HTTP status\n    code, both via status code numbers as well as their descriptive names.\n    Supports ability to adjust behavior to stop, message or warning.\n    Includes ability to use custom whisker template to have any configuration\n    of status code, short description, and verbose message. Currently \n    supports integration with 'crul', 'curl', and 'httr'."}, "rodd": {"categories": ["ExperimentalDesign"], "description": "A collection of functions for numerical construction of optimal discriminating designs. At the current moment T-optimal designs (which maximize the lower bound for the power of F-test for regression model discrimination), KL-optimal designs (for lognormal errors) and their robust analogues can be calculated with the package.  "}, "naivebayes": {"categories": ["MachineLearning", "MissingData"], "description": "In this implementation of the Naive Bayes classifier following class conditional distributions are available: Bernoulli, Categorical, Gaussian, Poisson and non-parametric representation of the class conditional density estimated via Kernel Density Estimation. Implemented classifiers handle missing data and can take advantage of sparse data."}, "agridat": {"categories": ["ExperimentalDesign"], "description": "Datasets from books, papers, and websites related to agriculture.\n    Example graphics and analyses are included. Data come from small-plot trials,\n    multi-environment trials, uniformity trials, yield monitors, and more."}, "pmmlTransformations": {"categories": ["ModelDeployment"], "description": "Allows for data to be transformed before using it to construct models. Builds structures to allow functions in the PMML package to\n    output transformation details in addition to the model in the resulting PMML file. The Predictive Model Markup Language (PMML) is an XML-based language which provides a way for applications to define machine learning, statistical and data mining models and to share models between PMML compliant applications. More information about the PMML industry standard and the Data Mining Group can be found at <http://www.dmg.org>. The generated PMML can be imported into any PMML consuming application, such as Zementis Predictive Analytics products, which integrate with web services, relational database systems and deploy natively on Hadoop in conjunction with Hive, Spark or Storm, as well as allow predictive analytics to be executed for IBM z Systems mainframe applications and real-time, streaming analytics platforms."}, "washdata": {"categories": ["Hydrology"], "description": "Urban water and sanitation survey dataset collected by Water and\n    Sanitation for the Urban Poor (WSUP) with technical support from \n    Valid International. These citywide surveys have been collecting data \n    allowing water and sanitation service levels across the entire city to be \n    characterised, while also allowing more detailed data to be collected in \n    areas of the city of particular interest. These surveys are intended to \n    generate useful information for others working in the water and sanitation\n    sector. Current release version includes datasets collected from a survey \n    conducted in Dhaka, Bangladesh in March 2017. This survey in Dhaka is one of \n    a series of surveys to be conducted by WSUP in various\n    cities in which they operate including Accra, Ghana; Nakuru, Kenya; \n    Antananarivo, Madagascar; Maputo, Mozambique; and, Lusaka, Zambia. This \n    package will be updated once the surveys in other cities are completed and \n    datasets have been made available."}, "batchtools": {"categories": ["HighPerformanceComputing"], "description": "As a successor of the packages 'BatchJobs' and 'BatchExperiments',\n    this package provides a parallel implementation of the Map function for high\n    performance computing systems managed by schedulers 'IBM Spectrum LSF'\n    (<https://www.ibm.com/products/hpc-workload-management>),\n    'OpenLava' (<https://www.openlava.org/>), 'Univa Grid Engine'/'Oracle Grid\n    Engine' (<https://www.univa.com/>), 'Slurm' (<https://slurm.schedmd.com/>),\n    'TORQUE/PBS'\n    (<https://adaptivecomputing.com/cherry-services/torque-resource-manager/>),\n    or 'Docker Swarm' (<https://docs.docker.com/engine/swarm/>).\n    A multicore and socket mode allow the parallelization on a local machines,\n    and multiple machines can be hooked up via SSH to create a makeshift\n    cluster. Moreover, the package provides an abstraction mechanism to define\n    large-scale computer experiments in a well-organized and reproducible way."}, "ghyp": {"categories": ["Distributions", "Finance"], "description": "Detailed functionality for working\n        with the univariate and multivariate Generalized Hyperbolic\n        distribution and its special cases (Hyperbolic (hyp), Normal\n        Inverse Gaussian (NIG), Variance Gamma (VG), skewed Student-t\n        and Gaussian distribution). Especially, it contains fitting\n        procedures, an AIC-based model selection routine, and functions\n        for the computation of density, quantile, probability, random\n        variates, expected shortfall and some portfolio optimization\n        and plotting routines as well as the likelihood ratio test. In\n        addition, it contains the Generalized Inverse Gaussian\n        distribution. See Chapter 3 of A. J. McNeil, R. Frey, and P. Embrechts. \n        Quantitative risk management: Concepts, techniques and tools. \n        Princeton University Press, Princeton (2005)."}, "psfmi": {"categories": ["MissingData"], "description": "\n\tPooling, backward and forward selection of linear, logistic and Cox regression models in \n\tmultiply imputed datasets. Backward and forward selection can be done \n\tfrom the pooled model using Rubin's Rules (RR), the D1, D2, D3, D4 and \n\tthe median p-values method. This is also possible for Mixed models. \n\tThe models can contain continuous, dichotomous, categorical and restricted \n\tcubic spline predictors and interaction terms between\tall these type of predictors. \n\tThe stability of the models\tcan be evaluated using bootstrapping and cluster \n\tbootstrapping. The package further contains functions to pool the model performance \n\tas ROC/AUC, R-squares, scaled Brier score, H&L test and calibration\tplots for logistic \n\tregression models. Internal validation can be\tdone with cross-validation or bootstrapping. \n\tThe adjusted intercept after shrinkage of pooled regression coefficients can be obtained. \n\tBackward and forward selection as part of internal validation is possible. \n\tA function to externally validate logistic prediction models in multiple imputed \n\tdatasets is available and a function to compare models. \n\tEekhout (2017) <doi:10.1186/s12874-017-0404-7>.\n\tWiel (2009) <doi:10.1093/biostatistics/kxp011>.\n\tMarshall (2009) <doi:10.1186/1471-2288-9-57>."}, "doRNG": {"categories": ["HighPerformanceComputing"], "description": "Provides functions to perform\n    reproducible parallel foreach loops, using independent\n    random streams as generated by L'Ecuyer's combined\n    multiple-recursive generator [L'Ecuyer (1999), <doi:10.1287/opre.47.1.159>].\n    It enables to easily convert standard %dopar% loops into\n    fully reproducible loops, independently of the number\n    of workers, the task scheduling strategy, or the chosen\n    parallel environment and associated foreach backend."}, "sdcSpatial": {"categories": ["OfficialStatistics"], "description": "Privacy protected raster maps \n  can be created from spatial point data. Protection\n  methods include smoothing of dichotomous variables by de Jonge and de Wolf (2016) \n  <doi:10.1007/978-3-319-45381-1_9>, continuous variables by de Wolf and \n  de Jonge (2018) <doi:10.1007/978-3-319-99771-1_23>, suppressing \n  revealing values and a generalization of the quad tree method by \n  Su\u00f1\u00e9, Rovira, Ib\u00e1\u00f1ez and Farr\u00e9 (2017) <doi:10.2901/EUROSTAT.C2017.001>."}, "extraDistr": {"categories": ["Distributions"], "description": "Density, distribution function, quantile function\n    and random generation for a number of univariate\n    and multivariate distributions. This package implements the\n    following distributions: Bernoulli, beta-binomial, beta-negative\n    binomial, beta prime, Bhattacharjee, Birnbaum-Saunders,\n    bivariate normal, bivariate Poisson, categorical, Dirichlet,\n    Dirichlet-multinomial, discrete gamma, discrete Laplace,\n    discrete normal, discrete uniform, discrete Weibull, Frechet,\n    gamma-Poisson, generalized extreme value, Gompertz,\n    generalized Pareto, Gumbel, half-Cauchy, half-normal, half-t,\n    Huber density, inverse chi-squared, inverse-gamma, Kumaraswamy,\n    Laplace, location-scale t, logarithmic, Lomax, multivariate\n    hypergeometric, multinomial, negative hypergeometric, \n    non-standard beta, normal mixture, Poisson mixture, Pareto,\n    power, reparametrized beta, Rayleigh, shifted Gompertz, Skellam,\n    slash, triangular, truncated binomial, truncated normal,\n    truncated Poisson, Tukey lambda, Wald, zero-inflated binomial,\n    zero-inflated negative binomial, zero-inflated Poisson."}, "eva": {"categories": ["ExtremeValue"], "description": "Goodness-of-fit tests for selection of r in the r-largest order\n    statistics (GEVr) model. Goodness-of-fit tests for threshold selection in the\n    Generalized Pareto distribution (GPD). Random number generation and density functions\n    for the GEVr distribution. Profile likelihood for return level estimation\n    using the GEVr and Generalized Pareto distributions. P-value adjustments for\n    sequential, multiple testing error control. Non-stationary fitting of GEVr and\n    GPD.\n    Bader, B., Yan, J. & Zhang, X. (2016) <doi:10.1007/s11222-016-9697-3>.\n    Bader, B., Yan, J. & Zhang, X. (2018) <doi:10.1214/17-AOAS1092>."}, "trip": {"categories": ["Spatial", "SpatioTemporal", "Tracking"], "description": "Functions for accessing and manipulating spatial data for animal\n    tracking, with straightforward coercion from and to other formats. Filter\n    for speed and create time spent maps from animal track data. There are\n    coercion methods to convert between 'trip' and 'ltraj' from 'adehabitatLT', \n    and between 'trip' and 'psp' and 'ppp' from 'spatstat'. Trip objects\n    can be created from raw or grouped data frames, and from types in the 'sp', \n    'sf', 'amt', 'trackeR', 'mousetrap', and other packages. "}, "LowRankQP": {"categories": ["Optimization"], "description": "Solves quadratic programming problems where the Hessian is represented as the product of two matrices."}, "quantregGrowth": {"categories": ["Environmetrics"], "description": "Fits non-crossing regression quantiles as a function of linear covariates and multiple smooth terms, including varying coefficients, via B-splines with L1-norm difference penalties. \n    The smoothing parameters are estimated as part of the model fitting, see Muggeo and others (2021) <doi:10.1177/1471082X20929802>. Monotonicity and concavity \n    constraints on the fitted curves are allowed, see Muggeo and others (2013) <doi:10.1007/s10651-012-0232-1> and also <doi:10.13140/RG.2.2.12924.85122> for some code examples."}, "FHtest": {"categories": ["Survival"], "description": "Functions to compare two or more survival curves with:\n             a) The Fleming-Harrington test for right-censored data based on permutations and on counting processes.\n             b) An extension of the Fleming-Harrington test for interval-censored data based on a permutation distribution and on a score vector distribution."}, "robeth": {"categories": ["Robust"], "description": "Locations problems, M-estimates of coefficients and scale\n        in linear regression, Weights for bounded influence regression,\n        Covariance matrix of the coefficient estimates, Asymptotic\n        relative efficiency of regression M-estimates, Robust testing\n        in linear models, High breakdown point regression, M-estimates\n        of covariance matrices, M-estimates for discrete generalized\n        linear models."}, "asbio": {"categories": ["Survival"], "description": "Contains functions from: Aho, K. (2014) Foundational and Applied Statistics for Biologists using R.  CRC/Taylor and Francis, Boca Raton, FL, ISBN: 978-1-4398-7338-0."}, "BayesDA": {"categories": ["Bayesian", "TeachingStatistics"], "description": "Functions for Bayesian Data Analysis, with datasets from\n        the book \"Bayesian data Analysis (second edition)\" by Gelman,\n        Carlin, Stern and Rubin. Not all datasets yet, hopefully\n        completed soon."}, "ggsn": {"categories": ["Spatial"], "description": "Adds north symbols (18 options) and scale bars in kilometers,\n    meters, nautical miles, or statue miles, to maps in geographic\n    or metric coordinates created with 'ggplot2' or 'ggmap'."}, "REBayes": {"categories": ["Bayesian"], "description": "Kiefer-Wolfowitz maximum likelihood estimation for mixture models\n    and some other density estimation and regression methods based on convex\n    optimization.  See Koenker and Gu (2017) REBayes: An R Package for Empirical\n    Bayes Mixture Methods, Journal of Statistical Software, 82, 1\u201326, \n    <doi:10.18637/jss.v082.i08>."}, "coxsei": {"categories": ["Survival"], "description": "Fit a CoxSEI (Cox type Self-Exciting Intensity) model to right-censored counting process data."}, "SkewHyperbolic": {"categories": ["Distributions"], "description": "Functions are provided for the density function,\n\t     distribution function, quantiles and random number\n\t     generation for the skew hyperbolic\n\t     t-distribution. There are also functions that fit\n\t     the distribution to data. There are functions for the\n\t     mean, variance, skewness, kurtosis and mode of a given\n\t     distribution and to calculate moments of any order about\n\t     any centre. To assess goodness of fit, there are\n\t     functions to generate a Q-Q plot, a P-P plot and a tail plot."}, "swgee": {"categories": ["MissingData"], "description": "Simulation extrapolation and inverse probability weighted generalized estimating equations method for longitudinal data with missing observations and measurement error in covariates. References: Yi, G. Y. (2008) <doi:10.1093/biostatistics/kxm054>; Cook, J. R. and Stefanski, L. A. (1994) <doi:10.1080/01621459.1994.10476871>; Little, R. J. A. and Rubin, D. B. (2002, ISBN:978-0-471-18386-0)."}, "SparseFactorAnalysis": {"categories": ["Psychometrics"], "description": "Multidimensional scaling provides a means of uncovering a latent structure underlying observed data, while estimating the number of latent dimensions.  This package presents a means for scaling binary and count data, for example the votes and word counts for legislators.  Future work will include an EM implementation and extend this work to ordinal and continuous data."}, "bgumbel": {"categories": ["Distributions"], "description": "Bimodal Gumbel distribution. General functions for performing extreme value analysis."}, "gumbel": {"categories": ["Distributions"], "description": "Provides probability functions (cumulative distribution and density functions), simulation function (Gumbel copula multivariate simulation) and estimation functions (Maximum Likelihood Estimation, Inference For Margins, Moment Based Estimation and Canonical Maximum Likelihood)."}, "copBasic": {"categories": ["Distributions"], "description": "Extensive functions for bivariate copula (bicopula) computations and related operations \n for bicopula theory. The lower, upper, product, and select other bicopula are implemented along \n with operations including the diagonal, survival copula, dual of a copula, co-copula, and\n numerical bicopula density. Level sets, horizontal and vertical sections are supported. Numerical \n derivatives and inverses of a bicopula are provided through which simulation is implemented. \n Bicopula composition, convex combination, and products also are provided. Support \n extends to the Kendall Function as well as the Lmoments thereof. Kendall Tau,\n Spearman Rho and Footrule, Gini Gamma, Blomqvist Beta, Hoeffding Phi, Schweizer-\n Wolff Sigma, tail dependency, tail order, skewness, and bivariate Lmoments are implemented, and \n positive/negative quadrant dependency, left (right) increasing (decreasing) are available. \n Other features include Kullback-Leibler divergence, Vuong procedure, spectral measure, and \n Lcomoments for inference, maximum likelihood, and AIC, BIC, and RMSE for goodness-of-fit. "}, "GUIDE": {"categories": ["Finance"], "description": "A nice GUI for financial DErivatives in R."}, "cosmoFns": {"categories": ["ChemPhys"], "description": "Package encapsulates standard expressions for distances, times, luminosities, and other quantities useful in observational cosmology, including molecular line observations.  Currently coded for a flat universe only."}, "BLModel": {"categories": ["Finance"], "description": "Posterior distribution in the Black-Litterman model is computed from a prior distribution given in the form of a time series of asset returns and a continuous distribution of views provided by the user as an external function."}, "cocron": {"categories": ["Psychometrics"], "description": "Statistical tests for the comparison between two or more alpha\n    coefficients based on either dependent or independent groups of individuals.\n    A web interface is available at http://comparingcronbachalphas.org. A plugin\n    for the R GUI and IDE RKWard is included. Please install RKWard from https://\n    rkward.kde.org to use this feature. The respective R package 'rkward' cannot be\n    installed directly from a repository, as it is a part of RKWard."}, "mco": {"categories": ["Optimization"], "description": "A collection of function to solve multiple criteria optimization problems \n  using genetic algorithms (NSGA-II). Also included is a collection of test functions."}, "intamap": {"categories": ["Spatial"], "description": "Provides classes and methods for automated\n    spatial interpolation."}, "PHeval": {"categories": ["Survival"], "description": "Provides tools for the evaluation of the goodness of fit and the predictive capacity of the proportional hazards model."}, "univOutl": {"categories": ["OfficialStatistics"], "description": "Well known outlier detection techniques in the univariate case. Methods to deal with skewed distribution are included too. The Hidiroglou-Berthelot (1986) method to search for outliers in ratios of historical data is implemented as well. When available, survey weights can be used in outliers detection."}, "strucchangeRcpp": {"categories": ["TimeSeries"], "description": "A fast implementation with additional experimental features for\n             testing, monitoring and dating structural changes in (linear)\n             regression models. 'strucchangeRcpp' features tests/methods from\n\t     the generalized fluctuation test framework as well as from\n\t     the F test (Chow test) framework. This includes methods to\n             fit, plot and test fluctuation processes (e.g. cumulative/moving\n             sum, recursive/moving estimates) and F statistics, respectively.\n             These methods are described in Zeileis et al. (2002)\n             <doi:10.18637/jss.v007.i02>.\n             Finally, the breakpoints in regression models with structural\n             changes can be estimated together with confidence intervals,\n             and their magnitude as well as the model fit can be evaluated\n             using a variety of statistical measures."}, "EntropyMCMC": {"categories": ["Bayesian"], "description": "Tools for Markov Chain Monte Carlo (MCMC) simulation and performance analysis. Simulate MCMC algorithms including adaptive MCMC, evaluate their convergence rate, and compare candidate MCMC algorithms for a same target density, based on entropy and Kullback-Leibler divergence criteria. MCMC algorithms can be simulated using provided functions, or imported from external codes. This package is based upon work starting with Chauveau, D. and Vandekerkhove, P. (2013) <doi:10.1051/ps/2012004> and next articles."}, "oro.dicom": {"categories": ["MedicalImaging"], "description": "Data input/output functions for data that conform to the\n    Digital Imaging and Communications in Medicine (DICOM) standard, part\n    of the Rigorous Analytics bundle."}, "mlmi": {"categories": ["MissingData"], "description": "Implements so called Maximum Likelihood Multiple Imputation as described by von Hippel and Bartlett (2021) <doi:10.1214/20-STS793>. A number of different imputations are available, by utilising the 'norm', 'cat' and 'mix' packages. Inferences can be performed either using combination rules similar to Rubin's or using a likelihood score based approach based on theory by Wang and Robins (1998) <doi:10.1093/biomet/85.4.935>."}, "BayesGWQS": {"categories": ["Bayesian"], "description": "Fits Bayesian grouped weighted quantile sum (BGWQS) regressions for one or more chemical groups with binary outcomes. Wheeler DC et al. (2019) <doi:10.1016/j.sste.2019.100286>."}, "RWeka": {"categories": ["MachineLearning", "NaturalLanguageProcessing"], "description": "An R interface to Weka (Version 3.9.3).\n   Weka is a collection of machine learning algorithms for data mining\n   tasks written in Java, containing tools for data pre-processing,\n   classification, regression, clustering, association rules, and\n   visualization.  Package 'RWeka' contains the interface code, the\n   Weka jar is in a separate package 'RWekajars'.  For more information\n   on Weka see <http://www.cs.waikato.ac.nz/ml/weka/>."}, "RcmdrPlugin.DoE": {"categories": ["ExperimentalDesign"], "description": "Provides a platform-independent GUI for design of experiments.\n The package is implemented as a plugin to the R-Commander, which is a more general \n graphical user interface for statistics in R based on tcl/tk. \n DoE functionality can be accessed through the menu Design that is added to the \n R-Commander menus."}, "norm": {"categories": ["MissingData"], "description": "An integrated set of functions for the analysis of \n  multivariate normal datasets with missing values, including implementation of\n  the EM algorithm, data augmentation, and multiple imputation."}, "tidycensus": {"categories": ["OfficialStatistics", "Spatial"], "description": "An integrated R interface to several United States Census Bureau \n    APIs (<https://www.census.gov/data/developers/data-sets.html>) and the US Census Bureau's \n    geographic boundary files. Allows R users to return Census and ACS data as \n    tidyverse-ready data frames, and optionally returns a list-column with feature geometry for mapping \n    and spatial analysis. "}, "BurStFin": {"categories": ["Finance"], "description": "A suite of functions for finance, including the estimation\n\tof variance matrices via a statistical factor model or\n\tLedoit-Wolf shrinkage."}, "plotMCMC": {"categories": ["Bayesian"], "description": "Markov chain Monte Carlo diagnostic plots. The purpose of the\n  package is to combine existing tools from the 'coda' and 'lattice' packages,\n  and make it easy to adjust graphical details."}, "ecp": {"categories": ["TimeSeries"], "description": "Implements various procedures for finding \n\t     multiple change-points from Matteson D. et al (2013)\n\t     <doi:10.1080/01621459.2013.849605>, Zhang W. et al (2017) \n\t     <doi:10.1109/ICDMW.2017.44>, Arlot S. et al (2019).\n\t     Two methods make use of dynamic  \n\t     programming and pruning, with no distributional \n\t     assumptions other than the existence of certain absolute \n\t     moments in one method. Hierarchical and exact search methods \n\t     are included. All methods return the set of estimated change-\n\t     points as well as other summary information."}, "MoEClust": {"categories": ["Cluster"], "description": "Clustering via parsimonious Gaussian Mixtures of Experts using the MoEClust models introduced by Murphy and Murphy (2020) <doi:10.1007/s11634-019-00373-8>. This package fits finite Gaussian mixture models with a formula interface for supplying gating and/or expert network covariates using a range of parsimonious covariance parameterisations from the GPCM family via the EM/CEM algorithm. Visualisation of the results of such models using generalised pairs plots and the inclusion of an additional noise component is also facilitated. A greedy forward stepwise search algorithm is provided for identifying the optimal model in terms of the number of components, the GPCM covariance parameterisation, and the subsets of gating/expert network covariates."}, "glmpath": {"categories": ["MachineLearning", "Survival"], "description": "A path-following algorithm for L1 regularized generalized linear models and Cox proportional hazards model."}, "SwimmeR": {"categories": ["SportsAnalytics"], "description": "The goal of the 'SwimmeR' package is to provide means of acquiring, and then analyzing, data from swimming (and diving) competitions.  To that end 'SwimmeR' allows results to be read in from .html sources, like 'Hy-Tek' real time results pages, '.pdf' files, 'ISL' results, 'Omega' results, and (on a development basis) '.hy3' files.  Once read in, 'SwimmeR' can convert swimming times (performances) between the computationally useful format of seconds reported to the '100ths' place (e.g. 95.37), and the conventional reporting format (1:35.37) used in the swimming community.  'SwimmeR' can also score meets in a variety of formats with user defined point values, convert times between courses ('LCM', 'SCM', 'SCY') and draw single elimination brackets, as well as providing a suite of tools for working cleaning swimming data.  This is a developmental package, not yet mature."}, "inegiR": {"categories": ["OfficialStatistics"], "description": "Provides functions to download and parse information from INEGI\n    (Official Mexican statistics agency). To learn more about the API, see <https://www.inegi.org.mx/servicios/api_indicadores.html>."}, "deldir": {"categories": ["Spatial"], "description": "Calculates the Delaunay triangulation and the Dirichlet\n\tor Voronoi tessellation (with respect to the entire plane) of\n\ta planar point set. Plots triangulations and tessellations in\n\tvarious ways.  Clips tessellations to sub-windows. Calculates\n\tperimeters of tessellations.  Summarises information about\n\tthe tiles of the tessellation.\tCalculates the centroidal\n\tVoronoi (Dirichlet) tessellation using Lloyd's algorithm."}, "ggdemetra": {"categories": ["TimeSeries"], "description": "Provides 'ggplot2' functions to return the results of seasonal and trading day adjustment \n    made by 'RJDemetra'. 'RJDemetra' is an 'R' interface around 'JDemetra+' (<https://github.com/jdemetra/jdemetra-app>),\n    the seasonal adjustment software officially recommended to the members of the European Statistical System and\n    the European System of Central Banks."}, "pdfCluster": {"categories": ["Cluster"], "description": "Cluster analysis via nonparametric density \n   estimation is performed. Operationally, the kernel method is used throughout to estimate\n   the density. Diagnostics methods for evaluating the quality of the clustering \n   are available. The package includes also a routine to estimate the \n   probability density function obtained by the kernel method, given a set of\n   data with arbitrary dimensions."}, "islasso": {"categories": ["MachineLearning"], "description": "An implementation of the induced smoothing (IS) idea to lasso regularization models to allow estimation and inference on the model coefficients (currently hypothesis testing only). Linear, logistic, Poisson and gamma regressions with several link functions are implemented. The algorithm is described in the original paper: Cilluffo, G., Sottile, G., La Grutta, S. and Muggeo, V. (2019) The Induced Smoothed lasso: A practical framework for hypothesis testing in high dimensional regression. <doi:10.1177/0962280219842890>, and discussed in a tutorial: Sottile, G., Cilluffo, G., and Muggeo, V. (2019) The R package islasso: estimation and hypothesis testing in lasso regression. <doi:10.13140/RG.2.2.16360.11521>."}, "sld": {"categories": ["Distributions"], "description": "The skew logistic distribution is a quantile-defined generalisation\n of the logistic distribution (van Staden and King 2015).  Provides random \n numbers, quantiles, probabilities, densities and density quantiles for the distribution.\n It provides Quantile-Quantile plots and method of L-Moments estimation \n (including asymptotic standard errors) for the distribution."}, "aRxiv": {"categories": ["WebTechnologies"], "description": "An interface to the API for 'arXiv',\n    a repository of electronic preprints for\n    computer science, mathematics, physics, quantitative biology,\n    quantitative finance, and statistics."}, "rerddap": {"categories": ["WebTechnologies"], "description": "General purpose R client for 'ERDDAP' servers. Includes\n    functions to search for 'datasets', get summary information on\n    'datasets', and fetch 'datasets', in either 'csv' or 'netCDF' format.\n    'ERDDAP' information: \n    <https://upwell.pfeg.noaa.gov/erddap/information.html>."}, "micemd": {"categories": ["MissingData"], "description": "Addons for the 'mice' package to perform multiple imputation using chained equations with two-level data. Includes imputation methods dedicated to sporadically and systematically missing values. Imputation of continuous, binary or count variables are available. Following the recommendations of Audigier, V. et al (2018) <doi:10.1214/18-STS646>, the choice of the imputation method for each variable can be facilitated by a default choice tuned according to the structure of the incomplete dataset. Allows parallel calculation and overimputation for 'mice'."}, "blaise": {"categories": ["OfficialStatistics"], "description": "Can be used to read and write a fwf with an accompanying 'Blaise' datamodel.\n    Blaise is the software suite built by Statistics Netherlands (CBS). It is essentially a \n    way to write and collect surveys and perform statistical analysis on the data. It stores its data in \n    fixed width format with an accompanying metadata file, this is the Blaise format. The package automatically \n    interprets this metadata and reads the file into an R dataframe.\n    When supplying a datamodel for writing, the dataframe will be automatically converted \n    to that format and checked for compatibility.\n    Supports dataframes, tibbles and LaF objects.\n    For more information about 'Blaise', see <https://blaise.com/products/general-information>. "}, "uGMAR": {"categories": ["TimeSeries"], "description": "Maximum likelihood estimation of univariate Gaussian Mixture Autoregressive (GMAR),\n    Student's t Mixture Autoregressive (StMAR), and Gaussian and Student's t Mixture Autoregressive (G-StMAR) models, \n    quantile residual tests, graphical diagnostics, forecast and simulate from GMAR, StMAR and G-StMAR processes. \n    Leena Kalliovirta, Mika Meitz, Pentti Saikkonen (2015) <doi:10.1111/jtsa.12108>, \n    Mika Meitz, Daniel Preve, Pentti Saikkonen (2021) <doi:10.1080/03610926.2021.1916531>,\n    Savi Virolainen (2021) <doi:10.1515/snde-2020-0060>."}, "depmixS4": {"categories": ["Cluster", "TimeSeries"], "description": "Fits latent (hidden) Markov models on mixed categorical and continuous (time series) data, otherwise known as dependent mixture models, see Visser & Speekenbrink (2010, <doi:10.18637/jss.v036.i07>)."}, "naniar": {"categories": ["MissingData"], "description": "Missing values are ubiquitous in data and need to be explored and\n    handled in the initial stages of analysis. 'naniar' provides data structures \n    and functions that facilitate the plotting of missing values and examination \n    of imputations. This allows missing data dependencies to be explored with \n    minimal deviation from the common work patterns of 'ggplot2' and tidy data. \n    The work is fully discussed at Tierney & Cook (2018) <arXiv:1809.02264>."}, "LAWBL": {"categories": ["Bayesian", "Psychometrics"], "description": "A variety of models to analyze latent variables based on Bayesian learning: the partially CFA (Chen, Guo, Zhang, & Pan, 2020) <doi:10.1037/met0000293>; generalized PCFA; partially confirmatory IRM (Chen, 2020) <doi:10.1007/s11336-020-09724-3>; Bayesian regularized EFA <doi:10.1080/10705511.2020.1854763>; Fully and partially EFA."}, "ClusterR": {"categories": ["Cluster"], "description": "Gaussian mixture models, k-means, mini-batch-kmeans, k-medoids and affinity propagation clustering with the option to plot, validate, predict (new data) and estimate the optimal number of clusters. The package takes advantage of 'RcppArmadillo' to speed up the computationally intensive parts of the functions. For more information, see (i) \"Clustering in an Object-Oriented Environment\" by Anja Struyf, Mia Hubert, Peter Rousseeuw (1997), Journal of Statistical Software, <doi:10.18637/jss.v001.i04>; (ii) \"Web-scale k-means clustering\" by D. Sculley (2010), ACM Digital Library, <doi:10.1145/1772690.1772862>; (iii) \"Armadillo: a template-based C++ library for linear algebra\" by Sanderson et al (2016), The Journal of Open Source Software, <doi:10.21105/joss.00026>; (iv) \"Clustering by Passing Messages Between Data Points\" by Brendan J. Frey and Delbert Dueck, Science 16 Feb 2007: Vol. 315, Issue 5814, pp. 972-976, <doi:10.1126/science.1136800>."}, "BLOQ": {"categories": ["MissingData"], "description": "It includes estimating the area under the concentrations\n        versus time curve (AUC) and its standard error for data with\n        Below the Limit of Quantification (BLOQ) observations. Two \n        approaches are implemented: direct estimation using censored maximum \n        likelihood, also by first imputing the BLOQ's \n        using various methods, then compute AUC and its standard \n        error using imputed data. Technical details can found in \n        Barnett, Helen Yvette, Helena Geys, Tom Jacobs, and Thomas Jaki. \n\t\t\"Methods for Non-Compartmental Pharmacokinetic Analysis With Observations \n\t\tBelow the Limit of Quantification.\" Statistics in Biopharmaceutical \n\t\tResearch (2020): 1-12.\n        (available online: \n        <https://www.tandfonline.com/doi/full/10.1080/19466315.2019.1701546>). "}, "lavaan.survey": {"categories": ["OfficialStatistics", "Psychometrics"], "description": "Fit structural equation models (SEM) including factor analysis,\n    multivariate regression models with latent variables and many other latent\n    variable models while correcting estimates, standard errors, and\n    chi-square-derived fit measures for a complex sampling design. \n    Incorporate clustering, stratification, sampling weights, and \n    finite population corrections into a SEM analysis.\n    Wrapper around packages lavaan and survey."}, "RSNNS": {"categories": ["MachineLearning"], "description": "The Stuttgart Neural Network Simulator (SNNS) is a library\n    containing many standard implementations of neural networks. This\n    package wraps the SNNS functionality to make it available from\n    within R. Using the 'RSNNS' low-level interface, all of the\n    algorithmic functionality and flexibility of SNNS can be accessed.\n    Furthermore, the package contains a convenient high-level\n    interface, so that the most common neural network topologies and\n    learning algorithms integrate seamlessly into R."}, "coxphf": {"categories": ["Survival"], "description": "Implements Firth's penalized maximum likelihood bias reduction method  for Cox regression\n  which has been shown to provide a solution in case of monotone likelihood (nonconvergence of likelihood function), see \n  Heinze and Schemper (2001) and Heinze and Dunkler (2008).\n  The program fits profile penalized likelihood confidence intervals which were proved to outperform\n  Wald confidence intervals."}, "hyper2": {"categories": ["Distributions"], "description": "A suite of routines for the hyperdirichlet distribution; supersedes the 'hyperdirichlet' package."}, "Rmagic": {"categories": ["MissingData"], "description": "MAGIC (Markov affinity-based graph imputation of cells) is a method for addressing technical noise in single-cell data, including under-sampling of mRNA molecules, often termed \"dropout\" which can severely obscure important gene-gene relationships. MAGIC shares information across similar cells, via data diffusion, to denoise the cell count matrix and fill in missing transcripts. Read more: van Dijk et al. (2018) <doi:10.1016/j.cell.2018.05.061>."}, "bayesGARCH": {"categories": ["Bayesian", "Finance"], "description": "Provides the bayesGARCH() function which performs the\n    Bayesian estimation of the GARCH(1,1) model with Student's t innovations as described in Ardia (2008) <doi:10.1007/978-3-540-78657-3>."}, "rmutil": {"categories": ["Distributions"], "description": "A toolkit of functions for nonlinear regression and repeated\n    measurements not to be used by itself but called by other Lindsey packages such\n    as 'gnlm', 'stable', 'growth', 'repeated', and 'event' \n    (available at <https://www.commanster.eu/rcode.html>)."}, "mokken": {"categories": ["Psychometrics"], "description": "Contains functions for performing Mokken\n   scale analysis on test and questionnaire data.\n   It includes an automated item selection algorithm, and various checks of model assumptions."}, "TripleR": {"categories": ["Psychometrics"], "description": "Social Relation Model (SRM) analyses for single or multiple\n    round-robin groups are performed. These analyses are either based on one\n    manifest variable, one latent construct measured by two manifest variables,\n    two manifest variables and their bivariate relations, or two latent\n    constructs each measured by two manifest variables. Within-group t-tests\n    for variance components and covariances are provided for single groups.\n    For multiple groups two types of significance tests are provided:\n    between-groups t-tests (as in SOREMO) and enhanced standard errors based on\n    Lashley and Bond (1997) <doi:10.1037/1082-989X.2.3.278>. Handling for missing values is provided."}, "BIFIEsurvey": {"categories": ["MissingData", "OfficialStatistics"], "description": "\n    Contains tools for survey statistics (especially in educational\n    assessment) for datasets with replication designs (jackknife, \n    bootstrap, replicate weights; see Kolenikov, 2010;\n    Pfefferman & Rao, 2009a, 2009b, <doi:10.1016/S0169-7161(09)70003-3>,\n    <doi:10.1016/S0169-7161(09)70037-9>); Shao, 1996, \n    <doi:10.1080/02331889708802523>). \n    Descriptive statistics, linear and logistic regression, \n    path models for manifest variables with measurement error \n    correction and two-level hierarchical regressions for weighted \n    samples are included. Statistical inference can be conducted for \n    multiply imputed datasets and nested multiply imputed datasets\n    and is in particularly suited for the analysis of plausible values\n    (for details see George, Oberwimmer & Itzlinger-Bruneforth, 2016; \n    Bruneforth, Oberwimmer & Robitzsch, 2016; Robitzsch, Pham &\n    Yanagida, 2016). The package development was supported by BIFIE \n    (Federal Institute for Educational Research, Innovation and Development \n    of the Austrian School System; Salzburg, Austria)."}, "bimets": {"categories": ["Econometrics"], "description": "Time series analysis, (dis)aggregation and manipulation, e.g. time series extension, merge, projection, lag, lead, delta, moving and cumulative average and product, selection by index, date and year-period, conversion to daily, monthly, quarterly, (semi)annually. Simultaneous equation models definition, estimation, simulation and forecasting with coefficient restrictions, error autocorrelation, exogenization, add-factors, impact and interim multipliers analysis, conditional equation evaluation, endogenous targeting and model renormalization, structural stability, stochastic simulation and forecast, optimal control."}, "SACOBRA": {"categories": ["Optimization"], "description": "Performs surrogate-assisted optimization for expensive black-box constrained problems."}, "smam": {"categories": ["SpatioTemporal", "Tracking"], "description": "Animal movement models including moving-resting process\n    with embedded Brownian motion according to\n    Yan et al. (2014) <doi:10.1007/s10144-013-0428-8>,\n    Pozdnyakov et al. (2017) <doi:10.1007/s11009-017-9547-6>,\n    Brownian motion with measurement error according to\n    Pozdnyakov et al. (2014) <doi:10.1890/13-0532.1>,\n    and moving-resting-handling process with embedded Brownian motion,\n    Pozdnyakov et al. (2018) <arXiv:1806.00849>."}, "cabootcrs": {"categories": ["Psychometrics"], "description": "Performs simple correspondence analysis on a two-way contingency table, \n    or multiple correspondence analysis (homogeneity analysis) \n    on data with p categorical variables,\n    and produces bootstrap-based elliptical confidence regions around the \n    projected coordinates for the category points. \n    Includes routines to plot the results in a variety of styles. \n    Also reports the standard numerical output for correspondence analysis."}, "CausalGAM": {"categories": ["CausalInference"], "description": "Implements various estimators for average\n  treatment effects - an inverse probability weighted (IPW) estimator, \n  an augmented inverse probability weighted (AIPW) estimator, and a standard\n  regression estimator - that make use of generalized additive models for\n  the treatment assignment model and/or outcome model. See: Glynn, Adam N.\n  and Kevin M. Quinn. 2010. \"An Introduction to the Augmented Inverse\n  Propensity Weighted Estimator.\" Political Analysis. 18: 36-56."}, "MedSurvey": {"categories": ["CausalInference"], "description": "It is a computer tool to conduct linear mediation analysis for complex surveys using multi-stage sampling and Balanced Repeated Replication (BRR). Specifically, the mediation analysis method using balanced repeated replications was proposed by Mai, Ha, and Soulakova (2019) <doi:10.1080/10705511.2018.1559065>. The current version can only handle continuous mediators and outcomes. The development of 'MedSurvey' was sponsored by American Lebanese Syrian Associated Charities (ALSAC). However, the contents of MedSurvey do not necessarily represent the policy of the ALSAC."}, "ncf": {"categories": ["Spatial"], "description": "Spatial (cross-)covariance and related geostatistical tools: the\n        nonparametric (cross-)covariance function , the spline correlogram, the\n        nonparametric phase coherence function, local indicators of spatial \n        association (LISA), (Mantel) correlogram, (Partial) Mantel test."}, "RStripe": {"categories": ["WebTechnologies"], "description": "A convenience interface for communicating with the Stripe payment processor to accept payments online. See <https://stripe.com> for more information."}, "esc": {"categories": ["MetaAnalysis"], "description": "Implementation of the web-based 'Practical Meta-Analysis Effect Size\n    Calculator' from David B. Wilson (<http://www.campbellcollaboration.org/escalc/html/EffectSizeCalculator-Home.php>)\n    in R. Based on the input, the effect size can be returned as standardized mean \n    difference, Cohen's f, Hedges' g, Pearson's r or Fisher's\n    transformation z, odds ratio or log odds, or eta squared effect size."}, "RPMM": {"categories": ["Cluster", "MachineLearning"], "description": "\n    Recursively Partitioned Mixture Model for Beta and Gaussian Mixtures.  \n    This is a model-based clustering algorithm that returns a hierarchy\n    of classes, similar to hierarchical clustering, but also similar to\n    finite mixture models."}, "spef": {"categories": ["Survival"], "description": "Functions for fitting semiparametric regression models for\n        panel count survival data. An overview of the package can be found \n        in Wang and Yan (2011) <doi:10.1016/j.cmpb.2010.10.005> and\n\tChiou et al. (2018) <doi:10.1111/insr.12271>."}, "robustarima": {"categories": ["TimeSeries"], "description": "Functions for fitting a linear regression model with ARIMA\n  errors using a filtered tau-estimate."}, "checkpoint": {"categories": ["ReproducibleResearch"], "description": "The goal of checkpoint is to solve the problem of package\n    reproducibility in R. Specifically, checkpoint allows you to install packages\n    as they existed on CRAN on a specific snapshot date as if you had a CRAN time\n    machine. To achieve reproducibility, the checkpoint() function installs the\n    packages required or called by your project and scripts to a local library\n    exactly as they existed at the specified point in time. Only those packages\n    are available to your project, thereby avoiding any package updates that came\n    later and may have altered your results. In this way, anyone using checkpoint's\n    checkpoint() can ensure the reproducibility of your scripts or projects at any\n    time. To create the snapshot archives, once a day (at midnight UTC) Microsoft\n    refreshes the Austria CRAN mirror on the \"Microsoft R Archived Network\"\n    server (<https://mran.microsoft.com/>). Immediately after completion\n    of the rsync mirror process, the process takes a snapshot, thus creating the\n    archive. Snapshot archives exist starting from 2014-09-17."}, "GPFDA": {"categories": ["FunctionalData"], "description": "Functionalities for modelling functional data with \n    multidimensional inputs, multivariate functional data, and non-separable \n    and/or non-stationary covariance structure of function-valued processes. \n    In addition, there are functionalities for functional regression models where \n    the mean function depends on scalar and/or functional covariates and \n    the covariance structure depends on functional covariates. \n    The development version of the package can be found on \n    <https://github.com/gpfda/GPFDA-dev>."}, "pamr": {"categories": ["MachineLearning", "Survival"], "description": "Some functions for sample classification in microarrays."}, "mirt": {"categories": ["MissingData", "Psychometrics"], "description": "Analysis of dichotomous and polytomous response data using\n    unidimensional and multidimensional latent trait models under the Item\n    Response Theory paradigm (Chalmers (2012) <doi:10.18637/jss.v048.i06>). \n    Exploratory and confirmatory models can be estimated with quadrature (EM) \n    or stochastic (MHRM) methods. Confirmatory\n    bi-factor and two-tier analyses are available for modeling item testlets.\n    Multiple group analysis and mixed effects designs also are available for\n    detecting differential item and test functioning as well as modeling\n    item and person covariates. Finally, latent class models such as the DINA,\n    DINO, multidimensional latent class, and several other discrete latent\n    variable models, including mixture and zero-inflated response models, \n    are supported."}, "units": {"categories": ["ChemPhys"], "description": "Support for measurement units in R vectors, matrices\n    and arrays: automatic propagation, conversion, derivation\n    and simplification of units; raising errors in case of unit\n    incompatibility. Compatible with the POSIXct, Date and difftime \n    classes. Uses the UNIDATA udunits library and unit database for \n    unit compatibility checking and conversion.\n    Documentation about 'units' is provided in the paper by Pebesma, Mailund &\n    Hiebert (2016, <doi:10.32614/RJ-2016-061>), included in this package as a\n    vignette; see 'citation(\"units\")' for details."}, "erer": {"categories": ["Econometrics"], "description": "Functions, datasets, and sample codes related to the book of 'Empirical Research in Economics: Growing up with R' by Dr. Changyou Sun are included. Marginal effects for binary or ordered choice models can be calculated. Static and dynamic Almost Ideal Demand System (AIDS) models can be estimated. A typical event analysis in finance can be conducted with several functions included."}, "beakr": {"categories": ["WebTechnologies"], "description": "A minimalist web framework for developing application programming \n    interfaces in R that provides a flexible framework for handling common \n    HTTP-requests, errors, logging, and an ability to integrate any R code as \n    server middle-ware."}, "ape": {"categories": ["Environmetrics"], "description": "Functions for reading, writing, plotting, and manipulating phylogenetic trees, analyses of comparative data in a phylogenetic framework, ancestral character analyses, analyses of diversification and macroevolution, computing distances from DNA sequences, reading and writing nucleotide sequences as well as importing from BioConductor, and several tools such as Mantel's test, generalized skyline plots, graphical exploration of phylogenetic data (alex, trex, kronoviz), estimation of absolute evolutionary rates and clock-like trees using mean path lengths and penalized likelihood, dating trees with non-contemporaneous sequences, translating DNA into AA sequences, and assessing sequence alignments. Phylogeny estimation can be done with the NJ, BIONJ, ME, MVR, SDM, and triangle methods, and several methods handling incomplete distance matrices (NJ*, BIONJ*, MVR*, and the corresponding triangle method). Some functions call external applications (PhyML, Clustal, T-Coffee, Muscle) whose results are returned into R."}, "RecordLinkage": {"categories": ["OfficialStatistics"], "description": "Provides functions for linking and deduplicating data sets.\n  Methods based on a stochastic approach are implemented as well as \n  classification algorithms from the machine learning domain. For details, \n  see our paper \"The RecordLinkage Package: Detecting Errors in Data\" \n  Sariyar M / Borg A (2010) <doi:10.32614/RJ-2010-017>. "}, "isotree": {"categories": ["MissingData"], "description": "Fast and multi-threaded implementation of\n\tisolation forest (Liu, Ting, Zhou (2008) <doi:10.1109/ICDM.2008.17>),\n\textended isolation forest (Hariri, Kind, Brunner (2018) <arXiv:1811.02141>),\n\tSCiForest (Liu, Ting, Zhou (2010) <doi:10.1007/978-3-642-15883-4_18>),\n\tfair-cut forest (Cortes (2021) <arXiv:2110:13402>),\n\trobust random-cut forest (Guha, Mishra, Roy, Schrijvers (2016) <http://proceedings.mlr.press/v48/guha16.html>),\n\tand customizable variations of them, for isolation-based outlier detection, clustered outlier detection,\n\tdistance or similarity approximation (Cortes (2019) <arXiv:1910.12362>),\n\tisolation kernel calculation (Ting, Zhu, Zhou (2018) <doi:10.1145/3219819.3219990>),\n\tand imputation of missing values (Cortes (2019) <arXiv:1911.06646>),\n\tbased on random or guided decision tree splitting, and providing different metrics for\n\tscoring anomalies based on isolation depth or density (Cortes (2021) <arXiv:2111.11639>).\n\tProvides simple heuristics for fitting the model to categorical columns and handling missing data,\n\tand offers options for varying between random and guided splits, and for using different splitting criteria."}, "W3CMarkupValidator": {"categories": ["WebTechnologies"], "description": "\n  R interface to a W3C Markup Validation service.\n  See <http://validator.w3.org/> for more information."}, "gamlss.cens": {"categories": ["Survival"], "description": "This is an add-on package to GAMLSS. The purpose of this\n        package is to allow users to fit interval response variables in\n        GAMLSS models. The main function gen.cens() generates a\n        censored version of an existing GAMLSS family distribution."}, "bsts": {"categories": ["Bayesian", "TimeSeries"], "description": "Time series regression using dynamic linear models fit using\n  MCMC. See Scott and Varian (2014) <doi:10.1504/IJMMNO.2014.059942>, among many\n  other sources."}, "clinPK": {"categories": ["Pharmacokinetics"], "description": "Calculates equations commonly used in clinical\n        pharmacokinetics and clinical pharmacology, such as equations\n        for dose individualization, compartmental pharmacokinetics,\n        drug exposure, anthropomorphic calculations, clinical\n        chemistry, and conversion of common clinical parameters. Where\n        possible and relevant, it provides multiple published and\n        peer-reviewed equations within the respective R function."}, "mkssd": {"categories": ["ExperimentalDesign"], "description": "Generates efficient balanced non-aliased multi-level k-circulant supersaturated designs by interchanging the elements of the generator vector. Attempts to generate a supersaturated design that has chisquare efficiency more than user specified efficiency level (mef). Displays the progress of generation of an efficient multi-level k-circulant design through a progress bar. The progress of 100% means that one full round of interchange is completed. More than one full round (typically 4-5 rounds) of interchange may be required for larger designs. "}, "magclass": {"categories": ["Spatial"], "description": "Data class for increased interoperability working with spatial-\n    temporal data together with corresponding functions and methods (conversions,\n    basic calculations and basic data manipulation). The class distinguishes\n    between spatial, temporal and other dimensions to facilitate the development\n    and interoperability of tools build for it. Additional features are name-based\n    addressing of data and internal consistency checks (e.g. checking for the right\n    data order in calculations)."}, "munfold": {"categories": ["Psychometrics"], "description": "Multidimensional unfolding using Schoenemann's algorithm for metric\n   and Procrustes rotation of unfolding results."}, "rsparkling": {"categories": ["ModelDeployment"], "description": "An extension package for 'sparklyr' that provides an R interface to\n    H2O Sparkling Water machine learning library (see <https://github.com/h2oai/sparkling-water> for more information)."}, "extRemes": {"categories": ["Environmetrics", "ExtremeValue"], "description": "General functions for performing extreme value analysis.  In particular, allows for inclusion of covariates into the parameters of the extreme-value distributions, as well as estimation through MLE, L-moments, generalized (penalized) MLE (GMLE), as well as Bayes.  Inference methods include parametric normal approximation, profile-likelihood, Bayes, and bootstrapping.  Some bivariate functionality and dependence checking (e.g., auto-tail dependence function plot, extremal index estimation) is also included.  For a tutorial, see Gilleland and Katz (2016) <doi:10.18637/jss.v072.i08> and for bootstrapping, please see Gilleland (2020) <doi:10.1175/JTECH-D-20-0070.1>."}, "splm": {"categories": ["Econometrics", "Spatial", "SpatioTemporal"], "description": "ML and GM estimation and diagnostic testing of econometric models for spatial panel data."}, "DChaos": {"categories": ["TimeSeries"], "description": "Chaos theory has been hailed as a revolution of thoughts and attracting ever increasing \n    attention of many scientists from diverse disciplines. Chaotic systems are nonlinear deterministic \n    dynamic systems which can behave like an erratic and apparently random motion. A relevant field\n    inside chaos theory and nonlinear time series analysis is the detection of a chaotic behaviour \n    from empirical time series data. One of the main features of chaos is the well known initial value \n    sensitivity property. Methods and techniques related to test the hypothesis of chaos try to quantify \n    the initial value sensitive property estimating the Lyapunov exponents. The DChaos package \n    provides different useful tools and efficient algorithms which test robustly the hypothesis of chaos \n    based on the Lyapunov exponent in order to know if the data generating process behind time series \n    behave chaotically or not."}, "medflex": {"categories": ["CausalInference"], "description": "Run flexible mediation analyses using natural effect models as described in \n  Lange, Vansteelandt and Bekaert (2012) <doi:10.1093/aje/kwr525>, \n  Vansteelandt, Bekaert and Lange (2012) <doi:10.1515/2161-962X.1014> \n  and Loeys, Moerkerke, De Smet, Buysse, Steen and Vansteelandt (2013) <doi:10.1080/00273171.2013.832132>."}, "micEconAids": {"categories": ["Econometrics"], "description": "Functions and tools\n   for analysing consumer demand\n   with the Almost Ideal Demand System (AIDS)\n   suggested by Deaton and Muellbauer (1980)."}, "mbsts": {"categories": ["TimeSeries"], "description": "Tools for data analysis with multivariate Bayesian structural time series (MBSTS) models.  Specifically, the package provides facilities for implementing general structural time series models, flexibly adding on different time series components (trend, season, cycle, and regression), simulating them, fitting them to multivariate correlated time series data, conducting feature selection on the regression component."}, "tidyRSS": {"categories": ["WebTechnologies"], "description": "\n    With the objective of including data from RSS feeds into your analysis, \n    'tidyRSS' parses RSS, Atom and JSON feeds and returns a tidy data frame."}, "dfmeta": {"categories": ["MetaAnalysis"], "description": "Meta-analysis approaches for Phase I dose finding early phases clinical trials in order to better suit requirements in terms of maximum tolerated dose (MTD) and maximal dose regimen (MDR). This package has currently three different approaches: (a) an approach proposed by Zohar et al, 2011, <doi:10.1002/sim.4121> (denoted as ZKO), (b) the Variance Weighted pooling analysis (called VarWT) and (c) the Random Effects Model Based (REMB) algorithm, where user can input his/her own model based approach or use the existing random effect logistic regression model (named as glimem) through the 'dfmeta' package."}, "restfulr": {"categories": ["WebTechnologies"], "description": "Models a RESTful service as if it were a nested R list."}, "gvc": {"categories": ["Econometrics"], "description": "Several tools for Global Value Chain ('GVC') analysis are\n    implemented."}, "missCompare": {"categories": ["MissingData"], "description": "Offers a convenient pipeline to test and compare various missing data\n  imputation algorithms on simulated and real data. These include simpler methods, such as mean and median\n  imputation and random replacement, but also include more sophisticated algorithms already implemented in popular \n  R packages, such as 'mi', described by Su et al. (2011) <doi:10.18637/jss.v045.i02>; 'mice', described by van Buuren \n  and Groothuis-Oudshoorn (2011) <doi:10.18637/jss.v045.i03>; 'missForest', described by Stekhoven and Buhlmann (2012) \n  <doi:10.1093/bioinformatics/btr597>; 'missMDA', described by Josse and Husson (2016) <doi:10.18637/jss.v070.i01>; and \n  'pcaMethods', described by Stacklies et al. (2007) <doi:10.1093/bioinformatics/btm069>. The central assumption behind \n  'missCompare' is that structurally different datasets (e.g. larger datasets with a large number of correlated variables \n  vs. smaller datasets with non correlated variables) will benefit differently from different missing data imputation \n  algorithms. 'missCompare' takes measurements of your dataset and sets up a sandbox to try a curated list of standard and \n  sophisticated missing data imputation algorithms and compares them assuming custom missingness patterns.\n  'missCompare' will also impute your real-life dataset for you after the selection of the best performing algorithm\n  in the simulations. The package also provides various post-imputation diagnostics and visualizations to help you \n  assess imputation performance.   "}, "rebmix": {"categories": ["Cluster"], "description": "Random univariate and multivariate finite mixture model generation, estimation, clustering, latent class analysis and classification. Variables can be continuous, discrete, independent or dependent and may follow normal, lognormal, Weibull, gamma, Gumbel, binomial, Poisson, Dirac, uniform or circular von Mises parametric families."}, "Bayesiangammareg": {"categories": ["Bayesian"], "description": "Adjust the Gamma regression models from a Bayesian perspective described by Cepeda and Urdinola (2012) <doi:10.1080/03610918.2011.600500>, modeling the parameters of mean and shape and using different link functions for the parameter associated to the mean. And calculates different adjustment statistics such as the Akaike information criterion and Bayesian information criterion."}, "acc": {"categories": ["Tracking"], "description": "Processes accelerometer data from uni-axial and tri-axial devices,\n    and generates data summaries. Also includes functions to plot, analyze, and\n    simulate accelerometer data."}, "dataRetrieval": {"categories": ["Hydrology"], "description": "Collection of functions to help retrieve U.S. Geological Survey\n    (USGS) and U.S. Environmental Protection Agency (EPA) water quality and\n    hydrology data from web services. USGS web services are discovered from \n    National Water Information System (NWIS) <https://waterservices.usgs.gov/> and <https://waterdata.usgs.gov/nwis>. \n    Both EPA and USGS water quality data are obtained from the Water Quality Portal <https://www.waterqualitydata.us/>."}, "flexrsurv": {"categories": ["Survival"], "description": "Package for parametric relative survival analyses. It allows to model non-linear and \n        non-proportional effects and both non proportional and non linear effects, using splines (B-spline and truncated power basis), Weighted Cumulative Index of Exposure effect, with correction model for \n        the life table. Both non proportional and non linear effects are described in \n\t\t\tRemontet, L. et al. (2007) <doi:10.1002/sim.2656> and \n\t\t\tMahboubi, A. et al. (2011) <doi:10.1002/sim.4208>. "}, "sjmisc": {"categories": ["MissingData"], "description": "Collection of miscellaneous utility functions, supporting data \n    transformation tasks like recoding, dichotomizing or grouping variables, \n    setting and replacing missing values. The data transformation functions \n    also support labelled data, and all integrate seamlessly into a \n    'tidyverse'-workflow."}, "BsMD": {"categories": ["ExperimentalDesign"], "description": "Bayes screening and model discrimination follow-up designs."}, "clock": {"categories": ["TimeSeries"], "description": "Provides a comprehensive library for date-time manipulations\n    using a new family of orthogonal date-time classes (durations, time\n    points, zoned-times, and calendars) that partition responsibilities so\n    that the complexities of time zones are only considered when they are\n    really needed. Capabilities include: date-time parsing, formatting,\n    arithmetic, extraction and updating of components, and rounding."}, "meta4diag": {"categories": ["MetaAnalysis"], "description": "Bayesian inference analysis for bivariate meta-analysis of diagnostic test studies using integrated nested Laplace approximation with INLA. A purpose built graphic user interface is available. The installation of R package INLA is compulsory for successful usage. The INLA package can be obtained from <https://www.r-inla.org>. We recommend the testing version, which can be downloaded by running: install.packages(\"INLA\", repos=c(getOption(\"repos\"), INLA=\"https://inla.r-inla-download.org/R/testing\"), dep=TRUE)."}, "FSMUMI": {"categories": ["MissingData"], "description": "Filling large gaps in low or uncorrelated multivariate time series uses a new fuzzy weighted similarity measure. It contains all required functions to create large missing consecutive values within time series and then fill these gaps, according to the paper Phan et al. (2018), <doi:10.1155/2018/9095683>. Performance indicators are also provided to compare similarity between two univariate signals (incomplete signal and imputed signal)."}, "nilde": {"categories": ["Optimization"], "description": "Routines for enumerating all existing nonnegative integer solutions of a linear Diophantine equation. The package provides routines for solving 0-1, bounded and unbounded knapsack problems; 0-1, bounded and unbounded subset sum problems; additive partitioning of natural numbers; and one-dimensional bin-packing problem."}, "retrosheet": {"categories": ["SportsAnalytics"], "description": "A collection of tools to import and structure the (currently) single-season\n    event, game-log, roster, and schedule data available from <https://www.retrosheet.org>.\n    In particular, the event (a.k.a. play-by-play) files can be especially difficult to parse.\n    This package does the parsing on those files, returning the requested data in the most\n    practical R structure to use for sabermetric or other analyses."}, "Rexperigen": {"categories": ["WebTechnologies"], "description": "Provides convenience functions to communicate with\n    an Experigen server: Experigen (<http://github.com/aquincum/experigen>)\n    is an online framework for creating  linguistic experiments,\n    and it stores the results on a dedicated server. This package can be\n    used to retrieve the results from the server, and it is especially\n    helpful with registered experiments, as authentication with the server\n    has to happen."}, "penMSM": {"categories": ["Survival"], "description": "Structured fusion Lasso penalized estimation of multi-state models with the penalty applied to absolute effects and absolute effect differences (i.e., effects on transition-type specific hazard rates)."}, "sFFLHD": {"categories": ["ExperimentalDesign"], "description": "Gives design points from a sequential full factorial-based\n Latin hypercube design, as described in Duan, Ankenman, Sanchez,\n and Sanchez (2015, Technometrics,\n <doi:10.1080/00401706.2015.1108233>)."}, "dynpred": {"categories": ["Survival"], "description": "The dynpred package contains functions for dynamic prediction in survival analysis."}, "seqDesign": {"categories": ["CausalInference", "ExperimentalDesign"], "description": "A modification of the preventive vaccine efficacy trial design of Gilbert, Grove et al. (2011, Statistical Communications in Infectious Diseases) is implemented, with application generally to individual-randomized clinical trials with multiple active treatment groups and a shared control group, and a study endpoint that is a time-to-event endpoint subject to right-censoring. The design accounts for the issues that the efficacy of the treatment/vaccine groups may take time to accrue while the multiple treatment administrations/vaccinations are given; there is interest in assessing the durability of treatment efficacy over time; and group sequential monitoring of each treatment group for potential harm, non-efficacy/efficacy futility, and high efficacy is warranted. The design divides the trial into two stages of time periods, where each treatment is first evaluated for efficacy in the first stage of follow-up, and, if and only if it shows significant treatment efficacy in stage one, it is evaluated for longer-term durability of efficacy in stage two. The package produces plots and tables describing operating characteristics of a specified design including an unconditional power for intention-to-treat and per-protocol/as-treated analyses; trial duration; probabilities of the different possible trial monitoring outcomes (e.g., stopping early for non-efficacy); unconditional power for comparing treatment efficacies; and distributions of numbers of endpoint events occurring after the treatments/vaccinations are given, useful as input parameters for the design of studies of the association of biomarkers with a clinical outcome (surrogate endpoint problem). The code can be used for a single active treatment versus control design and for a single-stage design."}, "colf": {"categories": ["Optimization"], "description": "Performs least squares constrained optimization on a linear objective function. It contains\n    a number of algorithms to choose from and offers a formula syntax similar to lm()."}, "tweedie": {"categories": ["Distributions"], "description": "Maximum likelihood computations for Tweedie families, including the series expansion (Dunn and Smyth, 2005; <doi:10.1007/s11222-005-4070-y>) and the Fourier inversion  (Dunn and Smyth, 2008; <doi:10.1007/s11222-007-9039-6>), and related methods."}, "rdpower": {"categories": ["Econometrics"], "description": "The regression discontinuity (RD) design is a popular quasi-experimental design for causal inference and policy evaluation. The 'rdpower' package provides tools to perform power, sample size and MDE calculations in RD designs: rdpower() calculates the power of an RD design, rdsampsi() calculates the required sample size to achieve a desired power and rdmde() calculates minimum detectable effects. See Cattaneo, Titiunik and Vazquez-Bare (2019) <https://rdpackages.github.io/references/Cattaneo-Titiunik-VazquezBare_2019_Stata.pdf> for further methodological details."}, "tokenizers": {"categories": ["NaturalLanguageProcessing"], "description": "Convert natural language text into tokens. Includes tokenizers for\n    shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs,\n    characters, shingled characters, lines, tweets, Penn Treebank, regular\n    expressions, as well as functions for counting characters, words, and sentences,\n    and a function for splitting longer texts into separate documents, each with\n    the same number of words.  The tokenizers have a consistent interface, and\n    the package is built on the 'stringi' and 'Rcpp' packages for  fast\n    yet correct tokenization in 'UTF-8'. "}, "topicdoc": {"categories": ["NaturalLanguageProcessing"], "description": "Calculates topic-specific diagnostics (e.g. mean token length, exclusivity) for \n    Latent Dirichlet Allocation and Correlated Topic Models fit using the 'topicmodels' package.\n    For more details, see Chapter 12 in Airoldi et al. (2014, ISBN:9781466504080), \n    pp 262-272 Mimno et al. (2011, ISBN:9781937284114), and Bischof et al. (2014) <arXiv:1206.4631v1>."}, "hyfo": {"categories": ["Hydrology"], "description": "Focuses on data processing and visualization in hydrology and\n    climate forecasting. Main function includes data extraction, data downscaling,\n    data resampling, gap filler of precipitation, bias correction of forecasting\n    data, flexible time series plot, and spatial map generation. It is a good pre-\n    processing and post-processing tool for hydrological and hydraulic modellers."}, "fMultivar": {"categories": ["Finance"], "description": "A collection of functions to manage, investigate and analyze\n        bivariate and multivariate data sets of financial returns."}, "kmc": {"categories": ["Survival"], "description": "Given constraints for right censored data, we use a recursive computational algorithm to calculate the the \"constrained\" Kaplan-Meier estimator. The constraint is assumed given in linear estimating equations or mean functions. We also illustrate how this leads to the empirical likelihood ratio test with right censored data and accelerated failure time model with given coefficients. EM algorithm from emplik package is used to get the initial value. The properties and performance of the EM algorithm is discussed in Mai Zhou and Yifan Yang (2015)<doi:10.1007/s00180-015-0567-9> and Mai Zhou and Yifan Yang (2017) <10.1002/wics.1400>. More applications could be found in Mai Zhou (2015) <doi:10.1201/b18598>."}, "gsDesign": {"categories": ["ExperimentalDesign"], "description": "Derives group sequential clinical trial designs and describes\n    their properties. Particular focus on time-to-event, binary, and\n    continuous outcomes. Largely based on methods described in\n    Jennison, Christopher and Turnbull, Bruce W., 2000,\n    \"Group Sequential Methods with Applications to Clinical Trials\"\n    ISBN: 0-8493-0316-8."}, "lasso2": {"categories": ["MachineLearning"], "description": "Routines and documentation for solving regression problems\n  while imposing an L1 constraint on the estimates, based on\n  the algorithm of Osborne et al. (1998)."}, "crossdes": {"categories": ["ExperimentalDesign"], "description": "Contains functions for the construction of carryover\n        balanced crossover designs. In addition contains functions to\n        check given designs for balance."}, "IPWboxplot": {"categories": ["MissingData"], "description": "Boxplots adapted to the happenstance of missing observations where drop-out probabilities can be given by the practitioner or  modelled  using auxiliary covariates. The paper of \"Zhang, Z., Chen, Z., Troendle, J. F. and Zhang, J.(2012) <doi:10.1111/j.1541-0420.2011.01712.x>\", proposes estimators of marginal quantiles based on the Inverse Probability Weighting method."}, "deductive": {"categories": ["OfficialStatistics"], "description": "Attempt to repair inconsistencies and missing values in\n        data records by using information from valid values and\n        validation rules restricting the data."}, "groundhog": {"categories": ["ReproducibleResearch"], "description": "Make R scripts reproducible, by ensuring that\n    every time a given script is run, the same version of the used packages are\n    loaded (instead of whichever version the user running the script happens to\n    have installed). This is achieved by using the command\n    groundhog.library() instead of the base command library(), and including a\n    date in the call. The date is used to call on the same version of the\n    package every time (the most recent version available at that date). \n    Load packages  from CRAN, GitHub, or Gitlab."}, "nbapalettes": {"categories": ["SportsAnalytics"], "description": "Palettes generated from NBA jersey colorways."}, "contactdata": {"categories": ["Epidemiology"], "description": "Data package for the supplementary data in Prem et al. (2017)\n    <doi:10.1371/journal.pcbi.1005697>.\n    Provides easy access to contact data for 152 countries, for use in\n    epidemiological, demographic or social sciences research."}, "toRvik": {"categories": ["SportsAnalytics"], "description": "A suite of functions to quickly scrape and tidy advanced metrics,\n    detailed player and game statistics, team and coach histories, and more from\n    Barttorvik<https://barttorvik.com/>."}, "WRSS": {"categories": ["Hydrology"], "description": "Water resources system simulator is a tool for simulation and analysis of large-scale water resources systems. 'WRSS' proposes functions and methods for construction, simulation and analysis of primary storage and hydropower water resources features (e.g. reservoirs, aquifers, and etc.) based on Standard Operating Policy (SOP). "}, "RoughSets": {"categories": ["MachineLearning"], "description": "Implementations of algorithms for data analysis based on the\n    rough set theory (RST) and the fuzzy rough set theory (FRST). We not only\n    provide implementations for the basic concepts of RST and FRST but also\n    popular algorithms that derive from those theories. The methods included in the\n    package can be divided into several categories based on their functionality:\n    discretization, feature selection, instance selection, rule induction and\n    classification based on nearest neighbors. RST was introduced by Zdzis\u0142aw\n    Pawlak in 1982 as a sophisticated mathematical tool to model and process\n    imprecise or incomplete information. By using the indiscernibility relation for\n    objects/instances, RST does not require additional parameters to analyze the\n    data. FRST is an extension of RST. The FRST combines concepts of vagueness and\n    indiscernibility that are expressed with fuzzy sets (as proposed by Zadeh, in\n    1965) and RST."}, "robust": {"categories": ["Robust"], "description": "Methods for robust statistics, a state of the art in the early\n  2000s, notably for robust regression and robust multivariate analysis."}, "apollo": {"categories": ["Econometrics"], "description": "Choice models are a widely used technique across numerous scientific disciplines. The Apollo package is a very flexible tool for the estimation and application \n    of choice models in R. Users are able to write their own \n    model functions or use a mix of already available ones. Random heterogeneity, \n    both continuous and discrete and at the level of individuals and \n    choices, can be incorporated for all models. There is support for both standalone \n    models and hybrid model structures.  Both classical \n    and Bayesian estimation is available, and multiple discrete \n    continuous models are covered in addition to discrete choice. \n    Multi-threading processing is supported for estimation and a large\n    number of pre and post-estimation routines, including for computing posterior\n    (individual-level) distributions are available. \n    For examples, a manual, and a support forum, visit \n    <http://www.ApolloChoiceModelling.com>. For more information on choice \n    models see Train, K. (2009) <isbn:978-0-521-74738-7> and Hess, \n    S. & Daly, A.J. (2014) <isbn:978-1-781-00314-5> for an overview \n    of the field."}, "primePCA": {"categories": ["MissingData"], "description": "Implements the primePCA algorithm, developed and analysed in Zhu, Z., Wang, T. and Samworth, R. J. (2019) High-dimensional principal component analysis with heterogeneous missingness. <arXiv:1906.12125>."}, "tswge": {"categories": ["TimeSeries"], "description": "Accompanies the text Applied Time Series Analysis with R, 2nd edition by Woodward, Gray, and Elliott. It is helpful for data analysis and for time series instruction."}, "survey": {"categories": ["OfficialStatistics", "Survival"], "description": "Summary statistics, two-sample tests, rank tests, generalised linear models, cumulative link models, Cox models, loglinear models, and general maximum pseudolikelihood estimation for multistage stratified, cluster-sampled, unequally weighted survey samples. Variances by Taylor series linearisation or replicate weights. Post-stratification, calibration, and raking. Two-phase subsampling designs. Graphics. PPS sampling without replacement. "}, "opusminer": {"categories": ["MachineLearning"], "description": "Provides a simple R interface to the OPUS Miner algorithm (implemented in C++) for finding the top-k productive, non-redundant itemsets from transaction data.  The OPUS Miner algorithm uses the OPUS search algorithm to efficiently discover the key associations in transaction data, in the form of self-sufficient itemsets, using either leverage or lift.  See <http://i.giwebb.com/index.php/research/association-discovery/> for more information in relation to the OPUS Miner algorithm."}, "Rmpfr": {"categories": ["NumericalMathematics"], "description": "Arithmetic (via S4 classes and methods) for\n  arbitrary precision floating point numbers, including transcendental\n  (\"special\") functions.  To this end, the package interfaces to\n  the 'LGPL' licensed 'MPFR' (Multiple Precision Floating-Point Reliable) Library\n  which itself is based on the 'GMP' (GNU Multiple Precision) Library."}, "ZRA": {"categories": ["TimeSeries"], "description": "Combines a forecast of a time series, using the function forecast(), with the dynamic plots from dygraphs."}, "ofGEM": {"categories": ["MetaAnalysis"], "description": "Offers a gene-based meta-analysis test with filtering to detect gene-environment interactions (GxE) with association data, proposed by Wang et al. (2018) <doi:10.1002/gepi.22115>. It first conducts a meta-filtering test to filter out unpromising SNPs by combining all samples in the consortia data. It then runs a test of omnibus-filtering-based GxE meta-analysis (ofGEM) that combines the strengths of the fixed- and random-effects meta-analysis with meta-filtering. It can also analyze data from multiple ethnic groups. "}, "cricketdata": {"categories": ["SportsAnalytics"], "description": "Data on international and other major cricket matches from \n  ESPNCricinfo <https://www.espncricinfo.com> and Cricsheet <https://cricsheet.org>.\n  This package provides some functions to download the data into tibbles ready\n  for analysis."}, "metaumbrella": {"categories": ["MetaAnalysis"], "description": "A comprehensive range of facilities to perform umbrella reviews with stratification of the evidence in R. The package accomplishes this aim by building on three core functions that: (i) automatically perform all required calculations in an umbrella review (including but not limited to meta-analyses), (ii) stratify evidence according to various classification criteria, and (iii) generate a visual representation of the results. Note that if you are not familiar with R, the core features of this package are available from a web browser (<https://www.metaumbrella.org/>)."}, "robCompositions": {"categories": ["MissingData"], "description": "Methods for analysis of compositional data including robust\n    methods  (<doi:10.1007/978-3-319-96422-5>), imputation of missing values (<doi:10.1016/j.csda.2009.11.023>), methods to replace \n    rounded zeros (<doi:10.1080/02664763.2017.1410524>, <doi:10.1016/j.chemolab.2016.04.011>, <doi:10.1016/j.csda.2012.02.012>), \n    count zeros (<doi:10.1177/1471082X14535524>), \n    methods to deal with essential zeros (<doi:10.1080/02664763.2016.1182135>), (robust) outlier\n    detection for compositional data, (robust) principal component analysis for\n    compositional data, (robust) factor analysis for compositional data, (robust)\n    discriminant analysis for compositional data (Fisher rule), robust regression\n    with compositional predictors, functional data analysis (<doi:10.1016/j.csda.2015.07.007>) and p-splines (<doi:10.1016/j.csda.2015.07.007>), \n    contingency (<doi:10.1080/03610926.2013.824980>) \n    and compositional tables (<doi:10.1111/sjos.12326>, <doi:10.1111/sjos.12223>, <doi:10.1080/02664763.2013.856871>) \n    and (robust) Anderson-Darling normality tests for\n    compositional data as well as popular log-ratio transformations (addLR, cenLR,\n    isomLR, and their inverse transformations). In addition, visualisation and\n    diagnostic tools are implemented as well as high and low-level plot functions\n    for the ternary diagram."}, "svmpath": {"categories": ["MachineLearning"], "description": "Computes the entire regularization path for the two-class svm classifier\n\t\twith essentially the same cost as a single SVM fit."}, "MAd": {"categories": ["MetaAnalysis"], "description": "A collection of functions for conducting a meta-analysis with mean differences data.  It uses recommended procedures as\tdescribed in The \tHandbook of Research Synthesis and Meta-Analysis\t(Cooper, Hedges, & Valentine, 2009). "}, "Distance": {"categories": ["Environmetrics"], "description": "A simple way of fitting detection functions to distance sampling\n    data for both line and point transects. Adjustment term selection, left and\n    right truncation as well as monotonicity constraints and binning are\n    supported. Abundance and density estimates can also be calculated (via a\n    Horvitz-Thompson-like estimator) if survey area information is provided. See\n    Miller et al. (2019) <doi:10.18637/jss.v089.i01> for more information on\n    methods and <https://examples.distancesampling.org/> for example analyses."}, "BTM": {"categories": ["NaturalLanguageProcessing"], "description": "Biterm Topic Models find topics in collections of short texts. \n    It is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns which are called biterms.\n    This in contrast to traditional topic models like Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis \n    which are word-document co-occurrence topic models.\n    A biterm consists of two words co-occurring in the same short text window.  \n    This context window can for example be a twitter message, a short answer on a survey, a sentence of a text or a document identifier. \n    The techniques are explained in detail in the paper 'A Biterm Topic Model For Short Text' by Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng (2013) <https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf>."}, "LAM": {"categories": ["Psychometrics"], "description": "\n    Includes some procedures for latent variable modeling with a \n    particular focus on multilevel data.\n    The 'LAM' package contains mean and covariance structure modelling\n    for multivariate normally distributed data (mlnormal(); Longford, 1987;\n    <doi:10.1093/biomet/74.4.817>), a general Metropolis-Hastings algorithm \n    (amh(); Roberts & Rosenthal, 2001, <doi:10.1214/ss/1015346320>) and \n    penalized maximum likelihood estimation (pmle(); Cole, Chu & Greenland, \n    2014; <doi:10.1093/aje/kwt245>)."}, "lsl": {"categories": ["Psychometrics"], "description": "Fits structural equation modeling via penalized likelihood."}, "MLCIRTwithin": {"categories": ["MissingData", "Psychometrics"], "description": "Framework for the Item Response Theory analysis of dichotomous and ordinal polytomous outcomes under the assumption of within-item multidimensionality and discreteness of the latent traits. The fitting algorithms allow for missing responses and for different item parametrizations and are based on the Expectation-Maximization paradigm. Individual covariates affecting the class weights may be included in the new version together with possibility of constraints on all model parameters."}, "seacarb": {"categories": ["Environmetrics"], "description": "Calculates parameters of the seawater carbonate system and assists the design of ocean acidification perturbation experiments."}, "automap": {"categories": ["Spatial"], "description": "Performs an automatic interpolation by automatically estimating the variogram and then calling gstat."}, "SCAT": {"categories": ["MissingData"], "description": "Conditional association test based on summary data from genome-wide association study (GWAS). SCAT adjusts for heterogeneity in SNP coverage that exists in summary data if SNPs are not present in all of the participating studies of a GWAS meta-analysis. This commonly happens when different reference panels are used in participating studies for genotype imputation. This could happen when ones simply do not have data for some SNPs (e.g. different array, or imputated data is not available). Without properly adjusting for this kind of heterogeneity leads to inflated false positive rate. SCAT can also be used to conduct conventional conditional analysis when coverage heterogeneity is absent. For more details, refer to Zhang et al. (2018) Brief Bioinform. 19(6):1337-1343. <doi:10.1093/bib/bbx072>. "}, "cglasso": {"categories": ["MissingData"], "description": "Conditional graphical lasso estimator is an extension of the graphical lasso proposed to estimate the conditional dependence structure of a set of p response variables given q predictors. This package provides suitable extensions developed to study datasets with censored and/or missing values. Standard conditional graphical lasso is available as a special case. Furthermore, the package provides an integrated set of core routines for visualization, analysis, and simulation of datasets with censored and/or missing values drawn from a Gaussian graphical model. Details about the implemented models can be found in Augugliaro et al. (2020b) <doi:10.1007/s11222-020-09945-7>, Augugliaro et al. (2020a) <doi:10.1093/biostatistics/kxy043>, Yin et al. (2001) <doi:10.1214/11-AOAS494> and Stadler et al. (2012) <doi:10.1007/s11222-010-9219-7>."}, "SDaA": {"categories": ["OfficialStatistics"], "description": "Functions and Datasets from Lohr, S. (1999), Sampling:\n        Design and Analysis, Duxbury."}, "tsBSS": {"categories": ["TimeSeries"], "description": "Different estimators are provided to solve the blind source separation problem for multivariate time series with stochastic volatility and supervised dimension reduction problem for multivariate time series. Different functions based on AMUSE and SOBI are also provided for estimating the dimension of the white noise subspace. The package is fully described in Nordhausen, Matilainen, Miettinen, Virta and Taskinen (2021) <doi:10.18637/jss.v098.i15>."}, "whitebox": {"categories": ["Spatial"], "description": "An R frontend for the 'WhiteboxTools' library, which is an advanced geospatial data analysis platform developed by Prof. John Lindsay at the University of Guelph's Geomorphometry and Hydrogeomatics Research Group. 'WhiteboxTools' can be used to perform common geographical information systems (GIS) analysis operations, such as cost-distance analysis, distance buffering, and raster reclassification. Remote sensing and image processing tasks include image enhancement (e.g. panchromatic sharpening, contrast adjustments), image mosaicing, numerous filtering operations, simple classification (k-means), and common image transformations. 'WhiteboxTools' also contains advanced tooling for spatial hydrological analysis (e.g. flow-accumulation, watershed delineation, stream network analysis, sink removal), terrain analysis (e.g. common terrain indices such as slope, curvatures, wetness index, hillshading; hypsometric analysis; multi-scale topographic position analysis), and LiDAR data processing. Suggested citation: Lindsay (2016) <doi:10.1016/j.cageo.2016.07.003>."}, "logitnorm": {"categories": ["Distributions"], "description": "Density, distribution, quantile and random generation function for the logitnormal distribution. Estimation of the mode and the first two moments. Estimation of distribution parameters."}, "MetaUtility": {"categories": ["MetaAnalysis"], "description": "Contains functions to estimate the proportion of effects stronger than a threshold\n    of scientific importance (function prop_stronger), to nonparametrically characterize the distribution of effects in a meta-analysis (calib_ests, pct_pval),\n    to make effect size conversions (r_to_d, r_to_z, z_to_r, d_to_logRR), to compute and format inference in a meta-analysis (format_CI, format_stat, tau_CI), to scrape results from \n    existing meta-analyses for re-analysis (scrape_meta, parse_CI_string, ci_to_var)."}, "RPPairwiseDesign": {"categories": ["ExperimentalDesign"], "description": "Using some association schemes to obtain a new series of resolvable partially pairwise balanced designs (RPPBD) and space-filling designs."}, "CARBayesdata": {"categories": ["Bayesian"], "description": "Spatio-temporal data from Scotland used in the vignettes accompanying the CARBayes (spatial modelling) and CARBayesST (spatio-temporal modelling) packages. Most of the data relate to the set of 271 Intermediate Zones (IZ)  that make up the 2001 definition of the  Greater Glasgow and Clyde health board. "}, "bets.covid19": {"categories": ["Epidemiology"], "description": "Implements likelihood inference for early epidemic analysis. BETS is short for the four key epidemiological events being modeled: Begin of exposure, End of exposure, time of Transmission, and time of Symptom onset. The package contains a dataset of the trajectory of confirmed cases during the coronavirus disease (COVID-19) early outbreak. More detail of the statistical methods can be found in Zhao et al. (2020) <arXiv:2004.07743>."}, "coarseDataTools": {"categories": ["Survival"], "description": "Functions to analyze coarse data.\n    Specifically, it contains functions to (1) fit parametric accelerated\n    failure time models to interval-censored survival time data, and (2)\n    estimate the case-fatality ratio in scenarios with under-reporting.\n    This package's development was motivated by applications to infectious\n    disease: in particular, problems with estimating the incubation period and\n    the case fatality ratio of a given disease.  Sample data files are included\n    in the package. See Reich et al. (2009) <doi:10.1002/sim.3659>, \n    Reich et al. (2012) <doi:10.1111/j.1541-0420.2011.01709.x>, and \n    Lessler et al. (2009) <doi:10.1016/S1473-3099(09)70069-6>."}, "ngspatial": {"categories": ["Spatial"], "description": "Provides tools for analyzing spatial data, especially non-\n    Gaussian areal data. The current version supports the sparse restricted\n    spatial regression model of Hughes and Haran (2013) <doi:10.1111/j.1467-9868.2012.01041.x>,\n\tthe centered autologistic model of Caragea and Kaiser (2009) <doi:10.1198/jabes.2009.07032>,\n\tand the Bayesian spatial filtering model of Hughes (2017) <arXiv:1706.04651>."}, "aftgee": {"categories": ["Survival"], "description": "A collection of methods for both the rank-based estimates and least-square estimates to the Accelerated Failure Time (AFT) model. For rank-based estimation, it provides approaches that include the computationally efficient Gehan's weight and the general's weight such as the logrank weight. Details of the rank-based estimation can be found in Chiou et al. (2014) <doi:10.1007/s11222-013-9388-2> and Chiou et al. (2015) <doi:10.1002/sim.6415>. For the least-square estimation, the estimating equation is solved with generalized estimating equations (GEE). Moreover, in multivariate cases, the dependence working correlation structure can be specified in GEE's setting. Details on the least-squares estimation can be found in Chiou et al. (2014) <doi:10.1007/s10985-014-9292-x>."}, "pid": {"categories": ["ExperimentalDesign"], "description": "A collection of scripts and data files for the statistics text: \n \"Process Improvement using Data\" <https://learnche.org/pid> and the online \n course \"Experimentation for Improvement\" found on Coursera. The package \n contains code for designed experiments, data sets and other convenience \n functions used in the book."}, "OwenQ": {"categories": ["Distributions"], "description": "Evaluates the Owen Q-function for an integer value of the degrees of freedom, by applying Owen's algorithm (1965) <doi:10.1093/biomet/52.3-4.437>. \n    It is useful for the calculation of the power of equivalence tests. "}, "estimraw": {"categories": ["MetaAnalysis"], "description": "Estimation of four-fold table cell frequencies (raw data) from risk ratios (relative risks), risk differences and odds ratios. While raw data can be useful for doing meta-analysis, such data is often not provided by primary studies (with summary statistics being solely presented). Therefore, based on summary statistics (namely, risk ratios, risk differences and odds ratios), this package estimates the value of each cell in a 2x2 table according to the equations described in Di Pietrantonj C (2006) <doi:10.1002/sim.2287>."}, "TSclust": {"categories": ["TimeSeries"], "description": "A set of measures of dissimilarity between time series to perform time series clustering. Metrics based on raw data, on generating models and on the forecast behavior are implemented. Some additional utilities related to time series clustering are also provided, such as clustering algorithms and cluster evaluation metrics."}, "ModelMap": {"categories": ["Spatial"], "description": "Creates sophisticated models of training data and validates the models with an independent test set, cross validation, or Out Of Bag (OOB) predictions on the training data. Create graphs and tables of the model validation results. Applies these models to GIS .img files of predictors to create detailed prediction surfaces. Handles large predictor files for map making, by reading in the .img files in chunks, and output to the .txt file the prediction for each data chunk, before reading the next chunk of data."}, "bayestestR": {"categories": ["Bayesian"], "description": "Provides utilities to describe posterior\n    distributions and Bayesian models. It includes point-estimates such as\n    Maximum A Posteriori (MAP), measures of dispersion (Highest Density\n    Interval - HDI; Kruschke, 2015 <doi:10.1016/C2012-0-00477-2>) and\n    indices used for null-hypothesis testing (such as ROPE percentage, pd\n    and Bayes factors)."}, "worldfootballR": {"categories": ["SportsAnalytics"], "description": "Allow users to obtain clean and tidy\n    football (soccer) game, team and player data. Data is collected from a\n    number of popular sites, including 'FBref',\n    transfer and valuations data from\n    'Transfermarkt'<https://www.transfermarkt.com/> and shooting location\n    and other match stats data from 'Understat'<https://understat.com/>\n    and 'fotmob'<https://www.fotmob.com/>. It gives users the\n    ability to access data more efficiently, rather than having to export\n    data tables to files before being able to complete their analysis."}, "irtplay": {"categories": ["Psychometrics"], "description": "Fit unidimensional item response theory (IRT) models to a mixture \n    of dichotomous and polytomous data, calibrate online item parameters \n    (i.e., pretest and operational items), estimate examinees' abilities, \n    and examine the IRT model-data fit on item-level in different ways \n    as well as provide useful functions related to unidimensional IRT models. \n    For the item parameter estimation, the marginal maximum likelihood estimation \n    via the expectation-maximization (MMLE-EM) algorithm \n    (Bock & Aitkin (1981) <doi:10.1007/BF02294168>) is used. \n    For the online calibration, the fixed item parameter calibration method \n    (Kim (2006) <doi:10.1111/j.1745-3984.2006.00021.x>) and \n    the fixed ability parameter calibration method\n    (Ban, Hanson, Wang, Yi, & Harris (2011) <doi:10.1111/j.1745-3984.2001.tb01123.x>) \n    are provided. For the ability estimation, several popular scoring methods \n    (e.g., MLE, EAP, and MAP) are implemented. In terms of assessing the IRT \n    model-data fit, one of distinguished features of this package is that it \n    gives not only well-known item fit statistics (e.g., chi-square (X2), \n    likelihood ratio chi-square (G2), infit and oufit statistics, and \n    S-X2 statistic (Ames & Penfield (2015) <doi:10.1111/emip.12067>)) \n    but also graphical displays to look at residuals between the observed \n    data and model-based predictions \n    (Hambleton, Swaminathan, & Rogers (1991, ISBN:9780803936478)). \n    In addition, there are many useful functions such as analyzing differential item \n    functioning, computing asymptotic variance-covariance matrices of item parameter \n    estimates (Li & Lissitz (2004) <doi:10.1111/j.1745-3984.2004.tb01109.x>), \n    importing item and/or ability parameters from popular IRT software, \n    running 'flexMIRT' (Cai, 2017) through R, generating simulated data, \n    computing the conditional distribution of observed scores using \n    the Lord-Wingersky recursion formula (Lord & Wingersky (1984) \n    <doi:10.1177/014662168400800409>), computing the loglikelihood of individual \n    items, computing the loglikelihood of abilities, computing item and test \n    information functions, computing item and test characteristic curve functions, \n    and plotting item and test characteristic curves and item and test information functions."}, "currentSurvival": {"categories": ["Survival"], "description": "The currentSurvival package contains functions for\n  the estimation of the current cumulative incidence (CCI) and\n  the current leukaemia-free survival (CLFS). The CCI is the \n  probability that a patient is alive and in any disease \n  remission (e.g. complete cytogenetic remission in chronic \n  myeloid leukaemia) after initiating his or her therapy (e.g. \n  tyrosine kinase therapy for chronic myeloid leukaemia). The \n  CLFS is the probability that a patient is alive and in any \n  disease remission after achieving the first disease remission."}, "fNonlinear": {"categories": ["Finance", "TimeSeries"], "description": "Provides a collection of functions for testing various aspects of\n\tunivariate time series including independence and neglected\n\tnonlinearities. Further provides functions to investigate the chaotic\n\tbehavior of time series processes and to simulate different types of chaotic\n\ttime series maps."}, "igcop": {"categories": ["Distributions"], "description": "Compute distributional quantities for an\n    Integrated Gamma (IG) or Integrated Gamma Limit (IGL) copula, such\n    as a cdf and density. Compute corresponding conditional quantities\n    such as the cdf and quantiles. Generate\n    data from an IG or IGL copula."}, "grf": {"categories": ["CausalInference", "Econometrics", "MachineLearning", "MissingData"], "description": "A pluggable package for forest-based statistical estimation and inference.\n    GRF currently provides methods for non-parametric least-squares regression,\n    quantile regression, survival regression and treatment effect estimation (optionally using instrumental\n    variables), with support for missing values."}, "prcr": {"categories": ["Cluster"], "description": "Provides an easy-to-use yet adaptable set of tools to conduct person-center analysis using a two-step clustering procedure. As described in Bergman and El-Khouri (1999) <doi:10.1002/(SICI)1521-4036(199910)41:6%3C753::AID-BIMJ753%3E3.0.CO;2-K>, hierarchical clustering is performed to determine the initial partition for the subsequent k-means clustering procedure."}, "coalitions": {"categories": ["Bayesian"], "description": "An implementation of a Bayesian framework for the opinion poll\n    based estimation of event probabilities in multi-party electoral systems\n    (Bender and Bauer (2018) <doi:10.21105/joss.00606>)."}, "rrcov": {"categories": ["Robust"], "description": "Robust Location and Scatter Estimation and Robust\n        Multivariate Analysis with High Breakdown Point:\n        principal component analysis (Filzmoser and Todorov (2013), <doi:10.1016/j.ins.2012.10.017>),\n        linear and quadratic discriminant analysis (Todorov and Pires (2007)),\n        multivariate tests (Todorov and Filzmoser (2010) <doi:10.1016/j.csda.2009.08.015>),\n        outlier detection (Todorov et al. (2010) <doi:10.1007/s11634-010-0075-2>).\n        See also Todorov and Filzmoser (2009) <ISBN-13:978-3838108148>,\n        Todorov and Filzmoser (2010) <doi:10.18637/jss.v032.i03> and\n        Boudt et al. (2019) <doi:10.1007/s11222-019-09869-x>."}, "MFPCA": {"categories": ["FunctionalData"], "description": "Calculate a multivariate functional principal component analysis\n    for data observed on different dimensional domains. The estimation algorithm\n    relies on univariate basis expansions for each element of the multivariate\n    functional data  (Happ & Greven, 2018) <doi:10.1080/01621459.2016.1273115>. \n    Multivariate and univariate functional data objects are\n    represented by S4 classes for this type of data implemented in the package\n    'funData'. For more details on the general concepts of both packages and a case \n    study, see Happ-Kurz (2020) <doi:10.18637/jss.v093.i05>."}, "Rphylopars": {"categories": ["MissingData"], "description": "Tools for performing phylogenetic comparative methods for datasets with with multiple observations per species (intraspecific variation or measurement error) and/or missing data (Goolsby et al. 2017). Performs ancestral state reconstruction and missing data imputation on the estimated evolutionary model, which can be specified as Brownian Motion, Ornstein-Uhlenbeck, Early-Burst, Pagel's lambda, kappa, or delta, or a star phylogeny."}, "ROI.plugin.qpoases": {"categories": ["Optimization"], "description": "Enhances the 'R' Optimization Infrastructure ('ROI') package\n         with the quadratic solver 'qpOASES'. More information about\n         'qpOASES' can be found at <https://projects.coin-or.org/qpOASES/>."}, "DSI": {"categories": ["OfficialStatistics"], "description": "'DataSHIELD' is an infrastructure and series of R packages that \n    enables the remote and 'non-disclosive' analysis of sensitive research data. \n    This package defines the API that is to be implemented by 'DataSHIELD' compliant \n    data repositories."}, "spind": {"categories": ["Spatial"], "description": "Functions for spatial methods based on generalized estimating equations (GEE) and\n  wavelet-revised methods (WRM), functions for scaling by wavelet multiresolution regression (WMRR),\n  conducting multi-model inference, and stepwise model selection. Further, contains functions \n  for spatially corrected model accuracy measures."}, "fitteR": {"categories": ["Distributions"], "description": "Systematic fit of hundreds of theoretical univariate distributions to empirical data via maximum likelihood estimation. Fits are reported and summarized by a data.frame, a csv file or a 'shiny' app (here with additional features like visual representation of fits). All output formats provide assessment of goodness-of-fit by the following methods: Kolmogorov-Smirnov test, Shapiro-Wilks test, Anderson-Darling test."}, "gbm": {"categories": ["MachineLearning", "Survival"], "description": "An implementation of extensions to Freund and Schapire's AdaBoost \n  algorithm and Friedman's gradient boosting machine. Includes regression \n  methods for least squares, absolute loss, t-distribution loss, quantile \n  regression, logistic, multinomial logistic, Poisson, Cox proportional hazards \n  partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and \n  Learning to Rank measures (LambdaMart). Originally developed by Greg Ridgeway."}, "IsingFit": {"categories": ["Psychometrics"], "description": "This network estimation procedure eLasso, which is based on the Ising model, combines l1-regularized logistic regression with model selection based on the Extended Bayesian Information Criterion (EBIC). EBIC is a fit measure that identifies relevant relationships between variables. The resulting network consists of variables as nodes and relevant relationships as edges. Can deal with binary data."}, "cpk": {"categories": ["Pharmacokinetics"], "description": "The package cpk provides simplified clinical pharmacokinetic functions for dose regimen design and modification at the point-of-care. Currently, the following functions are available: (1) ttc.fn for target therapeutic concentration, (2) dr.fn for dose rate, (3) di.fn for dosing interval, (4) dm.fn for maintenance dose, (5) bc.ttc.fn for back calculation, (6) ar.fn for accumulation ratio, (7) dpo.fn for orally administered dose, (8) cmax.fn for peak concentration, (9) css.fn for steady-state concentration, (10) cmin.fn for trough,(11) ct.fn for concentration-time predictions, (12) dlcmax.fn for calculating loading dose based on drug's maximum concentration, (13) dlar.fn for calculating loading dose based on drug's accumulation ratio, and (14) R0.fn for calculating drug infusion rate. Reference: Linares O, Linares A. Computational opioid prescribing: A novel application of clinical pharmacokinetics. J Pain Palliat Care Pharmacother 2011;25:125-135."}, "bain": {"categories": ["Bayesian"], "description": "Computes approximated adjusted fractional Bayes factors for\n    equality, inequality, and about equality constrained hypotheses.\n    For a tutorial on this method, see Hoijtink, Mulder, van Lissa, & Gu,\n    (2019) <doi:10.31234/osf.io/v3shc>. For applications in structural equation\n    modeling, see: Van Lissa, Gu, Mulder, Rosseel, Van Zundert, &\n    Hoijtink, (2021) <doi:10.1080/10705511.2020.1745644>. For the statistical\n    underpinnings, see Gu, Mulder, and Hoijtink (2018) <doi:10.1111/bmsp.12110>;\n    Hoijtink, Gu, & Mulder, J. (2019) <doi:10.1111/bmsp.12145>; Hoijtink, Gu, \n    Mulder, & Rosseel, (2019) <doi:10.31234/osf.io/q6h5w>."}, "liteq": {"categories": ["Databases"], "description": "Temporary and permanent message queues for R. Built on top of\n    'SQLite' databases. 'SQLite' provides locking, and makes it possible\n    to detect crashed consumers. Crashed jobs can be automatically marked\n    as \"failed\", or put in the queue again, potentially a limited number of times."}, "NGSSEML": {"categories": ["Bayesian", "TimeSeries"], "description": "Due to a large quantity of non-Gaussian time series and reliability data, the R-package non-Gaussian state-space with exact marginal likelihood is useful for modeling and forecasting non-Gaussian time series and reliability data via non-Gaussian state-space models with the exact marginal likelihood easily, see Gamerman, Santos and Franco (2013) <doi:10.1111/jtsa.12039> and Santos, Gamerman and Franco (2017) <doi:10.1109/TR.2017.2670142>. The package gives codes for formulating and specifying the non-Gaussian state-space models in the R language. Inferences for the parameters of the model can be made under the classical and Bayesian. Furthermore, prediction, filtering, and smoothing procedures can be used to perform inferences for the latent parameters. Applications include, e.g., count, volatility, piecewise exponential, and software reliability data."}, "citationchaser": {"categories": ["MetaAnalysis"], "description": "In searching for research articles, we often want to \n  obtain lists of references from across studies, and also obtain lists \n  of articles that cite a particular study. In systematic reviews, this \n  supplementary search technique is known as 'citation chasing': forward \n  citation chasing looks for all records citing one or more articles of \n  known relevance; backward citation chasing looks for all records \n  referenced in one or more articles. Traditionally, this process would \n  be done manually, and the resulting records would need to be checked \n  one-by-one against included studies in a review to identify potentially \n  relevant records that should be included in a review. This package \n  contains functions to automate this process by making use of the \n  Lens.org API. An input article list can be used to return a list of \n  all referenced records, and/or all citing records in the Lens.org \n  database (consisting of PubMed, PubMed Central, CrossRef, Microsoft \n  Academic Graph and CORE; <https://www.lens.org>). "}, "wildlifeDI": {"categories": ["SpatioTemporal", "Tracking"], "description": "Dynamic interaction refers to spatial-temporal associations\n    in the movements of two (or more) animals. This package provides tools for\n    calculating a suite of indices used for quantifying dynamic interaction with\n    wildlife telemetry data. For more information on each of the methods employed\n    see the references within. The package (as of version >= 0.3) also has new tools for\n    automating contact analysis in large tracking datasets. The package draws \n    heavily on the classes and methods developed in the 'adehabitat' packages."}, "formattable": {"categories": ["ReproducibleResearch"], "description": "Provides functions to create formattable vectors and data frames.\n    'Formattable' vectors are printed with text formatting, and formattable\n    data frames are printed with multiple types of formatting in HTML\n    to improve the readability of data presented in tabular form rendered in\n    web pages."}, "BayesLCA": {"categories": ["Bayesian", "Cluster", "Psychometrics"], "description": "Bayesian Latent Class Analysis using several different\n        methods."}, "distrSim": {"categories": ["Distributions"], "description": "S4-classes for setting up a coherent framework for simulation within the distr\n        family of packages."}, "PMCMRplus": {"categories": ["Environmetrics"], "description": "For one-way layout experiments the one-way ANOVA can\n\t     be performed as an omnibus test. All-pairs multiple comparisons \n\t     tests (Tukey-Kramer test, Scheffe test, LSD-test) \n\t     and many-to-one tests (Dunnett test) for normally distributed \n\t     residuals and equal within variance are available. Furthermore,\n\t     all-pairs tests (Games-Howell test, Tamhane's T2 test, \n\t     Dunnett T3 test, Ury-Wiggins-Hochberg test) and many-to-one\n\t     (Tamhane-Dunnett Test) for normally distributed residuals \n\t     and heterogeneous variances are provided. Van der Waerden's normal\n\t     scores test for omnibus, all-pairs and many-to-one tests is\n\t     provided for non-normally distributed residuals and homogeneous\n\t     variances. The Kruskal-Wallis, BWS and Anderson-Darling\n\t     omnibus test and all-pairs tests\n\t     (Nemenyi test, Dunn test, Conover test, Dwass-Steele-Critchlow-\n\t     Fligner test) as well as many-to-one (Nemenyi test, Dunn test,\n\t     U-test) are given for the analysis of variance by ranks. \n             Non-parametric trend tests (Jonckheere test, Cuzick test,\n\t     Johnson-Mehrotra test, Spearman test) are included. \n\t     In addition, a Friedman-test for one-way ANOVA with repeated \n\t     measures on ranks (CRBD) and Skillings-Mack test for unbalanced \n\t     CRBD is provided with consequent all-pairs tests (Nemenyi test, \n\t     Siegel test, Miller test, Conover test, Exact test)\n\t     and many-to-one tests (Nemenyi test, Demsar test, Exact test). \n\t     A trend can be tested with Pages's test. Durbin's test \n\t     for a two-way balanced incomplete block design (BIBD) is given \n\t     in this package as well as Gore's test for CRBD with multiple\n\t     observations per cell is given.  Outlier tests, Mandel's k- and\n\t     h statistic as well as functions for Type I error and Power \n\t     analysis as well as generic summary, print and plot methods \n             are provided."}, "flacco": {"categories": ["Optimization"], "description": "Tools and features for \"Exploratory Landscape Analysis (ELA)\" of\n\tsingle-objective continuous optimization problems.\n    Those features are able to quantify rather complex properties, such as the\n    global structure, separability, etc., of the optimization problems."}, "mscsweblm4r": {"categories": ["NaturalLanguageProcessing", "WebTechnologies"], "description": "R Client for the Microsoft Cognitive Services Web Language Model\n    REST API, including Break Into Words, Calculate Conditional\n    Probability, Calculate Joint Probability, Generate Next Words, and List\n    Available Models. A valid account MUST be registered at the Microsoft\n    Cognitive Services website <https://www.microsoft.com/cognitive-services/>\n    in order to obtain a (free) API key. Without an API key, this package will\n    not work properly."}, "RobAStBase": {"categories": ["Robust"], "description": "Base S4-classes and functions for robust asymptotic statistics."}, "difNLR": {"categories": ["Psychometrics"], "description": "Detection of differential item functioning (DIF) among dichotomously scored items and differential distractor functioning (DDF) among unscored items with non-linear regression  procedures based on generalized logistic regression models (Hladka & Martinkova, 2020, <doi:10.32614/RJ-2020-014>)."}, "magrittr": {"categories": ["WebTechnologies"], "description": "Provides a mechanism for chaining commands with a new\n    forward-pipe operator, %>%. This operator will forward a value, or the\n    result of an expression, into the next function call/expression.\n    There is flexible support for the type of right-hand side expressions.\n    For more information, see package vignette.  To quote Rene Magritte,\n    \"Ceci n'est pas un pipe.\""}, "DTAT": {"categories": ["ClinicalTrials"], "description": "Dose Titration Algorithm Tuning (DTAT) is a methodologic framework\n             allowing dose individualization to be conceived as a continuous\n             learning process that begins in early-phase clinical trials and\n             continues throughout drug development, on into clinical practice.\n             This package includes code that researchers may use to reproduce\n             or extend key results of the DTAT research programme, plus tools\n             for trialists to design and simulate a '3+3/PC' dose-finding study.\n             Please see Norris (2017) <doi:10.12688/f1000research.10624.3> and\n             Norris (2017) <doi:10.1101/240846>."}, "ompr": {"categories": ["Optimization"], "description": "Model mixed integer linear programs in an algebraic way directly in R.\n             The model is solver-independent and thus offers the possibility\n             to solve a model with different solvers. It currently only supports\n             linear constraints and objective functions. See the 'ompr'\n             website <https://dirkschumacher.github.io/ompr/> for more information,\n             documentation and examples."}, "data.table": {"categories": ["Finance", "HighPerformanceComputing", "TimeSeries"], "description": "Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, friendly and fast character-separated-value read/write. Offers a natural and flexible syntax, for faster development."}, "CBPS": {"categories": ["CausalInference"], "description": "Implements the covariate balancing propensity score (CBPS) proposed\n    by Imai and Ratkovic (2014) <doi:10.1111/rssb.12027>. The propensity score is\n    estimated such that it maximizes the resulting covariate balance as well as the\n    prediction of treatment assignment. The method, therefore, avoids an iteration\n    between model fitting and balance checking.  The package also implements optimal\n    CBPS from Fan et al. (in-press) <doi:10.1080/07350015.2021.2002159>,  \n    several extensions of the CBPS beyond the cross-sectional, binary treatment setting.\n    They include the CBPS for longitudinal settings so that it can be used in \n    conjunction with marginal structural models from Imai and Ratkovic (2015) \n    <doi:10.1080/01621459.2014.956872>, treatments with three- and four-valued treatment \n    variables, continuous-valued treatments from Fong, Hazlett, and Imai (2018) \n    <doi:10.1214/17-AOAS1101>, propensity score estimation with a large number of \n    covariates from Ning, Peng, and Imai (2020) <doi:10.1093/biomet/asaa020>, and the situation \n    with multiple distinct binary treatments administered simultaneously. In the future \n    it will be extended to other settings including the generalization of experimental \n    and instrumental variable estimates. "}, "autoFRK": {"categories": ["Spatial"], "description": "Automatic fixed rank kriging for (irregularly located)\n    spatial data using a class of basis functions with multi-resolution features\n    and ordered in terms of their resolutions. The model parameters are estimated\n    by maximum likelihood (ML) and the number of basis functions is determined\n    by Akaike's information criterion (AIC). For spatial data with either one\n    realization or independent replicates, the ML estimates and AIC are efficiently\n    computed using their closed-form expressions when no missing value occurs. Details \n    regarding the basis function construction, parameter estimation, and AIC calculation  \n    can be found in Tzeng and Huang (2018) <doi:10.1080/00401706.2017.1345701>. For\n    data with missing values, the ML estimates are obtained using the expectation-\n    maximization algorithm. Apart from the number of basis functions, there are\n    no other tuning parameters, making the method fully automatic. Users can also\n    include a stationary structure in the spatial covariance, which utilizes\n    'LatticeKrig' package."}, "carfima": {"categories": ["TimeSeries"], "description": "We provide a toolbox to fit a continuous-time fractionally integrated ARMA process (CARFIMA) on univariate and irregularly spaced time series data via both frequentist and Bayesian machinery. A general-order CARFIMA(p, H, q) model for p>q is specified in Tsai and Chan (2005) <doi:10.1111/j.1467-9868.2005.00522.x> and it involves p+q+2 unknown model parameters, i.e., p AR parameters, q MA parameters, Hurst parameter H, and process uncertainty (standard deviation) sigma. Also, the model can account for heteroscedastic measurement errors, if the information about measurement error standard deviations is known. The package produces their maximum likelihood estimates and asymptotic uncertainties using a global optimizer called the differential evolution algorithm. It also produces posterior samples of the model parameters via Metropolis-Hastings within a Gibbs sampler equipped with adaptive Markov chain Monte Carlo. These fitting procedures, however, may produce numerical errors if p>2. The toolbox also contains a function to simulate discrete time series data from CARFIMA(p, H, q) process given the model parameters and observation times. "}, "bivgeom": {"categories": ["Distributions"], "description": "Implements Roy's bivariate geometric model (Roy (1993) <doi:10.1006/jmva.1993.1065>): joint probability mass function, distribution function, survival function, random generation, parameter estimation, and more."}, "microsamplingDesign": {"categories": ["Pharmacokinetics"], "description": "\n  Find optimal microsampling designs for non-compartmental pharacokinetic analysis using a  general simulation methodology:\n  Algorithm III of Barnett, Helen, Helena Geys, Tom Jacobs, and Thomas Jaki. (2017) \"Optimal Designs for Non-Compartmental\n  Analysis of Pharmacokinetic Studies. (currently unpublished)\"\n  This methodology consist of (1) specifying a pharmacokinetic model\n  including variability among animals; (2) generating possible sampling times; (3)\n  evaluating performance of each time point choice on simulated data; (4)\n  generating possible schemes given a time point choice and additional constraints\n  and finally (5) evaluating scheme performance on simulated data. The default\n  settings differ from the article of Barnett and others, in the default pharmacokinetic model used and\n  the parameterization of variability among animals. Details can be found in the package vignette. A 'shiny'\n  web application is included, which guides users from model parametrization to\n  optimal microsampling scheme.   "}, "srvyr": {"categories": ["OfficialStatistics"], "description": "Use piping, verbs like 'group_by' and 'summarize', and other\n    'dplyr' inspired syntactic style when calculating summary statistics on survey\n    data using functions from the 'survey' package."}, "BayesCR": {"categories": ["Bayesian"], "description": "Propose a parametric fit for censored linear regression models based on SMSN distributions, from a Bayesian perspective. Also, generates SMSN random variables."}, "boilerpipeR": {"categories": ["NaturalLanguageProcessing", "WebTechnologies"], "description": "Generic Extraction of main text content from HTML files; removal\n    of ads, sidebars and headers using the boilerpipe \n    <https://github.com/kohlschutter/boilerpipe> Java library. The\n    extraction heuristics from boilerpipe show a robust performance for a wide\n    range of web site templates."}, "drtmle": {"categories": ["CausalInference"], "description": "Targeted minimum loss-based estimators of counterfactual means and\n    causal effects that are doubly-robust with respect both to consistency and\n    asymptotic normality (Benkeser et al (2017), <doi:10.1093/biomet/asx053>; MJ\n    van der Laan (2014), <doi:10.1515/ijb-2012-0038>)."}, "SnowballC": {"categories": ["NaturalLanguageProcessing"], "description": "An R interface to the C 'libstemmer' library that implements\n  Porter's word stemming algorithm for collapsing words to a common\n  root to aid comparison of vocabulary. Currently supported languages are\n  Danish, Dutch, English, Finnish, French, German, Hungarian, Italian,\n  Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish\n  and Turkish."}, "SCOR": {"categories": ["Optimization"], "description": "A non convex optimization package that optimizes any function under the criterion, combination of variables are on the surface of a unit sphere, as described in the paper : Das et al. (2019) <arXiv:1909.04024> ."}, "git2r": {"categories": ["WebTechnologies"], "description": "Interface to the 'libgit2' library, which is a pure C\n    implementation of the 'Git' core methods. Provides access to 'Git'\n    repositories to extract data and running some basic 'Git'\n    commands."}, "KFAS": {"categories": ["TimeSeries"], "description": "State space modelling is an efficient and flexible framework for \n    statistical inference of a broad class of time series and other data. KFAS \n    includes computationally efficient functions for Kalman filtering, smoothing, \n    forecasting, and simulation of multivariate exponential family state space models, \n    with observations from Gaussian, Poisson, binomial, negative binomial, and gamma \n    distributions. See the paper by Helske (2017) <doi:10.18637/jss.v078.i10> for details."}, "Runuran": {"categories": ["Bayesian", "Distributions"], "description": "Interface to the 'UNU.RAN' library for Universal Non-Uniform RANdom variate generators. \n\t     Thus it allows to build non-uniform random number generators from quite arbitrary\n\t     distributions. In particular, it provides an algorithm for fast numerical inversion\n\t     for distribution with given density function.\n\t     In addition, the package contains densities, distribution functions and quantiles\n\t     from a couple of distributions. "}, "rhosp": {"categories": ["Survival"], "description": "Evaluating risk (that a patient arises a side effect) during hospitalization is the main purpose of this package. Several methods (Parametric, non parametric and De Vielder estimation) to estimate the risk constant (R) are implemented in this package. There are also functions to simulate the different models of this issue in order to quantify the previous estimators. It is necessary to read at least the first six pages of the report to understand the topic."}, "walrus": {"categories": ["Robust"], "description": "A toolbox of common robust statistical tests, including robust\n  descriptives, robust t-tests, and robust ANOVA. It is also available as a\n  module for 'jamovi' (see <https://www.jamovi.org> for more information).\n  Walrus is based on the WRS2 package by Patrick Mair, which is in turn based on\n  the scripts and work of Rand Wilcox. These analyses are described in depth in\n  the book 'Introduction to Robust Estimation & Hypothesis Testing'."}, "Rcplex": {"categories": ["Optimization"], "description": "R interface to CPLEX solvers for linear, quadratic, and (linear and quadratic) mixed integer programs. Support for quadratically constrained programming is available. See the file \"INSTALL\" for details on how to install the Rcplex package in Linux/Unix-like and Windows systems. Support for sparse matrices is provided by an S3-style class \"simple_triplet_matrix\" from package slam and by objects from the Matrix package class hierarchy."}, "FKF.SP": {"categories": ["TimeSeries"], "description": "Fast and flexible Kalman filtering implementation utilizing sequential processing, designed for efficient parameter estimation through maximum likelihood estimation. Sequential processing is a univariate treatment of a multivariate series of observations and can benefit from computational efficiency over traditional Kalman filtering when independence is assumed in the variance of the disturbances of the measurement equation. Sequential processing is described in the textbook of Durbin and Koopman (2001, ISBN:978-0-19-964117-8). 'FKF.SP' was built upon the existing 'FKF' package and is, in general, a faster Kalman filter."}, "hydroscoper": {"categories": ["Hydrology"], "description": "R interface to the Greek National Data Bank for Hydrological and \n    Meteorological Information. It covers \n    Hydroscope's data sources and provides functions to transliterate, \n    translate and download them into tidy dataframes."}, "SAVER": {"categories": ["MissingData"], "description": "An implementation of a regularized regression prediction and \n    empirical Bayes method to recover the true gene expression profile in \n    noisy and sparse single-cell RNA-seq data. See Huang M, et al (2018) \n    <doi:10.1038/s41592-018-0033-z> for more details."}, "radsafer": {"categories": ["ChemPhys"], "description": "Provides functions for radiation safety, also known as\n    \"radiation protection\" and \"radiological control\". The science of \n    radiation protection is called \"health physics\" and its engineering \n    functions are called \"radiological engineering\". Functions in this \n    package cover many of the computations needed by radiation safety \n    professionals. Examples include: obtaining updated calibration and\n    source check values for radiation monitors to account for radioactive \n    decay in a reference source, simulating instrument readings to better\n    understand measurement uncertainty, correcting instrument readings \n    for geometry and ambient atmospheric conditions. Many of these \n    functions are described in Johnson and Kirby (2011, ISBN-13:  \n    978-1609134198). Utilities are also included for developing inputs \n    and processing outputs with radiation transport codes, such as MCNP, \n    a general-purpose Monte Carlo N-Particle code that can be used for \n    neutron, photon, electron, or coupled neutron/photon/electron transport\n    (Werner et. al. (2018) <doi:10.2172/1419730>)."}, "CAMAN": {"categories": ["MetaAnalysis"], "description": "Tools for the analysis of finite semiparametric mixtures.\n        These are useful when data is heterogeneous, e.g. in\n        pharmacokinetics or meta-analysis. The NPMLE and VEM algorithms\n        (flexible support size) and EM algorithms (fixed support size)\n        are provided for univariate (Bohning et al., 1992; \n        <doi:10.2307/2532756>) and bivariate data (Schlattmann et \n        al., 2015; <doi:10.1016/j.jclinepi.2014.08.013>)."}, "metaheuristicOpt": {"categories": ["Optimization"], "description": "An implementation of metaheuristic algorithms for continuous optimization. Currently, the package contains the implementations of 21 algorithms, as follows: particle swarm optimization (Kennedy and Eberhart, 1995), ant lion optimizer (Mirjalili, 2015 <doi:10.1016/j.advengsoft.2015.01.010>), grey wolf optimizer (Mirjalili et al., 2014 <doi:10.1016/j.advengsoft.2013.12.007>), dragonfly algorithm (Mirjalili, 2015 <doi:10.1007/s00521-015-1920-1>), firefly algorithm (Yang, 2009 <doi:10.1007/978-3-642-04944-6_14>), genetic algorithm (Holland, 1992, ISBN:978-0262581110), grasshopper optimisation algorithm (Saremi et al., 2017 <doi:10.1016/j.advengsoft.2017.01.004>), harmony search algorithm (Mahdavi et al., 2007 <doi:10.1016/j.amc.2006.11.033>), moth flame optimizer (Mirjalili, 2015 <doi:10.1016/j.knosys.2015.07.006>, sine cosine algorithm (Mirjalili, 2016 <doi:10.1016/j.knosys.2015.12.022>),  whale optimization algorithm (Mirjalili and Lewis, 2016 <doi:10.1016/j.advengsoft.2016.01.008>), clonal selection algorithm (Castro, 2002 <doi:10.1109/TEVC.2002.1011539>), differential evolution (Das & Suganthan, 2011), shuffled frog leaping (Eusuff, Landsey & Pasha, 2006), cat swarm optimization (Chu et al., 2006), artificial bee colony algorithm (Karaboga & Akay, 2009), krill-herd algorithm (Gandomi & Alavi, 2012), cuckoo search (Yang & Deb, 2009), bat algorithm (Yang, 2012), gravitational based search (Rashedi et al., 2009) and black hole optimization (Hatamlou, 2013)."}, "tssim": {"categories": ["TimeSeries"], "description": "Flexible simulation of time series using time\n    series components, including seasonal, calendar and outlier effects.\n    Algorithm described in Ollech, D. (2021) <doi:10.1515/jtse-2020-0028>."}, "deSolve": {"categories": ["DifferentialEquations", "Epidemiology"], "description": "Functions that solve initial value problems of a system\n        of first-order ordinary differential equations ('ODE'), of\n        partial differential equations ('PDE'), of differential\n        algebraic equations ('DAE'), and of delay differential\n        equations.  The functions provide an interface to the FORTRAN\n        functions 'lsoda', 'lsodar', 'lsode', 'lsodes' of the 'ODEPACK'\n        collection, to the FORTRAN functions 'dvode', 'zvode' and 'daspk'\n        and a C-implementation of solvers of the 'Runge-Kutta' family with\n        fixed or variable time steps.  The package contains routines\n        designed for solving 'ODEs' resulting from 1-D, 2-D and 3-D\n        partial differential equations ('PDE') that have been converted\n        to 'ODEs' by numerical differencing."}, "unrepx": {"categories": ["ExperimentalDesign"], "description": "Provides half-normal plots, reference plots, and Pareto plots\n    of effects from an unreplicated experiment, along with various \n    pseudo-standard-error measures, simulated reference distributions, \n    and other tools. Many of these methods are described in \n    Daniel C. (1959) <doi:10.1080/00401706.1959.10489866> and/or\n    Lenth R.V. (1989) <doi:10.1080/00401706.1989.10488595>, but some new\n    approaches are added and integrated in one package."}, "covid19france": {"categories": ["Epidemiology"], "description": "Imports and cleans 'opencovid19-fr'\n    <https://github.com/opencovid19-fr/data> data on COVID-19 in France."}, "mix": {"categories": ["MissingData"], "description": "Estimation/multiple imputation programs for mixed categorical\n    and continuous data."}, "mlr3spatiotempcv": {"categories": ["Spatial", "SpatioTemporal"], "description": "Extends the mlr3 ML framework with spatio-temporal resampling\n    methods to account for the presence of spatiotemporal autocorrelation\n    (STAC) in predictor variables. STAC may cause highly biased\n    performance estimates in cross-validation if ignored."}, "fmdates": {"categories": ["Finance"], "description": "Implements common date calculations relevant for specifying\n  the economic nature of financial market contracts that are typically defined\n  by International Swap Dealer Association (ISDA, <http://www2.isda.org>) legal\n  documentation. This includes methods to check whether dates are business\n  days in certain locales, functions to adjust and shift dates and time length\n  (or day counter) calculations."}, "gitlabr": {"categories": ["WebTechnologies"], "description": "Provides R functions to access the API of the project and\n    repository management web application 'GitLab'. For many common tasks\n    (repository file access, issue assignment and status, commenting)\n    convenience wrappers are provided, and in addition the full API can be\n    used by specifying request locations. 'GitLab' is open-source software\n    and can be self-hosted or used on <https://about.gitlab.com>."}, "qualV": {"categories": ["Environmetrics"], "description": "Qualitative methods for the validation of dynamic models.\n    It contains (i) an orthogonal set of deviance measures for absolute,\n    relative and ordinal scale and (ii) approaches accounting for time\n    shifts. The first approach transforms time to take time delays and speed\n    differences into account. The second divides the time series into\n    interval units according to their main features and finds the longest\n    common subsequence (LCS) using a dynamic programming algorithm."}, "textreuse": {"categories": ["NaturalLanguageProcessing"], "description": "Tools for measuring similarity among documents and detecting\n    passages which have been reused. Implements shingled n-gram, skip n-gram,\n    and other tokenizers; similarity/dissimilarity functions; pairwise\n    comparisons; minhash and locality sensitive hashing algorithms; and a\n    version of the Smith-Waterman local alignment algorithm suitable for\n    natural language."}, "plumber": {"categories": ["ModelDeployment", "WebTechnologies"], "description": "Gives the ability to automatically generate and serve an HTTP API\n    from R functions using the annotations in the R documentation around your\n    functions."}, "riverdist": {"categories": ["SpatioTemporal"], "description": "Reads river network shape files and computes network distances.\n    Also included are a variety of computation and graphical tools designed \n    for fisheries telemetry research, such as minimum home range, kernel density \n    estimation, and clustering analysis using empirical k-functions with \n    a bootstrap envelope.  Tools are also provided for editing the river \n    networks, meaning there is no reliance on external software."}, "optbdmaeAT": {"categories": ["ExperimentalDesign"], "description": "Computes A-, MV-, D- and E-optimal or near-optimal block designs for two-colour cDNA microarray experiments using the linear fixed effects and mixed effects models where the interest is in a comparison of all possible elementary treatment contrasts. The algorithms used in this package are based on the treatment exchange and array exchange algorithms of Debusho, Gemechu and Haines (2016, unpublished). The package also provides an optional method of using the graphical user interface (GUI) R package tcltk to ensure that it is user friendly."}, "refund": {"categories": ["FunctionalData"], "description": "Methods for regression for functional\n    data, including function-on-scalar, scalar-on-function, and\n    function-on-function regression. Some of the functions are applicable to\n    image data."}, "RSclient": {"categories": ["ModelDeployment", "WebTechnologies"], "description": "Client for Rserve, allowing to connect to Rserve instances and issue commands."}, "CoImp": {"categories": ["MissingData"], "description": "Copula based imputation method. A semiparametric imputation procedure for missing multivariate data based on conditional copula specifications."}, "distr": {"categories": ["Distributions", "Robust"], "description": "S4-classes and methods for distributions."}, "starma": {"categories": ["Spatial"], "description": "Statistical functions to identify, estimate and diagnose a Space-Time AutoRegressive Moving Average (STARMA) model."}, "EMCluster": {"categories": ["Cluster"], "description": "EM algorithms and several efficient\n        initialization methods for model-based clustering of finite\n        mixture Gaussian distribution with unstructured dispersion\n        in both of unsupervised and semi-supervised learning."}, "BMT": {"categories": ["Distributions"], "description": "Density, distribution, quantile function, random number generation for the BMT (Bezier-Montenegro-Torres) distribution. Torres-Jimenez C.J. and Montenegro-Diaz A.M. (2017) <arXiv:1709.05534>. Moments, descriptive measures and parameter conversion for different parameterizations of the BMT distribution. Fit of the BMT distribution to non-censored data by maximum likelihood, moment matching, quantile matching, maximum goodness-of-fit, also known as minimum distance, maximum product of spacing, also called maximum spacing, and minimum quantile distance, which can also be called maximum quantile goodness-of-fit. Fit of univariate distributions for non-censored data using maximum product of spacing estimation and minimum quantile distance estimation is also included."}, "scModels": {"categories": ["Distributions"], "description": "Provides functions for fitting discrete distribution models to count data.\n  Included are the Poisson, the negative binomial, the Poisson-inverse gaussian and, most importantly,\n  a new implementation of the Poisson-beta distribution (density, distribution and quantile\n  functions, and random number generator) together with a needed new implementation of\n  Kummer's function (also: confluent hypergeometric function of the first kind). Three\n  different implementations of the Gillespie algorithm allow data simulation based on the\n  basic, switching or bursting mRNA generating processes. Moreover, likelihood functions for\n  four variants of each of the three aforementioned distributions are also available.\n  The variants include one population and two population mixtures, both with and without\n  zero-inflation. The package depends on the 'MPFR' libraries (<https://www.mpfr.org/>) which need to be installed separately \n  (see description at <https://github.com/fuchslab/scModels>).\n  This package is supplement to the paper \"A mechanistic model for the negative binomial distribution of single-cell mRNA counts\" \n  by Lisa Amrhein, Kumar Harsha and Christiane Fuchs (2019) <doi:10.1101/657619> available on bioRxiv."}, "segmented": {"categories": ["Econometrics", "Environmetrics"], "description": "Given a regression model, segmented \u2018updates\u2019 it by adding one or more segmented \n  (i.e., piece-wise linear) relationships. Several variables with multiple breakpoints are allowed. The estimation method is discussed in Muggeo (2003, <doi:10.1002/sim.1545>) and \n  illustrated in Muggeo (2008, <https://www.r-project.org/doc/Rnews/Rnews_2008-1.pdf>). An approach for hypothesis testing is presented \n  in Muggeo (2016, <doi:10.1080/00949655.2016.1149855>), and interval estimation for the breakpoint is discussed in Muggeo (2017, <doi:10.1111/anzs.12200>). \n  Segmented mixed models, i.e. random effects in the change point, are discussed in\n  in Muggeo (2014, <doi:10.1177/1471082X13504721>)."}, "frenchdata": {"categories": ["Finance"], "description": "Download data sets from Kenneth's French finance data library site <http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html>, reads all the data subsets from the file. Allows R users to collect the data as\n    'tidyverse'-ready data frames."}, "markovchain": {"categories": ["Finance"], "description": "Functions and S4 methods to create and manage discrete time Markov\n    chains more easily. In addition functions to perform statistical (fitting\n    and drawing random variates) and probabilistic (analysis of their structural\n    proprieties) analysis are provided. See Spedicato (2017) <doi:10.32614/RJ-2017-036>."}, "TAQMNGR": {"categories": ["Finance"], "description": "Manager of tick-by-tick transaction data that performs 'cleaning', 'aggregation' and 'import' in an efficient and fast way. The package engine, written in C++, exploits the 'zlib' and 'gzstream' libraries to handle gzipped data without need to uncompress them. 'Cleaning' and 'aggregation' are performed according to Brownlees and Gallo (2006) <doi:10.1016/j.csda.2006.09.030>. Currently, TAQMNGR processes raw data from WRDS (Wharton Research Data Service, <https://wrds-web.wharton.upenn.edu/wrds/>)."}, "mfbvar": {"categories": ["TimeSeries"], "description": "Functions and tools for estimation of mixed-frequency Bayesian vector autoregressive (VAR) models. The package implements a state space-based VAR model that handles mixed frequencies of the data as proposed by Schorfheide and Song (2015) <doi:10.1080/07350015.2014.954707>, and extensions thereof developed by Ankargren, Unosson and Yang (2020) <doi:10.1515/jtse-2018-0034>, Ankargren and Joneus (2019) <arXiv:1912.02231>, and Ankargren and Joneus (2020) <doi:10.1016/j.ecosta.2020.05.007>. The models are estimated using Markov Chain Monte Carlo to numerically approximate the posterior distribution. Prior distributions that can be used include normal-inverse Wishart and normal-diffuse priors as well as steady-state priors. Stochastic volatility can be handled by common or factor stochastic volatility models."}, "MBESS": {"categories": ["Psychometrics"], "description": "Implements methods that are useful in designing research studies and analyzing data, with \n\tparticular emphasis on methods that are developed for or used within the behavioral, \n\teducational, and social sciences (broadly defined). That being said, many of the methods \n\timplemented within MBESS are applicable to a wide variety of disciplines. MBESS has a \n\tsuite of functions for a variety of related topics, such as effect sizes, confidence intervals \n\tfor effect sizes (including standardized effect sizes and noncentral effect sizes), sample size\n\tplanning (from the accuracy in parameter estimation [AIPE], power analytic, equivalence, and \n\tminimum-risk point estimation perspectives), mediation analysis, various properties of \n\tdistributions, and a variety of utility functions. MBESS (pronounced 'em-bes') was originally \n\tan acronym for 'Methods for the Behavioral, Educational, and Social Sciences,' but MBESS became\n\tmore general and now contains methods applicable and used in a wide variety of fields and is an \n\torphan acronym, in the sense that what was an acronym is now literally its name. MBESS has \n\tgreatly benefited from others, see <https://www3.nd.edu/~kkelley/site/MBESS.html> for a detailed \n\tlist of those that have contributed and other details."}, "MixSim": {"categories": ["Cluster"], "description": "The utility of this package is in simulating mixtures of Gaussian\n        distributions with different levels of overlap between mixture\n        components.  Pairwise overlap, defined as a sum of two\n        misclassification probabilities, measures the degree of\n        interaction between components and can be readily employed to\n        control the clustering complexity of datasets simulated from\n        mixtures. These datasets can then be used for systematic\n        performance investigation of clustering and finite mixture\n        modeling algorithms. Among other capabilities of 'MixSim', there\n        are computing the exact overlap for Gaussian mixtures,\n        simulating Gaussian and non-Gaussian data, simulating outliers\n        and noise variables, calculating various measures of agreement\n        between two partitionings, and constructing parallel\n        distribution plots for the graphical display of finite mixture\n        models."}, "parameters": {"categories": ["Distributions"], "description": "Utilities for processing the parameters of various\n    statistical models. Beyond computing p values, CIs, and other indices\n    for a wide variety of models (see list of supported models using the\n    function 'insight::supported_models()'), this package implements\n    features like bootstrapping or simulating of parameters and models,\n    feature reduction (feature extraction and variable selection) as well\n    as functions to describe data and variable characteristics (e.g.\n    skewness, kurtosis, smoothness or distribution)."}, "netmeta": {"categories": ["MetaAnalysis"], "description": "A comprehensive set of functions providing frequentist methods for network meta-analysis and supporting Schwarzer et al. (2015) <doi:10.1007/978-3-319-21416-0>, Chapter 8 \"Network Meta-Analysis\":\n - frequentist network meta-analysis following R\u00fccker (2012) <doi:10.1002/jrsm.1058>;\n - net heat plot and design-based decomposition of Cochran's Q according to Krahn et al. (2013) <doi:10.1186/1471-2288-13-35>;\n - measures characterizing the flow of evidence between two treatments by K\u00f6nig et al. (2013) <doi:10.1002/sim.6001>;\n - ranking of treatments (frequentist analogue of SUCRA) according to R\u00fccker & Schwarzer (2015) <doi:10.1186/s12874-015-0060-8>;\n - partial order of treatment rankings ('poset') and Hasse diagram for 'poset' (Carlsen & Bruggemann, 2014) <doi:10.1002/cem.2569>; (R\u00fccker & Schwarzer, 2017) <doi:10.1002/jrsm.1270>;\n - split direct and indirect evidence to check consistency (Dias et al., 2010) <doi:10.1002/sim.3767>, (Efthimiou et al., 2019) <doi:10.1002/sim.8158>;\n - league table with network meta-analysis results;\n - additive network meta-analysis for combinations of treatments (R\u00fccker et al., 2020) <doi:10.1002/bimj.201800167>;\n - network meta-analysis of binary data using the Mantel-Haenszel or non-central hypergeometric distribution method (Efthimiou et al., 2019) <doi:10.1002/sim.8158>;\n - 'comparison-adjusted' funnel plot (Chaimani & Salanti, 2012) <doi:10.1002/jrsm.57>;\n - automated drawing of network graphs described in R\u00fccker & Schwarzer (2016) <doi:10.1002/jrsm.1143>;\n - rankograms and ranking by SUCRA;\n - contribution matrix as described in Papakonstantinou et al. (2018) <doi:10.12688/f1000research.14770.3> and Davies et al. (2021) <arXiv:2107.02886>."}, "ggplot2": {"categories": ["Spatial", "TeachingStatistics"], "description": "A system for 'declaratively' creating graphics,\n    based on \"The Grammar of Graphics\". You provide the data, tell 'ggplot2'\n    how to map variables to aesthetics, what graphical primitives to use,\n    and it takes care of the details."}, "vegan": {"categories": ["Environmetrics", "Psychometrics", "Spatial"], "description": "Ordination methods, diversity analysis and other\n  functions for community and vegetation ecologists."}, "rgbif": {"categories": ["Spatial"], "description": "A programmatic interface to the Web Service methods\n    provided by the Global Biodiversity Information Facility (GBIF;\n    <https://www.gbif.org/developer/summary>). GBIF is a database\n    of species occurrence records from sources all over the globe.\n    rgbif includes functions for searching for taxonomic names,\n    retrieving information on data providers, getting species occurrence\n    records, getting counts of occurrence records, and using the GBIF\n    tile map service to make rasters summarizing huge amounts of data."}, "runjags": {"categories": ["Bayesian"], "description": "User-friendly interface utilities for MCMC models via\n    Just Another Gibbs Sampler (JAGS), facilitating the use of parallel\n    (or distributed) processors for multiple chains, automated control\n    of convergence and sample length diagnostics, and evaluation of the\n    performance of a model using drop-k validation or against simulated\n    data. Template model specifications can be generated using a standard\n    lme4-style formula interface to assist users less familiar with the\n    BUGS syntax.  A JAGS extension module provides additional distributions\n    including the Pareto family of distributions, the DuMouchel prior and\n    the half-Cauchy prior."}, "Davies": {"categories": ["Distributions"], "description": "Various utilities for the Davies distribution."}, "RMixtComp": {"categories": ["MissingData"], "description": "Mixture Composer <https://github.com/modal-inria/MixtComp> is a project to build mixture models with\n    heterogeneous data sets and partially missing data management. \n    It includes 8 models for real, categorical, counting, functional and ranking data."}, "evalITR": {"categories": ["CausalInference"], "description": "Provides various statistical methods for evaluating Individualized Treatment Rules under randomized data. The provided metrics include Population Average Value (PAV), Population Average Prescription Effect (PAPE), Area Under Prescription Effect Curve (AUPEC). It also provides the tools to analyze Individualized Treatment Rules under budget constraints. Detailed reference in Imai and Li (2019) <arXiv:1905.05389>."}, "rpostgis": {"categories": ["Databases", "Spatial"], "description": "Provides an interface between R and 'PostGIS'-enabled\n    'PostgreSQL' databases to transparently transfer spatial\n    data. Both vector (points, lines, polygons) and raster data are\n    supported in read and write modes. Also provides convenience\n    functions to execute common procedures in 'PostgreSQL/PostGIS'."}, "RInside": {"categories": ["HighPerformanceComputing"], "description": "C++ classes to embed R in C++ (and C) applications\n A C++ class providing the R interpreter is offered by this package\n making it easier to have \"R inside\" your C++ application. As R itself\n is embedded into your application, a shared library build of R is\n required. This works on Linux, OS X and even on Windows provided you\n use the same tools used to build R itself. Numerous examples are\n provided in the nine subdirectories of the examples/ directory of\n the installed package: standard, 'mpi' (for parallel computing), 'qt'\n (showing how to embed 'RInside' inside a Qt GUI application), 'wt'\n (showing how to build a \"web-application\" using the Wt toolkit),\n 'armadillo' (for 'RInside' use with 'RcppArmadillo'), 'eigen' (for\n 'RInside' use with 'RcppEigen'), and 'c_interface' for a basic C\n interface and 'Ruby' illustration.  The examples use 'GNUmakefile(s)'\n with GNU extensions, so a GNU make is required (and will use the\n 'GNUmakefile' automatically). 'Doxygen'-generated documentation of\n the C++ classes is available at the 'RInside' website as well."}, "BayesianFROC": {"categories": ["Bayesian"], "description": "Provides new methods for the so-called Free-response Receiver Operating Characteristic (FROC) analysis. The ultimate aim of FROC analysis is to compare observer performances, which means comparing characteristics, such as area under the curve (AUC) or figure of merit (FOM). In this package, we only use the notion of AUC for modality comparison, where by \"modality\",  we mean imaging methods such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET), ..., etc. So there is a problem that which imaging method is better to detect lesions from shadows in radiographs. To solve modality comparison issues, this package provides new methods using hierarchical Bayesian models proposed by the author of this package. Using this package, one can obtain at least one conclusion that which imaging methods are better for finding lesions in radiographs with the case of your data. Fitting FROC statistical models is sometimes not so good, it can easily confirm by drawing FROC curves and comparing these curves and the points constructed by False Positive fractions (FPFs) and True Positive Fractions (TPFs), we can validate the goodness of fit intuitively. Such validation is also implemented by the Chi square goodness of fit statistics in the Bayesian context which means that the parameter is not deterministic, thus by integrating it with the posterior predictive measure, we get a desired value. To compare modalities (imaging methods: MRI, CT, PET, ... , etc),  we evaluate AUCs for each modality. FROC is developed by Dev Chakraborty, his FROC model in his 1989 paper relies on the maximal likelihood methodology. The author modified and provided the alternative Bayesian FROC model. Strictly speaking, his model does not coincide with models in this package. In FROC context, we means by multiple reader and multiple case (MRMC) the case of the number of reader or modality is two or more. The MRMC data is available for functions of this package. I hope that medical researchers use not only the frequentist method but also alternative Bayesian methods. In medical research, many problems are considered under only frequentist methods, such as the notion of p-values. But p-value is sometimes misunderstood. Bayesian methods provide very simple, direct, intuitive answer for research questions. Combining frequentist methods with Bayesian methods, we can obtain more reliable answer for research questions. References: Dev Chakraborty (1989) Maximum likelihood analysis of free - response receiver operating characteristic (FROC) data. "}, "BayesMallows": {"categories": ["Bayesian", "MissingData"], "description": "An implementation of the Bayesian version of the Mallows rank model\n    (Vitelli et al., Journal of Machine Learning Research, 2018 <https://jmlr.org/papers/v18/15-481.html>;\n    Crispino et al., Annals of Applied Statistics, 2019 <doi:10.1214/18-AOAS1203>). Both Metropolis-Hastings \n    and sequential Monte Carlo algorithms for estimating the models are available. Cayley, footrule,\n    Hamming, Kendall, Spearman, and Ulam distances are supported in the models. The rank data to be\n    analyzed can be in the form of complete rankings, top-k rankings, partially missing rankings, as well\n    as consistent and inconsistent pairwise preferences. Several functions for plotting and studying the\n    posterior distributions of parameters are provided. The package also provides functions for estimating\n    the partition function (normalizing constant) of the Mallows rank model, both with the importance\n    sampling algorithm of Vitelli et al. and asymptotic approximation with the IPFP algorithm\n    (Mukherjee, Annals of Statistics, 2016 <doi:10.1214/15-AOS1389>)."}, "censusapi": {"categories": ["OfficialStatistics"], "description": "A wrapper for the U.S. Census Bureau APIs that returns data frames of \n\tCensus data and metadata. Available datasets include the \n\tDecennial Census, American Community Survey, Small Area Health Insurance Estimates,\n\tSmall Area Income and Poverty Estimates, Population Estimates and Projections, and more."}, "fixest": {"categories": ["CausalInference", "Econometrics"], "description": "Fast and user-friendly estimation of econometric models with multiple fixed-effects. Includes ordinary least squares (OLS), generalized linear models (GLM) and the negative binomial.\n    The core of the package is based on optimized parallel C++ code, scaling especially well for large data sets. The method to obtain the fixed-effects coefficients is based on Berge (2018) <https://wwwen.uni.lu/content/download/110162/1299525/file/2018_13>.\n    Further provides tools to export and view the results of several estimations with intuitive design to cluster the standard-errors."}, "PGM2": {"categories": ["ExperimentalDesign"], "description": "Construction method of nested resolvable designs from \n    a projective geometry defined on Galois field of order 2. The obtained\n    Resolvable designs are used to build uniform design. The presented results\n    are based on <https://eudml.org/doc/219563> and A. Boudraa et al. (See references)."}, "doFuture": {"categories": ["HighPerformanceComputing"], "description": "Provides a '%dopar%' adapter such that any type of futures can\n    be used as backends for the 'foreach' framework."}, "pdc": {"categories": ["TimeSeries"], "description": "Permutation Distribution Clustering is a clustering method for time series. Dissimilarity of time series is formalized as the divergence between their permutation distributions. The permutation distribution was proposed as measure of the complexity of a time series."}, "RcppBigIntAlgos": {"categories": ["NumericalMathematics"], "description": "Features the multiple polynomial quadratic sieve (MPQS) algorithm\n    for factoring large integers and a vectorized factoring function that\n    returns the complete factorization of an integer. The MPQS is based off of\n    the seminal work of Carl Pomerance (1984) <doi:10.1007/3-540-39757-4_17>\n    along with the modification of multiple polynomials introduced by Peter\n    Montgomery and J. Davis as outlined by Robert D. Silverman (1987)\n    <doi:10.1090/S0025-5718-1987-0866119-8>. Utilizes the C library\n    GMP (GNU Multiple Precision Arithmetic) and 'RcppThread' for factoring\n    integers in parallel. For smaller integers, a simple Elliptic\n    Curve algorithm is attempted followed by a constrained version of \n    Pollard's rho algorithm. The Pollard's rho algorithm is the same algorithm\n    used by the factorize function in the 'gmp' package."}, "marg": {"categories": ["Distributions"], "description": "Likelihood inference based on higher order approximations \n  for linear nonnormal regression models."}, "usethis": {"categories": ["ReproducibleResearch"], "description": "Automate package and project setup tasks that are\n    otherwise performed manually. This includes setting up unit testing,\n    test coverage, continuous integration, Git, 'GitHub', licenses,\n    'Rcpp', 'RStudio' projects, and more."}, "dygraphs": {"categories": ["TimeSeries"], "description": "An R interface to the 'dygraphs' JavaScript charting library\n    (a copy of which is included in the package). Provides rich facilities\n    for charting time-series data in R, including highly configurable\n    series- and axis-display and interactive features like zoom/pan and\n    series/point highlighting."}, "semds": {"categories": ["Psychometrics"], "description": "Fits a structural equation multidimensional scaling (SEMDS) model for asymmetric and three-way input dissimilarities. It assumes that the dissimilarities are measured with errors. The latent dissimilarities are estimated as factor scores within an SEM framework while the objects are represented in a low-dimensional space as in MDS. "}, "BayesBP": {"categories": ["Bayesian"], "description": "Smoothed lexis diagrams with Bayesian method specifically tailored to cancer \n             incidence data. Providing to calculating slope and constructing credible interval.\n             LC Chien et al. (2015) <doi:10.1080/01621459.2015.1042106>. \n             LH Chien et al. (2017) <doi:10.1002/cam4.1102>."}, "ggmcmc": {"categories": ["Bayesian"], "description": "Tools for assessing and diagnosing convergence of\n    Markov Chain Monte Carlo simulations, as well as for graphically display\n    results from full MCMC analysis. The package also facilitates the graphical\n    interpretation of models by providing flexible functions to plot the\n    results against observed variables, and functions to work with\n    hierarchical/multilevel batches of parameters\n    (Fern\u00e1ndez-i-Mar\u00edn, 2016 <doi:10.18637/jss.v070.i09>)."}, "funLBM": {"categories": ["FunctionalData"], "description": "The funLBM algorithm allows to simultaneously cluster the rows and the columns of a data matrix where each entry of the matrix is a function or a time series."}, "panelaggregation": {"categories": ["OfficialStatistics"], "description": "Aggregate Business Tendency Survey Data (and other qualitative\n    surveys) to time series at various aggregation levels. Run aggregation of\n    survey data in a speedy, re-traceable and a easily deployable way.\n    Aggregation is substantially accelerated by use of data.table.\n    This package intends to provide an interface that is less general and abstract than data.table but rather geared towards\n    survey researchers."}, "forecastHybrid": {"categories": ["TimeSeries"], "description": "Convenient functions for ensemble forecasts in R combining\n    approaches from the 'forecast' package. Forecasts generated from auto.arima(), ets(),\n    thetaf(), nnetar(), stlm(), tbats(), and snaive() can be combined with equal weights, weights\n    based on in-sample errors (introduced by Bates & Granger (1969) <doi:10.1057/jors.1969.103>),\n    or cross-validated weights. Cross validation for time series data with user-supplied models\n    and forecasting functions is also supported to evaluate model accuracy."}, "rts": {"categories": ["TimeSeries"], "description": "This framework aims to provide classes and methods for manipulating and processing of raster time series data (e.g. a time series of satellite images)."}, "js": {"categories": ["WebTechnologies"], "description": "A set of utilities for working with JavaScript syntax in R.\n    Includes tools to parse, tokenize, compile, validate, reformat, optimize \n    and analyze JavaScript code."}, "BLR": {"categories": ["Bayesian"], "description": "Bayesian Linear Regression."}, "GEOmap": {"categories": ["Spatial"], "description": "Set of routines for making map projections (forward and inverse), topographic maps, perspective plots, geological maps, geological map symbols, geological databases, interactive plotting and selection of focus regions."}, "lfe": {"categories": ["Econometrics"], "description": "Transforms away factors with many levels prior to doing an OLS.\n  Useful for estimating linear models with multiple group fixed effects, and for\n  estimating linear models which uses factors with many levels as pure control variables. See Gaure (2013) <doi:10.1016/j.csda.2013.03.024>\n  Includes support for instrumental variables, conditional F statistics for weak instruments,\n  robust and multi-way clustered standard errors, as well as limited mobility bias correction (Gaure 2014 <doi:10.1002/sta4.68>). \n  WARNING: This package is NOT under active development anymore, no further improvements are to be expected, and the package is at risk of being removed from CRAN."}, "glasso": {"categories": ["Psychometrics"], "description": "Estimation of a sparse inverse covariance matrix using a lasso (L1)\n             penalty. Facilities are provided for estimates along a path of values\n\t     for the regularization parameter."}, "cold": {"categories": ["MissingData"], "description": "Performs regression analysis for longitudinal count data,  \n   allowing for serial dependence among observations from a given \n   individual and two dimensional random effects on the linear predictor. \n   Estimation is via maximization of the exact likelihood of a suitably \n   defined model. Missing values and unbalanced data are allowed. \n   Details can be found in the accompanying scientific papers: \n   Goncalves & Cabral (2021, Journal of Statistical Software, \n   <doi:10.18637/jss.v099.i03>) and Goncalves et al. \n   (2007, Computational Statistics & Data Analysis, \n   <doi:10.1016/j.csda.2007.03.002>)."}, "Rglpk": {"categories": ["Optimization"], "description": "R interface to the GNU Linear Programming Kit.\n  'GLPK' is open source software for solving large-scale linear programming (LP),\n  mixed integer linear programming ('MILP') and other related problems."}, "geospt": {"categories": ["ExperimentalDesign", "Spatial"], "description": "Estimation of the variogram through trimmed mean, radial basis \n        functions (optimization, prediction and cross-validation), summary\n        statistics from cross-validation, pocket plot, and design of\n        optimal sampling networks through sequential and simultaneous\n        points methods."}, "RobustRankAggreg": {"categories": ["MetaAnalysis"], "description": "Methods for aggregating ranked lists, especially lists of\n    genes. It implements the Robust Rank Aggregation Kolde et al (2012)\n    <doi:10.1093/bioinformatics/btr709> and some other simple algorithms \n    for the task. RRA method uses a probabilistic model for aggregation that \n    is robust to noise and also facilitates the calculation of significance\n    probabilities for all the elements in the final ranking."}, "SimSCRPiecewise": {"categories": ["Survival"], "description": "Contains two functions for simulating survival data from piecewise exponential hazards with a proportional hazards adjustment for covariates. The first function SimUNIVPiecewise simulates univariate survival data based on a piecewise exponential hazard, covariate matrix and true regression vector. The second function SimSCRPiecewise semi-competing risks data based on three piecewise exponential hazards, three true regression vectors and three matrices of patient covariates (which can be different or the same). This simulates from the Semi-Markov model of Lee et al (2015) given patient covariates, regression parameters, patient frailties and baseline hazard functions."}, "distTails": {"categories": ["Distributions"], "description": "A full definition for Weibull tails and Full-Tails Gamma and tools for fitting these distributions to empirical tails. This package build upon the paper by del Castillo, Joan & Daoudi, Jalila & Serra, Isabel. (2012) <doi:10.1017/asb.2017.9>."}, "edmdata": {"categories": ["Psychometrics"], "description": "Collection of data sets from various assessments that can be used to \n    evaluate psychometric models. These data sets have been analyzed in the\n    following papers that introduced new methodology as part of the application section:\n    Yinghan Chen et al. (2021) <doi:10.1007/s11336-021-09750-9>,\n    Yinyin Chen et al. (2020) <doi:10.1007/s11336-019-09693-2>,\n    Culpepper, S. A. (2019a) <doi:10.1007/s11336-019-09683-4>,\n    Culpepper, S. A. (2019b) <doi:10.1007/s11336-018-9643-8>,\n    Culpepper, S. A., & Chen, Y. (2019) <doi:10.3102/1076998618791306>,\n    Culpepper, S. A., & Balamuta, J. J. (2017) <doi:10.1007/s11336-015-9484-7>,\n    and Culpepper, S. A. (2015) <doi:10.3102/1076998615595403>."}, "puniform": {"categories": ["MetaAnalysis"], "description": "Provides meta-analysis methods that correct for\n    publication bias and outcome reporting bias. Four methods and a visual tool \n    are currently included in the package. The p-uniform method as described in \n    van Assen, van Aert, and Wicherts (2015) <https:psycnet.apa.org/record/2014-48759-001> \n    can be used for estimating the average effect size, testing the null hypothesis \n    of no effect, and testing for publication bias using only the statistically \n    significant effect sizes of primary studies. The second method in the package \n    is the p-uniform* method as described in van Aert and van Assen (2019) \n    <doi:10.31222/osf.io/zqjr9>. This method is an extension of the p-uniform \n    method that allows for estimation of the average effect size and the \n    between-study variance in a meta-analysis, and uses both the statistically \n    significant and nonsignificant effect sizes. The third method in the package \n    is the hybrid method as described in van Aert and van Assen (2017) \n    <doi:10.3758/s13428-017-0967-6>. The hybrid method is a meta-analysis method \n    for combining an original study and replication and while taking into account \n    statistical significance of the  original study. The p-uniform and hybrid method \n    are based on the statistical theory that the distribution of p-values is \n    uniform conditional on the population effect size. The fourth method in the \n    package is the Snapshot Bayesian Hybrid Meta-Analysis Method as described in \n    van Aert and van Assen (2018) <doi:10.1371/journal.pone.0175302>. This method \n    computes posterior probabilities for four true effect sizes (no, small, medium, \n    and large) based on an original study and replication while taking into account \n    publication bias in the original study. The method can also be used for computing \n    the required sample size of the replication akin to power analysis in null \n    hypothesis significance testing. The meta-plot is a visual tool for meta-analysis \n    that provides information on the primary studies in the meta-analysis, the \n    results of the meta-analysis, and characteristics of the research on the effect \n    under study (van Assen et al., 2021). Helper functions to apply the \n    Correcting for Outcome Reporting Bias (CORB) method to correct for outcome \n    reporting bias in a meta-analysis (van Aert & Wicherts, 2021)."}, "ph2bye": {"categories": ["ExperimentalDesign"], "description": "Calculate the Bayesian posterior/predictive probability and\n    determine the sample size and stopping boundaries for single-arm Phase II design."}, "PhysicalActivity": {"categories": ["Tracking"], "description": "It provides a function \"wearingMarking\" for classification of monitor\n    wear and nonwear time intervals in accelerometer data collected to assess\n    physical activity. The package also contains functions for making plot for \n    accelerometer data and obtaining the summary of various information including \n    daily monitor wear time and the mean monitor wear time during valid days.      \n    \"deliveryPred\" and \"markDelivery\" can classify days for ActiGraph delivery by mail;\n    \"deliveryPreprocess\" can process accelerometry data for analysis by zeropadding incomplete \n    days and removing low activity days; \"markPAI\" can categorize physical activity\n    intensity level based on user-defined cut-points of accelerometer counts. It also\n    supports importing ActiGraph AGD files with \"readActigraph\" and \"queryActigraph\" functions."}, "bayesDP": {"categories": ["Bayesian"], "description": "Functions for data augmentation using the Bayesian discount prior \n    method for single arm and two-arm clinical trials, as described in Haddad \n    et al. (2017) <doi:10.1080/10543406.2017.1300907>. The discount power prior \n    methodology was developed in collaboration with the The Medical Device \n    Innovation Consortium (MDIC) Computer Modeling & Simulation Working Group."}, "RgoogleMaps": {"categories": ["Spatial", "WebTechnologies"], "description": "Serves two purposes: (i) Provide a\n        comfortable R interface to query the Google server for static\n        maps, and (ii) Use the map as a background image to overlay\n        plots within R. This requires proper coordinate scaling."}, "ESG": {"categories": ["Finance"], "description": "Presents a \"Scenarios\" class containing\n        general parameters, risk parameters and projection results.\n        Risk parameters are gathered together into a ParamsScenarios\n        sub-object. The general process for using this package is to\n        set all needed parameters in a Scenarios object, use the\n        customPathsGeneration method to proceed to the projection, then\n        use xxx_PriceDistribution() methods to get asset prices."}, "climdex.pcic": {"categories": ["Hydrology"], "description": "PCIC's implementation of Climdex routines for computation of\n    extreme climate indices. Further details on the extreme climate indices\n    can be found at <http://etccdi.pacificclimate.org/list_27_indices.shtml>\n    and in the package manual."}, "beyondWhittle": {"categories": ["TimeSeries"], "description": "Implementations of Bayesian parametric, nonparametric and semiparametric procedures for univariate and multivariate time series. The package is based on the methods presented in C. Kirch et al (2018) <doi:10.1214/18-BA1126> and A. Meier (2018) <https://opendata.uni-halle.de//handle/1981185920/13470>. It was supported by DFG grant KI 1443/3-1."}, "jackknifeKME": {"categories": ["Survival"], "description": "Computing the original and modified jackknife estimates of Kaplan-Meier estimators."}, "kmi": {"categories": ["MissingData", "Survival"], "description": "Performs a Kaplan-Meier multiple imputation to recover the missing potential censoring information from competing risks events, so that standard right-censored methods could be applied to the imputed data sets to perform analyses of the cumulative incidence functions (Allignol and Beyersmann, 2010 <doi:10.1093/biostatistics/kxq018>)."}, "simputation": {"categories": ["MissingData", "OfficialStatistics"], "description": "Easy to use interfaces to a number of imputation methods\n        that fit in the not-a-pipe operator of the 'magrittr' package."}, "ltsa": {"categories": ["TimeSeries"], "description": "Methods of developing linear time series modelling.\n Methods are given for loglikelihood computation, forecasting\n  and simulation."}, "PortfolioEffectHFT": {"categories": ["Finance"], "description": "R interface to PortfolioEffect cloud service for backtesting\n    high frequency trading (HFT) strategies, intraday portfolio analysis\n    and optimization. Includes auto-calibrating model pipeline for market\n    microstructure noise, risk factors, price jumps/outliers, tail risk\n    (high-order moments) and price fractality (long memory). Constructed\n    portfolios could use client-side market data or access HF intraday price\n    history for all major US Equities. See <https://www.portfolioeffect.com/>\n    for more information on the PortfolioEffect high frequency portfolio\n    analytics platform."}, "gRbase": {"categories": ["GraphicalModels"], "description": "The 'gRbase' package provides graphical modelling features\n    used by e.g. the packages 'gRain', 'gRim' and 'gRc'. 'gRbase' implements\n    graph algorithms including (i) maximum cardinality search (for marked\n    and unmarked graphs).\n    (ii) moralization, (iii) triangulation, (iv) creation of junction tree.\n    'gRbase' facilitates array operations,\n    'gRbase' implements functions for testing for conditional independence.\n    'gRbase' illustrates how hierarchical log-linear models may be\n    implemented and describes concept of graphical meta\n    data. \n    The facilities of the package are documented in the book by H\u00f8jsgaard,\n    Edwards and Lauritzen (2012,\n    <doi:10.1007/978-1-4614-2299-0>) and in the paper by \n    Dethlefsen and H\u00f8jsgaard, (2005, <doi:10.18637/jss.v014.i17>).\n    Please see 'citation(\"gRbase\")' for citation details. \n    NOTICE  'gRbase' requires that the packages graph,\n    'Rgraphviz' and 'RBGL' are installed from 'bioconductor'; for\n    installation instructions please refer to the web page given below."}, "bridgesampling": {"categories": ["Bayesian"], "description": "Provides functions for estimating marginal likelihoods, Bayes\n    factors, posterior model probabilities, and normalizing constants in general,\n    via different versions of bridge sampling (Meng & Wong, 1996, \n    <http://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).\n    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>."}, "dmri.tracking": {"categories": ["MedicalImaging"], "description": "It provides functions to apply the deterministic tracking algorithm - DiST (Wong et al 2016) <doi:10.1214/15-AOAS880> and to plot tractography results."}, "waveslim": {"categories": ["Finance", "TimeSeries"], "description": "Basic wavelet routines for time series (1D), image (2D) \n  and array (3D) analysis.  The code provided here is based on\n  wavelet methodology developed in Percival and Walden (2000);\n  Gencay, Selcuk and Whitcher (2001); the dual-tree complex wavelet\n  transform (DTCWT) from Kingsbury (1999, 2001) as implemented by\n  Selesnick; and Hilbert wavelet pairs (Selesnick 2001, 2002).  All\n  figures in chapters 4-7 of GSW (2001) are reproducible using this \n  package and R code available at the book website(s) below."}, "spBayesSurv": {"categories": ["Spatial", "Survival"], "description": "Provides several Bayesian survival models for spatial/non-spatial survival data: proportional hazards (PH), accelerated failure time (AFT), proportional odds (PO), and accelerated hazards (AH), a super model that includes PH, AFT, PO and AH as special cases, Bayesian nonparametric nonproportional hazards (LDDPM), generalized accelerated failure time (GAFT), and spatially smoothed Polya tree density estimation. The spatial dependence is modeled via frailties under PH, AFT, PO, AH and GAFT, and via copulas under LDDPM and PH. Model choice is carried out via the logarithm of the pseudo marginal likelihood (LPML), the deviance information criterion (DIC), and the Watanabe-Akaike information criterion (WAIC). See Zhou, Hanson and Zhang (2020) <doi:10.18637/jss.v092.i09>. "}, "som": {"categories": ["ChemPhys", "Cluster"], "description": "Self-Organizing Map (with application in gene clustering)."}, "GSE": {"categories": ["MissingData", "Robust"], "description": "Robust Estimation of Multivariate Location and Scatter in the\n        Presence of Cellwise and Casewise Contamination and Missing Data."}, "fbRads": {"categories": ["WebTechnologies"], "description": "Wrapper functions around the Facebook Marketing 'API' to create, read, update and delete custom audiences, images, campaigns, ad sets, ads and related content."}, "bayesLife": {"categories": ["Bayesian"], "description": "Making probabilistic projections of life expectancy for all countries of the world, using a Bayesian hierarchical model <doi:10.1007/s13524-012-0193-x>. Subnational projections are also supported."}, "spikeslab": {"categories": ["Bayesian"], "description": "Spike and slab for prediction and variable selection in linear regression models. Uses a generalized elastic net for variable selection."}, "footballpenaltiesBL": {"categories": ["SportsAnalytics"], "description": "Basic analysis of all penalties taken in the German men's Bundesliga\n  between the start of its inaugural season and May 2017. The main functions are\n  suitable printing and plotting functions. Flexible selection of a player is\n  supported via grep. Missed penalties can easily be included or excluded, depending\n  on the user's wishes."}, "SGL": {"categories": ["Survival"], "description": "Fit a regularized generalized linear model via penalized\n        maximum likelihood.  The model is fit for a path of values of\n        the penalty parameter. Fits linear, logistic and Cox models."}, "curl": {"categories": ["WebTechnologies"], "description": "The curl() and curl_download() functions provide highly\n    configurable drop-in replacements for base url() and download.file() with\n    better performance, support for encryption (https, ftps), gzip compression,\n    authentication, and other 'libcurl' goodies. The core of the package implements a\n    framework for performing fully customized requests where data can be processed\n    either in memory, on disk, or streaming via the callback or connection\n    interfaces. Some knowledge of 'libcurl' is recommended; for a more-user-friendly\n    web client see the 'httr' package which builds on this package with http\n    specific tools and logic."}, "endoSwitch": {"categories": ["CausalInference"], "description": "Maximum likelihood estimation of endogenous switching regression models from Heckman (1979) <doi:10.2307/1912352> and estimation of treatment effects.  "}, "ichimoku": {"categories": ["Finance"], "description": "An implementation of 'Ichimoku Kinko Hyo', also commonly known as\n    'cloud charts'. Static and interactive visualizations with tools for\n    creating, backtesting and development of quantitative 'ichimoku' strategies.\n    As described in Sasaki (1996, ISBN:4925152009), the technique is a refinement\n    on candlestick charting, originating from Japan and now in widespread use in\n    technical analysis worldwide. Translating as 'one-glance equilibrium chart',\n    it allows the price action and market structure of financial securities to\n    be determined 'at-a-glance'. Incorporates an interface with the OANDA\n    fxTrade API <https://developer.oanda.com/> for retrieving historical and\n    live streaming price data for major currencies, metals, commodities,\n    government bonds and stock indices."}, "tdROC": {"categories": ["Survival"], "description": "Compute time-dependent ROC curve from censored survival data using\n    nonparametric weight adjustments."}, "ASSA": {"categories": ["TimeSeries"], "description": "Functions to model and decompose time series into principal components using singular spectrum analysis (de Carvalho and Rua (2017) <doi:10.1016/j.ijforecast.2015.09.004>; de Carvalho et al (2012) <doi:10.1016/j.econlet.2011.09.007>)."}, "disaggR": {"categories": ["TimeSeries"], "description": "The twoStepsBenchmark() and threeRuleSmooth() functions allow you to \n    disaggregate a low-frequency time series with higher frequency time series, \n    using the French National Accounts methodology. The aggregated sum of the \n    resulting time series is strictly equal to the low-frequency time series within the \n    benchmarking window. Typically, the low-frequency time series is an annual one, \n    unknown for the last year, and the high frequency one is either quarterly or \n    monthly. See \"Methodology of quarterly national accounts\", Insee M\u00e9thodes \n    N\u00b0126, by Insee (2012, ISBN:978-2-11-068613-8, <https://www.insee.fr/en/information/2579410>)."}, "BayesFactor": {"categories": ["Bayesian"], "description": "A suite of functions for computing\n    various Bayes factors for simple designs, including contingency tables,\n    one- and two-sample designs, one-way designs, general ANOVA designs, and\n    linear regression."}, "MBSP": {"categories": ["Distributions"], "description": "Gibbs sampler for fitting multivariate Bayesian linear regression with shrinkage priors (MBSP), using the three parameter beta normal family. The method is described in Bai and Ghosh (2018) <doi:10.1016/j.jmva.2018.04.010>. "}, "sna": {"categories": ["Bayesian", "GraphicalModels", "Optimization"], "description": "A range of tools for social network analysis, including node and graph-level indices, structural distance and covariance methods, structural equivalence detection, network regression, random graph generation, and 2D/3D network visualization."}, "arfima": {"categories": ["TimeSeries"], "description": "Simulates, fits, and predicts long-memory and anti-persistent\n\ttime series, possibly mixed with ARMA, regression, transfer-function\n\tcomponents.\n\tExact methods (MLE, forecasting, simulation) are used.\n\tBug reports should be done via GitHub (at\n\t<https://github.com/JQVeenstra/arfima>), where the development version\n\tof this package lives; it can be installed using devtools."}, "xml2": {"categories": ["WebTechnologies"], "description": "Work with XML files using a simple, consistent\n    interface. Built on top of the 'libxml2' C library."}, "trackeR": {"categories": ["SpatioTemporal", "SportsAnalytics", "Tracking"], "description": "Provides infrastructure for handling running, cycling and swimming data from GPS-enabled tracking devices within R. The package provides methods to extract, clean and organise workout and competition data into session-based and unit-aware data objects of class 'trackeRdata' (S3 class). The information can then be visualised, summarised, and analysed through flexible and extensible methods. Frick and Kosmidis (2017) <doi:10.18637/jss.v082.i07>, which is updated and maintained as one of the vignettes, provides detailed descriptions of the package and its methods, and real-data demonstrations of the package functionality."}, "scholar": {"categories": ["WebTechnologies"], "description": "Provides functions to extract citation data from Google\n    Scholar.  Convenience functions are also provided for comparing\n    multiple scholars and predicting future h-index values."}, "modeltime.ensemble": {"categories": ["TimeSeries"], "description": "\n    A 'modeltime' extension that implements time series ensemble forecasting methods including model averaging, \n    weighted averaging, and stacking. These techniques are popular methods \n    to improve forecast accuracy and stability. Refer to papers such as \n    \"Machine-Learning Models for Sales Time Series Forecasting\" Pavlyshenko, B.M. (2019) <doi:10.3390>."}, "SCMA": {"categories": ["MetaAnalysis"], "description": "Perform meta-analysis of single-case experiments, including calculating various effect size measures (SMD, PND, PEM and NAP) and probability combining (additive and multiplicative method), as discussed in Bulte and Onghena (2013) <doi:10.22237/jmasm/1383280020>."}, "simPop": {"categories": ["OfficialStatistics"], "description": "Tools and methods to simulate populations for surveys based\n    on auxiliary data. The tools include model-based methods, calibration and\n    combinatorial optimization algorithms, see Templ, Kowarik and Meindl (2017) <doi:10.18637/jss.v079.i10>) and\n    Templ (2017) <doi:10.1007/978-3-319-50272-4>. The package was developed with support of\n    the International Household Survey Network, DFID Trust Fund TF011722 and funds\n    from the World bank."}, "OneStep": {"categories": ["Distributions"], "description": "Provide principally an eponymic function that numerically computes the Le Cam one-step estimator which is asymptotically efficient (see e.g. L. Le Cam (1956) <https://projecteuclid.org/euclid.bsmsp/1200501652>) and can be computed faster than the maximum likelihood estimator for large observation samples."}, "SingleCaseES": {"categories": ["MetaAnalysis"], "description": "\n  Provides R functions for calculating basic effect size indices for \n  single-case designs, including several non-overlap measures and parametric \n  effect size measures, and for estimating the gradual effects model developed \n  by Swan and Pustejovsky (2018) <doi:10.1080/00273171.2018.1466681>. \n  Standard errors and confidence intervals (based on the assumption that the outcome \n  measurements are mutually independent) are provided for the subset of effect sizes \n  indices with known sampling distributions."}, "SimSurvNMarker": {"categories": ["Survival"], "description": "Provides functions to simulate from joint survival and marker \n    models. The user can specific all basis functions of time, random or \n    deterministic covariates, random or deterministic left-truncation and \n    right-censoring times, and model parameters."}, "CTT": {"categories": ["Psychometrics"], "description": "A collection of common test and item analyses from a classical test theory (CTT) framework. Analyses can be applied to both dichotomous and polytomous data. Functions provide reliability analyses (alpha), item statistics, disctractor analyses, disattenuated correlations, scoring routines, and empirical ICCs."}, "GetTDData": {"categories": ["Finance"], "description": "Downloads and aggregates data for Brazilian government issued bonds directly from the website of Tesouro Direto <https://www.tesourodireto.com.br/>."}, "webutils": {"categories": ["WebTechnologies"], "description": "Parses http request data in application/json, multipart/form-data, \n    or application/x-www-form-urlencoded format. Includes example of hosting\n    and parsing html form data in R using either 'httpuv' or 'Rhttpd'."}, "fuzzyjoin": {"categories": ["OfficialStatistics"], "description": "Join tables together based not on whether columns\n  match exactly, but whether they are similar by some comparison.\n  Implementations include string distance and regular expression\n  matching."}, "elo": {"categories": ["SportsAnalytics"], "description": "A flexible framework for calculating Elo ratings and resulting\n    rankings of any two-team-per-matchup system (chess, sports leagues, 'Go',\n    etc.). This implementation is capable of evaluating a variety of matchups,\n    Elo rating updates, and win probabilities, all based on the basic Elo\n    rating system. It also includes methods to benchmark performance,\n    including logistic regression and Markov chain models."}, "Boruta": {"categories": ["MachineLearning"], "description": "An all relevant feature selection wrapper algorithm.\n It finds relevant features by comparing original attributes' importance with importance achievable at random, estimated using their permuted copies (shadows)."}, "LatticeKrig": {"categories": ["Spatial"], "description": "Methods for the interpolation of large spatial\n  datasets. This package follows a \"fixed rank Kriging\" approach but\n  provides a surface fitting method\n  that can approximate standard spatial data models.\n  Using a large number of basis functions allows for estimates that\n  can come close to interpolating the observations (a spatial model\n  with a small nugget variance.)  Moreover, the covariance model for\n  this method can approximate the Matern covariance family but also\n  allows for a multi-resolution model and supports efficient\n  computation of the profile likelihood for estimating covariance\n  parameters. This is accomplished through compactly supported basis\n  functions and a Markov random field model for the basis\n  coefficients. These features lead to sparse matrices for the\n  computations and this package makes of the R spam package for sparse\n  linear algebra.\n  An extension of this version over previous ones ( < 5.4 ) is the\n  support for different geometries besides a rectangular domain. The\n  Markov random field approach combined with a basis function\n  representation makes the implementation of different geometries\n  simple where only a few specific R functions need to be added with\n  most of the computation and evaluation done by generic routines that\n  have been tuned to be efficient.  One benefit of this package's\n  model/approach is the facility to do unconditional and conditional\n  simulation of the field for large numbers of arbitrary points. There\n  is also the flexibility for estimating non-stationary covariances\n  and also the case when the observations are a linear combination\n  (e.g. an integral) of the spatial process. Included are generic\n  methods for prediction, standard errors for prediction, plotting of\n  the estimated surface and conditional and unconditional simulation.\n  See the 'LatticeKrig' GitHub repository for a vignette of this package.\n  Development of this package was supported in part by the National\n  Science Foundation  Grant 1417857 and the National Center for\n  Atmospheric Research. "}, "DRDID": {"categories": ["CausalInference"], "description": "Implements the locally efficient doubly robust difference-in-differences (DiD)\n    estimators for the average treatment effect proposed by Sant'Anna and Zhao (2020)\n    <doi:10.1016/j.jeconom.2020.06.003>. The estimator combines inverse probability weighting and outcome\n    regression estimators (also implemented in the package) to form estimators with\n    more attractive statistical properties. Two different estimation methods can be used\n    to estimate the nuisance functions."}, "homals": {"categories": ["ChemPhys", "Psychometrics"], "description": "Performs a homogeneity analysis (multiple correspondence analysis) and various extensions. Rank restrictions on the category quantifications can be imposed (nonlinear PCA). The categories are transformed by means of optimal scaling with options for nominal, ordinal, and numerical scale levels (for rank-1 restrictions). Variables can be grouped into sets, in order to emulate regression analysis and canonical correlation analysis. "}, "agricolae": {"categories": ["Distributions", "ExperimentalDesign"], "description": "Original idea was presented in the thesis \"A statistical analysis tool for agricultural research\" to obtain the degree of Master on science, National Engineering University (UNI), Lima-Peru. Some experimental data for the examples come from the CIP and others research. Agricolae offers extensive functionality on experimental design especially for agricultural and plant breeding experiments, which can also be useful for other purposes. It supports planning of lattice, Alpha, Cyclic, Complete Block, Latin Square, Graeco-Latin Squares, augmented block, factorial, split and strip plot designs. There are also various analysis facilities for experimental data, e.g. treatment comparison procedures and several non-parametric tests comparison, biodiversity indexes and consensus cluster."}, "CopulaDTA": {"categories": ["MetaAnalysis"], "description": "Modelling of sensitivity and specificity on their natural scale\n    using copula based bivariate beta-binomial distribution to yield marginal\n    mean sensitivity and specificity. The intrinsic negative correlation between\n    sensitivity and specificity is modelled using a copula function. A forest plot\n    can be obtained for categorical covariates or for the model with intercept only.\n    Nyaga VN, Arbyn M, Aerts M (2017) <doi:10.18637/jss.v082.c01>."}, "brandwatchR": {"categories": ["WebTechnologies"], "description": "Interact with the 'Brandwatch' API <https://developers.brandwatch.com/docs>. \n  Allows you to authenticate to the API and obtain data for projects, queries, query groups tags and categories.\n  Also allows you to directly obtain mentions and aggregate data for a specified query or query group."}, "RcppQuantuccia": {"categories": ["Finance"], "description": "'QuantLib' bindings are provided for R using 'Rcpp' via an updated\n variant of the header-only 'Quantuccia' project (put together initially by Peter\n Caspers) offering an essential subset of 'QuantLib' (and now maintained separately\n for the calendaring subset). See the included file 'AUTHORS' for a full list of\n contributors to both 'QuantLib' and 'Quantuccia'."}, "fds": {"categories": ["FunctionalData"], "description": "Functional data sets."}, "hydrotoolbox": {"categories": ["Hydrology"], "description": "Read, plot, manipulate and process hydro-meteorological data records (with special features for Argentina and Chile data-sets)."}, "joinet": {"categories": ["MachineLearning"], "description": "Implements high-dimensional multivariate regression by stacked generalisation (Rauschenberger 2021 <doi:10.1093/bioinformatics/btab576>). For positively correlated outcomes, a single multivariate regression is typically more predictive than multiple univariate regressions. Includes functions for model fitting, extracting coefficients, outcome prediction, and performance measurement. If required, install MRCE or remMap from GitHub (<https://github.com/cran/MRCE>, <https://github.com/cran/remMap>)."}, "metatest": {"categories": ["MetaAnalysis"], "description": "Fits and tests meta regression models and generates a\n number of useful test statistics: next to t- and z-tests, the likelihood ratio,\n bartlett corrected likelihood ratio and permutation tests are performed on\n the model coefficients."}, "neuroim": {"categories": ["MedicalImaging"], "description": "A collection of data structures that represent\n    volumetric brain imaging data. The focus is on basic data handling for 3D\n    and 4D neuroimaging data. In addition, there are function to read and write\n    NIFTI files and limited support for reading AFNI files."}, "captr": {"categories": ["WebTechnologies"], "description": "Get text from images of text using Captricity Optical Character\n    Recognition (OCR) API. Captricity allows you to get text from handwritten\n    forms \u2014 think surveys \u2014 and other structured paper documents. And it can\n    output data in form a delimited file keeping field information intact. For more\n    information, read <https://shreddr.captricity.com/developer/overview/>."}, "nlmixr": {"categories": ["DifferentialEquations", "Pharmacokinetics"], "description": "Fit and compare nonlinear mixed-effects models in differential\n    equations with flexible dosing information commonly seen in pharmacokinetics\n    and pharmacodynamics (Almquist, Leander, and Jirstrand 2015 \n    <doi:10.1007/s10928-015-9409-1>). Differential equation solving is \n    by compiled C code provided in the 'RxODE' package\n    (Wang, Hallow, and James 2015 <doi:10.1002/psp4.12052>)."}, "validate": {"categories": ["OfficialStatistics"], "description": "Declare data validation rules and data quality indicators;\n        confront data with them and analyze or visualize the results.\n        The package supports rules that are per-field, in-record,\n        cross-record or cross-dataset. Rules can be automatically\n        analyzed for rule type and connectivity. Supports checks implied\n        by an SDMX DSD file as well. See also Van der Loo\n        and De Jonge (2018) <doi:10.1002/9781118897126>, Chapter 6\n        and the JSS paper (2021) <doi:10.18637/jss.v097.i10>."}, "confoundr": {"categories": ["CausalInference"], "description": "Implements three covariate-balance diagnostics for time-varying confounding and selection-bias in complex longitudinal data, as described in Jackson (2016) <doi:10.1097/EDE.0000000000000547> and Jackson (2019) <doi:10.1093/aje/kwz136>. Diagnostic 1 assesses measured confounding/selection-bias, diagnostic 2 assesses exposure-covariate feedback, and diagnostic 3 assesses residual confounding/selection-bias after inverse probability weighting or propensity score stratification. All diagnostics appropriately account for exposure history, can be adapted to assess a particular depth of covariate history, and can be implemented in right-censored data. Balance assessments can be obtained for all times, selected-times, or averaged across person-time. The balance measures are reported as tables or plots. These diagnostics can be applied to the study of multivariate exposures including time-varying exposures, direct effects, interaction, and censoring."}, "rmarkdown": {"categories": ["ReproducibleResearch"], "description": "Convert R Markdown documents into a variety of formats."}, "compound.Cox": {"categories": ["Survival"], "description": "Univariate feature selection and compound covariate methods under the Cox model with high-dimensional features (e.g., gene expressions).\n Available are survival data for non-small-cell lung cancer patients with gene expressions (Chen et al 2007 New Engl J Med) <doi:10.1056/NEJMoa060096>,\n statistical methods in Emura et al (2012 PLoS ONE) <doi:10.1371/journal.pone.0047627>,\n Emura & Chen (2016 Stat Methods Med Res) <doi:10.1177/0962280214533378>, and Emura et al. (2019)<doi:10.1016/j.cmpb.2018.10.020>.\n Algorithms for generating correlated gene expressions are also available."}, "stepp": {"categories": ["CausalInference"], "description": "A method to explore the treatment-covariate interactions in survival or generalized \n\tlinear model (GLM) for continuous, binomial and count data arising from two or more treatment \n\tarms of a clinical trial. A permutation distribution approach to inference is implemented, \n\tbased on permuting the covariate values within each treatment group."}, "boxr": {"categories": ["WebTechnologies"], "description": "An R interface for the remote file hosting service 'Box'\n    (<https://www.box.com/>). In addition to uploading and downloading files,\n    this package includes functions which mirror base R operations for local\n    files, (e.g. box_load(), box_save(), box_read(), box_setwd(), etc.), as well\n    as 'git' style functions for entire directories (e.g. box_fetch(),\n    box_push())."}, "KScorrect": {"categories": ["Distributions"], "description": "Implements the Lilliefors-corrected Kolmogorov-Smirnov test for use\n    in goodness-of-fit tests, suitable when population parameters are unknown and\n    must be estimated by sample statistics. P-values are estimated by simulation.\n    Can be used with a variety of continuous distributions, including normal,\n    lognormal, univariate mixtures of normals, uniform, loguniform, exponential,\n    gamma, and Weibull distributions. Functions to generate random numbers and\n    calculate density, distribution, and quantile functions are provided for use\n    with the log uniform and mixture distributions."}, "mlogit": {"categories": ["Econometrics"], "description": "Maximum likelihood estimation of random utility discrete\n             choice models. The software is described in Croissant (2020)\n              <doi:10.18637/jss.v095.i11> and the underlying methods in\n              Train (2009) <doi:10.1017/CBO9780511805271>."}, "ProjectionBasedClustering": {"categories": ["Cluster"], "description": "A clustering approach applicable to every projection method is proposed here. The two-dimensional scatter plot of any projection method can construct a topographic map which displays unapparent data structures by using distance and density information of the data. The generalized U*-matrix renders this visualization in the form of a topographic map, which can be used to automatically define the clusters of high-dimensional data. The whole system is based on Thrun and Ultsch, \"Using Projection based Clustering to Find Distance and Density based Clusters in High-Dimensional Data\" <doi:10.1007/s00357-020-09373-2>. Selecting the correct projection method will result in a visualization in which mountains surround each cluster. The number of clusters can be determined by counting valleys on the topographic map. Most projection methods are wrappers for already available methods in R. By contrast, the neighbor retrieval visualizer (NeRV) is based on C++ source code of the 'dredviz' software package, and the Curvilinear Component Analysis (CCA) is translated from 'MATLAB' ('SOM Toolbox' 2.0) to R."}, "bpbounds": {"categories": ["CausalInference"], "description": "Implementation of the nonparametric bounds for the average causal \n    effect under an instrumental variable model by Balke and Pearl (Bounds on \n    Treatment Effects from Studies with Imperfect Compliance, JASA, 1997, 92, \n    439, 1171-1176). The package can calculate bounds for a binary outcome, a \n    binary treatment/phenotype, and an instrument with either 2 or 3 \n    categories. The package implements bounds for situations where these 3 \n    variables are measured in the same dataset (trivariate data) or where the \n    outcome and instrument are measured in one study and the \n    treatment/phenotype and instrument are measured in another study \n    (bivariate data)."}, "frontier": {"categories": ["Econometrics"], "description": "Maximum Likelihood Estimation of\n   Stochastic Frontier Production and Cost Functions.\n   Two specifications are available:\n   the error components specification with time-varying efficiencies\n   (Battese and Coelli, 1992, <doi:10.1007/BF00158774>)\n   and a model specification in which the firm effects are directly \n   influenced by a number of variables (Battese and Coelli, 1995,\n   <doi:10.1007/BF01205442>)."}, "pcnetmeta": {"categories": ["MetaAnalysis"], "description": "Performs Bayesian arm-based network meta-analysis for datasets with binary, continuous, and count outcomes\n (Zhang et al., 2014 <doi:10.1177/1740774513498322>;\n  Lin et al., 2017 <doi:10.18637/jss.v080.i05>)."}, "OECD": {"categories": ["OfficialStatistics"], "description": "Search and extract data from the Organization for Economic \n    Cooperation and Development (OECD)."}, "repmis": {"categories": ["WebTechnologies"], "description": "Tools to load 'R' packages\n    and automatically generate BibTeX files citing them as well as load and\n    cache plain-text and 'Excel' formatted data stored on 'GitHub', and\n    from other sources."}, "XML": {"categories": ["WebTechnologies"], "description": "Many approaches for both reading and\n        creating XML (and HTML) documents (including DTDs), both local\n        and accessible via HTTP or FTP.  Also offers access to an\n        'XPath' \"interpreter\"."}, "daewr": {"categories": ["ExperimentalDesign"], "description": "Contains Data frames and functions used in the book \"Design and Analysis of Experiments with R\"."}, "emdi": {"categories": ["OfficialStatistics"], "description": "Functions that support estimating, assessing and mapping regional\n    disaggregated indicators. So far, estimation methods comprise direct estimation,\n    the model-based unit-level approach Empirical Best Prediction (see \"Small area\n    estimation of poverty indicators\" by Molina and Rao (2010) <doi:10.1002/cjs.10051>), \n    the area-level model (see \"Estimates of income for small places: An \n    application of James-Stein procedures to Census Data\" by Fay and Herriot (1979) \n    <doi:10.1080/01621459.1979.10482505>) and various extensions of it (adjusted variance \n    estimation methods, log and arcsin transformation, spatial, robust and measurement \n    error models), as well as their precision estimates. The assessment of the used model\n    is supported by a summary and diagnostic plots. For a suitable presentation of\n    estimates, map plots can be easily created. Furthermore, results can easily be\n    exported to excel. For a detailed description of the package and the methods used\n    see \"The R Package emdi for Estimating and Mapping Regionally Disaggregated Indicators\" \n    by Kreutzmann et al. (2019) <doi:10.18637/jss.v091.i07> and the second package vignette \n    \"A Framework for Producing Small Area Estimates Based on Area-Level Models in R\"."}, "Rirt": {"categories": ["Psychometrics"], "description": "Parameter estimation, computation of probability, information, and \n    (log-)likelihood, and visualization of item/test characteristic curves and\n    item/test information functions for three uni-dimensional item response theory\n    models: the 3-parameter-logistic model, generalized partial credit model, \n    and graded response model. The full documentation and tutorials are at \n    <https://github.com/xluo11/Rirt>."}, "APtools": {"categories": ["Survival"], "description": "We provide tools to estimate two prediction accuracy metrics,\n    the average positive predictive values (AP) as well as the well-known AUC\n    (the area under the receiver operator characteristic curve) for risk scores. \n    The outcome of interest is either binary or censored event time.\n    Note that for censored event time, our functions' estimates, the AP and the\n    AUC, are time-dependent for pre-specified time interval(s). A function that\n    compares the APs of two risk scores/markers is also included. Optional\n    outputs include positive predictive values and true positive fractions at\n    the specified marker cut-off values, and a plot of the time-dependent AP\n    versus time (available for event time data)."}, "bayesAB": {"categories": ["Bayesian"], "description": "A suite of functions that allow the user to analyze A/B test\n    data in a Bayesian framework. Intended to be a drop-in replacement for\n    common frequentist hypothesis test such as the t-test and chi-sq test."}, "zic": {"categories": ["Bayesian"], "description": "Provides MCMC algorithms for the analysis of\n        zero-inflated count models. The case of stochastic search\n        variable selection (SVS) is also considered.  All MCMC samplers\n        are coded in C++ for improved efficiency. A data set\n        considering the demand for health care is provided."}, "rrum": {"categories": ["Bayesian", "Psychometrics"], "description": "Implementation of Gibbs sampling algorithm for Bayesian Estimation\n    of the Reduced Reparameterized Unified Model ('rrum'), described by \n    Culpepper and Hudson (2017) <doi:10.1177/0146621617707511>."}, "cmvnorm": {"categories": ["Distributions"], "description": "Various utilities for the complex multivariate Gaussian distribution and complex Gaussian processes."}, "TruncatedNormal": {"categories": ["Distributions"], "description": "A collection of functions to deal with the truncated univariate and multivariate normal and Student distributions, described in Botev (2017) <doi:10.1111/rssb.12162> and Botev and L'Ecuyer (2015) <doi:10.1109/WSC.2015.7408180>."}, "foreach": {"categories": ["HighPerformanceComputing"], "description": "Support for the foreach looping construct.  Foreach is an\n        idiom that allows for iterating over elements in a collection,\n        without the use of an explicit loop counter.  This package in\n        particular is intended to be used for its return value, rather\n        than for its side effects.  In that sense, it is similar to the\n        standard lapply function, but doesn't require the evaluation\n        of a function.  Using foreach without side effects also\n        facilitates executing the loop in parallel."}, "multifamm": {"categories": ["FunctionalData"], "description": "An implementation for multivariate functional additive mixed\n    models (multiFAMM), see Volkmann et al. (2021, <arXiv:2103.06606>). It builds on developed methods for univariate sparse \n    functional regression models and multivariate functional principal component\n    analysis. This package contains the function to run a multiFAMM and some\n    convenience functions useful when working with large models. An additional \n    package on GitHub contains more convenience functions to reproduce the \n    analyses of the corresponding paper \n    (<https://github.com/alexvolkmann/multifammPaper>)."}, "TEQR": {"categories": ["ClinicalTrials", "ExperimentalDesign"], "description": "The TEQR package contains software to calculate the operating characteristics for the TEQR and the ACT designs.The TEQR (toxicity equivalence range) design is a toxicity based cumulative cohort design with added safety rules. The ACT (Activity constrained for toxicity) design  is also a cumulative cohort design with additional safety rules. The unique feature of this design is that dose is escalated based on lack of activity rather than on lack of toxicity and is de-escalated only if an unacceptable level of toxicity is experienced."}, "NFLSimulatoR": {"categories": ["SportsAnalytics"], "description": "The intent here is to enable the simulation of plays/drives and\n    evaluate game-play strategies in the National Football League (NFL).\n    Built-in strategies include going for it on fourth down and varying the \n    proportion of passing/rushing plays during a drive. The user should be\n    familiar with nflscrapR data before trying to write his/her own \n    strategies. This work is inspired by a blog post by Mike Lopez, \n    currently the  Director of Data and Analytics at the NFL, Lopez (2019) <https://statsbylopez.netlify.app/post/resampling-nfl-drives/>."}, "RNCEP": {"categories": ["Hydrology"], "description": "Contains functions to retrieve, organize, and visualize weather data from the NCEP/NCAR Reanalysis (<http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html>) and NCEP/DOE Reanalysis II (<http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis2.html>) datasets.  Data are queried via the Internet and may be obtained for a specified spatial and temporal extent or interpolated to a point in space and time.  We also provide functions to visualize these weather data on a map.  There are also functions to simulate flight trajectories according to specified behavior using either NCEP wind data or data specified by the user."}, "bamlss": {"categories": ["Bayesian"], "description": "Infrastructure for estimating probabilistic distributional regression models in a Bayesian framework.\n  The distribution parameters may capture location, scale, shape, etc. and every parameter may depend\n  on complex additive terms (fixed, random, smooth, spatial, etc.) similar to a generalized additive model.\n  The conceptual and computational framework is introduced in Umlauf, Klein, Zeileis (2019)\n  <doi:10.1080/10618600.2017.1407325> and the R package in Umlauf, Klein, Simon, Zeileis (2021)\n  <doi:10.18637/jss.v100.i04>."}, "InformativeCensoring": {"categories": ["MissingData", "Survival"], "description": "Multiple Imputation for Informative Censoring.\n    This package implements two methods. Gamma Imputation\n    described in <doi:10.1002/sim.6274> and Risk Score Imputation\n    described in <doi:10.1002/sim.3480>."}, "bayesdistreg": {"categories": ["Bayesian"], "description": "Implements Bayesian Distribution Regression methods. This package contains functions for three estimators (non-asymptotic, semi-asymptotic and asymptotic) and related routines for Bayesian Distribution Regression in Huang and Tsyawo (2018) <doi:10.2139/ssrn.3048658> which is also the recommended reference to cite for this package. The functions can be grouped into three (3) categories. The first computes the logit likelihood function and posterior densities under uniform and normal priors. The second contains Independence and Random Walk Metropolis-Hastings Markov Chain Monte Carlo (MCMC) algorithms as functions and the third category of functions are useful for semi-asymptotic and asymptotic Bayesian distribution regression inference."}, "bayesmix": {"categories": ["Bayesian", "Cluster", "GraphicalModels"], "description": "Fits finite mixture models of univariate Gaussian distributions using JAGS within a Bayesian framework."}, "SteinIV": {"categories": ["Econometrics"], "description": "Routines for computing different types of linear estimators, based on instrumental variables (IVs), including the semi-parametric Stein-like (SPS) estimator, originally introduced by Judge and Mittelhammer (2004)  <doi:10.1198/016214504000000430>. "}, "glogis": {"categories": ["Distributions"], "description": "Tools for the generalized logistic distribution (Type I,\n             also known as skew-logistic distribution), encompassing\n\t     basic distribution functions (p, q, d, r, score), maximum\n\t     likelihood estimation, and structural change methods."}, "QTLRel": {"categories": ["MissingData"], "description": "This software provides tools for quantitative trait mapping in populations such as advanced intercross lines where relatedness among individuals should not be ignored. It can estimate background genetic variance components, impute missing genotypes, simulate genotypes, perform a genome scan for putative quantitative trait loci (QTL), and plot mapping results. It also has functions to calculate identity coefficients from pedigrees, especially suitable for pedigrees that consist of a large number of generations, or estimate identity coefficients from genotypic data in certain circumstances."}, "sdcTable": {"categories": ["OfficialStatistics"], "description": "Methods for statistical disclosure control in\n    tabular data such as primary and secondary cell suppression as described for example\n    in Hundepol et al. (2012) <doi:10.1002/9781118348239> are covered in this package."}, "idendr0": {"categories": ["Cluster"], "description": "Interactive dendrogram that enables the user to select and\n    color clusters, to zoom and pan the dendrogram, and to visualize\n    the clustered data not only in a built-in heat map, but also in\n    'GGobi' interactive plots and user-supplied plots. \n    This is a backport of Qt-based 'idendro' \n    (<https://github.com/tsieger/idendro>) to base R graphics and \n    Tcl/Tk GUI."}, "zoo": {"categories": ["Econometrics", "Environmetrics", "Finance", "MissingData", "TimeSeries"], "description": "An S3 class with methods for totally ordered indexed\n             observations. It is particularly aimed at irregular time series\n             of numeric vectors/matrices and factors. zoo's key design goals\n             are independence of a particular index/date/time class and\n             consistency with ts and base R by providing methods to extend\n             standard generics."}, "bcrm": {"categories": ["ClinicalTrials", "ExperimentalDesign"], "description": "Implements a wide variety of one- and two-parameter Bayesian CRM\n    designs. The program can run interactively, allowing the user to enter outcomes\n    after each cohort has been recruited, or via simulation to assess operating\n    characteristics. See Sweeting et al. (2013): <doi:10.18637/jss.v054.i13>."}, "PerformanceAnalytics": {"categories": ["Finance"], "description": "Collection of econometric functions for performance and risk \n    analysis. In addition to standard risk and performance metrics, this \n    package aims to aid practitioners and researchers in utilizing the latest\n    research in analysis of non-normal return streams.  In general, it is most \n    tested on return (rather than price) data on a regular scale, but most \n    functions will work with irregular return data as well, and increasing\n    numbers of functions will work with P&L or price data where possible."}, "PKNCA": {"categories": ["Pharmacokinetics"], "description": "Compute standard Non-Compartmental Analysis (NCA) parameters for\n    typical pharmacokinetic analyses and summarize them."}, "parSim": {"categories": ["HighPerformanceComputing"], "description": "Perform flexible simulation studies using one or multiple computer cores.\n          The package is set up to be usable on high-performance clusters in addition\n          to being run locally, see examples on <https://github.com/SachaEpskamp/parSim>."}, "genieclust": {"categories": ["Cluster"], "description": "A retake on the Genie algorithm - a robust\n    hierarchical clustering method\n    (Gagolewski, Bartoszuk, Cena, 2016 <doi:10.1016/j.ins.2016.05.003>).\n    Now faster and more memory efficient; determining the whole hierarchy\n    for datasets of 10M points in low dimensional Euclidean spaces or\n    100K points in high-dimensional ones takes only 1-2 minutes.\n    Allows clustering with respect to mutual reachability distances\n    so that it can act as a noise point detector or a robustified version of\n    'HDBSCAN*' (that is able to detect a predefined number of\n    clusters and hence it does not dependent on the somewhat\n    fragile 'eps' parameter).\n    The package also features an implementation of economic inequity indices\n    (the Gini, Bonferroni index) and external cluster validity measures\n    (partition similarity scores; e.g., the adjusted Rand, Fowlkes-Mallows,\n    adjusted mutual information, pair sets index).\n    See also the 'Python' version of 'genieclust' available on 'PyPI', which\n    supports sparse data, more metrics, and even larger datasets."}, "irlba": {"categories": ["NumericalMathematics"], "description": "Fast and memory efficient methods for truncated singular value\n    decomposition and principal components analysis of large sparse and dense\n    matrices."}, "BayesFM": {"categories": ["Bayesian"], "description": "Collection of procedures to perform Bayesian analysis on a variety\n    of factor models. Currently, it includes: \"Bayesian Exploratory Factor \n    Analysis\" (befa) from G. Conti, S. Fr\u00fchwirth-Schnatter, J.J. Heckman, \n    R. Piatek (2014) <doi:10.1016/j.jeconom.2014.06.008>, an approach to \n    dedicated factor analysis with stochastic search on the structure of the \n    factor loading matrix. The number of latent factors, as well as the \n    allocation of the manifest variables to the factors, are not fixed a priori \n    but determined during MCMC sampling."}, "lpSolveAPI": {"categories": ["Optimization"], "description": "The lpSolveAPI package provides an R interface to 'lp_solve',\n    a Mixed Integer Linear Programming (MILP) solver with support for pure\n    linear, (mixed) integer/binary, semi-continuous and special ordered sets\n    (SOS) models."}, "stars": {"categories": ["Spatial", "SpatioTemporal"], "description": "Reading, manipulating, writing and plotting\n    spatiotemporal arrays (raster and vector data cubes) in 'R', using 'GDAL'\n    bindings provided by 'sf', and 'NetCDF' bindings by 'ncmeta' and 'RNetCDF'."}, "intsurv": {"categories": ["Survival"], "description": "Contains implementations of\n    integrative survival analysis routines, including\n    regular Cox cure rate model proposed by\n    Kuk and Chen (1992) <doi:10.1093/biomet/79.3.531>\n    via an EM algorithm proposed by\n    Sy and Taylor (2000) <doi:10.1111/j.0006-341X.2000.00227.x>,\n    regularized Cox cure rate model with elastic net penalty following\n    Masud et al. (2018) <doi:10.1177/0962280216677748>, and\n    Zou and Hastie (2005) <doi:10.1111/j.1467-9868.2005.00503.x>, and\n    weighted concordance index for cure models proposed by\n    Asano and Hirakawa (2017) <doi:10.1080/10543406.2017.1293082>."}, "wikipediatrend": {"categories": ["WebTechnologies"], "description": "A convenience wrapper for the Wikipedia page access statistics API \n  binding the 'pageviews' package and using an additional self composed data \n  source thus covering a time span from very late 2007 up to the present for \n  daily page views. "}, "revtools": {"categories": ["MetaAnalysis"], "description": "Researchers commonly need to summarize scientific information, a process known as 'evidence synthesis'. The first stage of a synthesis process (such as a systematic review or meta-analysis) is to download a list of references from academic search engines such as 'Web of Knowledge' or 'Scopus'. The traditional approach to systematic review is then to sort these data manually, first by locating and removing duplicated entries, and then screening to remove irrelevant content by viewing titles and abstracts (in that order). 'revtools' provides interfaces for each of these tasks. An alternative approach, however, is to draw on tools from machine learning to visualise patterns in the corpus. In this case, you can use 'revtools' to render ordinations of text drawn from article titles, keywords and abstracts, and interactively select or exclude individual references, words or topics."}, "rpatrec": {"categories": ["Finance"], "description": "Generating visual charting patterns and noise,\n    smoothing to find a signal in noisy time series and enabling\n    users to apply their findings to real life data."}, "textrank": {"categories": ["NaturalLanguageProcessing"], "description": "The 'textrank' algorithm is an extension of the 'Pagerank' algorithm for text. The algorithm allows to summarize text by calculating how sentences are related to one another. This is done by looking at overlapping terminology used in sentences in order to set up links between sentences. The resulting sentence network is next plugged into the 'Pagerank' algorithm which identifies the most important sentences in your text and ranks them. \n    In a similar way 'textrank' can also be used to extract keywords. A word network is constructed by looking if words are following one another. On top of that network the 'Pagerank' algorithm is applied to extract relevant words after which relevant words which are following one another are combined to get keywords.  \n    More information can be found in the paper from Mihalcea, Rada & Tarau, Paul (2004) <https://www.aclweb.org/anthology/W04-3252/>."}, "mvnfast": {"categories": ["HighPerformanceComputing"], "description": "Provides computationally efficient tools related to the\n    multivariate normal and Student's t distributions. The main functionalities\n    are: simulating multivariate random vectors, evaluating multivariate normal or\n    Student's t densities and Mahalanobis distances. These tools are very efficient\n    thanks to the use of C++ code and of the OpenMP API."}, "gte": {"categories": ["Survival"], "description": "Generalized Turnbull's estimator proposed by Dehghan and Duchesne\n    (2011)."}, "smacof": {"categories": ["Psychometrics"], "description": "Implements the following approaches for multidimensional scaling (MDS) based on stress minimization using majorization (smacof): ratio/interval/ordinal/spline MDS on symmetric dissimilarity matrices, MDS with external constraints on the configuration, individual differences scaling (idioscal, indscal), MDS with spherical restrictions, and ratio/interval/ordinal/spline unfolding (circular restrictions, row-conditional). Various tools and extensions like jackknife MDS, bootstrap MDS, permutation tests, MDS biplots, gravity models, unidimensional scaling, drift vectors (asymmetric MDS), classical scaling, and Procrustes are implemented as well.  "}, "LongMemoryTS": {"categories": ["TimeSeries"], "description": "Long Memory Time Series is a collection of functions for estimation, simulation and testing of long memory processes, spurious long memory processes and fractionally cointegrated systems. "}, "SCEPtER": {"categories": ["ChemPhys"], "description": "A pipeline for estimating the stellar\n        age, mass, and radius given observational \n        effective temperature, [Fe/H], and astroseismic\n\tparameters. The results are obtained adopting a maximum likelihood\n\ttechnique over a grid of pre-computed stellar models, as\n\tdescribed in Valle et al. (2014) <doi:10.1051/0004-6361/201322210>."}, "bipd": {"categories": ["MetaAnalysis"], "description": "We use a Bayesian approach to run individual patient data meta-analysis and network meta-analysis using 'JAGS'. The methods incorporate shrinkage methods and calculate patient-specific treatment effects as described in Seo et al. (2021) <doi:10.1002/sim.8859>. This package also includes user-friendly functions that impute missing data in an individual patient data using mice-related packages."}, "marked": {"categories": ["Environmetrics"], "description": "Functions for fitting various models to capture-recapture data\n    including mixed-effects Cormack-Jolly-Seber(CJS) and multistate models and\n    the multi-variate state model structure for survival\n    estimation and POPAN structured Jolly-Seber models for abundance estimation.\n    There are also Hidden Markov model (HMM) implementations of CJS and multistate\n    models with and without state uncertainty and a simulation capability for HMM\n    models."}, "textcat": {"categories": ["NaturalLanguageProcessing"], "description": "Text categorization based on n-grams."}, "metaconfoundr": {"categories": ["MetaAnalysis"], "description": "Visualize 'confounder' control in meta-analysis.\n    'metaconfoundr' is an approach to evaluating bias in studies used in\n    meta-analyses based on the causal inference framework. Study groups\n    create a causal diagram displaying their assumptions about the\n    scientific question. From this, they develop a list of important\n    'confounders'. Then, they evaluate whether studies controlled for\n    these variables well. 'metaconfoundr' is a toolkit to facilitate this\n    process and visualize the results as heat maps, traffic light plots,\n    and more."}, "freealg": {"categories": ["NumericalMathematics"], "description": "The free algebra in R; multivariate polynomials with non-commuting indeterminates."}, "dataverse": {"categories": ["WebTechnologies"], "description": "Provides access to Dataverse APIs <https://dataverse.org/> (versions 4-5),\n    enabling data search, retrieval, and deposit. For Dataverse versions <= 3.0,\n    use the archived 'dvn' package <https://cran.r-project.org/package=dvn>."}, "r2rtf": {"categories": ["ReproducibleResearch"], "description": "Create production-ready Rich Text Format (RTF) table and figure with flexible format."}, "loo": {"categories": ["Bayesian"], "description": "Efficient approximate leave-one-out cross-validation (LOO)\n    for Bayesian models fit using Markov chain Monte Carlo, as \n    described in Vehtari, Gelman, and Gabry (2017) \n    <doi:10.1007/s11222-016-9696-4>. \n    The approximation uses Pareto smoothed importance sampling (PSIS), \n    a new procedure for regularizing importance weights. \n    As a byproduct of the calculations, we also obtain approximate \n    standard errors for estimated predictive errors and for the comparison \n    of predictive errors between models. The package also provides methods \n    for using stacking and other model weighting techniques to average \n    Bayesian predictive distributions."}, "Rook": {"categories": ["WebTechnologies"], "description": "This package contains the Rook specification and\n convenience software for building and running Rook applications. To\n get started, be sure and read the 'Rook' help file first."}, "AGSDest": {"categories": ["ClinicalTrials"], "description": "Calculation of repeated confidence intervals as well as confidence\n    intervals based on the stage-wise ordering in group sequential designs and\n    adaptive group sequential designs. For adaptive group sequential designs\n    the confidence intervals are based on the conditional rejection probability\n    principle. Currently the procedures do not support the use of futility\n    boundaries or more than one adaptive interim analysis."}, "lakemorpho": {"categories": ["Hydrology"], "description": "Lake morphometry metrics are used by limnologists to understand,\n    among other things, the ecological processes in a lake. Traditionally, these\n    metrics are calculated by hand, with planimeters, and increasingly with\n    commercial GIS products. All of these methods work; however, they are either\n    outdated, difficult to reproduce, or require expensive licenses to use. The\n    'lakemorpho' package provides the tools to calculate a typical suite\n    of these metrics from an input elevation model and lake polygon. The metrics\n    currently supported are: fetch, major axis, minor axis, major/minor axis \n    ratio, maximum length, maximum width, mean width, maximum depth, mean depth, \n    shoreline development, shoreline length, surface area, and volume."}, "MonoPoly": {"categories": ["NumericalMathematics"], "description": "Functions for fitting monotone polynomials to data.\n             Detailed discussion of the methodologies used can be\n             found in Murray, Mueller and Turlach (2013)\n             <doi:10.1007/s00180-012-0390-5> and Murray, Mueller and\n             Turlach (2016) <doi:10.1080/00949655.2016.1139582>.  "}, "LindleyPowerSeries": {"categories": ["Distributions"], "description": "Computes the probability density function, the cumulative distribution function, the hazard rate function, the quantile function and random generation for Lindley Power Series distributions, see Nadarajah and Si (2018) <doi:10.1007/s13171-018-0150-x>."}, "IMIFA": {"categories": ["Cluster"], "description": "Provides flexible Bayesian estimation of Infinite Mixtures of Infinite Factor Analysers and related models, for nonparametrically clustering high-dimensional data, introduced by Murphy et al. (2020) <doi:10.1214/19-BA1179>. The IMIFA model conducts Bayesian nonparametric model-based clustering with factor analytic covariance structures without recourse to model selection criteria to choose the number of clusters or cluster-specific latent factors, mostly via efficient Gibbs updates. Model-specific diagnostic tools are also provided, as well as many options for plotting results, conducting posterior inference on parameters of interest, posterior predictive checking, and quantifying uncertainty."}, "rstream": {"categories": ["Distributions", "HighPerformanceComputing"], "description": "Unified object oriented interface for multiple independent streams of random numbers from different sources."}, "ecr": {"categories": ["Optimization"], "description": "Framework for building evolutionary algorithms for both single- and multi-objective continuous or discrete optimization problems. A set of predefined evolutionary building blocks and operators is included. Moreover, the user can easily set up custom objective functions, operators, building blocks and representations sticking to few conventions. The package allows both a black-box approach for standard tasks (plug-and-play style) and a much more flexible white-box approach where the evolutionary cycle is written by hand."}, "simcdm": {"categories": ["Psychometrics"], "description": "Provides efficient R and 'C++' routines to simulate cognitive diagnostic\n    model data for Deterministic Input, Noisy \"And\" Gate ('DINA') and\n    reduced Reparameterized Unified Model ('rRUM') from \n    Culpepper and Hudson (2017) <doi:10.1177/0146621617707511>,\n    Culpepper (2015) <doi:10.3102/1076998615595403>, and\n    de la Torre (2009) <doi:10.3102/1076998607309474>."}, "inca": {"categories": ["OfficialStatistics"], "description": "Specific functions are provided for rounding real weights to integers and performing an integer programming algorithm for calibration problems. They are useful for census-weights adjustments, or for performing linear regression with integer parameters. This research was supported in part by the U.S. Department of Agriculture, National Agriculture Statistics Service. The findings and conclusions in this publication are those of the authors and should not be construed to represent any official USDA, or US Government determination or policy."}, "Rsagacmd": {"categories": ["Spatial"], "description": "Provides an R scripting interface to the open-source 'SAGA-GIS' \n    (System for Automated Geoscientific Analyses Geographical Information\n    System) software. 'Rsagacmd' dynamically generates R functions for every\n    'SAGA-GIS' geoprocessing tool based on the user's currently installed\n    'SAGA-GIS' version. These functions are contained within an S3 object\n    and are accessed as a named list of libraries and tools. This structure\n    facilitates an easier scripting experience by organizing the large number\n    of 'SAGA-GIS' geoprocessing tools (>700) by their respective library.\n    Interactive scripting can fully take advantage of code autocompletion tools\n    (e.g. in 'Rstudio'), allowing for each tools syntax to be quickly\n    recognized. Furthermore, the most common types of spatial data (via the\n    'raster', 'terra', 'sp', and 'sf' packages) along with non-spatial data are\n    automatically passed from R to the 'SAGA-GIS' command line tool for\n    geoprocessing operations, and the results are loaded as the appropriate R\n    object. Outputs from individual 'SAGA-GIS' tools can also be chained using\n    pipes from the 'magrittr' and 'dplyr' packages to combine complex\n    geoprocessing operations together in a single statement. 'SAGA-GIS' is\n    available under a GPLv2 / LGPLv2 licence from\n    <https://sourceforge.net/projects/saga-gis/> including Windows x86/x64\n    binaries. SAGA-GIS is also included in Debian/Ubuntu default software\n    repositories and is available for macOS using homebrew (<https://brew.sh/>)\n    from the osgeo/osgeo4mac (<https://github.com/OSGeo/homebrew-osgeo4mac>)\n    formula tap, as well as being bundled within the 'QGIS' application bundle\n    for macOS. Rsagacmd has currently been tested on 'SAGA-GIS' versions\n    from 2.3.1 to 8.0.1 on Windows, Linux and macOS."}, "changepoint": {"categories": ["TimeSeries"], "description": "Implements various mainstream and specialised changepoint methods for finding single and multiple changepoints within data.  Many popular non-parametric and frequentist methods are included.  The cpt.mean(), cpt.var(), cpt.meanvar() functions should be your first point of call."}, "PeerPerformance": {"categories": ["Finance"], "description": "Provides functions to perform the peer performance\n    analysis of funds' returns as described in Ardia and Boudt (2018) <doi:10.1016/j.jbankfin.2017.10.014>."}, "coronavirus": {"categories": ["Epidemiology"], "description": "Provides a daily summary of the Coronavirus (COVID-19) cases by state/province. Data source: Johns Hopkins University Center for Systems Science and Engineering (JHU CCSE) Coronavirus <https://systems.jhu.edu/research/public-health/ncov/>."}, "fmri": {"categories": ["ChemPhys", "MedicalImaging"], "description": "Contains R-functions to perform an fMRI analysis as described in\n             Polzehl and Tabelow (2019) <doi:10.1007/978-3-030-29184-6>,\n             Tabelow et al. (2006) <doi:10.1016/j.neuroimage.2006.06.029>,\n             Polzehl et al. (2010) <doi:10.1016/j.neuroimage.2010.04.241>,\n             Tabelow and Polzehl (2011) <doi:10.18637/jss.v044.i11>."}, "RandMeta": {"categories": ["MetaAnalysis"], "description": "A novel numerical algorithm that provides functionality for estimating the exact 95% confidence interval of the location parameter in the random effects model, and is much faster than the naive method. Works best when the number of studies is between 6-20."}, "pawacc": {"categories": ["Tracking"], "description": "This is a collection of functions to process, format and store accelerometer data."}, "httpuv": {"categories": ["ModelDeployment", "WebTechnologies"], "description": "Provides low-level socket and protocol support for handling\n    HTTP and WebSocket requests directly from within R. It is primarily\n    intended as a building block for other packages, rather than making it\n    particularly easy to create complete web applications using httpuv alone.\n    httpuv is built on top of the libuv and http-parser C libraries, both of\n    which were developed by Joyent, Inc. (See LICENSE file for libuv and\n    http-parser license information.)"}, "postGIStools": {"categories": ["Spatial"], "description": "Functions to convert geometry and 'hstore' data types from\n    'PostgreSQL' into standard R objects, as well as to simplify\n    the import of R data frames (including spatial data frames) into 'PostgreSQL'.\n    Note: This package is deprecated. For new projects, we recommend \n    using the 'sf' package to interface with geodatabases."}, "gravitas": {"categories": ["TimeSeries"], "description": "Provides tools for systematically exploring large quantities of \n             temporal data across cyclic temporal granularities\n             (deconstructions of time) by visualizing probability distributions.\n             Cyclic time granularities can be circular, quasi-circular or \n             aperiodic. 'gravitas' computes cyclic\n             single-order-up or multiple-order-up granularities, check the\n             feasibility of creating plots for any two cyclic granularities\n             and recommend probability distributions plots for exploring\n             periodicity in the data."}, "ergm": {"categories": ["GraphicalModels"], "description": "An integrated set of tools to analyze and simulate networks based on exponential-family random graph models (ERGMs). 'ergm' is a part of the Statnet suite of packages for network analysis. See Hunter, Handcock, Butts, Goodreau, and Morris (2008) <doi:10.18637/jss.v024.i03> and Krivitsky, Hunter, Morris, and Klumb (2021) <arXiv:2106.04997>."}, "BioMark": {"categories": ["ChemPhys"], "description": "Variable selection methods are provided for several classification methods: the lasso/elastic net, PCLDA, PLSDA, and several t-tests. Two approaches for selecting cutoffs can be used, one based on the stability of model coefficients under perturbation, and the other on higher criticism."}, "AquaEnv": {"categories": ["ChemPhys"], "description": "Toolbox for the experimental aquatic chemist, focused on \n        acidification and CO2 air-water exchange. It contains all elements to\n        model the pH, the related CO2 air-water exchange, and\n        aquatic acid-base chemistry for an arbitrary marine,\n        estuarine or freshwater system. It contains a suite of tools for \n        sensitivity analysis, visualisation, modelling of chemical batches, \n        and can be used to build dynamic models of aquatic systems. \n        As from version 1.0-4, it also contains functions to calculate \n        the buffer factors. "}, "spacetime": {"categories": ["MissingData", "Spatial", "SpatioTemporal"], "description": "Classes and methods for spatio-temporal data, including space-time regular lattices, sparse lattices, irregular data, and trajectories; utility functions for plotting data as map sequences (lattice or animation) or multiple time series; methods for spatial and temporal selection and subsetting, as well as for spatial/temporal/spatio-temporal matching or aggregation, retrieving coordinates, print, summary, etc."}, "LPM": {"categories": ["Hydrology"], "description": "Apply Univariate Long Memory Models,\n    Apply Multivariate Short Memory Models To Hydrological Dataset,\n    Estimate Intensity Duration Frequency curve to rainfall series."}, "exactextractr": {"categories": ["Spatial"], "description": "Provides a replacement for the 'extract' function from the 'raster' package\n    that is suitable for extracting raster values using 'sf' polygons."}, "WhatIf": {"categories": ["CausalInference"], "description": "Inferences about counterfactuals are essential for prediction,\n      answering what if questions, and estimating causal effects.\n      However, when the counterfactuals posed are too far from the data at\n      hand, conclusions drawn from well-specified statistical analyses\n      become based largely on speculation hidden in convenient modeling\n      assumptions that few would be willing to defend. Unfortunately,\n      standard statistical approaches assume the veracity of the model\n      rather than revealing the degree of model-dependence, which makes this\n      problem hard to detect. WhatIf offers easy-to-apply methods to\n      evaluate counterfactuals that do not require sensitivity testing over\n      specified classes of models. If an analysis fails the tests offered\n      here, then we know that substantive inferences will be sensitive to at\n      least some modeling choices that are not based on empirical evidence,\n      no matter what method of inference one chooses to use. WhatIf\n      implements the methods for evaluating counterfactuals discussed in\n      Gary King and Langche Zeng, 2006, \"The Dangers of Extreme\n      Counterfactuals,\" Political Analysis 14 (2) <doi:10.1093/pan/mpj004>; \n      and Gary King and Langche Zeng, 2007, \"When Can History Be Our Guide? The \n      Pitfalls of Counterfactual Inference,\" International Studies \n      Quarterly 51 (March) <doi:10.1111/j.1468-2478.2007.00445.x>."}, "RCurl": {"categories": ["WebTechnologies"], "description": "A wrapper for 'libcurl' <https://curl.se/libcurl/>\n\tProvides functions to allow one to compose general HTTP requests\n        and provides convenient functions to fetch URIs, get & post\n        forms, etc. and process the results returned by the Web server.\n        This provides a great deal of control over the HTTP/FTP/...\n        connection and the form of the request while providing a\n        higher-level interface than is available just using R socket\n        connections.  Additionally, the underlying implementation is\n        robust and extensive, supporting FTP/FTPS/TFTP (uploads and\n        downloads), SSL/HTTPS, telnet, dict, ldap, and also supports\n        cookies, redirects, authentication, etc."}, "alpaca": {"categories": ["CausalInference", "Econometrics"], "description": "Provides a routine to partial out factors with many levels during the\n  optimization of the log-likelihood function of the corresponding generalized linear model (glm).\n  The package is based on the algorithm described in Stammann (2018) <arXiv:1707.01815> and is\n  restricted to glm's that are based on maximum likelihood estimation and non-linear. It also offers\n  an efficient algorithm to recover estimates of the fixed effects in a post-estimation routine and \n  includes robust and multi-way clustered standard errors. Further the package provides analytical \n  bias corrections for binary choice models (logit and probit) derived by Fernandez-Val \n  and Weidner (2016) <doi:10.1016/j.jeconom.2015.12.014> and Hinz, Stammann, and Wanner (2020) \n  <arXiv:2004.12655>."}, "gmailr": {"categories": ["WebTechnologies"], "description": "An interface to the 'Gmail' 'RESTful' API.  Allows access to\n    your 'Gmail' messages, threads, drafts and labels."}, "metaBLUE": {"categories": ["MetaAnalysis"], "description": "The sample mean and standard deviation are two commonly used statistics in meta-analyses, \n    but some trials use other summary statistics such as the median and quartiles to report the results. \n    Therefore, researchers need to transform those information back to the sample mean and \n    standard deviation. This package implemented sample mean estimators by Luo et al. (2016) <arXiv:1505.05687>, sample standard deviation estimators by Wan et al. (2014) <arXiv:1407.8038>, and the best linear unbiased estimators (BLUEs) of location and scale parameters by Yang et al. (2018, submitted) based on sample quantiles derived summaries in a meta-analysis."}, "set6": {"categories": ["NumericalMathematics"], "description": "An object-oriented package for mathematical sets, upgrading the current gold-standard {sets}. Many forms of mathematical sets are implemented, including (countably finite) sets, tuples, intervals (countably infinite or uncountable), and fuzzy variants. Wrappers extend functionality by allowing symbolic representations of complex operations on sets, including unions, (cartesian) products, exponentiation, and differences (asymmetric and symmetric)."}, "PSweight": {"categories": ["CausalInference"], "description": "Supports propensity score weighting analysis of observational studies and randomized trials. Enables the estimation and inference of average causal effects with binary and multiple treatments using overlap weights (ATO), inverse probability of treatment weights (ATE), average treatment effect among the treated weights (ATT), matching weights (ATM) and entropy weights (ATEN), with and without propensity score trimming. These weights are members of the family of balancing weights introduced in Li, Morgan and Zaslavsky (2018) <doi:10.1080/01621459.2016.1260466> and Li and Li (2019) <doi:10.1214/19-AOAS1282>."}, "ssizeRNA": {"categories": ["ExperimentalDesign"], "description": "We propose a procedure for sample size calculation while\n    controlling false discovery rate for RNA-seq experimental design. Our\n    procedure depends on the Voom method proposed for RNA-seq data analysis\n    by Law et al. (2014) <doi:10.1186/gb-2014-15-2-r29> and the sample size \n    calculation method proposed for microarray experiments by Liu and Hwang \n    (2007) <doi:10.1093/bioinformatics/btl664>. We develop a set of functions\n    that calculates appropriate sample sizes for two-sample t-test for RNA-seq\n    experiments with fixed or varied set of parameters. The outputs also contain a\n    plot of power versus sample size, a table of power at different sample sizes,\n    and a table of critical test values at different sample sizes. \n    To install this package, please use \n    'source(\"http://bioconductor.org/biocLite.R\"); biocLite(\"ssizeRNA\")'. \n    For R version 3.5 or greater, please use  \n    'if(!requireNamespace(\"BiocManager\", quietly = TRUE)){install.packages(\"BiocManager\")}; BiocManager::install(\"ssizeRNA\")'."}, "NPflow": {"categories": ["Cluster"], "description": "Dirichlet process mixture of multivariate normal, skew normal or skew t-distributions\n             modeling oriented towards flow-cytometry data preprocessing applications. Method is \n             detailed in: Hejblum, Alkhassimn, Gottardo, Caron & Thiebaut (2019) <doi:10.1214/18-AOAS1209>."}, "UnifiedDoseFinding": {"categories": ["ClinicalTrials", "Pharmacokinetics"], "description": "In many phase I trials, the design goal is to find the dose associated with a certain target toxicity rate. In some trials, the goal can be to find the dose with a certain weighted sum of rates of various toxicity grades. For others, the goal is to find the dose with a certain mean value of a continuous response. This package provides the setup and calculations needed to run a dose-finding trial with non-binary endpoints and performs simulations to assess design\u2019s operating characteristics under various scenarios. Three dose finding designs are included in this package: unified phase I design (Ivanova et al. (2009) <doi:10.1111/j.1541-0420.2008.01045.x>), Quasi-CRM/Robust-Quasi-CRM (Yuan et al. (2007) <doi:10.1111/j.1541-0420.2006.00666.x>, Pan et al. (2014) <doi:10.1371/journal.pone.0098147>) and generalized BOIN design (Mu et al. (2018) <doi:10.1111/rssc.12263>). The toxicity endpoints can be handled with these functions including equivalent toxicity score (ETS), total toxicity burden (TTB), general continuous toxicity endpoints, with incorporating ordinal grade toxicity information into dose-finding procedure. These functions allow customization of design characteristics to vary sample size, cohort sizes, target dose-limiting toxicity (DLT) rates, discrete or continuous toxicity score, and incorporate safety and/or stopping rules."}, "PLMIX": {"categories": ["Psychometrics"], "description": "Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi.org/10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi/10.1002/sim.6224>."}, "swirlify": {"categories": ["TeachingStatistics"], "description": "A set of tools for writing and sharing interactive courses\n    to be used with swirl."}, "FRAPO": {"categories": ["Finance"], "description": "Accompanying package of the book 'Financial Risk Modelling\n        and Portfolio Optimisation with R', second edition. The data sets used in the book are contained in this package."}, "CDNmoney": {"categories": ["Econometrics"], "description": "Components of Canadian Credit Aggregates and Monetary Aggregates with continuity adjustments."}, "mblm": {"categories": ["Robust"], "description": "Provides linear models based on Theil-Sen\n        single median and Siegel repeated medians. They are very robust\n        (29 or 50 percent breakdown point, respectively), and if no\n        outliers are present, the estimators are very similar to OLS."}, "languagelayeR": {"categories": ["WebTechnologies"], "description": "Improve your text analysis with languagelayer\n        <https://languagelayer.com>, a powerful language detection\n        API."}, "geosphere": {"categories": ["Spatial"], "description": "Spherical trigonometry for geographic applications. That is, compute distances and related measures for angular (longitude/latitude) locations. "}, "gamboostMSM": {"categories": ["Survival"], "description": "Contains infrastructure for using mboost::gamboost() in order to estimate multistate models."}, "causact": {"categories": ["Bayesian"], "description": "Accelerate Bayesian analytics workflows in 'R' through interactive modelling,\n    visualization, and inference. Define probabilistic graphical models using directed\n    acyclic graphs (DAGs) as a unifying language for business stakeholders, statisticians, \n    and programmers. This package relies on the sleek and elegant 'greta' package for \n    Bayesian inference. 'greta', in turn, is an interface into 'TensorFlow' from 'R'. \n    Install 'greta' using instructions available here: <https://www.causact.com/install-tensorflow-greta-and-causact.html>.\n    See <https://github.com/flyaflya/causact> or <https://www.causact.com/> for more documentation."}, "Rmalschains": {"categories": ["MachineLearning", "Optimization"], "description": "An implementation of an algorithm family for continuous\n    optimization called memetic algorithms with local search chains\n    (MA-LS-Chains). Memetic algorithms are hybridizations of genetic\n    algorithms with local search methods. They are especially suited\n    for continuous optimization."}, "maxcombo": {"categories": ["Survival"], "description": "Functions for comparing survival curves using the max-combo test at a single timepoint or repeatedly at successive respective timepoints while controlling type I error (i.e., the group sequential setting), as published by Prior (2020) <doi:10.1177/0962280220931560>.  The max-combo test is a generalization of the weighted log-rank test, which itself is a generalization of the log-rank test, which is a commonly used statistical test for comparing survival curves, e.g., during or after a clinical trial as part of an effort to determine if a new drug or therapy is more effective at delaying undesirable outcomes than an established drug or therapy or a placebo."}, "thurstonianIRT": {"categories": ["Psychometrics"], "description": "Fit Thurstonian Item Response Theory (IRT) models in R. This \n  package supports fitting Thurstonian IRT models and its extensions using \n  'Stan', 'lavaan', or 'Mplus' for the model estimation. Functionality for \n  extracting results, making predictions, and simulating data is provided as \n  well. References: \n  Brown & Maydeu-Olivares (2011) <doi:10.1177/0013164410375112>;\n  B\u00fcrkner et al. (2019) <doi:10.1177/0013164419832063>."}, "metacart": {"categories": ["MetaAnalysis"], "description": "Meta-CART integrates classification and regression trees (CART) into meta-analysis. Meta-CART is a flexible approach to identify interaction effects between moderators in meta-analysis. The method is described in Dusseldorp et al. (2014) <doi:10.1037/hea0000018> and Li et al. (2017) <doi:10.1111/bmsp.12088>."}, "expsmooth": {"categories": ["Econometrics", "TimeSeries"], "description": "Data sets from the book \"Forecasting with exponential smoothing: the state space approach\" by \n\tHyndman, Koehler, Ord and Snyder (Springer, 2008)."}, "timsac": {"categories": ["Finance", "TimeSeries"], "description": "Functions for statistical analysis, prediction and control of time\n series based mainly on Akaike and Nakagawa (1988) <ISBN 978-90-277-2786-2>."}, "MetaStan": {"categories": ["MetaAnalysis"], "description": "Performs Bayesian meta-analysis, meta-regression and model-based meta-analysis \n             using 'Stan'. Includes binomial-normal hierarchical models and option to use \n             weakly informative priors for the heterogeneity parameter and the treatment effect \n             parameter which are described in Guenhan, Roever, and Friede (2020) <doi:10.1002/jrsm.1370>."}, "cocor": {"categories": ["Psychometrics"], "description": "Statistical tests for the comparison between two correlations based on either independent or dependent groups. Dependent correlations can either be overlapping or nonoverlapping. A web interface is available on the website <http://comparingcorrelations.org>. A plugin for the R GUI and IDE RKWard is included. Please install RKWard from <https://rkward.kde.org> to use this feature. The respective R package 'rkward' cannot be installed directly from a repository, as it is a part of RKWard."}, "phylin": {"categories": ["MissingData"], "description": "The spatial interpolation of genetic distances between\n\t     samples is based on a modified kriging method that\n\t     accepts a genetic distance matrix and generates a map of\n\t     probability of lineage presence. This package also offers\n\t     tools to generate a map of  potential contact zones\n\t     between groups with user-defined thresholds in the tree\n\t     to account for old and recent divergence. Additionally,\n\t     it has functions for IDW interpolation using genetic data\n\t     and midpoints."}, "disclap": {"categories": ["Distributions"], "description": "The discrete Laplace exponential family for use in fitting generalized linear models."}, "RTDE": {"categories": ["Distributions", "ExtremeValue"], "description": "Robust tail dependence estimation for bivariate models. This package is based on two papers by the authors:'Robust and bias-corrected estimation of the coefficient of tail dependence' and 'Robust and bias-corrected estimation of probabilities of extreme failure sets'. This work was supported by a research grant (VKR023480) from VILLUM FONDEN and an international project for scientific cooperation (PICS-6416)."}, "equateIRT": {"categories": ["Psychometrics"], "description": "Computation of direct, chain and average (bisector) equating coefficients with \n  standard errors using Item Response Theory (IRT) methods for dichotomous items \n  (Battauz (2013) <doi:10.1007/s11336-012-9316-y>, \n  Battauz (2015) <doi:10.18637/jss.v068.i07>). \n  Test scoring can be performed by true score equating and observed score equating methods. \n  DIF detection can be performed using a Wald-type test  \n  (Battauz (2018) <doi:10.1007/s10260-018-00442-w>)."}, "imputeR": {"categories": ["MissingData"], "description": "Multivariate Expectation-Maximization (EM) based imputation framework that offers several different algorithms. These include regularisation methods like Lasso and Ridge regression, tree-based models and dimensionality reduction methods like PCA and PLS."}, "cfma": {"categories": ["CausalInference"], "description": "Performs causal functional mediation analysis (CFMA) for functional treatment, functional mediator, and functional outcome. This package includes two functional mediation model types: (1) a concurrent mediation model and (2) a historical influence mediation model. See Zhao et al. (2018), Functional Mediation Analysis with an Application to Functional Magnetic Resonance Imaging Data, <arXiv:1805.06923> for details."}, "model4you": {"categories": ["MachineLearning"], "description": "Model-based trees for subgroup analyses in clinical trials and\n  model-based forests for the estimation and prediction of personalised\n  treatment effects (personalised models). Currently partitioning of linear\n  models, lm(), generalised linear models, glm(), and Weibull models,\n  survreg(), is supported.  Advanced plotting functionality is supported for\n  the trees and a test for parameter heterogeneity is provided for the\n  personalised models. For details on model-based trees for subgroup analyses\n  see Seibold, Zeileis and Hothorn (2016) <doi:10.1515/ijb-2015-0032>; for\n  details on model-based forests for estimation of individual treatment effects\n  see Seibold, Zeileis and Hothorn (2017) <doi:10.1177/0962280217693034>."}, "dosearch": {"categories": ["CausalInference", "MissingData"], "description": "Identification of causal effects from arbitrary observational and experimental probability distributions via do-calculus and standard probability manipulations using a search-based algorithm by Tikka et al. (2021) <doi:10.18637/jss.v099.i05>. Allows for the presence of mechanisms related to selection bias (Bareinboim, E. and Tian, J. (2015) <http://ftp.cs.ucla.edu/pub/stat_ser/r445.pdf>), transportability (Bareinboim, E. and Pearl, J. (2014) <http://ftp.cs.ucla.edu/pub/stat_ser/r443.pdf>), missing data (Mohan, K. and Pearl, J. and Tian., J. (2013) <http://ftp.cs.ucla.edu/pub/stat_ser/r410.pdf>) and arbitrary combinations of these. Also supports identification in the presence of context-specific independence (CSI) relations through labeled directed acyclic graphs (LDAG). For details on CSIs see Corander et al. (2019) <doi:10.1016/j.apal.2019.04.004>. "}, "ppmSuite": {"categories": ["MissingData"], "description": "Provides a suite of functions that fit models that use PPM type priors for partitions.\n                Models include hierarchical Gaussian and probit ordinal models with  a (covariate \n                dependent) PPM.  If a covariate dependent product partition model is selected, \n                then all the options detailed in Page, G.L.; Quintana, F.A. (2018) \n                <doi:10.1007/s11222-017-9777-z> are available.  If covariate values are missing, \n                then the approach detailed in Page, G.L.; Quintana, F.A.; Mueller, P (2020) \n                <doi:10.1080/10618600.2021.1999824> is employed.  Also included in the package is \n                a function that fits a Gaussian likelihood spatial product  partition model that is \n                detailed in Page, G.L.; Quintana, F.A. (2016)  <doi:10.1214/15-BA971>, and \n                multivariate PPM change point models that are detailed in Quinlan, J.J.; Page, G.L.; \n                Castro, L.M. (2021) <arXiv:2201.07830>."}, "routr": {"categories": ["WebTechnologies"], "description": "In order to make sure that web request ends up in the correct \n    handler function a router is often used. 'routr' is a package implementing a\n    simple but powerful routing functionality for R based servers. It is a fully\n    functional 'fiery' plugin, but can also be used with other 'httpuv' based\n    servers."}, "errum": {"categories": ["Bayesian", "Psychometrics"], "description": "Perform a Bayesian estimation of the exploratory reduced\n    reparameterized unified model (ErRUM) described by Culpepper and Chen (2018)\n    <doi:10.3102/1076998618791306>."}, "lbfgs": {"categories": ["Optimization"], "description": "A wrapper built around the libLBFGS optimization library by Naoaki Okazaki. The lbfgs package implements both the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) and the Orthant-Wise Quasi-Newton Limited-Memory (OWL-QN) optimization algorithms. The L-BFGS algorithm solves the problem of minimizing an objective, given its gradient, by iteratively computing approximations of the inverse Hessian matrix. The OWL-QN algorithm finds the optimum of an objective plus the L1-norm of the problem's parameters. The package offers a fast and memory-efficient implementation of these optimization routines, which is particularly suited for high-dimensional problems."}, "EloRating": {"categories": ["SportsAnalytics"], "description": "Provides functions to quantify animal dominance hierarchies. The major focus is on Elo rating and its ability to deal with temporal dynamics in dominance interaction sequences. For static data, David's score and de Vries' I&SI are also implemented. In addition, the package provides functions to assess transitivity, linearity and stability of dominance networks. See Neumann et al (2011) <doi:10.1016/j.anbehav.2011.07.016> for an introduction."}, "biglm": {"categories": ["HighPerformanceComputing"], "description": "Regression for data too large to fit in memory."}, "footBayes": {"categories": ["SportsAnalytics"], "description": "This is the first package allowing for the estimation,\n             visualization and prediction of the most well-known \n             football models: double Poisson, bivariate Poisson,\n             Skellam, student_t. The package allows Hamiltonian\n             Monte Carlo (HMC) estimation through the underlying Stan\n             environment and Maximum Likelihood estimation (MLE, for \n             'static' models only). The model construction relies on\n             the most well-known football references, such as \n             Dixon and Coles (1997) <doi:10.1111/1467-9876.00065>,\n             Karlis and Ntzoufras (2003) <doi:10.1111/1467-9884.00366> and\n             Egidi, Pauli and Torelli (2018) <doi:10.1177/1471082X18798414>."}, "cat": {"categories": ["MissingData"], "description": "Performs analysis of categorical-variable with missing values. Implements methods from Schafer, JL, Analysis of Incomplete Multivariate Data, Chapman and Hall."}, "Ecume": {"categories": ["Distributions"], "description": "We implement (or re-implements in R) a variety of statistical tools. They are focused on non-parametric two-sample (or k-sample) distribution comparisons in the univariate or multivariate case. See the vignette for more info."}, "SvyNom": {"categories": ["Survival"], "description": "Builds, evaluates and validates a nomogram with survey data\n    and right-censored outcomes. As described in Capanu (2015)\n    <doi:10.18637/jss.v064.c01>, the package contains functions to create\n    the nomogram, validate it using bootstrap, as well as produce the\n    calibration plots."}, "missingHE": {"categories": ["MissingData"], "description": "Contains a suite of functions for health economic evaluations with missing outcome data. \n  The package can fit different types of statistical models under a fully Bayesian approach using the software 'JAGS' (which should be installed locally and which is loaded in 'missingHE' via the 'R' package 'R2jags'). \n  Three classes of models can be fitted under a variety of missing data assumptions: selection models, pattern mixture models and hurdle models.\n  In addition to model fitting, 'missingHE' provides a set of specialised functions to assess model convergence and fit, and to summarise the statistical and economic results using different types of measures and graphs. \n  The methods implemented are described in Mason (2018) <doi:10.1002/hec.3793>, Molenberghs (2000) <doi:10.1007/978-1-4419-0300-6_18> and Gabrio (2019) <doi:10.1002/sim.8045>."}, "fsMTS": {"categories": ["TimeSeries"], "description": "Implements feature selection routines for multivariate time series (MTS). \n    The list of implemented algorithms includes: \n    own lags (independent MTS components), \n    distance-based (using external structure, e.g. Pfeifer and Deutsch (1980) <doi:10.2307/1268381>),\n    cross-correlation (see  Schelter et al. (2006, ISBN:9783527406234)),\n    graphical LASSO (see Haworth and Cheng (2014) <https://www.gla.ac.uk/media/Media_401739_smxx.pdf>),\n    random forest (see Pavlyuk (2020) \"Random Forest Variable Selection for Sparse Vector Autoregressive Models\" in Contributions to Statistics, in production),\n    least angle regression (see Gelper and Croux (2008) <https://lirias.kuleuven.be/retrieve/16024>), \n    mutual information (see  Schelter et al. (2006, ISBN:9783527406234), Liu et al. (2016) <doi:10.1109/ChiCC.2016.7554480>),\n    and partial spectral coherence (see Davis et al.(2016) <doi:10.1080/10618600.2015.1092978>). \n    In addition, the package implements functions for ensemble feature selection (using feature ranking and majority voting).\n    The package is implemented within Dmitry Pavlyuk's research project No. 1.1.1.2/VIAA/1/16/112 \"Spatiotemporal urban traffic modelling using big data\"."}, "freqdom": {"categories": ["FunctionalData", "TimeSeries"], "description": "Implementation of dynamic principal component\n    analysis (DPCA), simulation of VAR and VMA processes and frequency domain tools. \n    These frequency domain methods for dimensionality reduction of multivariate time series\n    were introduced by David Brillinger in his book Time Series (1974). We follow implementation\n    guidelines as described in Hormann, Kidzinski and Hallin (2016),\n    Dynamic Functional Principal Component <doi:10.1111/rssb.12076>."}, "RaschSampler": {"categories": ["Psychometrics"], "description": "MCMC based sampling of binary matrices with fixed margins as used in exact Rasch model tests. "}, "robsurvey": {"categories": ["OfficialStatistics", "Robust"], "description": "Robust (outlier-resistant) estimators of finite population\n    characteristics like of means, totals, ratios, regression, etc. Available\n    methods are M- and GM-estimators of regression, weight reduction,\n    trimming, and winsorization. The package extends the 'survey'\n    <https://CRAN.R-project.org/package=survey> package."}, "fame": {"categories": ["Finance", "TimeSeries"], "description": "Read and write FAME databases."}, "Pade": {"categories": ["NumericalMathematics"], "description": "Given a vector of Taylor series coefficients of sufficient length\n    as input, the function returns the numerator and denominator coefficients\n    for the Pad\u00e9 approximant of appropriate order (Baker, 1975)\n    <ISBN:9780120748556>."}, "FinAsym": {"categories": ["Finance"], "description": "This package accomplishes two tasks: a) it classifies\n        implicit trading activity from quotes in OTC markets using the\n        algorithm of Lee and Ready (1991); b) based on information for\n        trade initiation, the package computes the probability of\n        informed trading of Easley and O'Hara (1987)."}, "Delaporte": {"categories": ["Distributions"], "description": "Provides probability mass, distribution, quantile, random-variate\n    generation, and method-of-moments parameter-estimation functions for the\n    Delaporte distribution with parameterization based on Vose (2008)\n    <isbn:9780470512845>. The Delaporte is a discrete probability distribution\n    which can be considered the convolution of a negative binomial distribution\n    with a Poisson distribution. Alternatively, it can be considered a counting\n    distribution with both Poisson and negative binomial components. It has been\n    studied in actuarial science as a frequency distribution which has more\n    variability than the Poisson, but less than the negative binomial."}, "rstiefel": {"categories": ["Bayesian"], "description": "Simulation of random orthonormal matrices from linear and quadratic exponential family distributions on the Stiefel manifold. The most general type of distribution covered is the matrix-variate  Bingham-von Mises-Fisher distribution. Most of the simulation methods are presented in Hoff(2009) \"Simulation of the Matrix Bingham-von Mises-Fisher Distribution, With Applications to Multivariate and Relational Data\" <doi:10.1198/jcgs.2009.07177>. The package also includes functions for optimization on the Stiefel manifold based on algorithms described in Wen and Yin (2013) \"A feasible method for optimization with orthogonality constraints\" <doi:10.1007/s10107-012-0584-1>. "}, "llogistic": {"categories": ["Distributions"], "description": "Density, distribution function, quantile function and random generation for the L-Logistic distribution with parameters m and phi. The parameter m is the median of the distribution."}, "OneR": {"categories": ["MachineLearning"], "description": "Implements the One Rule (OneR) Machine Learning classification algorithm (Holte, R.C. (1993) <doi:10.1023/A:1022631118932>) with enhancements for sophisticated handling of numeric data and missing values together with extensive diagnostic functions. It is useful as a baseline for machine learning models and the rules are often helpful heuristics."}, "yuima": {"categories": ["TimeSeries"], "description": "Simulation and Inference for SDEs and Other Stochastic Processes."}, "fastLink": {"categories": ["MissingData", "OfficialStatistics"], "description": "Implements a Fellegi-Sunter probabilistic record linkage model that allows for missing data\n    and the inclusion of auxiliary information. This includes functionalities to conduct a merge of two \n    datasets under the Fellegi-Sunter model using the Expectation-Maximization algorithm. In addition, \n    tools for preparing, adjusting, and summarizing data merges are included. The package implements methods \n    described in Enamorado, Fifield, and Imai (2019) \u201dUsing a Probabilistic Model to Assist Merging of \n    Large-scale Administrative Records\u201d, American Political Science Review and is available \n    at <http://imai.fas.harvard.edu/research/linkage.html>."}, "permutations": {"categories": ["NumericalMathematics"], "description": "Manipulates invertible functions from a finite set to itself.  Can transform from word form to cycle form and back."}, "SpatialExtremes": {"categories": ["ExtremeValue", "Spatial"], "description": "Tools for the statistical modelling of spatial extremes using max-stable processes, copula or Bayesian hierarchical models. More precisely, this package allows (conditional) simulations from various parametric max-stable models, analysis of the extremal spatial dependence, the fitting of such processes using composite likelihoods or least square (simple max-stable processes only), model checking and selection and prediction. Other approaches (although not completely in agreement with the extreme value theory) are available such as the use of (spatial) copula and Bayesian hierarchical models assuming the so-called conditional assumptions. The latter approaches is handled through an (efficient) Gibbs sampler. Some key references: Davison et al. (2012) <doi:10.1214/11-STS376>, Padoan et al. (2010) <doi:10.1198/jasa.2009.tm08577>, Dombry et al. (2013) <doi:10.1093/biomet/ass067>."}, "hht": {"categories": ["TimeSeries"], "description": "Builds on the EMD package to provide additional tools for empirical mode decomposition (EMD) and Hilbert spectral analysis. It also implements the ensemble empirical decomposition (EEMD) and the complete ensemble empirical mode decomposition (CEEMD) methods to avoid mode mixing and intermittency problems found in EMD analysis.  The package comes with several plotting methods that can be used to view intrinsic mode functions, the HHT spectrum, and the Fourier spectrum. "}, "rema": {"categories": ["MetaAnalysis"], "description": "The rema package implements a permutation-based approach for binary \n\tmeta-analyses of 2x2 tables, founded on conditional logistic regression, \n\tthat provides more reliable statistical tests when heterogeneity is \n\tobserved in rare event data (Zabriskie et al. 2021 <doi:10.1002/sim.9142>). \n\tTo adjust for the effect of heterogeneity, this method conditions on the \n\tsufficient statistic of a proxy for the heterogeneity effect as opposed to\n\testimating the heterogeneity variance. While this results in the model not\n\tstrictly falling under the random-effects framework, it is akin to a \n\trandom-effects approach in that it assumes differences in variability due \n\tto treatment. Further, this method does not rely on large-sample \n\tapproximations or continuity corrections for rare event data. This method\n\tuses the permutational distribution of the test statistic instead of\n\tasymptotic approximations for inference. The number of observed events \n\tdrives the computation complexity for creating this permutational \n\tdistribution. Accordingly, for this method to be computationally feasible,\n\tit should only be applied to meta-analyses with a relatively low number of\n\tobserved events. To create this permutational distribution, a network \n\talgorithm, based on the work of Mehta et al. (1992) <doi:10.2307/1390598> \n\tand Corcoran et al. (2001) <doi:10.1111/j.0006-341x.2001.00941.x>, is \n\temployed using C++ and integrated into the package."}, "subselect": {"categories": ["ChemPhys"], "description": "A collection of functions which (i) assess the quality of variable subsets as surrogates for a full data set, in either an exploratory data analysis or in the context of a multivariate linear model, and (ii) search for subsets which are optimal under various criteria. Theoretical support for the heuristic search methods and exploratory data analysis criteria is in Cadima, Cerdeira, Minhoto (2003, <doi:10.1016/j.csda.2003.11.001>). Theoretical support for the leap and bounds algorithm and the criteria for the general multivariate linear model is in Duarte Silva (2001, <doi:10.1006/jmva.2000.1920>). There is a package vignette \"subselect\", which includes additional references."}, "BASS": {"categories": ["Bayesian"], "description": "Bayesian fitting and sensitivity analysis methods for adaptive\n    spline surfaces described in <doi:10.18637/jss.v094.i08>. Built to handle continuous and categorical inputs as well as\n    functional or scalar output. An extension of the methodology in Denison, Mallick\n    and Smith (1998) <doi:10.1023/A:1008824606259>."}, "rodeo": {"categories": ["DifferentialEquations"], "description": "Provides an R6 class and several utility methods to\n    facilitate the implementation of models based on ordinary\n    differential equations. The heart of the package is a code generator\n    that creates compiled 'Fortran' (or 'R') code which can be passed to\n    a numerical solver. There is direct support for solvers contained\n    in packages 'deSolve' and 'rootSolve'."}, "NBAloveR": {"categories": ["SportsAnalytics"], "description": "Provides interface to the online basketball data resources such as\n      Basketball reference API <https://www.basketball-reference.com/> and helps\n      R users analyze basketball data."}, "joineR": {"categories": ["Survival"], "description": "Analysis of repeated measurements and time-to-event data via random\n    effects joint models. Fits the joint models proposed by Henderson and colleagues\n    <doi:10.1093/biostatistics/1.4.465> (single event time) and by Williamson and\n    colleagues (2008) <doi:10.1002/sim.3451> (competing risks events time) to a\n    single continuous repeated measure. The time-to-event data is modelled using a \n    (cause-specific) Cox proportional hazards regression model with time-varying \n    covariates. The longitudinal outcome is modelled using a linear mixed effects\n    model. The association is captured by a latent Gaussian process. The model is \n    estimated using am Expectation Maximization algorithm. Some plotting functions \n    and the variogram are also included. This project is funded by the Medical \n    Research Council (Grant numbers G0400615 and MR/M013227/1)."}, "zCompositions": {"categories": ["MissingData"], "description": "Principled methods for the imputation of zeros, left-censored and missing data in\n    compositional data sets (Palarea-Albaladejo and Martin-Fernandez (2015) <doi:10.1016/j.chemolab.2015.02.019>)."}, "dlookr": {"categories": ["MissingData"], "description": "A collection of tools that support data diagnosis, exploration, and transformation. \n    Data diagnostics provides information and visualization of missing values and outliers and \n    unique and negative values to help you understand the distribution and quality of your data. \n    Data exploration provides information and visualization of the descriptive statistics of \n    univariate variables, normality tests and outliers, correlation of two variables, and \n    relationship between target variable and predictor. Data transformation supports binning \n    for categorizing continuous variables, imputates missing values and outliers, resolving skewness. \n    And it creates automated reports that support these three tasks."}, "rtweet": {"categories": ["WebTechnologies"], "description": "An implementation of calls designed to collect and organize\n    Twitter data via Twitter's REST and stream Application Program\n    Interfaces (API), which can be found at the following URL:\n    <https://developer.twitter.com/en/docs>."}, "pks": {"categories": ["Psychometrics"], "description": "Fitting and testing probabilistic knowledge structures,\n  especially the basic local independence model (BLIM, Doignon & Flamagne,\n  1999), using the minimum discrepancy maximum likelihood (MDML) method\n  (Heller & Wickelmaier, 2013 <doi:10.1016/j.endm.2013.05.145>)."}, "dtw": {"categories": ["TimeSeries"], "description": "A comprehensive implementation of dynamic time warping\n    (DTW) algorithms in R.  DTW computes the optimal (least cumulative\n    distance) alignment between points of two time series.  Common DTW\n    variants covered include local (slope) and global (window)\n    constraints, subsequence matches, arbitrary distance definitions,\n    normalizations, minimum variance matching, and so on.  Provides\n    cumulative distances, alignments, specialized plot styles, etc.,\n    as described in Giorgino (2009) <doi:10.18637/jss.v031.i07>."}, "network": {"categories": ["GraphicalModels"], "description": "Tools to create and modify network objects.  The network class can represent a range of relational data types, and supports arbitrary vertex/edge/graph attributes."}, "seasonal": {"categories": ["TimeSeries"], "description": "Easy-to-use interface to X-13-ARIMA-SEATS, the seasonal adjustment\n    software by the US Census Bureau. It offers full access to almost all\n    options and outputs of X-13, including X-11 and SEATS, automatic ARIMA model\n    search, outlier detection and support for user defined holiday variables,\n    such as Chinese New Year or Indian Diwali. A graphical user interface can be\n    used through the 'seasonalview' package. Uses the X-13-binaries from the\n    'x13binary' package."}, "adegenet": {"categories": ["Epidemiology"], "description": "Toolset for the exploration of genetic and genomic\n    data. Adegenet provides formal (S4) classes for storing and handling\n    various genetic data, including genetic markers with varying ploidy\n    and hierarchical population structure ('genind' class), alleles counts\n    by populations ('genpop'), and genome-wide SNP data ('genlight'). It\n    also implements original multivariate methods (DAPC, sPCA), graphics,\n    statistical tests, simulation tools, distance and similarity measures,\n    and several spatial methods. A range of both empirical and simulated\n    datasets is also provided to illustrate various methods."}, "robets": {"categories": ["TimeSeries"], "description": "We provide an outlier robust alternative of the function ets() in the 'forecast' package of Hyndman and Khandakar (2008) <doi:10.18637/jss.v027.i03>. For each method of a class of exponential smoothing variants we made a robust alternative. The class includes methods with a damped trend and/or seasonal components. The robust method is developed by robustifying every aspect of the original exponential smoothing variant. We provide robust forecasting equations, robust initial values, robust smoothing parameter estimation and a robust information criterion. The method is described in more detail in Crevits and Croux (2016) <doi:10.13140/RG.2.2.11791.18080>."}, "eha": {"categories": ["Survival"], "description": "Parametric proportional hazards fitting with left truncation and\n        right censoring for common families of distributions, piecewise constant \n        hazards, and discrete models. Parametric accelerated failure time models\n        for left truncated and right censored data. Proportional hazards\n        models for tabular and register data. Sampling of risk sets in Cox \n        regression, selections in the Lexis diagram, bootstrapping. \n        Brostr\u00f6m (2012) <doi:10.1201/9781315373942>."}, "registr": {"categories": ["FunctionalData"], "description": "A method for registering curves (functional data) that are generated from exponential family distributions.  This \n    implements the algorithms described in 'Wrobel et al. (2019)' <doi:10.1111/biom.12963>. Curve registration is an active area of \n    research in functional data analysis, and can be used to better understand patterns in functional data by separating curves \n    into phase and amplitude variability. This software handles both binary and continuous functional data, and is\n    especially applicable in accelerometry and wearable technology."}, "shiny": {"categories": ["TeachingStatistics", "WebTechnologies"], "description": "Makes it incredibly easy to build interactive web\n    applications with R. Automatic \"reactive\" binding between inputs and\n    outputs and extensive prebuilt widgets make it possible to build\n    beautiful, responsive, and powerful applications with minimal effort."}, "crunchy": {"categories": ["WebTechnologies"], "description": "To facilitate building custom dashboards on the Crunch data\n    platform <https://crunch.io/>, the 'crunchy' package provides tools for\n    working with 'shiny'. These tools include utilities to manage authentication\n    and authorization automatically and custom stylesheets to help match the\n    look and feel of the Crunch web application. The package also includes\n    several gadgets for use in 'RStudio'."}, "BHH2": {"categories": ["ExperimentalDesign"], "description": "Functions and data sets reproducing some examples in\n             Box, Hunter and Hunter II.  Useful for statistical design\n             of experiments, especially factorial experiments.  "}, "KenSyn": {"categories": ["MetaAnalysis"], "description": "Demo and dataset accompaying the books :\n\tDe l'analyse des r\u00e9seaux exp\u00e9rimentaux \u00e0 la m\u00e9ta-analyse: M\u00e9thodes et applications avec le logiciel R pour les sciences agronomiques et environnementales (Published 2018-06-28, Quae, for french version) by David Makowski, Francois Piraux and Francois Brun - <https://www.quae.com/produit/1514/9782759228164/de-l-analyse-des-reseaux-experimentaux-a-la-meta-analyse>\n\tKnowledge Synthesis in Agriculture : from Experimental Network to Meta-Analysis (in preparation for 2018-06, Springer , for English version) by David Makowski, Francois Piraux and Francois Brun\n\tA full description of all the material is in both books.\n\tACKNOWLEDGMENTS : The French network \"RMT modeling and data analysis for agriculture\" (<http://www.modelia.org>) have contributed to the development of this R package. This project and network are lead by ACTA (French Technical Institute for Agriculture) and was funded by a grant from the Ministry of Agriculture and Fishing of France."}, "meteo": {"categories": ["Hydrology"], "description": "Spatio-temporal geostatistical mapping of meteorological data. Global spatio-temporal models calculated using publicly available data are stored in package."}, "blocksdesign": {"categories": ["CausalInference", "ExperimentalDesign"], "description": "Constructs treatment and block designs for linear treatment models\n  with crossed or nested block factors. The treatment design can be any feasible \n  linear model and the block design can be any feasible combination of crossed or \n  nested block factors. The block design is a sum of one or more block factors\n  and the block design is optimized sequentially with the levels of each successive\n  block factor optimized conditional on all previously optimized block factors. \n  D-optimality is used throughout except for square or rectangular lattice block designs \n  which are constructed algebraically using mutually orthogonal Latin squares.\n  Crossed block designs with interaction effects are optimized using a weighting scheme\n  which allows for differential weighting of first and second-order block effects. \n  Outputs include a table showing the allocation of treatments to blocks and tables showing\n  the achieved D-efficiency factors for each block and treatment design.  \n  Edmondson, R.N. Multi-level Block Designs for Comparative Experiments. \n  JABES 25, 500\u2013522 (2020) <doi:10.1007/s13253-020-00416-0>."}, "ptsuite": {"categories": ["ExtremeValue"], "description": "Various estimation methods for the shape parameter of Pareto\n    distributed data. This package contains functions for various estimation \n    methods such as maximum likelihood \n    (Newman, 2005)<doi:10.1016/j.cities.2012.03.001>, \n    Hill's estimator (Hill, 1975)<doi:10.1214/aos/1176343247>, \n    least squares (Zaher et al., 2014)<doi:10.9734/BJMCS/2014/10890>, \n    method of moments (Rytgaard, 1990)<doi:10.2143/AST.20.2.2005443>, \n    percentiles (Bhatti et al., 2018)<doi:10.1371/journal.pone.0196456>,\n    and weighted least squares (Nair et al., 2019) to estimate the shape \n    parameter of Pareto distributed data. It also provides both a heuristic \n    method (Hubert et al., 2013)<doi:10.1016/j.csda.2012.07.011> and a \n    goodness of fit test \n    (Gulati and Shapiro, 2008)<doi:10.1007/978-0-8176-4619-6> for testing for \n    Pareto data as well as a method for generating Pareto distributed data."}, "mapproj": {"categories": ["Spatial"], "description": "Converts latitude/longitude into projected coordinates."}, "pps": {"categories": ["OfficialStatistics"], "description": "Functions to select samples using PPS (probability proportional to size) sampling. The package also includes a function for stratified simple random sampling, a function to compute joint inclusion probabilities for Sampford's method of PPS sampling, and a few utility functions. The user's guide pps-ug.pdf is included in the .../pps/doc directory. The methods are described in standard survey sampling theory books such as Cochran's \"Sampling Techniques\"; see the user's guide for references."}, "mapSpain": {"categories": ["Spatial"], "description": "Administrative Boundaries of Spain at several levels\n    (Autonomous Communities, Provinces, Municipalities) based on the\n    'GISCO' 'Eurostat' database <https://ec.europa.eu/eurostat/web/gisco>\n    and 'CartoBase SIANE' from 'Instituto Geografico Nacional'\n    <https://www.ign.es/>.  It also provides a 'leaflet' plugin and the\n    ability of downloading and processing static tiles."}, "multcomp": {"categories": ["ClinicalTrials", "Survival"], "description": "Simultaneous tests and confidence intervals\n  for general linear hypotheses in parametric models, including \n  linear, generalized linear, linear mixed effects, and survival models.\n  The package includes demos reproducing analyzes presented\n  in the book \"Multiple Comparisons Using R\" (Bretz, Hothorn, \n  Westfall, 2010, CRC Press)."}, "linpk": {"categories": ["Pharmacokinetics"], "description": "Generate concentration-time profiles from linear pharmacokinetic\n  (PK) systems, possibly with first-order absorption or zero-order infusion,\n  possibly with one or more peripheral compartments, and possibly under\n  steady-state conditions. Single or multiple doses may be specified. Secondary\n  (derived) PK parameters (e.g. Cmax, Ctrough, AUC, Tmax, half-life, etc.) are\n  computed."}, "lordif": {"categories": ["Psychometrics"], "description": "Analysis of Differential Item Functioning (DIF) for\n        dichotomous and polytomous items using an iterative hybrid of\n        ordinal logistic regression and item response theory (IRT)."}, "GWmodel": {"categories": ["Spatial"], "description": "Techniques from a particular branch of spatial statistics,termed geographically-weighted (GW) models. GW models suit situations when data are not described well by some global model, but where there are spatial regions where a suitably localised calibration provides a better description. 'GWmodel' includes functions to calibrate: GW summary statistics (Brunsdon et al., 2002)<doi:10.1016/s0198-9715(01)00009-6>, GW principal components analysis (Harris et al., 2011)<doi:10.1080/13658816.2011.554838>, GW discriminant analysis (Brunsdon et al., 2007)<doi:10.1111/j.1538-4632.2007.00709.x> and various forms of GW regression (Brunsdon et al., 1996)<doi:10.1111/j.1538-4632.1996.tb00936.x>; some of which are provided in basic and robust (outlier resistant) forms."}, "face": {"categories": ["FunctionalData"], "description": "We implement the Fast Covariance Estimation for \n             Sparse Functional Data paper published in Statistics and Computing <doi:10.1007/s11222-017-9744-8>."}, "highr": {"categories": ["ReproducibleResearch"], "description": "Provides syntax highlighting for R source code. Currently it\n    supports LaTeX and HTML output. Source code of other languages is supported\n    via Andre Simon's highlight package (<http://www.andre-simon.de>)."}, "crimCV": {"categories": ["Cluster"], "description": "A finite mixture of Zero-Inflated Poisson (ZIP) models for analyzing criminal trajectories."}, "pracma": {"categories": ["DifferentialEquations", "NumericalMathematics"], "description": "\n    Provides a large number of functions from numerical analysis and\n    linear algebra, numerical optimization, differential equations,\n    time series, plus some well-known special mathematical functions.\n    Uses 'MATLAB' function names where appropriate to simplify porting."}, "foieGras": {"categories": ["SpatioTemporal", "Tracking"], "description": "Fits continuous-time random walk and correlated random walk state-space models for quality control animal tracking data ('Argos', processed light-level 'geolocation', 'GPS'). Template Model Builder ('TMB') is used for fast estimation. The 'Argos' data can be: (older) least squares-based locations; (newer) Kalman filter-based locations with error ellipse information; or a mixture of both. The models estimate two sets of location states corresponding to: 1) each observation, which are (usually) irregularly timed; and 2) user-specified time intervals (regular or irregular). Latent variable models are provided to estimate move persistence along tracks as an index of behaviour. Track simulation functions are provided. 'Jonsen I', 'McMahon CR', 'Patterson TA', 'Auger-M\u00e9th\u00e9 M', 'Harcourt R', 'Hindell MA', 'Bestley S' (2019) Movement responses to environment: fast inference of variation among southern elephant seals with a mixed effects model. Ecology 100:e02566 <doi:10.1002/ecy.2566>."}, "RGENERATE": {"categories": ["TimeSeries"], "description": "A method 'generate()' is implemented in this package for the random\n    generation of vector time series according to models obtained by 'RMAWGEN',\n    'vars' or other packages.  This package was created to generalize the\n    algorithms of the 'RMAWGEN' package for the analysis and generation of any\n    environmental vector time series."}, "pkdata": {"categories": ["Pharmacokinetics"], "description": "Prepare pharmacokinetic/pharmacodynamic (PK/PD) data for PK/PD analyses. \n    This package provides functions to standardize infusion and bolus dose data while\n    linking it to drug level or concentration data."}, "hydropeak": {"categories": ["Hydrology"], "description": "An important environmental impact on running water ecosystems \n    is caused by hydropeaking - the discontinuous release of turbine water \n    because of peaks of energy demand. An event-based algorithm is implemented \n    to detect flow fluctuations referring to increase events (IC) and decrease \n    events (DC). For each event, a set of parameters related to the fluctuation \n    intensity is calculated. The framework is introduced in Greimel et al. (2016) \n    \"A method to detect and characterize sub-daily flow fluctuations\" \n    <doi:10.1002/hyp.10773> and can be used to identify different fluctuation \n    types according to the potential source: e.g., sub-daily flow fluctuations \n    caused by hydropeaking, rainfall, or snow and glacier melt.\n    This is a companion to the package 'hydroroute', which is used to detect and \n    follow hydropower plant-specific hydropeaking waves at the sub-catchment \n    scale and to describe how hydropeaking flow parameters change along the\n    longitudinal flow path as proposed and validated in Greimel et al. (2022)."}, "DHS.rates": {"categories": ["OfficialStatistics"], "description": "Calculates key indicators such as fertility rates (Total Fertility Rate (TFR), General Fertility Rate (GFR), \n  and Age Specific Fertility Rate (ASFR)) using Demographic and Health Survey (DHS) women/individual data, \n  childhood mortality probabilities and rates such as Neonatal Mortality Rate (NNMR), Post-neonatal Mortality Rate (PNNMR), \n  Infant Mortality Rate (IMR), Child Mortality Rate (CMR), and Under-five Mortality Rate (U5MR), and adult mortality indicators \n  such as the Age Specific Mortality Rate (ASMR), Age Adjusted Mortality Rate (AAMR), Age Specific Maternal Mortality Rate (ASMMR),\n  Age Adjusted Maternal Mortality Rate (AAMMR), Age Specific Pregnancy Related Mortality Rate (ASPRMR), \n  Age Adjusted Pregnancy Related Mortality Rate (AAPRMR), Maternal Mortality Ratio (MMR) and Pregnancy Related Mortality Ratio (PRMR).  \n  In addition to the indicators, the 'DHS.rates' package estimates sampling errors indicators such as Standard Error (SE), \n  Design Effect (DEFT), Relative Standard Error (RSE) and Confidence Interval (CI). \n  The package is developed according to the DHS methodology of calculating the fertility indicators and \n  the childhood mortality rates outlined in the \n  \"Guide to DHS Statistics\" (Croft, Trevor N., Aileen M. J. Marshall, Courtney K. Allen, et al. 2018, <https://dhsprogram.com/Data/Guide-to-DHS-Statistics/index.cfm>) \n  and the DHS methodology of estimating the sampling errors indicators outlined in \n  the \"DHS Sampling and Household Listing Manual\" (ICF International 2012, <https://dhsprogram.com/pubs/pdf/DHSM4/DHS6_Sampling_Manual_Sept2012_DHSM4.pdf>)."}, "longCatEDA": {"categories": ["OfficialStatistics"], "description": "Methods for plotting categorical longitudinal and time-series data by mapping individuals to the vertical space (each horizontal line represents a participant), time (or repeated measures) to the horizontal space, categorical (or discrete) states as facets using color or shade, and events to points using plotting characters. Sorting individuals in the vertical space and (or) stratifying them by groups can reveal patterns in the changes over time."}, "rmoo": {"categories": ["Optimization"], "description": "A multiobjective optimization package based on K. Deb's \n    algorithm and inspired in 'GA' package by Luca Scrucca (2017) <doi:10.32614/RJ-2017-008>. \n    The 'rmoo' package is a framework for multi- and many-objective optimization, \n    allowing to work with representation of real numbers, permutations and \n    binaries, offering a high range of configurations."}, "IsingSampler": {"categories": ["Psychometrics"], "description": "Sample states from the Ising model and compute the probability of states. Sampling can be done for any number of nodes, but due to the intractibility of the Ising model the distribution can only be computed up to ~10 nodes."}, "simPH": {"categories": ["Survival"], "description": "Simulates and plots quantities of interest (relative\n    hazards, first differences, and hazard ratios) for linear coefficients,\n    multiplicative interactions, polynomials, penalised splines, and\n    non-proportional hazards, as well as stratified survival curves from Cox\n    Proportional Hazard models. It also simulates and plots marginal effects\n    for multiplicative interactions. Methods described in Gandrud (2015)\n    <doi:10.18637/jss.v065.i03>."}, "RATest": {"categories": ["Econometrics"], "description": "A collection of randomization tests, data sets and examples. The current version focuses on five testing problems and their implementation in empirical work. First, it facilitates the empirical researcher to test for particular hypotheses, such as comparisons of means, medians, and variances from k populations using robust permutation tests, which asymptotic validity holds under very weak assumptions, while retaining the exact rejection probability in finite samples when the underlying distributions are identical. Second, the description and implementation of a permutation test for testing the continuity assumption of the baseline covariates in the sharp regression discontinuity design (RDD) as in Canay and Kamat (2018) <https://goo.gl/UZFqt7>. More specifically, it allows the user to select a set of covariates and test the aforementioned hypothesis using a permutation test based on the Cramer-von Misses test statistic. Graphical inspection of the empirical CDF and histograms for the variables of interest is also supported in the package. Third, it provides the practitioner with an effortless implementation of a permutation test based on the martingale decomposition of the empirical process for testing for heterogeneous treatment effects in the presence of an estimated nuisance parameter as in Chung and Olivares (2021) <doi:10.1016/j.jeconom.2020.09.015>. Fourth, this version considers the two-sample goodness-of-fit testing problem under covariate adaptive randomization and implements a permutation test based on a prepivoted Kolmogorov-Smirnov test statistic. Lastly, it implements an asymptotically valid permutation test based on the quantile process for the hypothesis of constant quantile treatment effects in the presence of an estimated nuisance parameter."}, "SynthTools": {"categories": ["MissingData"], "description": "A set of functions to support experimentation in the utility of partially synthetic data sets.  All functions compare an observed data set to one or a set of partially synthetic data sets derived from the observed data to (1) check that data sets have identical attributes, (2) calculate overall and specific variable perturbation rates, (3) check for potential logical inconsistencies, and (4) calculate confidence intervals and standard errors of desired variables in multiple imputed data sets. Confidence interval and standard error formulas have options for either synthetic data sets or multiple imputed data sets. For more information on the formulas and methods used, see Reiter & Raghunathan (2007) <doi:10.1198/016214507000000932>."}, "TrialSize": {"categories": ["ClinicalTrials"], "description": "Functions and Examples in Sample Size Calculation in\n        Clinical Research."}, "cmprsk": {"categories": ["Epidemiology", "Survival"], "description": "Estimation, testing and regression modeling of\n subdistribution functions in competing risks, as described in Gray\n (1988), A class of K-sample tests for comparing the cumulative\n incidence of a competing risk, Ann. Stat. 16:1141-1154\n <doi:10.1214/aos/1176350951>, and Fine JP and\n Gray RJ (1999), A proportional hazards model for the subdistribution\n of a competing risk, JASA, 94:496-509, <doi:10.1080/01621459.1999.10474144>."}, "corona": {"categories": ["Epidemiology"], "description": "Manipulate and view coronavirus data and other societally relevant data at a basic level."}, "cmaesr": {"categories": ["Optimization"], "description": "Pure R implementation of the Covariance Matrix Adaptation -\n    Evolution Strategy (CMA-ES) with optional restarts (IPOP-CMA-ES)."}, "SemiMarkov": {"categories": ["Survival"], "description": "Functions for fitting multi-state semi-Markov models to longitudinal data. A parametric maximum likelihood estimation method adapted to deal with Exponential, Weibull and Exponentiated Weibull distributions is considered. Right-censoring can be taken into account and both constant and time-varying covariates can be included using a Cox proportional model. Reference: A. Krol and P. Saint-Pierre (2015) \t<doi:10.18637/jss.v066.i06>."}, "runner": {"categories": ["TimeSeries"], "description": "Lightweight library for rolling windows operations. Package enables\n  full control over the window length, window lag and a time indices. With a runner \n  one can apply any R function on a rolling windows. The package eases work with \n  equally and unequally spaced time series."}, "pso": {"categories": ["Optimization"], "description": "Provides an implementation of particle swarm optimisation consistent with the standard PSO 2007/2011 by Maurice Clerc. Additionally a number of ancillary routines are provided for easy testing and graphics."}, "tsdecomp": {"categories": ["TimeSeries"], "description": "ARIMA-model-based decomposition of quarterly and \n monthly time series data.\n The methodology is developed and described, among others, in \n Burman (1980) <doi:10.2307/2982132> and \n Hillmer and Tiao (1982) <doi:10.2307/2287770>."}, "httr": {"categories": ["WebTechnologies"], "description": "Useful tools for working with HTTP organised by HTTP verbs\n    (GET(), POST(), etc). Configuration functions make it easy to control\n    additional request components (authenticate(), add_headers() and so\n    on)."}, "assemblerr": {"categories": ["Pharmacokinetics"], "description": "Construct pharmacometric nonlinear mixed effect models by combining \n    predefined model components and automatically generate model code for NONMEM. \n    Models are created by combining parameter and observation models, algebraic \n    relationships, compartments, and flows. Pharmacokinetic models can be assembled\n    from the higher-order components: absorption, distribution, and elimination. \n    The generated code is optimized for performance by recognizing, for example,\n    linear differential equations or differential equations with an analytic \n    solution."}, "RYandexTranslate": {"categories": ["WebTechnologies"], "description": "'Yandex Translate' (https://translate.yandex.com/) is a statistical machine translation system.\n\tThe system translates separate words, complete texts, and webpages.\n\tThis package can be used to detect language from text and to translate it to supported target language.\n\tFor more info: https://tech.yandex.com/translate/doc/dg/concepts/About-docpage/ ."}, "Mediana": {"categories": ["ClinicalTrials"], "description": "Provides a general framework for clinical trial simulations based\n    on the Clinical Scenario Evaluation (CSE) approach. The package supports a\n    broad class of data models (including clinical trials with continuous, binary,\n    survival-type and count-type endpoints as well as multivariate outcomes that are\n    based on combinations of different endpoints), analysis strategies and commonly\n    used evaluation criteria."}, "mmeta": {"categories": ["MetaAnalysis"], "description": "A novel multivariate meta-analysis."}, "mssm": {"categories": ["TimeSeries"], "description": "Provides methods to perform parameter estimation and \n  make analysis of multivariate observed outcomes through time which depends \n  on a latent state variable. All methods scale well in the dimension \n  of the observed outcomes at each time point. The package contains an \n  implementation of a Laplace approximation, particle filters like \n  suggested by Lin, Zhang, Cheng, & Chen (2005)\n  <doi:10.1198/016214505000000349>, and the gradient and observed information\n  matrix approximation suggested by Poyiadjis, Doucet, & Singh (2011) \n  <doi:10.1093/biomet/asq062>."}, "kofnGA": {"categories": ["Optimization"], "description": "Provides a function that uses a genetic algorithm to search for a subset\n of size k from the integers 1:n, such that a user-supplied objective function \n is minimized at that subset.  The selection step is done by tournament selection \n based on ranks, and elitism may be used to retain a portion of the best solutions \n from one generation to the next. Population objective function values may \n optionally be evaluated in parallel."}, "missMethods": {"categories": ["MissingData"], "description": "Supply functions for the creation and handling of missing\n    data as well as tools to evaluate missing data methods. Nearly all\n    possibilities of generating missing data discussed by Santos et al.\n    (2019) <doi:10.1109/ACCESS.2019.2891360> and some additional are\n    implemented.  Functions are supplied to compare parameter estimates\n    and imputed values to true values to evaluate missing data methods.\n    Evaluations of these types are done, for example, by Cetin-Berber et\n    al. (2019) <doi:10.1177/0013164418805532> and Kim et al. (2005)\n    <doi:10.1093/bioinformatics/bth499>."}, "rsdmx": {"categories": ["OfficialStatistics", "WebTechnologies"], "description": "Set of classes and methods to read data and metadata documents\n  exchanged through the Statistical Data and Metadata Exchange (SDMX) framework,\n  currently focusing on the SDMX XML standard format (SDMX-ML)."}, "metagam": {"categories": ["MetaAnalysis"], "description": "Meta-analysis of generalized additive\n    models and generalized additive mixed models. A typical use case is\n    when data cannot be shared across locations, and an overall meta-analytic\n    fit is sought. 'metagam' provides functionality for removing individual\n    participant data from models computed using the 'mgcv' and 'gamm4' packages such\n    that the model objects can be shared without exposing individual data.\n    Furthermore, methods for meta-analysing these fits are provided. The implemented\n    methods are described in Sorensen et al. (2020), <doi:10.1016/j.neuroimage.2020.117416>,\n    extending previous works by Schwartz and Zanobetti (2000)\n    and Crippa et al. (2018) <doi:10.6000/1929-6029.2018.07.02.1>."}, "distributionsrd": {"categories": ["Distributions"], "description": "A library of density, distribution function, quantile function, (bounded) raw moments and random generation for a collection of distributions relevant for the firm size literature. Additionally, the package contains tools to fit these distributions using maximum likelihood and evaluate these distributions based on (i) log-likelihood ratio and (ii) deviations between the empirical and parametrically implied moments of the distributions. We add flexibility by allowing the considered distributions to be combined into piecewise composite or finite mixture distributions, as well as to be used when truncated. See Dewitte (2020) <https://hdl.handle.net/1854/LU-8644700> for a description and application of methods available in this package. "}, "causalsens": {"categories": ["CausalInference"], "description": "The causalsens package provides functions to perform sensitivity analyses and to study how various assumptions about selection bias affects estimates of causal effects."}, "DTSg": {"categories": ["MissingData", "TimeSeries"], "description": "Basic time series functionalities such as listing of missing\n    values, application of arbitrary aggregation as well as rolling (asymmetric)\n    window functions and automatic detection of periodicity. As it is mainly\n    based on 'data.table', it is fast and - in combination with the 'R6'\n    package - offers reference semantics. In addition to its native R6\n    interface, it provides an S3 interface for those who prefer the latter.\n    Finally yet importantly, its functional approach allows for incorporating\n    functionalities from many other packages."}, "disk.frame": {"categories": ["HighPerformanceComputing"], "description": "A disk-based data manipulation tool for working with \n  large-than-RAM datasets. Aims to lower the barrier-to-entry for \n  manipulating large datasets by adhering closely to popular and \n  familiar data manipulation paradigms like 'dplyr' verbs and \n  'data.table' syntax."}, "BaSkePro": {"categories": ["Bayesian"], "description": "Tool to perform Bayesian inference of carcass processing/transport strategy and bone attrition from archaeofaunal skeletal profiles characterized by percentages of MAU (Minimum Anatomical Units). The approach is based on a generative model for skeletal profiles that replicates the two phases of formation of any faunal assemblage: initial accumulation as a function of human transport strategies and subsequent attrition.Two parameters define this model: 1) the transport preference (alpha), which can take any value between - 1 (mostly axial contribution) and 1 (mostly appendicular contribution) following strategies constructed as a function of butchering efficiency of different anatomical elements and the results of ethnographic studies, and 2) degree of attrition (beta), which can vary between 0 (no attrition) and 10 (maximum attrition) and relates the survivorship of bone elements to their maximum bone density. Starting from uniform prior probability distribution functions of alpha and beta, a Monte Carlo Markov Chain sampling based on a random walk Metropolis-Hasting algorithm is adopted to derive the posterior probability distribution functions, which are then available for interpretation. During this process, the likelihood of obtaining the observed percentages of MAU given a pair of parameter values is estimated by the inverse of the Chi2 statistic, multiplied by the proportion of elements within a 1 percent of the observed value. See Ana B. Marin-Arroyo, David Ocio (2018).<doi:10.1080/08912963.2017.1336620>."}, "seriation": {"categories": ["Cluster"], "description": "Infrastructure for ordering objects with an implementation of several\n    seriation/sequencing/ordination techniques to reorder matrices, dissimilarity\n    matrices, and dendrograms. Also provides (optimally) reordered heatmaps,\n    color images and clustering visualizations like dissimilarity plots, and\n    visual assessment of cluster tendency plots (VAT and iVAT). Hahsler et al (2008) <doi:10.18637/jss.v025.i03>."}, "DiceView": {"categories": ["ExperimentalDesign"], "description": "View 2D/3D sections, contour plots, mesh of excursion sets for computer experiments designs, surrogates or test functions."}, "clv": {"categories": ["Cluster"], "description": "Package contains most of the popular internal and external\n        cluster validation methods ready to use for the most of the\n        outputs produced by functions coming from package \"cluster\".\n        Package contains also functions and examples of usage for\n        cluster stability approach that might be applied to algorithms\n        implemented in \"cluster\" package as well as user defined\n        clustering algorithms."}, "aod": {"categories": ["Econometrics", "Environmetrics"], "description": "Provides a set of functions to analyse\n        overdispersed counts or proportions. Most of the methods are\n        already available elsewhere but are scattered in different\n        packages. The proposed functions should be considered as\n        complements to more sophisticated methods such as generalized\n        estimating equations (GEE) or generalized linear mixed effect\n        models (GLMM)."}, "iCellR": {"categories": ["MissingData"], "description": "A toolkit that allows scientists to work with data from single cell sequencing technologies such as scRNA-seq, scVDJ-seq, scATAC-seq, CITE-Seq and Spatial Transcriptomics (ST). Single (i) Cell R package ('iCellR') provides unprecedented flexibility at every step of the analysis pipeline, including normalization, clustering, dimensionality reduction, imputation, visualization, and so on. Users can design both unsupervised and supervised models to best suit their research. In addition, the toolkit provides 2D and 3D interactive visualizations, differential expression analysis, filters based on cells, genes and clusters, data merging, normalizing for dropouts, data imputation methods, correcting for batch differences, pathway analysis, tools to find marker genes for clusters and conditions, predict cell types and pseudotime analysis. See Khodadadi-Jamayran, et al (2020) <doi:10.1101/2020.05.05.078550>  and Khodadadi-Jamayran, et al (2020) <doi:10.1101/2020.03.31.019109> for more details."}, "NetworkComparisonTest": {"categories": ["Psychometrics"], "description": "This permutation based hypothesis test, suited for Gaussian and binary data, \n    assesses the difference between two networks based on several invariance measures \n    (e.g., network structure invariance, global strength invariance, edge invariance). \n    Network structures are estimated with l1-regularized partial correlations (Gaussian data) \n    or with l1-regularized logistic regression (eLasso, binary data). Suited for comparison \n    of independent and dependent samples. For dependent samples, only supported for data of \n    one group which is measured twice. See van Borkulo et al. (2017) \n    <doi:10.13140/RG.2.2.29455.38569>."}, "libgeos": {"categories": ["Spatial"], "description": "Provides the Open Source Geometry Engine ('GEOS') as a\n  C API that can be used to write high-performance C and C++\n  geometry operations using R as an interface. Headers are provided\n  to make linking to and using these functions from C++ code as\n  easy and as safe as possible. This package contains an internal\n  copy of the 'GEOS' library to guarantee the best possible\n  consistency on multiple platforms."}, "concreg": {"categories": ["Survival"], "description": "Implements concordance regression which can be used to estimate generalized odds of concordance.\n\tCan be used for non- and semi-parametric survival analysis with non-proportional hazards, for binary and \n    for continuous outcome data. The method was introduced by Dunkler, Schemper and Heinze (2010) <doi:10.1093/bioinformatics/btq035>."}, "coalescentMCMC": {"categories": ["Bayesian"], "description": "Flexible framework for coalescent analyses in R. It includes a main function running the  MCMC algorithm, auxiliary functions for tree rearrangement, and some functions to compute population genetic parameters. Extended description can be found in Paradis (2020) <doi:10.1201/9780429466700>. For details on the MCMC algorithm, see Kuhner et al. (1995) <doi:10.1093/genetics/140.4.1421> and Drummond et al. (2002) <doi:10.1093/genetics/161.3.1307>."}, "spacefillr": {"categories": ["Distributions"], "description": "Generates random and quasi-random space-filling sequences. Supports the following sequences: 'Halton', 'Sobol', 'Owen'-scrambled 'Sobol',  'Owen'-scrambled 'Sobol' with errors distributed as blue noise, progressive jittered, progressive multi-jittered ('PMJ'), 'PMJ' with blue noise, 'PMJ02', and 'PMJ02' with blue noise. Includes a 'C++' 'API'. Methods derived from \"Constructing Sobol sequences with better two-dimensional projections\" (2012) <doi:10.1137/070709359> S. Joe and F. Y. Kuo, \"Progressive Multi-Jittered Sample Sequences\" (2018) <https://graphics.pixar.com/library/ProgressiveMultiJitteredSampling/paper.pdf> Christensen, P., Kensler, A. and Kilpatrick, C., and \"A Low-Discrepancy Sampler that Distributes Monte Carlo Errors as a Blue Noise in Screen Space\" (2019) E. Heitz, B. Laurent, O. Victor, C. David and I. Jean-Claude, <doi:10.1145/3306307.3328191>. "}, "geotopbricks": {"categories": ["Hydrology"], "description": "It analyzes raster maps and other information as input/output\n    files from the Hydrological Distributed Model GEOtop. It contains functions\n    and methods to import maps and other keywords from geotop.inpts file. Some\n    examples with simulation cases of GEOtop 2.x/3.x are presented in the package.\n    Any information about the GEOtop Distributed Hydrological Model source code\n    is available on www.geotop.org. Technical details about the model are\n    available in Endrizzi et al, 2014\n    (<http://www.geosci-model-dev.net/7/2831/2014/gmd-7-2831-2014.html>)."}, "rARPACK": {"categories": ["NumericalMathematics"], "description": "Previously an R wrapper of the 'ARPACK' library\n    <http://www.caam.rice.edu/software/ARPACK/>, and now a shell of the\n    R package 'RSpectra', an R interface to the 'Spectra' library\n    <http://yixuan.cos.name/spectra/> for solving large scale\n    eigenvalue/vector problems. The current version of 'rARPACK'\n    simply imports and exports the functions provided by 'RSpectra'.\n    New users of 'rARPACK' are advised to switch to the 'RSpectra' package."}, "decompr": {"categories": ["Econometrics"], "description": "Three global value chain (GVC) decompositions are implemented. \n    The Leontief decomposition derives the value added origin of exports by \n    country and industry as in Hummels, Ishii and Yi (2001). The Koopman, \n    Wang and Wei (2014) decomposition splits country-level exports into 9 \n    value added components, and the Wang, Wei and Zhu (2013) decomposition \n    splits bilateral exports into 16 value added components. Various GVC \n    indicators based on these decompositions are computed in the \n    complimentary 'gvc' package. \n    \u2014 References: \u2014\n    Hummels, D., Ishii, J., & Yi, K. M. (2001). The nature and growth of \n       vertical specialization in world trade. Journal of international \n       Economics, 54(1), 75-96.\n    Koopman, R., Wang, Z., & Wei, S. J. (2014). Tracing value-added and double \n       counting in gross exports. American Economic Review, 104(2), 459-94.\n    Wang, Z., Wei, S. J., & Zhu, K. (2013). Quantifying international production \n       sharing at the bilateral and sector levels (No. w19677). \n       National Bureau of Economic Research."}, "flexmix": {"categories": ["Cluster", "Environmetrics", "Psychometrics"], "description": "A general framework for finite mixtures of regression\n  models using the EM algorithm is implemented. The E-step and all\n  data handling are provided, while the M-step can be supplied by the\n  user to easily define new models. Existing drivers implement\n  mixtures of standard linear models, generalized linear models and\n  model-based clustering."}, "keras": {"categories": ["HighPerformanceComputing", "ModelDeployment"], "description": "Interface to 'Keras' <https://keras.io>, a high-level neural\n  networks 'API'. 'Keras' was developed with a focus on enabling fast experimentation,\n  supports both convolution based networks and recurrent networks (as well as\n  combinations of the two), and runs seamlessly on both 'CPU' and 'GPU' devices."}, "speedglm": {"categories": ["HighPerformanceComputing"], "description": "Fitting linear models and generalized linear models to large data sets by updating algorithms."}, "DtD": {"categories": ["Finance"], "description": "Provides fast methods to work with Merton's distance to default \n  model introduced in Merton (1974) <doi:10.1111/j.1540-6261.1974.tb03058.x>. \n  The methods includes simulation and estimation of the parameters."}, "coneproj": {"categories": ["Optimization"], "description": "Routines doing cone projection and quadratic programming, as well as doing estimation and inference for constrained parametric regression and shape-restricted regression problems. See Mary C. Meyer (2013)<doi:10.1080/03610918.2012.659820> for more details."}, "OTRselect": {"categories": ["CausalInference"], "description": "A penalized regression framework that can simultaneously estimate \n    the optimal treatment strategy and identify important variables. \n    Appropriate for either censored or uncensored continuous response."}, "StAMPP": {"categories": ["MissingData"], "description": "Allows users to calculate pairwise Nei's Genetic Distances (Nei 1972), pairwise Fixation\n Indexes (Fst) (Weir & Cockerham 1984) and also Genomic Relationship matrixes following Yang et al. (2010) in mixed and single\n ploidy populations. Bootstrapping across loci is implemented during Fst calculation to generate confidence intervals and p-values\n around pairwise Fst values. StAMPP utilises SNP genotype data of any ploidy level (with the ability to handle missing data) and is coded to  \n utilise multithreading where available to allow efficient analysis of large datasets. StAMPP is able to handle genotype data from genlight objects \n allowing integration with other packages such adegenet.\n Please refer to LW Pembleton, NOI Cogan & JW Forster, 2013, Molecular Ecology Resources, 13(5), 946-952. <doi:10.1111/1755-0998.12129> for the appropriate citation and user manual. Thank you in advance."}, "skewt": {"categories": ["Distributions"], "description": "Density, distribution function, quantile function and\n        random generation for the skewed t distribution of Fernandez\n        and Steel."}, "marcher": {"categories": ["SpatioTemporal", "Tracking"], "description": "A set of tools for likelihood-based estimation, model selection and testing of two- and three-range shift and migration models for animal movement data as described in Gurarie et al. (2017) <doi:10.1111/1365-2656.12674>.  Provided movement data (X, Y and Time), including irregularly sampled data, functions estimate the time, duration and location of one or two range shifts, as well as the ranging area and auto-correlation structure of the movment.  Tests assess, for example, whether the shift was \"significant\", and whether a two-shift migration was a true return migration."}, "slackr": {"categories": ["WebTechnologies"], "description": "'Slack' <https://slack.com/> provides a service for teams to\n    collaborate by sharing messages, images, links, files and more.\n    Functions are provided that make it possible to interact with the\n    'Slack' platform 'API'. When you need to share information or data\n    from R, rather than resort to copy/ paste in e-mails or other services\n    like 'Skype' <https://www.skype.com/en/>, you can use this package to\n    send well-formatted output from multiple R objects and expressions to\n    all teammates at the same time with little effort. You can also send\n    images from the current graphics device, R objects, and upload files."}, "errors": {"categories": ["ChemPhys"], "description": "Support for measurement errors in R vectors, matrices and arrays:\n    automatic uncertainty propagation and reporting.\n    Documentation about 'errors' is provided in the paper by Ucar, Pebesma &\n    Azcorra (2018, <doi:10.32614/RJ-2018-075>), included in this package as a\n    vignette; see 'citation(\"errors\")' for details."}, "ForestFit": {"categories": ["Distributions"], "description": "Developed for the following tasks. 1 ) Computing the probability density function,\n             cumulative distribution function, random generation, and estimating the parameters\n \t\t\t of the eleven mixture models. 2 ) Point estimation of the parameters of two - \n\t\t\t parameter Weibull distribution using twelve methods and three - parameter Weibull \n\t\t\t distribution using nine methods. 3 ) The Bayesian inference for the three - \n\t\t\t parameter Weibull distribution. 4 ) Estimating parameters of the three - parameter\n\t\t\t Birnbaum - Saunders, generalized exponential, and Weibull distributions fitted to\n\t\t\t grouped data using three methods including approximated maximum likelihood, \n\t\t\t expectation maximization, and maximum likelihood. 5 ) Estimating the parameters\n\t\t\t of the gamma, log-normal, and Weibull mixture models fitted to the grouped data\n\t\t\t through the EM algorithm, 6 ) Estimating parameters of the nonlinear height curve\n\t\t\t fitted to the height - diameter observation, 7 ) Estimating parameters, computing\n\t\t\t probability density function, cumulative distribution function, and generating\n\t\t\t realizations from gamma shape mixture model introduced by Venturini et al. (2008)\n\t\t\t <doi:10.1214/07-AOAS156> , 8 ) The Bayesian inference, computing probability\n\t\t\t density function, cumulative distribution function, and generating realizations\n\t\t\t from four-parameter Johnson SB distribution, 9 ) Robust multiple linear regression\n\t\t\t analysis when error term follows skewed t distribution, 10 ) Estimating \n\t\t\t parameters of a given distribution fitted to grouped data using method of maximum\n\t\t\t likelihood, and 11 ) Estimating parameters of the Johnson SB distribution through \n\t\t\t the Bayesian, method of moment, conditional maximum likelihood, and two - percentile \n\t\t\t method."}, "mixSPE": {"categories": ["Distributions"], "description": "Mixtures of skewed and elliptical distributions are implemented using mixtures of multivariate skew \n    power exponential and power exponential distributions, respectively. A generalized expectation-maximization \n    framework is used for parameter estimation. Methodology for mixtures of power exponential distributions is \n    from Dang et al. (2015) <doi:10.1111/biom.12351>."}, "polyaAeppli": {"categories": ["Distributions"], "description": "Functions for evaluating the mass density, cumulative distribution function, quantile function and random variate generation for the Polya-Aeppli distribution, also known as the geometric compound Poisson distribution.  More information on the implementation can be found at Conrad J. Burden (2014) <arXiv:1406.2780>."}, "fastcox": {"categories": ["Survival"], "description": "We implement a cocktail algorithm, a good mixture of coordinate decent, the majorization-minimization principle and the strong rule, for computing the solution paths of the elastic net penalized Cox's proportional hazards model. The package is an implementation of Yang, Y. and Zou, H. (2013) DOI: <doi:10.4310/SII.2013.v6.n2.a1>."}, "randomLCA": {"categories": ["Cluster", "Psychometrics"], "description": "Fits standard and random effects latent class models. The single level random effects model is described in Qu et al <doi:10.2307/2533043> and the two level random effects model in Beath and Heller <doi:10.1177/1471082X0800900302>. Examples are given for their use in diagnostic testing."}, "biotic": {"categories": ["Hydrology"], "description": "Calculates a range of UK freshwater invertebrate biotic indices\n    including BMWP, Whalley, WHPT, Habitat-specific BMWP, AWIC, LIFE and PSI."}, "rma.exact": {"categories": ["MetaAnalysis"], "description": "Compute an exact CI for the population mean under a random effects model. The routines implement the algorithm described in Michael, Thronton, Xie, and Tian (2017) <https://haben-michael.github.io/research/Exact_Inference_Meta.pdf>."}, "nlmrt": {"categories": ["Optimization"], "description": "Replacement for nls() tools for working with nonlinear least squares problems.\n      The calling structure is similar to, but much simpler than, that of the nls()\n      function. Moreover, where nls() specifically does NOT deal with small or zero\n      residual problems, nlmrt is quite happy to solve them. It also attempts to be\n      more robust in finding solutions, thereby avoiding 'singular gradient' messages\n      that arise in the Gauss-Newton method within nls(). The Marquardt-Nash approach\n      in nlmrt generally works more reliably to get a solution, though this may be \n      one of a set of possibilities, and may also be statistically unsatisfactory.\n      Added print and summary as of August 28, 2012."}, "covRobust": {"categories": ["Robust"], "description": "The cov.nnve() function implements robust covariance estimation\n        by the nearest neighbor variance estimation (NNVE) method of\n        Wang and Raftery (2002) <doi:10.1198/016214502388618780>."}, "httpcache": {"categories": ["WebTechnologies"], "description": "In order to improve performance for HTTP API clients, 'httpcache'\n    provides simple tools for caching and invalidating cache. It includes the\n    HTTP verb functions GET, PUT, PATCH, POST, and DELETE, which are drop-in\n    replacements for those in the 'httr' package. These functions are cache-aware\n    and provide default settings for cache invalidation suitable for RESTful\n    APIs; the package also enables custom cache-management strategies.\n    Finally, 'httpcache' includes a basic logging framework to facilitate the\n    measurement of HTTP request time and cache performance."}, "psoptim": {"categories": ["Optimization"], "description": "Particle swarm optimization - a basic variant."}, "missMDA": {"categories": ["MissingData", "Psychometrics"], "description": "Imputation of incomplete continuous or categorical datasets; Missing values are imputed with a principal component analysis (PCA), a multiple correspondence analysis (MCA) model or a multiple factor analysis (MFA) model; Perform multiple imputation with and in PCA or MCA."}, "seer": {"categories": ["TimeSeries"], "description": "A novel meta-learning framework for forecast model selection using time series features. Many applications require a large number of time series to be forecast. Providing better forecasts for these time series is important in decision and policy making. We propose a classification framework which selects forecast models based on features calculated from the time series. We call this framework FFORMS (Feature-based FORecast Model Selection). FFORMS builds a mapping that relates the features of time series to the best forecast model using a random forest. 'seer' package is the implementation of the FFORMS algorithm. For more details see our paper at <https://www.monash.edu/business/econometrics-and-business-statistics/research/publications/ebs/wp06-2018.pdf>."}, "SamplerCompare": {"categories": ["Bayesian"], "description": "A framework for running sets of MCMC samplers on sets of distributions with a variety of tuning parameters, along with plotting functions to visualize the results of those simulations."}, "ohoegdm": {"categories": ["Psychometrics"], "description": "Perform a Bayesian estimation of the ordinal exploratory \n    Higher-order General Diagnostic Model (OHOEGDM) for Polytomous Data \n    described by Culpepper, S. A. and Balamuta, J. J. (In Press) <doi:10.1080/00273171.2021.1985949>."}, "compareC": {"categories": ["Survival"], "description": "Proposed by Harrell, the C index or concordance C, is considered an overall measure of discrimination in survival analysis between a survival outcome that is possibly right censored and a predictive-score variable, which can represent a measured biomarker or a composite-score output from an algorithm that combines multiple biomarkers. This package aims to statistically compare two C indices with right-censored survival outcome, which commonly arise from a paired design and thus resulting two correlated C indices."}, "simfinapi": {"categories": ["Finance"], "description": "Through simfinapi, you can intuitively access the 'SimFin'\n    Web-API (<https://simfin.com/>) to make 'SimFin' data easily available\n    in R. To obtain an 'SimFin' API key (and thus to use this package),\n    you need to register at <https://simfin.com/login>."}, "hts": {"categories": ["TimeSeries"], "description": "Provides methods for analysing and forecasting hierarchical and \n    grouped time series. The available forecast methods include bottom-up,\n    top-down, optimal combination reconciliation (Hyndman et al. 2011) \n    <doi:10.1016/j.csda.2011.03.006>, and trace minimization reconciliation\n    (Wickramasuriya et al. 2018) <doi:10.1080/01621459.2018.1448825>."}, "profmem": {"categories": ["HighPerformanceComputing"], "description": "A simple and light-weight API for memory profiling of R expressions.  The profiling is built on top of R's built-in memory profiler ('utils::Rprofmem()'), which records every memory allocation done by R (also native code)."}, "bayesnec": {"categories": ["Pharmacokinetics"], "description": "Implementation of No-Effect-Concentration estimation that uses 'brms' (see Burkner (2017)<doi:10.18637/jss.v080.i01>; Burkner (2018)<doi:10.32614/RJ-2018-017>; Carpenter 'et al.' (2017)<doi:10.18637/jss.v076.i01> to fit concentration(dose)-response data using Bayesian methods for the purpose of estimating 'ECX' values, but more particularly 'NEC' (see Fox (2010)<doi:10.1016/j.ecoenv.2009.09.012>. This package expands and supersedes an original version implemented in R2jags, see Fisher, Ricardo and Fox (2020)<doi:10.5281/ZENODO.3966864>."}, "CoClust": {"categories": ["Cluster"], "description": "A copula based clustering algorithm that finds clusters according to the complex multivariate dependence structure of the data generating process. The updated version of the algorithm is described in Di Lascio, F.M.L. and Giannerini, S. (2016). \"Clustering dependent observations with copula functions\". Statistical Papers, p.1-17. <doi:10.1007/s00362-016-0822-3>."}, "randomForest": {"categories": ["Environmetrics", "MachineLearning", "MissingData"], "description": "Classification and regression based on a forest of trees using random inputs, based on Breiman (2001) <doi:10.1023/A:1010933404324>."}, "mixdist": {"categories": ["Cluster"], "description": "Fit finite mixture distribution models to grouped data and conditional data by maximum likelihood using a combination of a Newton-type algorithm and the EM algorithm."}, "pwr": {"categories": ["ClinicalTrials"], "description": "Power analysis functions along the lines of Cohen (1988)."}, "rcbsubset": {"categories": ["CausalInference"], "description": "Tools for optimal subset matching of treated units\n\tand control units in observational studies, with support\n\tfor refined covariate balance constraints, (including\n\tfine and near-fine balance as special cases). A close \n\trelative is the 'rcbalance' package.  See Pimentel, \n\tet al.(2015) <doi:10.1080/01621459.2014.997879>\n\tand Pimentel and Kelz (2020) \n\t<doi:10.1080/01621459.2020.1720693>. The rrelaxiv \n\tpackage, which provides an alternative solver for\n\tthe underlying network flow problems, carries an\n\tacademic license and is not available on CRAN, but\n\tmay be downloaded from Github at \n\t<https://github.com/josherrickson/rrelaxiv/>."}, "spant": {"categories": ["MedicalImaging"], "description": "Tools for reading, visualising and processing Magnetic Resonance\n    Spectroscopy data. The package includes methods for spectral fitting: Wilson\n    (2021) <doi:10.1002/mrm.28385> and spectral alignment: Wilson (2018)\n    <doi:10.1002/mrm.27605>. "}, "webchem": {"categories": ["ChemPhys"], "description": "Chemical information from around the web. This package interacts \n    with a suite of web services for chemical information. Sources include: Alan\n    Wood's Compendium of Pesticide Common Names, Chemical Identifier Resolver,\n    ChEBI, Chemical Translation Service, ChemIDplus, ChemSpider, ETOX,\n    Flavornet, NIST Chemistry WebBook, OPSIN, PAN Pesticide Database, PubChem,\n    SRS, Wikidata."}, "SamplingStrata": {"categories": ["OfficialStatistics"], "description": "In the field of stratified sampling design, this package offers an approach for the determination of the best stratification of a sampling frame, the one that ensures the minimum sample cost under the condition to satisfy precision constraints in a multivariate and multidomain case. This approach is based on the use of the genetic algorithm: each solution (i.e. a particular partition in strata of the sampling frame) is considered as an individual in a population; the fitness of all individuals is evaluated applying the Bethel-Chromy algorithm to calculate the sampling size satisfying precision constraints on the target estimates. Functions in the package allows to: (a) analyse the obtained results of the optimisation step; (b) assign the new strata labels to the sampling frame; (c) select a sample from the new frame accordingly to the best allocation. Functions for the execution of the genetic algorithm are a modified version of the functions in the 'genalg' package. M.Ballin, G.Barcaroli (2020) <arXiv:2004.09366> \"R package SamplingStrata: new developments and extension to Spatial Sampling\".  "}, "RCEIM": {"categories": ["Optimization"], "description": "An implementation of a stochastic heuristic method for performing multidimensional function optimization. The method is inspired in the Cross-Entropy Method. It does not relies on derivatives, neither imposes particularly strong requirements into the function to be optimized. Additionally, it takes profit from multi-core processing to enable optimization of time-consuming functions."}, "iccbeta": {"categories": ["Psychometrics"], "description": "A function and vignettes for computing an intraclass correlation\n    described in Aguinis & Culpepper (2015) <doi:10.1177/1094428114563618>.\n    This package quantifies the share of variance in a dependent variable that\n    is attributed to group heterogeneity in slopes."}, "anacor": {"categories": ["Psychometrics"], "description": "Performs simple and canonical CA (covariates on rows/columns) on a two-way frequency table (with missings) by means of SVD. Different scaling methods (standard, centroid, Benzecri, Goodman) as well as various plots including confidence ellipsoids are provided. "}, "mknapsack": {"categories": ["Optimization"], "description": "Package solves multiple knapsack optimisation problem. \n    Given a set of items, each with volume and value, \n    it will allocate them to knapsacks of a given size in a way that\n    value of top N knapsacks is as large as possible."}, "panelvar": {"categories": ["Econometrics"], "description": "We extend two general methods of moment estimators to panel vector \n    autoregression models (PVAR) with p lags of endogenous variables, predetermined \n    and strictly exogenous variables. This general PVAR model contains the first \n    difference GMM estimator by Holtz-Eakin et al. (1988) <doi:10.2307/1913103>, \n    Arellano and Bond (1991) <doi:10.2307/2297968> and the system GMM estimator \n    by Blundell and Bond (1998) <doi:10.1016/S0304-4076(98)00009-8>. We also \n    provide specification tests (Hansen overidentification test, lag selection \n    criterion and stability test of the PVAR polynomial) and classical structural \n    analysis for PVAR models such as orthogonal and generalized impulse response \n    functions, bootstrapped confidence intervals for impulse response analysis and \n    forecast error variance decompositions."}, "ensembleBMA": {"categories": ["Bayesian", "TimeSeries"], "description": "Bayesian Model Averaging to create probabilistic forecasts\n        from ensemble forecasts and weather observations\n <https://stat.uw.edu/sites/default/files/files/reports/2007/tr516.pdf>."}, "edina": {"categories": ["Bayesian", "Psychometrics"], "description": "Perform a Bayesian estimation of the exploratory \n    deterministic input, noisy and gate (EDINA)\n    cognitive diagnostic model described by Chen et al. (2018)\n    <doi:10.1007/s11336-017-9579-4>."}, "zoon": {"categories": ["ReproducibleResearch"], "description": "Reproducible and remixable species distribution modelling.\n    The package reads user submitted modules from an online repository, runs\n    full species distribution modelling workflows and returns output that is\n    fully reproducible. For examples and detailed discussion refer to: N.Golding\n    et al. (2017) 'The zoon r package for reproducible and shareable species\n    distribution modelling'. Methods in Ecology and Evolution.\n    <doi:10.1111/2041-210X.12858>. The package 'SDMTools' is used for testing\n    and, though this package is archived, you can access it here if \n    needed, <https://cran.r-project.org/src/contrib/Archive/SDMTools/>. "}, "SemiCompRisks": {"categories": ["Survival"], "description": "Hierarchical multistate models are considered to perform the analysis of independent/clustered semi-competing risks data. The package allows to choose the specification for model components from a range of options giving users substantial flexibility, including: accelerated failure time or proportional hazards regression models; parametric or non-parametric specifications for baseline survival functions and cluster-specific random effects distribution; a Markov or semi-Markov specification for terminal event following non-terminal event. While estimation is mainly performed within the Bayesian paradigm, the package also provides the maximum likelihood estimation approach for several parametric models. The package also includes functions for univariate survival analysis as complementary analysis tools."}, "fBasics": {"categories": ["Distributions", "Finance"], "description": "Provides a collection of functions to \n    explore and to investigate basic properties of financial returns \n    and related quantities.\n    The covered fields include techniques of explorative data analysis\n    and the investigation of distributional properties, including\n    parameter estimation and hypothesis testing. Even more there are\n    several utility functions for data handling and management."}, "SimplicialCubature": {"categories": ["NumericalMathematics"], "description": "Provides methods to integrate functions over m-dimensional simplices\n in n-dimensional Euclidean space.  There are exact methods for polynomials and\n adaptive methods for integrating an arbitrary function.  "}, "BasketballAnalyzeR": {"categories": ["SportsAnalytics"], "description": "Contains data and code to accompany  the book \n             P. Zuccolotto and M. Manisera (2020) Basketball Data Science. Applications with R. CRC Press. ISBN 9781138600799."}, "WikipediR": {"categories": ["WebTechnologies"], "description": "A wrapper for the MediaWiki API, aimed particularly at the\n    Wikimedia 'production' wikis, such as Wikipedia. It can be used to retrieve\n    page text, information about users or the history of pages, and elements of\n    the category tree."}, "tidyBdE": {"categories": ["OfficialStatistics"], "description": "Tools to download data series from 'Banco de Espa\u00f1a' ('BdE')\n    on 'tibble' format. 'Banco de Espa\u00f1a' is the national central bank\n    and, within the framework of the Single Supervisory Mechanism ('SSM'),\n    the supervisor of the Spanish banking system along with the European\n    Central Bank. This package is in no way sponsored endorsed or\n    administered by 'Banco de Espa\u00f1a'."}, "textir": {"categories": ["NaturalLanguageProcessing"], "description": "Multinomial (inverse) regression inference for text documents and associated attributes.  For details see: Taddy (2013 JASA) Multinomial Inverse Regression for Text Analysis <arXiv:1012.2098> and Taddy (2015, AoAS), Distributed Multinomial Regression, <arXiv:1311.6139>. A minimalist partial least squares routine is also included.  Note that the topic modeling capability of earlier 'textir' is now a separate package, 'maptpx'."}, "psychotools": {"categories": ["Psychometrics"], "description": "Infrastructure for psychometric modeling such as data classes (for\n  item response data and paired comparisons), basic model fitting functions (for\n  Bradley-Terry, Rasch, parametric logistic IRT, generalized partial credit,\n  rating scale, multinomial processing tree models), extractor functions for\n  different types of parameters (item, person, threshold, discrimination,\n  guessing, upper asymptotes), unified inference and visualizations, and various\n  datasets for illustration.  Intended as a common lightweight and efficient\n  toolbox for psychometric modeling and a common building block for fitting\n  psychometric mixture models in package \"psychomix\" and trees based on\n  psychometric models in package \"psychotree\"."}, "rslurm": {"categories": ["HighPerformanceComputing"], "description": "Functions that simplify submitting R scripts to a 'Slurm' \n    workload manager, in part by automating the division of embarrassingly\n    parallel calculations across cluster nodes."}, "Microsoft365R": {"categories": ["WebTechnologies"], "description": "An interface to the 'Microsoft 365' (formerly known as 'Office 365') suite of cloud services, building on the framework supplied by the 'AzureGraph' package. Enables access from R to data stored in 'Teams', 'SharePoint Online' and 'OneDrive', including the ability to list drive folder contents, upload and download files, send messages, and retrieve data lists. Also provides a full-featured 'Outlook' email client, with the ability to send emails and manage emails and mail folders."}, "NlinTS": {"categories": ["TimeSeries"], "description": "Models for non-linear time series analysis and causality detection. The main functionalities of this package consist of an implementation of the classical causality test (C.W.J.Granger 1980) <doi:10.1016/0165-1889(80)90069-X>,  and a non-linear version of it based on feed-forward neural networks. This package contains also an implementation of the Transfer Entropy <doi:10.1103/PhysRevLett.85.461>, and the continuous Transfer Entropy using an approximation based on the k-nearest neighbors <doi:10.1103/PhysRevE.69.066138>. There are also some other useful tools, like the VARNN (Vector Auto-Regressive Neural Network) prediction model, the Augmented test of stationarity, and the discrete and continuous entropy and mutual information."}, "RSpectra": {"categories": ["NumericalMathematics"], "description": "R interface to the 'Spectra' library\n    <https://spectralib.org/> for large-scale eigenvalue and SVD\n    problems. It is typically used to compute a few\n    eigenvalues/vectors of an n by n matrix, e.g., the k largest eigenvalues,\n    which is usually more efficient than eigen() if k << n. This package\n    provides the 'eigs()' function that does the similar job as in 'Matlab',\n    'Octave', 'Python SciPy' and 'Julia'. It also provides the 'svds()' function\n    to calculate the largest k singular values and corresponding\n    singular vectors of a real matrix. The matrix to be computed on can be\n    dense, sparse, or in the form of an operator defined by the user."}, "GSM": {"categories": ["Cluster", "Distributions"], "description": "Implementation of a Bayesian approach for estimating a mixture of gamma distributions in which the mixing occurs over the shape parameter. This family provides a flexible and novel approach for modeling heavy-tailed distributions, it is computationally efficient, and it only requires to specify a prior distribution for a single parameter."}, "metapack": {"categories": ["MetaAnalysis"], "description": "Contains functions performing Bayesian inference for meta-analytic and network meta-analytic models through Markov chain Monte Carlo algorithm. Currently, the package implements Hui Yao, Sungduk Kim, Ming-Hui Chen, Joseph G. Ibrahim, Arvind K. Shah, and Jianxin Lin (2015) <doi:10.1080/01621459.2015.1006065> and Hao Li, Daeyoung Lim, Ming-Hui Chen, Joseph G. Ibrahim, Sungduk Kim, Arvind K. Shah, Jianxin Lin (2021) <doi:10.1002/sim.8983>. For maximal computational efficiency, the Markov chain Monte Carlo samplers for each model, written in C++, are fine-tuned. This software has been developed under the auspices of the National Institutes of Health and Merck & Co., Inc., Kenilworth, NJ, USA."}, "rjags": {"categories": ["Bayesian", "Cluster", "GraphicalModels"], "description": "Interface to the JAGS MCMC library."}, "GPCMlasso": {"categories": ["Psychometrics"], "description": "Provides a framework to detect Differential Item Functioning (DIF) in Generalized Partial Credit Models (GPCM) and special cases of the GPCM as proposed by Schauberger and Mair (2019) <doi:10.3758/s13428-019-01224-2>. A joint model is set up where DIF is explicitly parametrized and penalized likelihood estimation is used for parameter selection. The big advantage of the method called GPCMlasso is that several variables can be treated simultaneously and that both continuous and categorical variables can be used to detect DIF."}, "kza": {"categories": ["TimeSeries"], "description": "Time Series Analysis including break detection, spectral analysis, KZ Fourier Transforms."}, "hunspell": {"categories": ["NaturalLanguageProcessing"], "description": "Low level spell checker and morphological analyzer based on the \n    famous 'hunspell' library <https://hunspell.github.io>. The package can analyze\n    or check individual words as well as parse text, latex, html or xml documents.\n    For a more user-friendly interface use the 'spelling' package which builds on\n    this package to automate checking of files, documentation and vignettes in all\n    common formats."}, "sftrack": {"categories": ["Tracking"], "description": "Modern classes for tracking and movement data, building\n    on 'sf' spatial infrastructure, and early theoretical work from\n    Turchin (1998, ISBN: 9780878938476), and Calenge et al. (2009)\n    <doi:10.1016/j.ecoinf.2008.10.002>. Tracking data are series of\n    locations with at least 2-dimensional spatial coordinates (x,y), a\n    time index (t), and individual identification (id) of the object\n    being monitored; movement data are made of trajectories, i.e. the\n    line representation of the path, composed by steps (the\n    straight-line segments connecting successive locations). 'sftrack'\n    is designed to handle movement of both living organisms and\n    inanimate objects."}, "uptasticsearch": {"categories": ["Databases"], "description": "\n    'Elasticsearch' is an open-source, distributed, document-based datastore\n    (<https://www.elastic.co/products/elasticsearch>).\n    It provides an 'HTTP' 'API' for querying the database and extracting datasets, but that\n    'API' was not designed for common data science workflows like pulling large batches of\n    records and normalizing those documents into a data frame that can be used as a training\n    dataset for statistical models. 'uptasticsearch' provides an interface for 'Elasticsearch'\n    that is explicitly designed to make these data science workflows easy and fun."}, "dfped": {"categories": ["ClinicalTrials"], "description": "A unified method for designing and analysing dose-finding trials in paediatrics, while bridging information from adults, is proposed in the 'dfped' package. The dose range can be calculated under three extrapolation methods: linear, allometry and maturation adjustment, using pharmacokinetic (PK) data. To do this, it is assumed that target exposures are the same in both populations. The working model and prior distribution parameters of the dose-toxicity and dose-efficacy relationships can be obtained using early phase adult toxicity and efficacy data at several dose levels through 'dfped' package. Priors are used into the dose finding process through a Bayesian model selection or adaptive priors, to facilitate adjusting the amount of prior information to differences between adults and children. This calibrates the model to adjust for misspecification if the adult and paediatric data are very different. User can use his/her own Bayesian model written in Stan code through the 'dfped' package. A template of this model is proposed in the examples of the corresponding R functions in the package. Finally, in this package you can find a simulation function for one trial or for more than one trial. These methods are proposed by Petit et al, (2016) <doi:10.1177/0962280216671348>."}, "RRreg": {"categories": ["OfficialStatistics"], "description": "\n    Univariate and multivariate methods to analyze randomized response \n    (RR) survey designs (e.g., Warner, S. L. (1965). Randomized response: A \n    survey technique for eliminating evasive answer bias. Journal of the \n    American Statistical Association, 60, 63\u201369, <doi:10.2307/2283137>). \n    Besides univariate estimates of true proportions, RR variables can be used \n    for correlations, as dependent variable in a logistic regression (with or \n    without random effects), or as predictors in a linear regression\n    (Heck, D. W., & Moshagen, M. (2018). RRreg: An R package for correlation and \n    regression analyses of randomized response data. Journal of Statistical \n    Software, 85(2), 1\u201329, <doi:10.18637/jss.v085.i02>). For simulations and \n    the estimation of statistical power, RR data can be generated according to \n    several models. The implemented methods also allow to test the link between \n    continuous covariates and dishonesty in cheating paradigms such as the \n    coin-toss or dice-roll task (Moshagen, M., & Hilbig, B. E. (2017). \n    The statistical analysis of cheating paradigms. Behavior Research Methods, \n    49, 724\u2013732, <doi:10.3758/s13428-016-0729-x>)."}, "lightgbm": {"categories": ["MachineLearning", "ModelDeployment"], "description": "Tree based algorithms can be improved by introducing boosting frameworks. \n    'LightGBM' is one such framework, based on Ke, Guolin et al. (2017) <https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision>.\n    This package offers an R interface to work with it.\n    It is designed to be distributed and efficient with the following advantages:\n        1. Faster training speed and higher efficiency.\n        2. Lower memory usage.\n        3. Better accuracy.\n        4. Parallel learning supported.\n        5. Capable of handling large-scale data.\n    In recognition of these advantages, 'LightGBM' has been widely-used in many winning solutions of machine learning competitions.\n    Comparison experiments on public datasets suggest that 'LightGBM' can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. In addition, parallel experiments suggest that in certain circumstances, 'LightGBM' can achieve a linear speed-up in training time by using multiple machines."}, "lubridate": {"categories": ["ReproducibleResearch", "TimeSeries"], "description": "Functions to work with date-times and time-spans:\n    fast and user friendly parsing of date-time data, extraction and\n    updating of components of a date-time (years, months, days, hours,\n    minutes, and seconds), algebraic manipulation on date-time and\n    time-span objects. The 'lubridate' package has a consistent and\n    memorable syntax that makes working with dates easy and fun.  Parts of\n    the 'CCTZ' source code, released under the Apache 2.0 License, are\n    included in this package. See <https://github.com/google/cctz> for\n    more details."}, "mbend": {"categories": ["NumericalMathematics"], "description": "Bending non-positive-definite (symmetric) matrices to positive-definite, using weighted and unweighted methods.\n   Jorjani, H., et al. (2003) <doi:10.3168/jds.S0022-0302(03)73646-7>.\n   Schaeffer, L. R. (2014) <http://animalbiosciences.uoguelph.ca/~lrs/ELARES/PDforce.pdf>."}, "Rlinkedin": {"categories": ["WebTechnologies"], "description": "A series of functions that allow users\n    to access the 'LinkedIn' API to get information about connections,\n    search for people and jobs, share updates with their network,\n    and create group discussions.  For more information about using\n    the API please visit <https://developer.linkedin.com/>."}, "lsa": {"categories": ["NaturalLanguageProcessing"], "description": "The basic idea of latent semantic analysis (LSA) is, \n  that text do have a higher order (=latent semantic) structure which, \n  however, is obscured by word usage (e.g. through the use of synonyms \n  or polysemy). By using conceptual indices that are derived statistically \n  via a truncated singular value decomposition (a two-mode factor analysis) \n  over a given document-term matrix, this variability problem can be overcome. "}, "censReg": {"categories": ["Econometrics", "Survival"], "description": "Maximum Likelihood estimation of censored regression (Tobit) models\n   with cross-sectional and panel data."}, "sparseFLMM": {"categories": ["FunctionalData"], "description": "Estimation of functional linear mixed models for irregularly or\n    sparsely sampled data based on functional principal component analysis."}, "gstat": {"categories": ["Spatial", "SpatioTemporal"], "description": "Variogram modelling; simple, ordinary and universal point or block (co)kriging; spatio-temporal kriging; sequential Gaussian or indicator (co)simulation; variogram and variogram map plotting utility functions; supports sf and stars."}, "SphericalCubature": {"categories": ["NumericalMathematics"], "description": "Provides several methods to integrate functions over the unit\n sphere and ball in n-dimensional Euclidean space.  Routines for converting to/from\n multivariate polar/spherical coordinates are also provided."}, "abess": {"categories": ["MachineLearning"], "description": "Extremely efficient toolkit for solving the best subset selection problem <arXiv:2110.09697>. This package is its R interface. The package implements and generalizes algorithms designed in <doi:10.1073/pnas.2014241117> that exploits a novel sequencing-and-splicing technique to guarantee exact support recovery and globally optimal solution in polynomial times for linear model. It also supports best subset selection for logistic regression, Poisson regression, Cox proportional hazard model, Gamma regression, multiple-response regression, multinomial logistic regression, ordinal regression, (sequential) principal component analysis, and robust principal component analysis. The other valuable features such as the best subset of group selection <arXiv:2104.12576> and sure independence screening <doi:10.1111/j.1467-9868.2008.00674.x> are also provided.  "}, "geojson": {"categories": ["Spatial"], "description": "Classes for 'GeoJSON' to make working with 'GeoJSON' easier.\n    Includes S3 classes for 'GeoJSON' classes with brief summary output,\n    and a few methods such as extracting and adding bounding boxes,\n    properties, and coordinate reference systems; working with \n    newline delimited 'GeoJSON'; linting through the 'geojsonlint' \n    package; and serializing to/from 'Geobuf' binary 'GeoJSON' \n    format."}, "metabolic": {"categories": ["MetaAnalysis"], "description": "Dataset and functions from the meta-analysis published in Medicine & Science in Sports & Exercise. \n    It contains all the data and functions to reproduce the analysis.\n    \"Effectiveness of HIIE versus MICT in Improving Cardiometabolic Risk Factors in Health and Disease: A Meta-analysis\".\n    Felipe Mattioni Maturana, Peter Martus, Stephan Zipfel, Andreas M Nie\u00df (2020) <doi:10.1249/MSS.0000000000002506>."}, "rtdists": {"categories": ["Distributions"], "description": "Provides response time distributions (density/PDF,\n       distribution function/CDF, quantile function, and random\n       generation): (a) Ratcliff diffusion model (Ratcliff &\n       McKoon, 2008, <doi:10.1162/neco.2008.12-06-420>) based on C\n       code by Andreas and Jochen Voss and (b) linear ballistic\n       accumulator (LBA; Brown & Heathcote, 2008,\n       <doi:10.1016/j.cogpsych.2007.12.002>) with different\n       distributions underlying the drift rate."}, "R6": {"categories": ["Databases"], "description": "Creates classes with reference semantics, similar to R's built-in\n    reference classes. Compared to reference classes, R6 classes are simpler\n    and lighter-weight, and they are not built on S4 classes so they do not\n    require the methods package. These classes allow public and private\n    members, and they support inheritance, even when the classes are defined in\n    different packages."}, "CVXR": {"categories": ["Optimization"], "description": "An object-oriented modeling language for disciplined\n    convex programming (DCP) as described in Fu, Narasimhan, and Boyd\n    (2020, <doi:10.18637/jss.v094.i14>). It allows the user to\n    formulate convex optimization problems in a natural way following\n    mathematical convention and DCP rules. The system analyzes the\n    problem, verifies its convexity, converts it into a canonical\n    form, and hands it off to an appropriate solver to obtain the\n    solution. Interfaces to solvers on CRAN and elsewhere are\n    provided, both commercial and open source."}, "caribou": {"categories": ["SpatioTemporal", "Tracking"], "description": "Estimation of population size of migratory caribou herds based on large scale \n             aggregations monitored by radio telemetry. It implements the methodology found \n             in the article by Rivest et al. (1998) about caribou abundance estimation. It \n             also includes a function based on the Lincoln-Petersen Index as applied to \n             radio telemetry data by White and Garrott (1990)."}, "effects": {"categories": ["Econometrics", "MachineLearning", "TeachingStatistics"], "description": "\n  Graphical and tabular effect displays, e.g., of interactions, for \n  various statistical models with linear predictors."}, "dqrng": {"categories": ["Distributions", "HighPerformanceComputing"], "description": "Several fast random number generators are provided as C++\n  header only libraries: The PCG family by O'Neill (2014\n  <https://www.cs.hmc.edu/tr/hmc-cs-2014-0905.pdf>) as well as\n  Xoroshiro128+ and Xoshiro256+ by Blackman and Vigna (2018\n  <arXiv:1805.01407>). In addition fast functions for generating random\n  numbers according to a uniform, normal and exponential distribution\n  are included. The latter two use the Ziggurat algorithm originally\n  proposed by Marsaglia and Tsang (2000, <doi:10.18637/jss.v005.i08>).\n  These functions are exported to R and as a C++ interface and are\n  enabled for use with the default 64 bit generator from the PCG family,\n  Xoroshiro128+ and Xoshiro256+ as well as the 64 bit version of the 20 rounds\n  Threefry engine (Salmon et al., 2011 <doi:10.1145/2063384.2063405>) as\n  provided by the package 'sitmo'."}, "cloudml": {"categories": ["ModelDeployment"], "description": "Interface to the Google Cloud Machine Learning Platform\n  <https://cloud.google.com/ml-engine>, which provides cloud tools for training machine\n  learning models."}, "spsurvey": {"categories": ["OfficialStatistics", "Spatial"], "description": "A design-based approach to statistical inference, with a focus on spatial data. Spatially balanced samples are selected using the Generalized Random Tessellation Stratified (GRTS) algorithm. The GRTS algorithm can be applied to finite resources (point geometries) and infinite resources (linear / linestring and areal / polygon geometries) and flexibly accommodates a diverse set of sampling design features, including stratification, unequal inclusion probabilities, proportional (to size) inclusion probabilities, legacy (historical) sites, a minimum distance between sites, and two options for replacement sites (reverse hierarchical order and nearest neighbor). Data are analyzed using a wide range of analysis functions that perform categorical variable analysis, continuous variable analysis, attributable risk analysis, risk difference analysis, relative risk analysis, change analysis, and trend analysis. spsurvey can also be used to summarize objects, visualize objects, select samples that are not spatially balanced, select panel samples, measure the amount of spatial balance in a sample, adjust design weights, and more."}, "dfcomb": {"categories": ["ExperimentalDesign"], "description": "Phase I/II adaptive dose-finding design for combination\n   studies where toxicity rates are supposed to increase with both agents."}, "DIFlasso": {"categories": ["Psychometrics"], "description": "Performs DIFlasso as proposed by Tutz and Schauberger (2015) <doi:10.1007/s11336-013-9377-6>, a method to detect DIF (Differential Item Functioning) in Rasch Models. It can handle settings with many variables and also metric variables. "}, "lmomRFA": {"categories": ["ExtremeValue"], "description": "Functions for regional frequency analysis using the methods\n  of J. R. M. Hosking and J. R. Wallis (1997), \"Regional frequency analysis:\n  an approach based on L-moments\"."}, "jack": {"categories": ["NumericalMathematics"], "description": "Symbolic calculation and evaluation of the Jack polynomials, zonal polynomials, and Schur polynomials. Mainly based on Demmel & Koev's paper (2006) <doi:10.1090/S0025-5718-05-01780-1>. Zonal polynomials and Schur polynomials are particular cases of Jack polynomials. Zonal polynomials appear in random matrix theory. Schur polynomials appear in the field of combinatorics."}, "bReeze": {"categories": ["Environmetrics"], "description": "A collection of functions to analyse, visualize and interpret wind data\n         and to calculate the potential energy production of wind turbines."}, "rcdk": {"categories": ["ChemPhys"], "description": "Allows the user to access functionality in the\n    'CDK', a Java framework for chemoinformatics. This allows the user to load\n    molecules, evaluate fingerprints, calculate molecular descriptors and so on.\n    In addition, the 'CDK' API allows the user to view structures in 2D."}, "pcFactorStan": {"categories": ["Bayesian", "Psychometrics"], "description": "Provides convenience functions and pre-programmed\n    Stan models related to the paired comparison factor model. Its purpose\n    is to make fitting paired comparison data using Stan easy. This\n    package is described in Pritikin (2020) <doi:10.1016/j.heliyon.2020.e04821>."}, "QRM": {"categories": ["Distributions", "ExtremeValue"], "description": "Provides functions/methods to accompany the book\n Quantitative Risk Management: Concepts, Techniques and Tools by\n Alexander J. McNeil, Ruediger Frey, and Paul Embrechts."}, "simFrame": {"categories": ["MissingData"], "description": "A general framework for statistical simulation, which allows researchers to make use of a wide range of simulation designs with minimal programming effort.  The package provides functionality for drawing samples from a distribution or a finite population, for adding outliers and missing values, as well as for visualization of the simulation results.  It follows a clear object-oriented design and supports parallel computing to increase computational performance."}, "norm2": {"categories": ["MissingData"], "description": "Functions for parameter estimation, Bayesian posterior simulation\n        and multiple imputation from incomplete multivariate data under a\n        normal model."}, "NormalLaplace": {"categories": ["Distributions"], "description": "Functions for the normal Laplace distribution. The package is\n        under development and provides only limited functionality.\n\tDensity, distribution and quantile functions, random number generation,\n\tand moments are provided."}, "qgraph": {"categories": ["GraphicalModels", "Psychometrics"], "description": "Weighted network visualization and analysis, as well as Gaussian graphical model computation. See Epskamp et al. (2012) <doi:10.18637/jss.v048.i04>."}, "adehabitatMA": {"categories": ["Spatial"], "description": "A collection of tools to deal with raster maps."}, "MGMM": {"categories": ["MissingData"], "description": "Parameter estimation and classification for Gaussian Mixture Models (GMMs) in the presence of missing data. This package complements existing implementations by allowing for both missing elements in the input vectors and full (as opposed to strictly diagonal) covariance matrices. Estimation is performed using an expectation conditional maximization algorithm that accounts for missingness of both the cluster assignments and the vector components. The output includes the marginal cluster membership probabilities; the mean and covariance of each cluster; the posterior probabilities of cluster membership; and a completed version of the input data, with missing values imputed to their posterior expectations. For additional details, please see McCaw ZR, Julienne H, Aschard H. \"MGMM: an R package for fitting Gaussian Mixture Models on Incomplete Data.\" <doi:10.1101/2019.12.20.884551>."}, "GWSDAT": {"categories": ["Hydrology"], "description": "Shiny application for the analysis of groundwater\n    monitoring data, designed to work with simple time-series data for\n    solute concentration and ground water elevation, but can also plot\n    non-aqueous phase liquid (NAPL) thickness if required. Also provides\n    the import of a site basemap in GIS shapefile format."}, "meta.shrinkage": {"categories": ["MetaAnalysis"], "description": "Implement meta-analyses for simultaneously estimating individual means with shrinkage,\n isotonic regression and pretests. Include our original implementation of the isotonic regression via the pool-adjacent-violators algorithm (PAVA) algorithm.\n This methodology is published in Taketomi et al.(2021) <doi:10.3390/axioms10040267>."}, "link2GI": {"categories": ["Spatial"], "description": "Functions to simplify the linking of open source GIS and remote sensing related command line interfaces."}, "texmex": {"categories": ["ExtremeValue"], "description": "Statistical extreme value modelling of threshold excesses, maxima\n    and multivariate extremes. Univariate models for threshold excesses and maxima\n    are the Generalised Pareto, and Generalised Extreme Value model respectively.\n    These models may be fitted by using maximum (optionally penalised-)likelihood,\n    or Bayesian estimation, and both classes of models may be fitted with covariates\n    in any/all model parameters. Model diagnostics support the fitting process.\n    Graphical output for visualising fitted models and return level estimates is\n    provided. For serially dependent sequences, the intervals declustering algorithm\n    of Ferro and Segers (2003) <doi:10.1111/1467-9868.00401> is provided, with\n    diagnostic support to aid selection of threshold and declustering horizon.\n    Multivariate modelling is performed via the conditional approach of Heffernan\n    and Tawn (2004) <doi:10.1111/j.1467-9868.2004.02050.x>, with graphical tools for\n    threshold selection and to diagnose estimation convergence."}, "ivmodel": {"categories": ["CausalInference"], "description": "Carries out instrumental variable\n    estimation of causal effects, including power analysis, sensitivity analysis,\n    and diagnostics. See Kang, Jiang, Zhao, and Small (2020) <http://pages.cs.wisc.edu/~hyunseung/> for details."}, "mixture": {"categories": ["Cluster", "MissingData"], "description": "An implementation of 14 parsimonious mixture models for model-based clustering or model-based classification. Gaussian, Student's t, generalized hyperbolic, variance-gamma or skew-t mixtures are available. All approaches work with missing data. Celeux and Govaert (1995) <doi:10.1016/0031-3203(94)00125-6>, Browne and McNicholas (2014) <doi:10.1007/s11634-013-0139-1>, Browne and McNicholas (2015) <doi:10.1002/cjs.11246>."}, "modelsummary": {"categories": ["Econometrics"], "description": "Create beautiful and customizable tables to summarize several\n    statistical models side-by-side. Draw coefficient plots, multi-level\n    cross-tabs, dataset summaries, balance tables (a.k.a. \"Table 1s\"), and\n    correlation matrices. This package supports dozens of statistical models, and\n    it can produce tables in HTML, LaTeX, Word, Markdown, PDF, PowerPoint, Excel,\n    RTF, JPG, or PNG. Tables can easily be embedded in 'Rmarkdown' or 'knitr'\n    dynamic documents. Details can be found in Arel-Bundock (2022)\n    <doi:10.18637/jss.v103.i01>."}, "DIFplus": {"categories": ["Psychometrics"], "description": "Clustered or multilevel data structures are common in the assessment of differential item functioning (DIF), particularly in the context of large-scale assessment programs. This package allows users to implement extensions of the Mantel-Haenszel DIF detection procedures in the presence of multilevel data based on the work of Begg (1999) <doi:10.1111/j.0006-341X.1999.00302.x>, Begg & Paykin (2001) <doi:10.1080/00949650108812115>, \n\tand French & Finch (2013) <doi:10.1177/0013164412472341>."}, "casebase": {"categories": ["Survival"], "description": "Fit flexible and fully parametric hazard regression models to survival data with single event type or multiple \n    competing causes via logistic and multinomial regression. Our formulation allows for arbitrary functional forms \n    of time and its interactions with other predictors for time-dependent hazards and hazard ratios. From the \n    fitted hazard model, we provide functions to readily calculate and plot cumulative incidence and survival \n    curves for a given covariate profile. This approach accommodates any log-linear hazard function of \n    prognostic time, treatment, and covariates, and readily allows for non-proportionality. We also provide \n    a plot method for visualizing incidence density via population time plots. Based on the case-base sampling \n    approach of Hanley and Miettinen (2009) <doi:10.2202/1557-4679.1125>, Saarela and Arjas (2015) <doi:10.1111/sjos.12125>, \n    and Saarela (2015) <doi:10.1007/s10985-015-9352-x>."}, "epitools": {"categories": ["Epidemiology"], "description": "Tools for training and practicing epidemiologists including methods for two-way and multi-way contingency tables."}, "ZIM": {"categories": ["TimeSeries"], "description": "Analyze count time series with excess zeros. \n    Two types of statistical models are supported: Markov regression by Yang et al.\n    (2013) <doi:10.1016/j.stamet.2013.02.001> and state-space models by Yang et al. \n    (2015) <doi:10.1177/1471082X14535530>. They are also known as observation-driven and \n    parameter-driven models respectively in the time series literature. The functions used for \n    Markov regression or observation-driven models can also be used to fit ordinary regression models \n    with independent data under the zero-inflated Poisson (ZIP) or zero-inflated negative binomial (ZINB) \n    assumption. Besides, the package contains some miscellaneous functions to compute density, distribution, \n    quantile, and generate random numbers from ZIP and ZINB distributions."}, "truncreg": {"categories": ["Econometrics"], "description": "Estimation of models for truncated Gaussian variables by maximum likelihood."}, "gamboostLSS": {"categories": ["MachineLearning"], "description": "Boosting models for fitting generalized additive models for\n  location, shape and scale ('GAMLSS') to potentially high dimensional\n  data."}, "RoBMA": {"categories": ["Bayesian", "MetaAnalysis"], "description": "A framework for estimating ensembles of meta-analytic models\n    (assuming either presence or absence of the effect, heterogeneity, and\n    publication bias). The RoBMA framework uses Bayesian model-averaging to \n    combine the competing meta-analytic models into a model ensemble, weights \n    the posterior parameter distributions based on posterior model probabilities \n    and uses Bayes factors to test for the presence or absence of the\n    individual components (e.g., effect vs. no effect; Barto\u0161 et al., 2021, \n    <doi:10.31234/osf.io/kvsp7>; Maier, Barto\u0161 & Wagenmakers, in press, \n    <doi:10.31234/osf.io/u4cns>). Users can define a wide range of non-informative \n    or informative prior distributions for the effect size, heterogeneity, \n    and publication bias components (including selection models and PET-PEESE). \n    The package provides convenient functions for summary, visualizations, and \n    fit diagnostics."}, "funHDDC": {"categories": ["Cluster", "FunctionalData"], "description": "The funHDDC algorithm  allows to cluster functional univariate (Bouveyron and Jacques, 2011, <doi:10.1007/s11634-011-0095-6>) or multivariate data (Schmutz et al., 2018) by modeling each group within a specific functional subspace."}, "Rbeast": {"categories": ["Bayesian", "TimeSeries"], "description": "Interpretation of time series data is affected by model choices. Different models can give different or even contradicting estimates of patterns, trends, and mechanisms for the same data\u2013a limitation alleviated by the Bayesian estimator of abrupt change,seasonality, and trend (BEAST) of this package. BEAST seeks to improve time series decomposition by forgoing the \"single-best-model\" concept and embracing all competing models into the inference via a Bayesian model averaging scheme. It is a flexible tool to uncover abrupt changes (i.e., change-points), cyclic variations (e.g., seasonality), and nonlinear trends in time-series observations. BEAST not just tells when changes occur but also quantifies how likely the detected changes are true. It detects not just piecewise linear trends but also arbitrary nonlinear trends. BEAST is applicable to real-valued time series data of all kinds, be it for remote sensing, economics, climate sciences, ecology, and hydrology. Example applications include its use to identify regime shifts in ecological data, map forest disturbance and land degradation from satellite imagery, detect market trends in economic data, pinpoint anomaly and extreme events in climate data, and unravel system dynamics in biological data. Details on BEAST are reported in Zhao et al. (2019) <doi:10.1016/j.rse.2019.04.034>."}, "BayesVarSel": {"categories": ["Bayesian"], "description": "Conceived to calculate Bayes factors in Linear models and then to provide a formal Bayesian answer to testing and variable selection problems. From a theoretical side, the emphasis in this package is placed on the prior distributions and it allows a wide range of them: Jeffreys (1961); Zellner and Siow(1980)<doi:10.1007/bf02888369>; Zellner and Siow(1984); Zellner (1986)<doi:10.2307/2233941>; Fernandez et al. (2001)<doi:10.1016/s0304-4076(00)00076-2>; Liang et al. (2008)<doi:10.1198/016214507000001337>  and Bayarri et al. (2012)<doi:10.1214/12-aos1013>. The interaction with the package is through a friendly interface that syntactically mimics the well-known lm() command of R. The resulting objects can be easily explored providing the user very valuable information (like marginal, joint and conditional inclusion probabilities of potential variables; the highest posterior probability model, HPM; the median probability model, MPM) about the structure of the true -data generating- model. Additionally, this package incorporates abilities to handle problems with a large number of potential explanatory variables through parallel and heuristic versions of the main commands, Garcia-Donato and Martinez-Beneito (2013)<doi:10.1080/01621459.2012.742443>. It also allows problems with p>n and p>>n and also incorporates routines to handle problems with variable selection with factors."}, "matlib": {"categories": ["NumericalMathematics"], "description": "A collection of matrix functions for teaching and learning matrix\n    linear algebra as used in multivariate statistical methods. These functions are\n    mainly for tutorial purposes in learning matrix algebra ideas using R. In some\n    cases, functions are provided for concepts available elsewhere in R, but where\n    the function call or name is not obvious. In other cases, functions are provided\n    to show or demonstrate an algorithm. In addition, a collection of functions are\n    provided for drawing vector diagrams in 2D and 3D."}, "ucminf": {"categories": ["Optimization"], "description": "An algorithm for general-purpose unconstrained non-linear optimization.\n             The algorithm is of quasi-Newton type with BFGS updating of the inverse\n             Hessian and soft line search with a trust region type monitoring of the\n             input to the line search algorithm. The interface of 'ucminf' is\n             designed for easy interchange with 'optim'."}, "duckduckr": {"categories": ["WebTechnologies"], "description": "Programmatic access to the DuckDuckGo Instant Answer API <https://api.duckduckgo.com/api>."}, "mefa": {"categories": ["Environmetrics"], "description": "A framework package aimed to provide standardized computational environment for specialist work via object classes to represent the data coded by samples, taxa and segments (i.e. subpopulations, repeated measures). It supports easy processing of the data along with cross tabulation and relational data tables for samples and taxa. An object of class \u2018mefa\u2019 is a project specific compendium of the data and can be easily used in further analyses. Methods are provided for extraction, aggregation, conversion, plotting, summary and reporting of \u2018mefa\u2019 objects. Reports can be generated in plain text or LaTeX format. Vignette contains worked examples."}, "errorlocate": {"categories": ["OfficialStatistics"], "description": "Errors in data can be located and removed using validation rules from package \n   'validate'. See also Van der Loo and De Jonge (2018) <doi:10.1002/9781118897126>,\n   chapter 7."}, "MonetDB.R": {"categories": ["HighPerformanceComputing"], "description": "Allows to pull data from MonetDB into R. "}, "tripEstimation": {"categories": ["Spatial", "SpatioTemporal", "Tracking"], "description": "Data handling and estimation functions for animal movement\n    estimation from archival or satellite tags. Helper functions are included\n    for making image summaries binned by time interval from Markov Chain Monte Carlo\n    simulations. "}, "polyCub": {"categories": ["NumericalMathematics"], "description": "Numerical integration of continuously differentiable\n    functions f(x,y) over simple closed polygonal domains.\n    The following cubature methods are implemented:\n    product Gauss cubature (Sommariva and Vianello, 2007,\n    <doi:10.1007/s10543-007-0131-2>),\n    the simple two-dimensional midpoint rule\n    (wrapping 'spatstat.geom' functions),\n    adaptive cubature for radially symmetric functions via line\n    integrate() along the polygon boundary (Meyer and Held, 2014,\n    <doi:10.1214/14-AOAS743>, Supplement B),\n    and integration of the bivariate Gaussian density based on\n    polygon triangulation.\n    For simple integration along the axes, the 'cubature' package\n    is more appropriate."}, "distributional": {"categories": ["Distributions"], "description": "Vectorised distribution objects with tools for manipulating, \n    visualising, and using probability distributions. Designed to allow model\n    prediction outputs to return distributions rather than their parameters, \n    allowing users to directly interact with predictive distributions in a\n    data-oriented workflow. In addition to providing generic replacements for\n    p/d/q/r functions, other useful statistics can be computed including means,\n    variances, intervals, and highest density regions."}, "FrF2.catlg128": {"categories": ["ExperimentalDesign"], "description": "Catalogues of resolution IV regular\n        fractional factorial designs in 128 runs are provided \n        for up to 33 2-level factors. The catalogues are complete, \n        excluding resolution IV designs without 5-letter words, \n        because these do not add value for a search for unblocked \n        clear designs. The previous package version\n        1.0 with complete catalogues up to 24 runs (24 runs and a\n        namespace added later) can be downloaded from the authors\n        website."}, "ade4": {"categories": ["Environmetrics", "MissingData", "Psychometrics", "Spatial"], "description": "Tools for multivariate data analysis. Several methods are provided for the analysis (i.e., ordination) of one-table (e.g., principal component analysis, correspondence analysis), two-table (e.g., coinertia analysis, redundancy analysis), three-table (e.g., RLQ analysis) and K-table (e.g., STATIS, multiple coinertia analysis). The philosophy of the package is described in Dray and Dufour (2007) <doi:10.18637/jss.v022.i04>."}, "ICGOR": {"categories": ["Survival"], "description": "Generalized Odds Rate Hazards (GORH) model is a flexible model of fitting survival data, including the Proportional Hazards (PH) model and the Proportional Odds (PO) Model as special cases. This package fit the GORH model with interval censored data."}, "Mcomp": {"categories": ["Econometrics", "TimeSeries"], "description": "\n  The 1001 time series from the M-competition (Makridakis et al. 1982) <doi:10.1002/for.3980010202> and the 3003 time series from the IJF-M3 competition (Makridakis and Hibon, 2000) <doi:10.1016/S0169-2070(00)00057-1>."}, "TFisher": {"categories": ["MetaAnalysis"], "description": "We provide the cumulative distribution function (CDF),\n    quantile, and statistical power calculator for a collection of \n    thresholding Fisher's p-value combination methods, including Fisher's \n    p-value combination method, truncated product method and, in particular, soft-thresholding\n    Fisher's p-value combination method which is proven to be optimal in some\n    context of signal detection. The p-value calculator for the omnibus version\n    of these tests are also included. For reference, please see Hong Zhang and Zheyang Wu. \"TFisher Tests: Optimal and Adaptive Thresholding for Combining p-Values\", submitted."}, "fable.prophet": {"categories": ["TimeSeries"], "description": "Allows prophet models from the 'prophet' package to be used in a tidy workflow with the modelling interface of 'fabletools'. This extends 'prophet' to provide enhanced model specification and management, performance evaluation methods, and model combination tools."}, "mde": {"categories": ["MissingData"], "description": "Correct identification and handling of missing data is one of the most important steps in any analysis. To aid this process, 'mde' provides a very easy to use yet robust framework to quickly get an idea of where the missing data\n             lies and therefore find the most appropriate action to take.\n             Graham WJ (2009) <doi:10.1146/annurev.psych.58.110405.085530>. "}, "sievePH": {"categories": ["MissingData"], "description": "Implements semiparametric estimation and testing procedures for a continuous, possibly multivariate, mark-specific hazard ratio (treatment/placebo) of an event of interest in a randomized treatment efficacy trial with a time-to-event endpoint, as described in Juraska M and Gilbert PB (2013), Mark-specific hazard ratio model with multivariate continuous marks: an application to vaccine efficacy. Biometrics 69(2):328 337 <doi:10.1111/biom.12016>, and in Juraska M and Gilbert PB (2015), Mark-specific hazard ratio model with missing multivariate marks. Lifetime Data Analysis 22(4): 606-25 <doi:10.1007/s10985-015-9353-9>. The former considers continuous multivariate marks fully observed in all subjects who experience the event of interest, whereas the latter extends the previous work to allow multivariate marks that are subject to missingness-at-random. For models with missing marks, two estimators are implemented based on (i) inverse probability weighting (IPW) of complete cases, and (ii) augmentation of the IPW estimating functions by leveraging correlations between the mark and auxiliary data to 'impute' the expected profile score vectors for subjects with missing marks. The augmented IPW estimator is doubly robust and recommended for use with incomplete mark data. The methods make two key assumptions: (i) the time-to-event is assumed to be conditionally independent of the mark given treatment, and (ii) the weight function in the semiparametric density ratio/biased sampling model is assumed to be exponential. Diagnostic testing procedures for evaluating validity of both assumptions are implemented. Summary and plotting functions are provided for estimation and inferential results."}, "multinma": {"categories": ["MetaAnalysis"], "description": "Network meta-analysis and network meta-regression models for \n    aggregate data, individual patient data, and mixtures of both individual \n    and aggregate data using multilevel network meta-regression as described by\n    Phillippo et al. (2020) <doi:10.1111/rssa.12579>. Models are estimated in a\n    Bayesian framework using 'Stan'."}, "CIAAWconsensus": {"categories": ["MetaAnalysis"], "description": "Calculation of consensus values for atomic weights, isotope amount ratios, and isotopic abundances with the associated uncertainties using multivariate meta-regression approach for consensus building."}, "iterLap": {"categories": ["Bayesian"], "description": "The iterLap (iterated Laplace approximation) algorithm approximates a\n             general (possibly non-normalized) probability density on R^p, by repeated\n             Laplace approximations to the difference between current approximation \n             and true density (on log scale). The final approximation is a mixture of\n             multivariate normal distributions and might be used for example as a\n             proposal distribution for importance sampling (eg in Bayesian applications). \n             The algorithm can be seen as a computational generalization of the Laplace \n             approximation suitable for skew or multimodal densities."}, "xmeta": {"categories": ["MetaAnalysis"], "description": "A toolbox for meta-analysis. This package includes (1) a robust multivariate meta-analysis of continuous or binary outcomes; (2) a bivariate Egger's test for detecting small study effects; (3) Galaxy Plot: A New Visualization Tool of Bivariate Meta-Analysis Studies; and (4) a bivariate T&F method accounting for publication bias in bivariate meta-analysis, based on symmetry of the galaxy plot."}, "mi": {"categories": ["MissingData"], "description": "The mi package provides functions for data manipulation, imputing missing values in an approximate Bayesian framework, diagnostics of the models used to generate the imputations, confidence-building mechanisms to validate some of the assumptions of the imputation algorithm, and functions to analyze multiply imputed data sets with the appropriate degree of sampling uncertainty."}, "multiplex": {"categories": ["Psychometrics"], "description": "Algebraic procedures for the analysis of multiple social networks are delivered with this \n\t    package as described in Ostoic (2020) <doi:10.18637/jss.v092.i11>. Among other things, it \n\t    makes it possible to create and manipulate multiplex, multimode, and multilevel network data \n\t    with different formats. There are effective ways available to treat multiple networks with \n\t    routines that combine algebraic systems like the partially ordered semigroup or the semiring \n\t    structure with the relational bundles occurring in different types of multivariate network \n\t    data sets. It also provides an algebraic approach for affiliation networks through Galois \n\t    derivations between families of the pairs of subsets in the two domains."}, "plotSEMM": {"categories": ["Psychometrics"], "description": "Contains a graphical user interface to generate the diagnostic\n    plots proposed by Bauer (2005; <doi:10.1207/s15328007sem1204_1>), \n    Pek & Chalmers (2015; <doi:10.1080/10705511.2014.937790>), and\n    Pek, Chalmers, R. Kok, & Losardo (2015; <doi:10.3102/1076998615589129>) to investigate\n    nonlinear bivariate relationships in latent regression models using structural\n    equation mixture models (SEMMs)."}, "adagio": {"categories": ["Optimization"], "description": "\n    The R package 'adagio' will provide methods and algorithms for\n    discrete optimization, e.g. knapsack and subset sum procedures,\n\tderivative-free Nelder-Mead and Hooke-Jeeves minimization, and\n\tsome (evolutionary) global optimization functions."}, "rugarch": {"categories": ["Finance", "TimeSeries"], "description": "ARFIMA, in-mean, external regressors and various GARCH flavors, with methods for fit, forecast, simulation, inference and plotting."}, "rrcovHD": {"categories": ["Robust"], "description": "Robust multivariate methods for high dimensional data including\n        outlier detection (Filzmoser and Todorov (2013) <doi:10.1016/j.ins.2012.10.017>), \n        robust sparse PCA (Croux et al. (2013) <doi:10.1080/00401706.2012.727746>, Todorov and Filzmoser (2013) <doi:10.1007/978-3-642-33042-1_31>), \n        robust PLS (Todorov and Filzmoser (2014) <doi:10.17713/ajs.v43i4.44>), \n        and robust sparse classification (Ortner et al. (2020) <doi:10.1007/s10618-019-00666-8>)."}, "adehabitatHS": {"categories": ["Spatial"], "description": "A collection of tools for the analysis of habitat selection."}, "TeachingDemos": {"categories": ["TeachingStatistics"], "description": "Demonstration functions that can be used in a classroom to demonstrate statistical concepts, or on your own to better understand the concepts or the programming."}, "foster": {"categories": ["MissingData"], "description": "Set of tools to streamline the modeling of the relationship between \n    satellite imagery time series or any other environmental information, \n    such as terrain elevation, with forest structural attributes derived from \n    3D point cloud data and their subsequent imputation over the broader \n    landscape. "}, "gdistance": {"categories": ["Spatial"], "description": "Provides classes and functions to calculate various distance measures and routes in heterogeneous geographic spaces represented as grids. The package implements measures to model dispersal histories first presented by van Etten and Hijmans (2010) <doi:10.1371/journal.pone.0012060>. Least-cost distances as well as more complex distances based on (constrained) random walks can be calculated. The distances implemented in the package are used in geographical genetics, accessibility indicators, and may also have applications in other fields of geospatial analysis."}, "SCEPtERbinary": {"categories": ["ChemPhys"], "description": "SCEPtER pipeline for estimating the stellar age for double-lined detached binary systems. The observational constraints adopted in the recovery are the effective temperature, the metallicity [Fe/H], the mass, and the radius of the two stars. The results are obtained adopting a maximum likelihood technique over a grid of pre-computed stellar models."}, "mixR": {"categories": ["Cluster"], "description": "Performs maximum likelihood estimation for finite mixture models for families including Normal, Weibull, Gamma and Lognormal by using EM algorithm, together with Newton-Raphson algorithm or bisection method when necessary. It also conducts mixture model selection by using information criteria or bootstrap likelihood ratio test. The data used for mixture model fitting can be raw data or binned data. The model fitting process is accelerated by using R package 'Rcpp'."}, "dsa": {"categories": ["TimeSeries"], "description": "Seasonal- and calendar adjustment of time series\n    with daily frequency using the DSA approach developed by Ollech,\n    Daniel (2018): Seasonal adjustment of daily time series. Bundesbank\n    Discussion Paper 41/2018."}, "stabs": {"categories": ["MachineLearning"], "description": "Resampling procedures to assess the stability of selected variables\n    with additional finite sample error control for high-dimensional variable\n    selection procedures such as Lasso or boosting. Both, standard stability\n    selection (Meinshausen & Buhlmann, 2010, <doi:10.1111/j.1467-9868.2010.00740.x>) \n    and complementary pairs stability selection with improved error bounds \n    (Shah & Samworth, 2013, <doi:10.1111/j.1467-9868.2011.01034.x>) are\n    implemented. The package can be combined with arbitrary user specified\n    variable selection approaches."}, "rakeR": {"categories": ["Spatial"], "description": "Functions for performing spatial microsimulation ('raking')\n    in R."}, "geogrid": {"categories": ["Spatial"], "description": "Turn irregular polygons (such as geographical regions) into regular or hexagonal grids.\n    This package enables the generation of regular (square) and hexagonal grids through the package \n    'sp' and then assigns the content of the existing polygons to the new grid using \n    the Hungarian algorithm, Kuhn (1955) (<doi:10.1007/978-3-540-68279-0_2>). \n    This prevents the need for manual generation of hexagonal grids or regular grids \n    that are supposed to reflect existing geography."}, "fda.usc": {"categories": ["FunctionalData"], "description": "Routines for exploratory and descriptive analysis of functional data such as depth measurements, atypical curves detection, regression models, supervised classification, unsupervised classification and functional analysis of variance."}, "controlTest": {"categories": ["Survival"], "description": "Nonparametric two-sample procedure for comparing survival quantiles."}, "lpc": {"categories": ["Survival"], "description": "Implements the LPC method of Witten&Tibshirani(Annals of Applied Statistics 2008) for identification of significant genes in a microarray experiment."}, "mclust": {"categories": ["Cluster", "Distributions", "Environmetrics"], "description": "Gaussian finite mixture models fitted via EM algorithm for\n  model-based clustering, classification, and density estimation, \n  including Bayesian regularization, dimension reduction for \n  visualisation, and resampling-based inference."}, "inline": {"categories": ["HighPerformanceComputing"], "description": "Functionality to dynamically define R functions and S4 methods\n with 'inlined' C, C++ or Fortran code supporting the .C and .Call calling\n conventions."}, "breakfast": {"categories": ["TimeSeries"], "description": "A developing software suite for multiple change-point detection/estimation (data segmentation) in data sequences."}, "dbmss": {"categories": ["Spatial"], "description": "Simple computation of spatial statistic functions of distance to characterize the spatial structures of mapped objects, following Marcon, Traissac, Puech, and Lang (2015) <doi:10.18637/jss.v067.c03>.\n  Includes classical functions (Ripley's K and others) and more recent ones used by spatial economists (Duranton and Overman's Kd, Marcon and Puech's M). \n  Relies on 'spatstat' for some core calculation."}, "SpatialTools": {"categories": ["Spatial"], "description": "Tools for spatial data analysis.  Emphasis on kriging.  Provides functions for prediction and simulation.  Intended to be relatively straightforward, fast, and flexible."}, "bcpa": {"categories": ["SpatioTemporal", "Tracking"], "description": "The Behavioral Change Point Analysis (BCPA) is a method of\n    identifying hidden shifts in the underlying parameters of a time series,\n    developed specifically to be applied to animal movement data which is\n    irregularly sampled.  The method is based on: E. Gurarie, R. Andrews and \n    K. Laidre A novel method for identifying behavioural changes in animal \n    movement data (2009) Ecology Letters 12:5 395-408. A development version is \n    on <https://github.com/EliGurarie/bcpa>. NOTE: the BCPA method may be useful \n    for any univariate, irregularly sampled Gaussian time-series, but animal \n    movement analysts are encouraged to apply correlated velocity change point \n    analysis as implemented in the smoove package, as of this writing on GitHub \n    at <https://github.com/EliGurarie/smoove>. An example of a univariate analysis\n    is provided in the UnivariateBCPA vignette. "}, "Counterfactual": {"categories": ["CausalInference"], "description": "Implements the estimation and inference methods for counterfactual analysis described in Chernozhukov, Fernandez-Val and Melly (2013) <doi:10.3982/ECTA10582> \"Inference on Counterfactual Distributions,\" Econometrica, 81(6). The counterfactual distributions considered are the result of changing either the marginal distribution of covariates related to the outcome variable of interest, or the conditional distribution of the outcome given the covariates. They can be applied to estimate quantile treatment effects and wage decompositions."}, "cccp": {"categories": ["Optimization"], "description": "Routines for solving convex optimization problems with cone constraints by means of interior-point methods. The implemented algorithms are partially ported from CVXOPT, a Python module for convex optimization (see <http://cvxopt.org> for more information). "}, "MomTrunc": {"categories": ["Distributions"], "description": "It computes arbitrary products moments (mean vector and variance-covariance matrix), for some double truncated (and folded) multivariate distributions. These distributions belong to the family of selection elliptical distributions, which includes well known skewed distributions as the unified skew-t distribution (SUT) and its particular cases as the extended skew-t (EST), skew-t (ST) and the symmetric student-t (T) distribution. Analogous normal cases unified skew-normal (SUN), extended skew-normal (ESN), skew-normal (SN), and symmetric normal (N) are also included. Density, probabilities and random deviates are also offered for these members."}, "timechange": {"categories": ["TimeSeries"], "description": "Efficient routines for manipulation of date-time objects while\n   accounting for time-zones and daylight saving times. The package includes\n   utilities for updating of date-time components (year, month, day etc.),\n   modification of time-zones, rounding of date-times, period addition and\n   subtraction etc. Parts of the 'CCTZ' source code, released under the Apache\n   2.0 License, are included in this package. See\n   <https://github.com/google/cctz> for more details."}, "normalp": {"categories": ["Distributions"], "description": "A collection of utilities referred to Exponential Power distribution, also known as General Error Distribution (see Mineo, A.M. and Ruggieri, M. (2005), A software Tool for the Exponential Power Distribution: The normalp package. In Journal of Statistical Software, Vol. 12, Issue 4)."}, "dataone": {"categories": ["WebTechnologies"], "description": "Provides read and write access to data and metadata from\n    the DataONE network <https://www.dataone.org> of data repositories.  \n    Each DataONE repository implements a consistent repository application \n    programming interface. Users call methods in R to access these remote \n    repository functions, such as methods to query the metadata catalog, get \n    access to metadata for particular data packages, and read the data objects \n    from the data repository. Users can also insert and update data objects on \n    repositories that support these methods."}, "twangContinuous": {"categories": ["CausalInference"], "description": "Provides functions for propensity score\n        estimation and weighting for continuous exposures as described in Zhu, Y., \n        Coffman, D. L., & Ghosh, D. (2015). A boosting algorithm for\n        estimating generalized propensity scores with continuous treatments.\n        Journal of Causal Inference, 3(1), 25-40. <doi:10.1515/jci-2014-0022>."}, "tpr": {"categories": ["Survival", "TimeSeries"], "description": "Regression models for temporal process responses with\n        time-varying coefficient."}, "CRTgeeDR": {"categories": ["MissingData"], "description": "Implements a semi-parametric GEE estimator accounting for missing data with Inverse-probability weighting (IPW) and for imbalance in covariates with augmentation (AUG). The estimator IPW-AUG-GEE is Doubly robust (DR)."}, "dual": {"categories": ["NumericalMathematics"], "description": "Automatic differentiation is achieved by using dual numbers without\n  providing hand-coded gradient functions. The output value of a mathematical \n  function is returned with the values of its exact first derivative \n  (or gradient). For more details see Baydin, Pearlmutter, Radul, and Siskind\n  (2018) <http://jmlr.org/papers/volume18/17-468/17-468.pdf>."}, "fredr": {"categories": ["TimeSeries"], "description": "An R client for the 'Federal Reserve Economic Data'\n    ('FRED') API <https://research.stlouisfed.org/docs/api/>.  Functions\n    to retrieve economic time series and other data from 'FRED'."}, "pwrRasch": {"categories": ["Psychometrics"], "description": "Statistical power simulation for testing the Rasch Model based on a three-way analysis of variance design with mixed classification."}, "distr6": {"categories": ["Distributions"], "description": "An R6 object oriented distributions package. Unified\n    interface for 42 probability distributions and 11 kernels including\n    functionality for multiple scientific types. Additionally\n    functionality for composite distributions and numerical imputation.\n    Design patterns including wrappers and decorators are described in\n    Gamma et al. (1994, ISBN:0-201-63361-2). For quick reference of\n    probability distributions including d/p/q/r functions and results we\n    refer to McLaughlin, M. P. (2001). Additionally Devroye (1986,\n    ISBN:0-387-96305-7) for sampling the Dirichlet distribution, Gentle\n    (2009) <doi:10.1007/978-0-387-98144-4> for sampling the Multivariate\n    Normal distribution and Michael et al. (1976) <doi:10.2307/2683801>\n    for sampling the Wald distribution."}, "GrassmannOptim": {"categories": ["Optimization"], "description": "Optimizing a function F(U), where U is a semi-orthogonal matrix and F is invariant under an orthogonal transformation of U."}, "MCMCglmm": {"categories": ["Bayesian", "Psychometrics", "Survival"], "description": "Fits Multivariate Generalised Linear Mixed Models (and related models) using Markov chain Monte Carlo techniques (Hadfield 2010 J. Stat. Soft.). "}, "mkin": {"categories": ["DifferentialEquations"], "description": "Calculation routines based on the FOCUS Kinetics Report (2006,\n  2014).  Includes a function for conveniently defining differential equation\n  models, model solution based on eigenvalues if possible or using numerical\n  solvers.  If a C compiler (on windows: 'Rtools') is installed, differential\n  equation models are solved using automatically generated C functions.\n  Heteroscedasticity can be taken into account using variance by variable or\n  two-component error models as described by Ranke and Meinecke (2018)\n  <doi:10.3390/environments6120124>.  Interfaces to several nonlinear\n  mixed-effects model packages are available, some of which are described by\n  Ranke et al. (2021) <doi:10.3390/environments8080071>.  Please note that no\n  warranty is implied for correctness of results or fitness for a particular\n  purpose."}, "rchess": {"categories": ["SportsAnalytics"], "description": "R package for chess validations, pieces movements and check\n    detection. Also integrates functions to plot chess boards given a\n    Forsyth Edwards and Portable Game notations."}, "mimi": {"categories": ["MissingData"], "description": "Generalized low-rank models for mixed and incomplete data frames. The main function may be used for dimensionality reduction of imputation of numeric, binary and count data (simultaneously). Main effects such as column means, group effects, or effects of row-column side information (e.g. user/item attributes in recommendation system) may also be modelled in addition to the low-rank model. Genevi\u00e8ve Robin, Olga Klopp, Julie Josse, \u00c9ric Moulines, Robert Tibshirani (2018) <arXiv:1806.09734>."}, "ICEbox": {"categories": ["MachineLearning"], "description": "Implements Individual Conditional Expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. ICE plots refine Friedman's partial dependence plot by graphing the functional relationship between the predicted response and a covariate of interest for individual observations. Specifically, ICE plots highlight the variation in the fitted values across the range of a covariate of interest, suggesting where and to what extent they may exist."}, "Require": {"categories": ["ReproducibleResearch"], "description": "A single key function, 'Require' that wraps 'install.packages',\n    'remotes::install_github', 'versions::install.versions', and 'base::require'\n    that allows for reproducible workflows. As with other functions in a\n    reproducible workflow, this package emphasizes functions that return the \n    same result whether it is the first or subsequent times running the function.\n    Maturing."}, "BayesianNetwork": {"categories": ["Bayesian"], "description": "A 'Shiny' web application for creating interactive Bayesian Network models,\n    learning the structure and parameters of Bayesian networks, and utilities for classic\n    network analysis."}, "knitLatex": {"categories": ["ReproducibleResearch"], "description": "Provides several helper functions for working with 'knitr' and 'LaTeX'.\n  It includes 'xTab' for creating traditional 'LaTeX' tables, 'lTab' for generating\n  'longtable' environments, and 'sTab' for generating a 'supertabular' environment.\n  Additionally, this package contains a knitr_setup() function which fixes a\n  well-known bug in 'knitr', which distorts the 'results=\"asis\"' command when used\n  in conjunction with user-defined commands; and a com command (<<com=TRUE>>=)\n  which renders the output from 'knitr' as a 'LaTeX' command."}, "match2C": {"categories": ["CausalInference"], "description": "Multivariate matching in observational studies typically has two goals: 1. to construct \n    treated and control groups that have similar distribution of observed covariates and 2. to produce \n    matched pairs or sets that are homogeneous in a few priority variables. This packages implements a\n    network-flow-based method built around a tripartite graph that can simultaneously achieve both goals.\n    The package also implements a template matching algorithm using a variant of the tripartite graph \n    design. A brief description of the workflow and some examples are given in the vignette. A more elaborated\n    tutorial can be found at <https://www.researchgate.net/publication/359513837_Tutorial_for_R_Package_match2C>."}, "rpinterest": {"categories": ["WebTechnologies"], "description": "Get information (boards, pins and\n    users) from the Pinterest <http://www.pinterest.com>\n    API."}, "CHsharp": {"categories": ["Cluster"], "description": "Functions for use in perturbing data prior to use of nonparametric smoothers\n             and clustering.  "}, "covsep": {"categories": ["FunctionalData"], "description": "Functions for testing if the covariance structure of 2-dimensional data\n    (e.g. samples of surfaces X_i = X_i(s,t)) is separable, i.e. if covariance(X) =\n    C_1 x C_2.\n    A complete descriptions of the implemented tests can be found in the paper\n    Aston, John A. D.; Pigoli, Davide; Tavakoli, Shahin. Tests for separability in\n    nonparametric covariance operators of random surfaces. Ann. Statist. 45 (2017),\n    no. 4, 1431\u20131461. <doi:10.1214/16-AOS1495> <https://projecteuclid.org/euclid.aos/1498636862> <arXiv:1505.02023>."}, "generalCorr": {"categories": ["CausalInference"], "description": "Since causal paths from data are important for all sciences, the\n    package provides many sophisticated functions. causeSummBlk() and causeSum2Blk()\n    give easy-to-interpret causal paths.  Let Z denote control variables and compare \n    two flipped kernel regressions: X=f(Y, Z)+e1 and Y=g(X,Z)+e2. Our criterion Cr1 \n    says that if |e1*Y|>|e2*X| then variation in X is more \"exogenous or independent\"\n    than in Y and causal path is X to Y. Criterion Cr2 requires |e2|<|e1|. These\n    inequalities between many absolute values are quantified by four orders of \n    stochastic dominance. Our third criterion Cr3 for the causal path X to Y\n    requires new generalized partial correlations to satisfy |r*(x|y,z)|< |r*(y|x,z)|.\n    The function parcorVec() reports generalized partials between the first\n    variable and all others.  The package provides several R functions including\n    get0outliers() for outlier detection, bigfp() for numerical integration by the\n    trapezoidal rule, stochdom2() for stochastic dominance, pillar3D() for 3D charts,\n    canonRho() for generalized canonical correlations, depMeas() measures nonlinear\n    dependence, and causeSummary(mtx) reports summary of causal paths among matrix columns\n    is easiest to use. Portfolio selection: decileVote(), momentVote(), dif4mtx(), \n    exactSdMtx() can rank several stocks.  Several functions whose names begin with 'boot' provide bootstrap\n    statistical inference including a new bootGcRsq() test for \"Granger-causality\" \n    allowing nonlinear relations. A new tool for evaluation of out-of-sample\n    portfolio performance is outOFsamp(). See six vignettes of the package for theory\n    and usage tips. See Vinod (2019) \\doi{10.1080/03610918.2015.1122048}."}, "udpipe": {"categories": ["NaturalLanguageProcessing"], "description": "This natural language processing toolkit provides language-agnostic\n    'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency\n    parsing' of raw text. Next to text parsing, the package also allows you to train\n    annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided\n    at <https://universaldependencies.org/format.html>. The techniques are explained\n    in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0\n    with UDPipe', available at <doi:10.18653/v1/K17-3009>. \n    The toolkit also contains functionalities for commonly used data manipulations on texts \n    which are enriched with the output of the parser. Namely functionalities and algorithms \n    for collocations, token co-occurrence, document term matrix handling, \n    term frequency inverse document frequency calculations,\n    information retrieval metrics (Okapi BM25), handling of multi-word expressions,\n    keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) \n    sentiment scoring and semantic similarity analysis."}, "nls2": {"categories": ["Optimization"], "description": "Adds brute force and multiple starting values to nls."}, "ptw": {"categories": ["TimeSeries"], "description": "Parametric Time Warping aligns patterns, i.e. it aims to\n        put corresponding features at the same locations. The algorithm\n        searches for an optimal polynomial describing the warping. It\n        is possible to align one sample to a reference, several samples\n        to the same reference, or several samples to several\n        references. One can choose between calculating individual\n        warpings, or one global warping for a set of samples and one\n        reference. Two optimization criteria are implemented: RMS (Root\n        Mean Square error) and WCC (Weighted Cross Correlation). Both\n\twarping of peak profiles and of peak lists are supported. A\n\tvignette for the latter is contained in the inst/doc directory\n\tof the source package - the vignette source can be found on\n\tthe package github site."}, "maicChecks": {"categories": ["CausalInference"], "description": "A collection of easy-to-implement tools for checking whether a MAIC can be conducted. An alternative way of calculating weights is also included. These methods are introduced in Glimm & Yau (2021) <arXiv:2108.01896>."}, "BayesTree": {"categories": ["Bayesian", "MachineLearning"], "description": "This is an implementation of BART:Bayesian Additive Regression Trees,\n        by Chipman, George, McCulloch (2010)."}, "tidysynth": {"categories": ["CausalInference"], "description": "A synthetic control offers a way of evaluating the effect of an intervention in comparative case studies. The package makes a number of improvements when implementing the method in R. These improvements allow users to inspect, visualize, and tune the synthetic control more easily. A key benefit of a tidy implementation is that the entire preparation process for building the synthetic control can be accomplished in a single pipe. For more information on the synthetic control method, see Abadie et al. (2003) <doi:10.1257/000282803321455188>."}, "teigen": {"categories": ["Cluster"], "description": "Fits mixtures of multivariate t-distributions (with eigen-decomposed covariance structure) via the expectation conditional-maximization algorithm under a clustering or classification paradigm."}, "raveio": {"categories": ["MedicalImaging"], "description": "Includes multiple cross-platform read/write interfaces for\n    'RAVE' project. 'RAVE' stands for \"R analysis and visualization of human \n    intracranial electroencephalography data\". The whole project aims at \n    providing powerful free-source package that analyze brain recordings from \n    patients with electrodes placed on the cortical surface or inserted into \n    the brain. 'raveio' as part of this project provides tools to read/write \n    neurophysiology data from/to 'RAVE' file structure, as well as several \n    popular formats including  'EDF(+)', 'Matlab', 'BIDS-iEEG', and 'HDF5', \n    etc. Documentation and examples about 'RAVE' project are provided at \n    <https://openwetware.org/wiki/RAVE>, and the paper by John F. Magnotti, \n    Zhengjia Wang, Michael S. Beauchamp (2020) \n    <doi:10.1016/j.neuroimage.2020.117341>; see 'citation(\"raveio\")' for \n    details."}, "MCAvariants": {"categories": ["Psychometrics"], "description": "Provides two variants of multiple correspondence analysis (ca):\n                     multiple ca and ordered multiple ca via orthogonal polynomials of Emerson."}, "nhlapi": {"categories": ["SportsAnalytics"], "description": "Retrieves and processes the data exposed by the open 'NHL' API. This includes information on players, teams, games, tournaments, drafts, standings, schedules and other endpoints. A lower-level interface to access the data via URLs directly is also provided."}, "ibd": {"categories": ["ExperimentalDesign"], "description": "A collection of several utility functions related to binary incomplete block designs. The package contains function to generate A- and D-efficient binary incomplete block designs with given numbers of treatments, number of blocks and block size. The package also contains function to generate an incomplete block design with specified concurrence matrix. There are functions to generate balanced treatment incomplete block designs and incomplete block designs for test versus control treatments comparisons with specified concurrence matrix. Package also allows performing analysis of variance of data and computing estimated marginal means of factors from experiments using a connected incomplete block design. Tests of hypothesis of treatment contrasts in incomplete block design set up is supported. "}, "timetk": {"categories": ["TimeSeries"], "description": "\n    Easy visualization, wrangling, and feature engineering of time series data for \n    forecasting and machine learning prediction. Consolidates and extends time series functionality \n    from packages including 'dplyr', 'stats', 'xts', 'forecast', 'slider', 'padr', 'recipes', and 'rsample'."}, "RMariaDB": {"categories": ["Databases"], "description": "Implements a DBI-compliant interface to MariaDB\n    (<https://mariadb.org/>) and MySQL (<https://www.mysql.com/>)\n    databases."}, "combinat": {"categories": ["NumericalMathematics"], "description": "routines for combinatorics"}, "estmeansd": {"categories": ["MetaAnalysis"], "description": "Implements the methods of McGrath et al. (2020) \n    <doi:10.1177/0962280219889080> and Cai et al. (2021) \n    <doi:10.1177/09622802211047348> for estimating the sample mean and standard \n    deviation from commonly reported quantiles in meta-analysis. These methods \n    can be applied to studies that report the sample median, sample size, and \n    one or both of (i) the sample minimum and maximum values and (ii) the first \n    and third quartiles. "}, "stockfish": {"categories": ["SportsAnalytics"], "description": "An implementation of the UCI open communication protocol that\n    ships with 'Stockfish' <https://stockfishchess.org/>, a very popular,\n    open source, powerful chess engine written in C++."}, "ATmet": {"categories": ["ChemPhys"], "description": "A collection of functions for smart sampling and sensitivity analysis for metrology applications, including computationally expensive problems."}, "longmemo": {"categories": ["Finance"], "description": "Datasets and Functionality from\n  'Jan Beran' (1994). Statistics for Long-Memory Processes; Chapman & Hall.\n  Estimation of Hurst (and more) parameters for fractional Gaussian noise,\n  'fARIMA' and 'FEXP' models."}, "FixedPoint": {"categories": ["NumericalMathematics"], "description": "For functions that take and return vectors (or scalars), this package provides 8 algorithms for finding fixed point vectors (vectors for which the inputs and outputs to the function are the same vector). These algorithms include Anderson (1965) acceleration <doi:10.1145/321296.321305>, epsilon extrapolation methods (Wynn 1962 <doi:10.2307/2004051>) and minimal polynomial methods (Cabay and Jackson 1976 <doi:10.1137/0713060>)."}, "CADFtest": {"categories": ["Econometrics", "Finance", "TimeSeries"], "description": "Hansen's (1995) Covariate-Augmented\n        Dickey-Fuller (CADF) test. The only required argument is y, the\n        Tx1 time series to be tested. If no stationary covariate X is\n        passed to the procedure, then an ordinary ADF test is\n        performed. The p-values of the test are computed using the\n        procedure illustrated in Lupi (2009)."}, "party": {"categories": ["Environmetrics", "MachineLearning", "Survival"], "description": "A computational toolbox for recursive partitioning.\n  The core of the package is ctree(), an implementation of\n  conditional inference trees which embed tree-structured \n  regression models into a well defined theory of conditional\n  inference procedures. This non-parametric class of regression\n  trees is applicable to all kinds of regression problems, including\n  nominal, ordinal, numeric, censored as well as multivariate response\n  variables and arbitrary measurement scales of the covariates. \n  Based on conditional inference trees, cforest() provides an\n  implementation of Breiman's random forests. The function mob()\n  implements an algorithm for recursive partitioning based on\n  parametric models (e.g. linear models, GLMs or survival\n  regression) employing parameter instability tests for split\n  selection. Extensible functionality for visualizing tree-structured\n  regression models is available. The methods are described in\n  Hothorn et al. (2006) <doi:10.1198/106186006X133933>,\n  Zeileis et al. (2008) <doi:10.1198/106186008X319331> and \n  Strobl et al. (2007) <doi:10.1186/1471-2105-8-25>."}, "pbapply": {"categories": ["HighPerformanceComputing"], "description": "A lightweight package that adds\n  progress bar to vectorized R functions\n  ('*apply'). The implementation can easily be added\n  to functions where showing the progress is\n  useful (e.g. bootstrap). The type and style of the\n  progress bar (with percentages or remaining time)\n  can be set through options.\n  Supports several parallel processing backends."}, "rLTP": {"categories": ["WebTechnologies"], "description": "R interface to the 'LTP'-Cloud service for Natural Language Processing\n    in Chinese (http://www.ltp-cloud.com/)."}, "rust": {"categories": ["Distributions"], "description": "Uses the generalized ratio-of-uniforms (RU) method to simulate\n    from univariate and (low-dimensional) multivariate continuous distributions.\n    The user specifies the log-density, up to an additive constant. The RU\n    algorithm is applied after relocation of mode of the density to zero, and\n    the user can choose a tuning parameter r. For details see Wakefield, Gelfand\n    and Smith (1991) <doi:10.1007/BF01889987>, Efficient generation of random\n    variates via the ratio-of-uniforms method, Statistics and Computing (1991)\n    1, 129-133.  A Box-Cox variable transformation can be used to make the input\n    density suitable for the RU method and to improve efficiency.  In the\n    multivariate case rotation of axes can also be used to improve efficiency.\n    From version 1.2.0 the 'Rcpp' package \n    <https://cran.r-project.org/package=Rcpp> can be used to improve efficiency."}, "nlreg": {"categories": ["ChemPhys"], "description": "Likelihood inference based on higher order approximations \n  for nonlinear models with possibly non constant variance."}, "tm.plugin.mail": {"categories": ["NaturalLanguageProcessing"], "description": "A plug-in for the tm text mining framework providing mail handling\n  functionality."}, "covid19.analytics": {"categories": ["Epidemiology"], "description": "Load and analyze updated time series worldwide data of reported cases for the Novel Coronavirus Disease (COVID-19) from different sources, including the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) data repository <https://github.com/CSSEGISandData/COVID-19>, \"Our World in Data\" <https://github.com/owid/> among several others. The datasets reporting the COVID-19 cases are available in two main modalities, as a time series sequences and aggregated data for the last day with greater spatial resolution. Several analysis, visualization and modelling functions are available in the package that will allow the user to compute and visualize total number of cases, total number of changes and growth rate globally or for an specific geographical location, while at the same time generating models using these trends; generate interactive visualizations and generate Susceptible-Infected-Recovered (SIR) model for the disease spread."}, "svars": {"categories": ["TimeSeries"], "description": "Implements data-driven identification methods for structural vector autoregressive (SVAR) models as described in Lange et al. (2021) <doi:10.18637/jss.v097.i05>. \n             Based on an existing VAR model object (provided by e.g. VAR() from the 'vars' package), the structural \n             impact matrix is obtained via data-driven identification techniques (i.e. changes in volatility (Rigobon, R. (2003) <doi:10.1162/003465303772815727>),  patterns of GARCH (Normadin, M., Phaneuf, L. (2004) <doi:10.1016/j.jmoneco.2003.11.002>),\n             independent component analysis (Matteson, D. S, Tsay, R. S., (2013) <doi:10.1080/01621459.2016.1150851>), least dependent innovations (Herwartz, H., Ploedt, M., (2016) <doi:10.1016/j.jimonfin.2015.11.001>), \n             smooth transition in variances (Luetkepohl, H., Netsunajev, A. (2017) <doi:10.1016/j.jedc.2017.09.001>) or non-Gaussian maximum likelihood (Lanne, M., Meitz, M., Saikkonen, P. (2017) <doi:10.1016/j.jeconom.2016.06.002>))."}, "corpora": {"categories": ["NaturalLanguageProcessing"], "description": "Utility functions for the statistical analysis of corpus frequency data.\n        This package is a companion to the open-source course \"Statistical Inference: \n        A Gentle Introduction for Computational Linguists and Similar Creatures\" ('SIGIL')."}, "DoE.base": {"categories": ["ExperimentalDesign"], "description": "Creates full factorial experimental designs and designs based on orthogonal arrays for (industrial) experiments. Provides diverse quality criteria. Provides utility functions for the class design, which is also used by other packages for designed experiments."}, "tvm": {"categories": ["Finance"], "description": "Functions for managing cashflows and interest rate curves."}, "fdatest": {"categories": ["FunctionalData"], "description": "Implementation of the Interval Testing Procedure for functional data in different frameworks (i.e., one or two-population frameworks, functional linear models) by means of different basis expansions (i.e., B-spline, Fourier, and phase-amplitude Fourier). The current version of the package requires functional data evaluated on a uniform grid; it automatically projects each function on a chosen functional basis; it performs the entire family of multivariate tests; and, finally, it provides the matrix of the p-values of the previous tests and the vector of the corrected p-values. The functional basis, the coupled or uncoupled scenario, and the kind of test can be chosen by the user. The package provides also a plotting function creating a graphical output of the procedure: the p-value heat-map, the plot of the corrected p-values, and the plot of the functional data."}, "MKinfer": {"categories": ["MissingData"], "description": "Computation of various confidence intervals (Altman et al. (2000), ISBN:978-0-727-91375-3; Hedderich and Sachs (2018), ISBN:978-3-662-56657-2) including bootstrapped versions (Davison and Hinkley (1997), ISBN:978-0-511-80284-3) as well as Hsu (Hedderich and Sachs (2018), ISBN:978-3-662-56657-2), permutation (Janssen (1997), <doi:10.1016/S0167-7152(97)00043-6>), bootstrap (Davison and Hinkley (1997), ISBN:978-0-511-80284-3) and multiple imputation (Barnard and Rubin (1999), <doi:10.1093/biomet/86.4.948>) t-test. Graphical visualization by volcano plots."}, "dina": {"categories": ["Bayesian", "Psychometrics"], "description": "Estimate the Deterministic Input, Noisy \"And\" Gate (DINA)\n    cognitive diagnostic model parameters using the Gibbs sampler described\n    by Culpepper (2015) <doi:10.3102/1076998615595403>."}, "UnivRNG": {"categories": ["Distributions"], "description": "Pseudo-random number generation of 17 univariate distributions proposed by Demirtas. (2005) <doi:10.22237/jmasm/1114907220>."}, "oce": {"categories": ["Environmetrics", "SpatioTemporal"], "description": "Supports the analysis of Oceanographic data, including 'ADCP'\n    measurements, measurements made with 'argo' floats, 'CTD' measurements,\n    sectional data, sea-level time series, coastline and topographic data, etc.\n    Provides specialized functions for calculating seawater properties such as\n    potential temperature in either the 'UNESCO' or 'TEOS-10' equation of state.\n    Produces graphical displays that conform to the conventions of the\n    Oceanographic literature. This package is discussed extensively by\n    Kelley (2018) \"Oceanographic Analysis with R\" <doi:10.1007/978-1-4939-8844-0>."}, "ganalytics": {"categories": ["WebTechnologies"], "description": "Functions for querying the 'Google Analytics' core reporting,\n    real-time, multi-channel funnel and management APIs, as well as the 'Google\n    Tag Manager' (GTM) API. Write methods are also provided for the management\n    and GTM APIs so that you can change tag, property or view settings, for\n    example. Define reporting queries using natural R expressions instead of\n    being concerned as much about API technical intricacies like query syntax,\n    character code escaping, and API limitations."}, "mar1s": {"categories": ["TimeSeries"], "description": "Multiplicative AR(1) with Seasonal is a stochastic\n  process model built on top of AR(1). The package provides the\n  following procedures for MAR(1)S processes: fit, compose, decompose,\n  advanced simulate and predict."}, "mvGPS": {"categories": ["CausalInference"], "description": "\n    Methods for estimating and utilizing the multivariate generalized\n    propensity score (mvGPS) for multiple continuous exposures described in\n    Williams, J.R, and Crespi, C.M. (2020) <arXiv:2008.13767>. The methods allow\n    estimation of a dose-response surface relating the joint distribution of multiple\n    continuous exposure variables to an outcome. Weights are constructed assuming a\n    multivariate normal density for the marginal and conditional distribution of\n    exposures given a set of confounders. Confounders can be different for different\n    exposure variables. The weights are designed to achieve balance across all\n    exposure dimensions and can be used to estimate dose-response surfaces."}, "MBNMAtime": {"categories": ["MetaAnalysis"], "description": "Fits Bayesian time-course models for model-based network meta-analysis (MBNMA) that allows inclusion of multiple\n  time-points from studies. Repeated measures over time are accounted for within studies by applying different time-course functions,\n  following the method of Pedder et al. (2019) <doi:10.1002/jrsm.1351>. \n  The method allows synthesis of studies with multiple follow-up measurements that can account for time-course for a single or multiple \n  treatment comparisons. Several general time-course functions are provided; others may be added \n  by the user. Various characteristics can be flexibly added to the models, such as correlation between time points and shared \n  class effects. The consistency of direct and indirect evidence in the network can be assessed using unrelated mean effects \n  models and/or by node-splitting."}, "blocklength": {"categories": ["TimeSeries"], "description": "A set of functions to select the optimal block-length for a \n    dependent bootstrap (block-bootstrap). Includes the Hall, Horowitz, and Jing\n    (1995) <doi:10.1093/biomet/82.3.561> cross-validation method and the \n    Politis and White (2004) <doi:10.1081/ETC-120028836> Spectral Density \n    Plug-in method, including the Patton, Politis, and White (2009)\n    <doi:10.1080/07474930802459016> correction with a corresponding set of S3\n    plot methods."}, "frbs": {"categories": ["MachineLearning"], "description": "An implementation of various learning algorithms based on fuzzy rule-based systems (FRBSs) for dealing with classification and regression tasks. Moreover, it allows to construct an FRBS model defined by human experts. \n    FRBSs are based on the concept of fuzzy sets, proposed by Zadeh in 1965, which aims at\n    representing the reasoning of human experts in a set of IF-THEN rules, to\n    handle real-life problems in, e.g., control, prediction and inference, data\n    mining, bioinformatics data processing, and robotics. FRBSs are also known\n    as fuzzy inference systems and fuzzy models. During the modeling of an\n    FRBS, there are two important steps that need to be conducted: structure\n    identification and parameter estimation. Nowadays, there exists a wide\n    variety of algorithms to generate fuzzy IF-THEN rules automatically from\n    numerical data, covering both steps. Approaches that have been used in the\n    past are, e.g., heuristic procedures, neuro-fuzzy techniques, clustering\n    methods, genetic algorithms, squares methods, etc. Furthermore, in this\n    version we provide a universal framework named 'frbsPMML', which is adopted\n    from the Predictive Model Markup Language (PMML), for representing FRBS\n    models. PMML is an XML-based language to provide a standard for describing\n    models produced by data mining and machine learning algorithms. Therefore,\n    we are allowed to export and import an FRBS model to/from 'frbsPMML'.\n    Finally, this package aims to implement the most widely used standard\n    procedures, thus offering a standard package for FRBS modeling to the R\n    community."}, "tclust": {"categories": ["Cluster", "Robust"], "description": "Provides functions for robust trimmed clustering. The methods are described in Garcia-Escudero (2008) <doi:10.1214/07-AOS515>, Fritz et al. (2012) <doi:10.18637/jss.v047.i12>, Garcia-Escudero et al. (2011)  <doi:10.1007/s11222-010-9194-z> and others."}, "mcga": {"categories": ["Optimization"], "description": "Machine coded genetic algorithm (MCGA) is a fast tool for\n    real-valued optimization problems. It uses the byte\n    representation of variables rather than real-values. It\n    performs the classical crossover operations (uniform) on these\n    byte representations. Mutation operator is also similar to\n    classical mutation operator, which is to say, it changes a\n    randomly selected byte value of a chromosome by +1 or -1 with\n    probability 1/2. In MCGAs there is no need for\n    encoding-decoding process and the classical operators are\n    directly applicable on real-values. It is fast and can handle a\n    wide range of a search space with high precision. Using a\n    256-unary alphabet is the main disadvantage of this algorithm\n    but a moderate size population is convenient for many problems.\n    Package also includes multi_mcga function for multi objective\n    optimization problems. This function sorts the chromosomes\n    using their ranks calculated from the non-dominated sorting\n    algorithm."}, "KONPsurv": {"categories": ["Survival"], "description": "The K-sample omnibus non-proportional hazards (KONP) tests are powerful non-parametric tests for comparing K (>=2) hazard functions based on right-censored data (Gorfine, Schlesinger and Hsu, 2020, <doi:10.1177/0962280220907355>). These tests are consistent against any differences between the hazard functions of the groups. The KONP tests are often more powerful than other existing tests, especially under non-proportional hazard functions."}, "edmcr": {"categories": ["MissingData"], "description": "Implements various general algorithms to estimate missing elements\n   of a Euclidean (squared) distance matrix.  \n   Includes optimization methods based on semi-definite programming found in\n   Alfakih, Khadani, and Wolkowicz (1999)<doi:10.1023/A:1008655427845>, \n   a non-convex position formulation by Fang and O'Leary (2012)<doi:10.1080/10556788.2011.643888>, and \n   a dissimilarity parameterization formulation by Trosset (2000)<doi:10.1023/A:1008722907820>.\n   When the only non-missing\n   distances are those on the minimal spanning tree, the guided random search\n   algorithm will complete the matrix while preserving the minimal spanning tree following\n   Rahman and Oldford (2018)<doi:10.1137/16M1092350>.\n   Point configurations in specified dimensions can be determined from the completions. \n   Special problems such as the sensor localization problem, \n   as for example in Krislock and Wolkowicz (2010)<doi:10.1137/090759392>,\n   as well as reconstructing\n   the geometry of a molecular structure, as for example in \n   Hendrickson (1995)<doi:10.1137/0805040>, can also be solved.\n   These and other methods are described in the thesis of Adam Rahman(2018)<https://hdl.handle.net/10012/13365>."}, "randomizeR": {"categories": ["ClinicalTrials"], "description": "This tool enables the user to choose a randomization procedure\n    based on sound scientific criteria. It comprises the generation of\n    randomization sequences as well the assessment of randomization procedures\n    based on carefully selected criteria. Furthermore, 'randomizeR' provides a\n    function for the comparison of randomization procedures."}, "sbw": {"categories": ["CausalInference"], "description": "Implements the Stable Balancing Weights by Zubizarreta (2015) <doi:10.1080/01621459.2015.1023805>. These are the weights of minimum variance that approximately balance the empirical distribution of the observed covariates. For an overview, see Chattopadhyay, Hase and Zubizarreta (2020) <doi:10.1002/(ISSN)1097-0258>. To solve the optimization problem in 'sbw', the default solver is 'quadprog', which is readily available through CRAN. The solver 'osqp' is also posted on CRAN. To enhance the performance of 'sbw', users are encouraged to install other solvers such as 'gurobi' and 'Rmosek', which require special installation. For the installation of gurobi and pogs, please follow the instructions at <https://www.gurobi.com/documentation/9.1/quickstart_mac/r_ins_the_r_package.html> and <http://foges.github.io/pogs/stp/r>."}, "tsiR": {"categories": ["Epidemiology"], "description": "An implementation of the time-series Susceptible-Infected-Recovered (TSIR) model using a number of different fitting options for infectious disease time series data. The manuscript based on this package can be found here <doi:10.1371/journal.pone.0185528>. The method implemented here is described by Finkenstadt and Grenfell (2000) <doi:10.1111/1467-9876.00187>."}, "fAssets": {"categories": ["Finance"], "description": "Provides a  collection of functions \n  to manage, to investigate and to analyze data sets of financial \n  assets from different points of view."}, "dbarts": {"categories": ["Bayesian"], "description": "Fits Bayesian additive regression trees (BART; Chipman, George, and McCulloch (2010) <doi:10.1214/09-AOAS285>) while allowing the updating of predictors or response so that BART can be incorporated as a conditional model in a Gibbs/Metropolis-Hastings sampler. Also serves as a drop-in replacement for package 'BayesTree'."}, "bnnSurvival": {"categories": ["Survival"], "description": "Implements a bootstrap aggregated (bagged) version of\n    the k-nearest neighbors survival probability prediction method (Lowsky et\n    al. 2013). In addition to the bootstrapping of training samples, the\n    features can be subsampled in each baselearner to break the correlation\n    between them. The Rcpp package is used to speed up the computation."}, "reticulate": {"categories": ["HighPerformanceComputing", "ModelDeployment", "NumericalMathematics"], "description": "Interface to 'Python' modules, classes, and functions. When calling\n    into 'Python', R data types are automatically converted to their equivalent 'Python'\n    types. When values are returned from 'Python' to R they are converted back to R\n    types. Compatible with all versions of 'Python' >= 2.7."}, "modeltime": {"categories": ["TimeSeries"], "description": "\n    The time series forecasting framework for use with the 'tidymodels' ecosystem. \n    Models include ARIMA, Exponential Smoothing, and additional time series models\n    from the 'forecast' and 'prophet' packages. Refer to \"Forecasting Principles & Practice, Second edition\" \n    (<https://otexts.com/fpp2/>).\n    Refer to \"Prophet: forecasting at scale\" \n    (<https://research.facebook.com/blog/2017/02/prophet-forecasting-at-scale/>.)."}, "Rcsdp": {"categories": ["Optimization"], "description": "R interface to the CSDP semidefinite programming library. Installs version 6.1.1 of CSDP from the COIN-OR website if required. An existing installation of CSDP may be used by passing the proper configure arguments to the installation command. See the INSTALL file for further details."}, "sleekts": {"categories": ["TimeSeries"], "description": "Compute Time series Resistant Smooth 4253H, twice smoothing method."}, "BTdecayLasso": {"categories": ["SportsAnalytics"], "description": "We apply Bradley-Terry Model to estimate teams' ability in paired comparison data. Exponential Decayed Log-likelihood function is applied for dynamic approximation of current rankings and Lasso penalty is applied for variance reduction and grouping. The main algorithm applies the Augmented Lagrangian Method described by Masarotto and Varin (2012) <doi:10.1214/12-AOAS581>."}, "segclust2d": {"categories": ["Tracking"], "description": "Provides two methods for segmentation and joint segmentation/clustering of\n    bivariate time-series. Originally intended for ecological segmentation\n    (home-range and behavioural modes) but easily applied on other series,\n    the package also provides tools for analysing outputs from R packages 'moveHMM' and 'marcher'.\n    The segmentation method is a bivariate extension of  Lavielle's method available in 'adehabitatLT' \n    (Lavielle, 1999 <doi:10.1016/S0304-4149(99)00023-X> and 2005 <doi:10.1016/j.sigpro.2005.01.012>).\n    This method rely on dynamic programming for efficient segmentation.\n    The segmentation/clustering method alternates steps of dynamic programming with an Expectation-Maximization algorithm.\n    This is an extension of Picard et al (2007) <doi:10.1111/j.1541-0420.2006.00729.x> method \n    (formerly available in 'cghseg' package) to the bivariate case.\n    The method is fully described in Patin et al (2018) <doi:10.1101/444794>."}, "RJDemetra": {"categories": ["TimeSeries"], "description": "Interface around 'JDemetra+' (<https://github.com/jdemetra/jdemetra-app>), the seasonal adjustment software officially\n    recommended to the members of the European Statistical System (ESS) and the European System of Central Banks.\n    It offers full access to all options and outputs of 'JDemetra+', including the two leading seasonal adjustment methods\n    TRAMO/SEATS+ and X-12ARIMA/X-13ARIMA-SEATS."}, "MendelianRandomization": {"categories": ["CausalInference", "MetaAnalysis"], "description": "Encodes several methods for performing Mendelian randomization\n    analyses with summarized data. Summarized data on genetic associations with the\n    exposure and with the outcome can be obtained from large consortia. These data\n    can be used for obtaining causal estimates using instrumental variable methods."}, "howzatR": {"categories": ["SportsAnalytics"], "description": "Helping to calculate cricket specific problems in a tidy & simple manner."}, "SLBDD": {"categories": ["TimeSeries"], "description": "Programs for analyzing large-scale time series data. They include functions for automatic specification and estimation of univariate time series, for clustering time series, for multivariate outlier detections, for quantile plotting of many time series, for dynamic factor models and for creating input data for deep learning programs. Examples of using the package can be found in the Wiley book 'Statistical Learning with Big Dependent Data' by Daniel Pe\u00f1a and Ruey S. Tsay (2021). ISBN 9781119417385."}, "multipleNCC": {"categories": ["Survival"], "description": "Fit Cox proportional hazard models with a weighted \n  partial likelihood. It handles one or multiple endpoints, additional matching \n  and makes it possible to reuse controls for other endpoints."}, "survIDINRI": {"categories": ["Survival"], "description": "Performs inference for a class of measures to compare competing risk prediction models with censored survival data. The class includes the integrated discrimination improvement index (IDI) and category-less net reclassification index (NRI)."}, "ftsspec": {"categories": ["FunctionalData"], "description": "Functions for estimating spectral density operator of functional\n    time series (FTS) and comparing the spectral density operator of two\n    functional time series, in a way that allows detection of differences of\n    the spectral density operator in frequencies and along the curve length."}, "BayesGOF": {"categories": ["Bayesian"], "description": "A Bayesian data modeling scheme that performs four interconnected tasks: (i) characterizes the uncertainty of the elicited parametric prior; (ii) provides exploratory diagnostic for checking prior-data conflict; (iii) computes the final statistical prior density estimate; and (iv) executes macro- and micro-inference. Primary reference is Mukhopadhyay, S. and Fletcher, D. 2018 paper \"Generalized Empirical Bayes via Frequentist Goodness of Fit\" (<https://www.nature.com/articles/s41598-018-28130-5>). "}, "kiwisR": {"categories": ["Hydrology"], "description": "A wrapper for querying 'WISKI' databases via the 'KiWIS' 'REST' API. 'WISKI' is an 'SQL' relational database \n  used for the collection and storage of water data developed by KISTERS and 'KiWIS' is a 'REST' service that provides\n  access to 'WISKI' databases via HTTP requests (<https://water.kisters.de/en/technology-trends/kisters-and-open-data/>). \n  Contains a list of default databases (called 'hubs') and also allows users to provide their own 'KiWIS' URL. \n  Supports the entire query process- from metadata to specific time series values. All data is returned as tidy tibbles."}, "piecewiseSEM": {"categories": ["Psychometrics"], "description": "Implements piecewise structural equation modeling from a single\n    list of structural equations, with new methods for non-linear, latent, and\n    composite variables, standardized coefficients, query-based prediction and\n    indirect effects. See <http://jslefche.github.io/piecewiseSEM/> for more."}, "FeedbackTS": {"categories": ["TimeSeries"], "description": "Analysis of fragmented time directionality to investigate feedback in time series. Tools provided by the package allow the analysis of feedback for a single time series and the analysis of feedback for a set of time series collected across a spatial domain."}, "OptimalDesign": {"categories": ["ExperimentalDesign"], "description": "Algorithms for D-, A-, I-, and c-optimal designs. Some of the functions in this package require the 'gurobi' software and its accompanying R package. For their installation, please follow the instructions at <https://www.gurobi.com> and the file gurobi_inst.txt, respectively."}, "wordnet": {"categories": ["NaturalLanguageProcessing"], "description": "An interface to WordNet using the Jawbone Java API to WordNet.\n  WordNet (<https://wordnet.princeton.edu/>) is a large lexical database of\n  English.  Nouns, verbs, adjectives and adverbs are grouped into sets of\n  cognitive synonyms (synsets), each expressing a distinct concept.  Synsets\n  are interlinked by means of conceptual-semantic and lexical relations.\n  Please note that WordNet(R) is a registered tradename.  Princeton\n  University makes WordNet available to research and commercial users\n  free of charge provided the terms of their license\n  (<https://wordnet.princeton.edu/license-and-commercial-use>) are followed,\n  and proper reference is made to the project using an appropriate\n  citation (<https://wordnet.princeton.edu/citing-wordnet>)."}, "fpp3": {"categories": ["TimeSeries"], "description": "\n    All data sets required for the examples and exercises in the book\n    \"Forecasting: principles and practice\" by Rob J Hyndman and George Athanasopoulos\n    <https://OTexts.com/fpp3/>.  All packages required to run the examples are also\n    loaded."}, "cna": {"categories": ["CausalInference"], "description": "Provides comprehensive functionalities for causal modeling with Coincidence Analysis (CNA), which is a configurational comparative method of causal data analysis that was first introduced in Baumgartner (2009) <doi:10.1177/0049124109339369>, and generalized in Baumgartner & Ambuehl (2018) <doi:10.1017/psrm.2018.45>. CNA is designed to recover INUS-causation from data, which is particularly relevant for analyzing processes featuring conjunctural causation (component causation) and equifinality (alternative causation). CNA is currently the only method for INUS-discovery that allows for multiple effects (outcomes/endogenous factors), meaning it can analyze common-cause and causal chain structures."}, "freesurferformats": {"categories": ["MedicalImaging"], "description": "Provides functions to read and write neuroimaging data in various file formats, with a focus on 'FreeSurfer' <http://freesurfer.net/> formats. This includes, but is not limited to, the following file formats: 1) MGH/MGZ format files, which can contain multi-dimensional images or other data. Typically they contain time-series of three-dimensional brain scans acquired by magnetic resonance imaging (MRI). They can also contain vertex-wise measures of surface morphometry data. The MGH format is named after the Massachusetts General Hospital, and the MGZ format is a compressed version of the same format. 2) 'FreeSurfer' morphometry data files in binary 'curv' format. These contain vertex-wise surface measures, i.e., one scalar value for each vertex of a brain surface mesh. These are typically values like the cortical thickness or brain surface area at each vertex. 3) Annotation file format. This contains a brain surface parcellation derived from a cortical atlas. 4) Surface file format. Contains a brain surface mesh, given by a list of vertices and a list of faces."}, "zipfR": {"categories": ["Distributions", "NaturalLanguageProcessing"], "description": "Statistical models and utilities for the analysis of word frequency distributions.\n\tThe utilities include functions for loading, manipulating and visualizing word frequency\n\tdata and vocabulary growth curves.  The package also implements several statistical \n\tmodels for the distribution of word frequencies in a population.  (The name of this package \n\tderives from the most famous word frequency distribution, Zipf's law.)"}, "commonsMath": {"categories": ["NumericalMathematics"], "description": "Java JAR files for the Apache Commons Mathematics Library for use by users and other packages."}, "ggsoccer": {"categories": ["SportsAnalytics"], "description": "The 'ggplot2' package provides a powerful set of tools \n  for visualising and investigating data. The 'ggsoccer' package provides a \n  set of functions for elegantly displaying and exploring soccer event data\n  with 'ggplot2'. Providing extensible layers and themes, it is designed to\n  work smoothly with a variety of popular sports data providers."}, "MPS": {"categories": ["Distributions"], "description": "Developed for computing the probability density function, computing the cumulative distribution function, computing the quantile function, random generation, drawing q-q plot, and estimating the parameters of 24 G-family of statistical distributions via the maximum product spacing approach introduced in <https://www.jstor.org/stable/2345411>. The set of families contains: beta G distribution, beta exponential G distribution, beta extended G distribution, exponentiated G distribution, exponentiated exponential Poisson G distribution, exponentiated generalized G distribution, exponentiated Kumaraswamy G distribution, gamma type I G distribution, gamma type II G distribution, gamma uniform G distribution, gamma-X generated of log-logistic family of G distribution, gamma-X family of modified beta exponential G distribution, geometric exponential Poisson G distribution, generalized beta G distribution, generalized transmuted G distribution, Kumaraswamy G distribution, log gamma type I G distribution, log gamma type II G distribution, Marshall Olkin G distribution, Marshall Olkin Kumaraswamy G distribution, modified beta G distribution, odd log-logistic G distribution, truncated-exponential skew-symmetric G distribution, and Weibull G distribution."}, "crs": {"categories": ["Optimization"], "description": "Regression splines that handle a mix of continuous and categorical (discrete) data often encountered in applied settings. I would like to gratefully acknowledge support from the Natural Sciences and Engineering Research Council of Canada (NSERC, <https://www.nserc-crsng.gc.ca>), the Social Sciences and Humanities Research Council of Canada (SSHRC, <https://www.sshrc-crsh.gc.ca>), and the Shared Hierarchical Academic Research Computing Network (SHARCNET, <https://www.sharcnet.ca>)."}, "rtf": {"categories": ["ReproducibleResearch"], "description": "A set of R functions to output Rich Text Format (RTF) files with high resolution tables and graphics that may be edited with a standard word processor such as Microsoft Word."}, "EValue": {"categories": ["CausalInference", "MetaAnalysis"], "description": "Conducts sensitivity analyses for unmeasured confounding, selection bias, and measurement error (individually or in combination; VanderWeele & Ding (2017) <doi:10.7326/M16-2607>; Smith & VanderWeele (2019) <doi:10.1097/EDE.0000000000001032>; VanderWeele & Li (2019) <doi:10.1093/aje/kwz133>; Smith & VanderWeele (2021) <arXiv:2005.02908>). Also conducts sensitivity analyses for unmeasured confounding in meta-analyses (Mathur & VanderWeele (2020a) <doi:10.1080/01621459.2018.1529598>; Mathur & VanderWeele (2020b) <doi:10.1097/EDE.0000000000001180>) and for additive measures of effect modification (Mathur et al., under review).  "}, "profile": {"categories": ["HighPerformanceComputing"], "description": "Defines a data structure for profiler data, and methods to read and\n    write from the 'Rprof' and 'pprof' file formats."}, "flashClust": {"categories": ["Cluster"], "description": "Fast implementation of hierarchical clustering"}, "Bolstad": {"categories": ["Bayesian", "TeachingStatistics"], "description": "A set of R functions and data sets for the book Introduction to Bayesian Statistics, Bolstad, W.M. (2017), John Wiley & Sons ISBN 978-1-118-09156-2."}, "bayesCT": {"categories": ["Bayesian", "MissingData"], "description": "Simulation and analysis of Bayesian adaptive clinical trials for binomial, Gaussian, and time-to-event data types, \n      incorporates historical data and allows early stopping for futility or early success. The package uses novel \n      and efficient Monte Carlo methods for estimating Bayesian posterior probabilities, evaluation of loss to follow up, \n      and imputation of incomplete data. The package has the functionality for dynamically incorporating historical data \n      into the analysis via the power prior or non-informative priors."}, "vasicek": {"categories": ["Distributions"], "description": "Provide a collection of miscellaneous R functions\n    related to the Vasicek distribution with the intent to make\n    the lives of risk modelers easier."}, "ltm": {"categories": ["MissingData", "Psychometrics"], "description": "Analysis of multivariate dichotomous and polytomous data using latent trait models under the Item Response Theory approach. It includes the Rasch, the Two-Parameter Logistic, the Birnbaum's Three-Parameter, the Graded Response, and the Generalized Partial Credit Models."}, "nnfor": {"categories": ["TimeSeries"], "description": "Automatic time series modelling with neural networks. \n    Allows fully automatic, semi-manual or fully manual specification of networks. For details of the\n\tspecification methodology see: (i) Crone and Kourentzes (2010) <doi:10.1016/j.neucom.2010.01.017>;\n\tand (ii) Kourentzes et al. (2014) <doi:10.1016/j.eswa.2013.12.011>."}, "mschart": {"categories": ["ReproducibleResearch"], "description": "Create native charts for 'Microsoft PowerPoint' and 'Microsoft Word' documents. \n These can then be edited and annotated. Functions are provided to let users create charts, modify \n and format their content. The chart's underlying data is automatically saved within the \n 'Word' document or 'PowerPoint' presentation. It extends package 'officer' that does \n not contain any feature for 'Microsoft' native charts production. "}, "arm": {"categories": ["Bayesian", "TeachingStatistics"], "description": "Functions to accompany A. Gelman and J. Hill, Data Analysis Using Regression and Multilevel/Hierarchical Models, Cambridge University Press, 2007."}, "sgeostat": {"categories": ["Spatial"], "description": "An Object-oriented Framework for Geostatistical Modeling in S+ \n  containing functions for variogram estimation, variogram fitting and kriging\n  as well as some plot functions. Written entirely in S, therefore works only\n  for small data sets in acceptable computing time."}, "relsurv": {"categories": ["Survival"], "description": "Contains functions for analysing relative survival data, including nonparametric estimators of net (marginal relative) survival, relative survival ratio, crude mortality, methods for fitting  and checking additive and multiplicative regression models, transformation approach, methods for dealing with population mortality tables. Work has been described in Pohar Perme, Pavlic (2018) <doi:10.18637/jss.v087.i08>."}, "terra": {"categories": ["Spatial", "SpatioTemporal"], "description": "Methods for spatial data analysis with raster and vector data. Raster methods allow for low-level data manipulation as well as high-level global, local, zonal, and focal computation. The predict and interpolate methods facilitate the use of regression type (interpolation, machine learning) models for spatial prediction, including with satellite remote sensing data. Processing of very large files is supported. See the manual and tutorials on <https://rspatial.org/terra/> to get started. 'terra' is very similar to the 'raster' package; but 'terra' can do more, is easier to use, and it is faster."}, "carData": {"categories": ["TeachingStatistics"], "description": "\n  Datasets to Accompany J. Fox and S. Weisberg, \n  An R Companion to Applied Regression, Third Edition, Sage (2019)."}, "swagger": {"categories": ["WebTechnologies"], "description": "A collection of 'HTML', 'JavaScript', and 'CSS' assets that\n  dynamically generate beautiful documentation from a 'Swagger' compliant API:\n  <https://swagger.io/specification/>."}, "gets": {"categories": ["Econometrics", "Finance"], "description": "Automated General-to-Specific (GETS) modelling of the mean and variance of a regression, and indicator saturation methods for detecting and testing for structural breaks in the mean, see Pretis, Reade and Sucarrat (2018) <doi:10.18637/jss.v086.i03>."}, "bookdown": {"categories": ["ReproducibleResearch"], "description": "Output formats and utilities for authoring books and technical documents with R Markdown."}, "riskRegression": {"categories": ["CausalInference", "Survival"], "description": "Implementation of the following methods for event history analysis.\n    Risk regression models for survival endpoints also in the presence of competing\n    risks are fitted using binomial regression based on a time sequence of binary\n    event status variables. A formula interface for the Fine-Gray regression model\n    and an interface for the combination of cause-specific Cox regression models.\n    A toolbox for assessing and comparing performance of risk predictions (risk\n    markers and risk prediction models). Prediction performance is measured by the\n    Brier score and the area under the ROC curve for binary possibly time-dependent\n    outcome. Inverse probability of censoring weighting and pseudo values are used\n    to deal with right censored data. Lists of risk markers and lists of risk models\n    are assessed simultaneously. Cross-validation repeatedly splits the data, trains\n    the risk prediction models on one part of each split and then summarizes and\n    compares the performance across splits."}, "abc": {"categories": ["Bayesian"], "description": "Implements several ABC algorithms for\n        performing parameter estimation, model selection, and goodness-of-fit.\n        Cross-validation tools are also available for measuring the\n        accuracy of ABC estimates, and to calculate the\n        misclassification probabilities of different models."}, "statebins": {"categories": ["Spatial"], "description": "The 'cartogram' heatmaps generated by the included methods \n    are an alternative to choropleth maps for the United States\n    and are based on work by the Washington Post graphics department in their report\n    on \"The states most threatened by trade\" \n    (<http://www.washingtonpost.com/wp-srv/special/business/states-most-threatened-by-trade/>).\n    \"State bins\" preserve as much of the geographic placement of the states as \n    possible but have the look and feel of a traditional heatmap. Functions are \n    provided that allow for use of a binned, discrete scale, a continuous scale \n    or manually specified colors depending on what is needed for the underlying data."}, "equate": {"categories": ["Psychometrics"], "description": "Contains methods for observed-score linking\n  and equating under the single-group, equivalent-groups,\n  and nonequivalent-groups with anchor test(s) designs.\n  Equating types include identity, mean, linear, general\n  linear, equipercentile, circle-arc, and composites of\n  these. Equating methods include synthetic, nominal\n  weights, Tucker, Levine observed score, Levine true\n  score, Braun/Holland, frequency estimation, and chained\n  equating. Plotting and summary methods, and methods for\n  multivariate presmoothing and bootstrap error estimation\n  are also provided."}, "distcrete": {"categories": ["Distributions"], "description": "Creates discretised versions of continuous \n      distribution functions by mapping continuous values \n      to an underlying discrete grid, based on a (uniform) \n      frequency of discretisation, a valid discretisation \n      point, and an integration range. For a review of \n      discretisation methods, see \n      Chakraborty (2015) <doi:10.1186/s40488-015-0028-6>."}, "rosetteApi": {"categories": ["WebTechnologies"], "description": "'Rosette' is an API for multilingual text analysis and information\n    extraction. More information can be found at <https://developer.rosette.com>."}, "cdlTools": {"categories": ["OfficialStatistics"], "description": "Downloads USDA National Agricultural Statistics Service (NASS) \n    cropscape data for a specified state. Utilities for fips, abbreviation, \n    and name conversion are also provided. Full functionality requires an \n    internet connection, but data sets can be cached for later off-line use."}, "rcdd": {"categories": ["Optimization"], "description": "R interface to (some of) cddlib\n    (<https://github.com/cddlib/cddlib>).\n    Converts back and forth between two representations of a convex polytope:\n    as solution of a set of linear equalities and inequalities and as\n    convex hull of set of points and rays.\n    Also does linear programming and redundant generator elimination\n    (for example, convex hull in n dimensions).  All functions can use exact\n    infinite-precision rational arithmetic."}, "conicfit": {"categories": ["NumericalMathematics"], "description": "Geometric circle fitting with Levenberg-Marquardt (a, b, R), Levenberg-Marquardt reduced (a, b), Landau, Spath and Chernov-Lesort. Algebraic circle fitting with Taubin, Kasa, Pratt and Fitzgibbon-Pilu-Fisher. Geometric ellipse fitting with ellipse LMG (geometric parameters) and conic LMA (algebraic parameters). Algebraic ellipse fitting with Fitzgibbon-Pilu-Fisher and Taubin."}, "irrNA": {"categories": ["MissingData"], "description": "Provides coefficients of interrater reliability that are generalized to cope with randomly incomplete (i.e. unbalanced) datasets without any imputation of missing values or any (row-wise or column-wise) omissions of actually available data. Applied to complete (balanced) datasets, these generalizations yield the same results as the common procedures, namely the Intraclass Correlation according to McGraw & Wong (1996) \\doi{10.1037/1082-989X.1.1.30} and the Coefficient of Concordance according to Kendall & Babington Smith (1939) \\doi{10.1214/aoms/1177732186}."}, "misty": {"categories": ["MissingData"], "description": "Miscellaneous functions for descriptive statistics (e.g., frequency table, cross tabulation, multilevel descriptive statistics, multilevel R-squared measures, within-group and between-group correlation matrix, various effect size measures), data management (e.g., grand-mean and group-mean centering, recode variables and reverse code items, scale and group scores, reading and writing SPSS and Excel files), missing data (e.g., descriptive statistics for missing data, missing data pattern, Little's test of Missing Completely at Random, and auxiliary variable analysis), item analysis (e.g., coefficient alpha and omega, confirmatory factor analysis), and  statistical analysis (e.g., confidence intervals, collinearity diagnostics, analysis of variance, Levene's test, t-test, z-test, sample size determination)."}, "orderly": {"categories": ["ReproducibleResearch"], "description": "Order, create and store reports from R.  By defining a\n    lightweight interface around the inputs and outputs of an\n    analysis, a lot of the repetitive work for reproducible research\n    can be automated.  We define a simple format for organising and\n    describing work that facilitates collaborative reproducible\n    research and acknowledges that all analyses are run multiple\n    times over their lifespans."}, "EpiContactTrace": {"categories": ["Epidemiology"], "description": "Routines for epidemiological contact tracing\n    and visualisation of network of contacts."}, "mvtnorm": {"categories": ["Distributions", "Finance"], "description": "Computes multivariate normal and t probabilities, quantiles,\n  random deviates and densities."}, "FastRWeb": {"categories": ["ModelDeployment", "WebTechnologies"], "description": "Infrastrcture for creating rich, dynamic web content using R scripts while maintaining very fast response time."}, "rminer": {"categories": ["MachineLearning"], "description": "Facilitates the use of data mining algorithms in classification and regression (including time series forecasting) tasks by presenting a short and coherent set of functions. Versions: 1.4.6 / 1.4.5 / 1.4.4 new automated machine learning (AutoML) and ensembles, via improved fit(), mining() and mparheuristic() functions, and new categorical preprocessing, via improved delevels() function; 1.4.3 new metrics (e.g., macro precision, explained variance), new \"lssvm\" model and improved mparheuristic() function; 1.4.2 new \"NMAE\" metric, \"xgboost\" and \"cv.glmnet\" models (16 classification and 18 regression models); 1.4.1 new tutorial and more robust version; 1.4 - new classification and regression models, with a total of 14 classification and 15 regression methods, including: Decision Trees, Neural Networks, Support Vector Machines, Random Forests, Bagging and Boosting; 1.3 and 1.3.1 - new classification and regression metrics; 1.2 - new input importance methods via improved Importance() function; 1.0 - first version."}, "tokenizers.bpe": {"categories": ["NaturalLanguageProcessing"], "description": "Unsupervised text tokenizer focused on computational efficiency. Wraps the 'YouTokenToMe' library <https://github.com/VKCOM/YouTokenToMe> which is an implementation of fast Byte Pair Encoding (BPE) <https://www.aclweb.org/anthology/P16-1162>."}, "googleVis": {"categories": ["SpatioTemporal", "WebTechnologies"], "description": "R interface to Google's chart tools, allowing users\n    to create interactive charts based on data frames. Charts\n    are displayed locally via the R HTTP help server. A modern\n    browser with an Internet connection is required. The data \n    remains local and is not uploaded to Google."}, "Cyclops": {"categories": ["Survival"], "description": "This model fitting tool incorporates cyclic coordinate descent and\n    majorization-minimization approaches to fit a variety of regression models\n    found in large-scale observational healthcare data.  Implementations focus\n    on computational optimization and fine-scale parallelization to yield\n    efficient inference in massive datasets.  Please see:\n    Suchard, Simpson, Zorych, Ryan and Madigan (2013) <doi:10.1145/2414416.2414791>."}, "partsm": {"categories": ["TimeSeries"], "description": "Basic functions to fit and predict periodic autoregressive time series models. These models are discussed in the book P.H. Franses (1996) \"Periodicity and Stochastic Trends in Economic Time Series\", Oxford University Press. Data set analyzed in that book is also provided. NOTE: the package was orphaned during several years. It is now only maintained, but no major enhancements are expected, and the maintainer cannot provide any support. "}, "PSF": {"categories": ["TimeSeries"], "description": "Pattern Sequence Based Forecasting (PSF) takes univariate\n    time series data as input and assist to forecast its future values.\n    This algorithm forecasts the behavior of time series\n    based on similarity of pattern sequences. Initially, clustering is done with the\n    labeling of samples from database. The labels associated with samples are then\n    used for forecasting the future behaviour of time series data. The further\n    technical details and references regarding PSF are discussed in Vignette."}, "TSTutorial": {"categories": ["TimeSeries"], "description": "Interactive laboratory of Time Series based in Box-Jenkins methodology."}, "limSolve": {"categories": ["Optimization"], "description": "Functions that (1) find the minimum/maximum of a linear or quadratic function:\n  min or max (f(x)), where f(x) = ||Ax-b||^2 or f(x) = sum(a_i*x_i)\n  subject to equality constraints Ex=f and/or inequality constraints Gx>=h,\n  (2) sample an underdetermined- or overdetermined system Ex=f subject to Gx>=h, and if applicable Ax~=b,      \n  (3) solve a linear system Ax=B for the unknown x. It includes banded and tridiagonal linear systems. "}, "XML2R": {"categories": ["WebTechnologies"], "description": "XML2R is a framework that reduces the effort required to transform\n    XML content into number of tables while preserving parent to child\n    relationships."}, "RMixtCompUtilities": {"categories": ["MissingData"], "description": "Mixture Composer <https://github.com/modal-inria/MixtComp> is a project to build mixture models with\n    heterogeneous data sets and partially missing data management. This package contains graphical, getter and some utility \n    functions to facilitate the analysis of 'MixtComp' output."}, "mr.raps": {"categories": ["CausalInference"], "description": "Mendelian randomization is a method of identifying and estimating a confounded causal effect using genetic instrumental variables. This packages implements methods for two-sample Mendelian randomization with summary statistics by using Robust Adjusted Profile Score (RAPS). References: Qingyuan Zhao, Jingshu Wang, Jack Bowden, Dylan S. Small. Statistical inference in two-sample summary-data Mendelian randomization using robust adjusted profile score. <arXiv:1801.09652>."}, "bayesanova": {"categories": ["Bayesian"], "description": "Provides a Bayesian version of the analysis of variance based on a three-component Gaussian mixture for which a Gibbs sampler produces posterior draws. For details about the Bayesian ANOVA based on Gaussian mixtures, see Kelter (2019) <arXiv:1906.07524>."}, "RxCEcolInf": {"categories": ["Bayesian"], "description": "Fits the R x C inference model described in Greiner and\n        Quinn (2009) <doi:10.1111/j.1467-985X.2008.00551.x> and Greiner and\n        Quinn (2010) <doi:10.1214/10-AOAS353>.\n\tAllows incorporation of survey results."}, "revdbayes": {"categories": ["Bayesian", "Distributions", "ExtremeValue"], "description": "Provides functions for the Bayesian analysis of extreme value\n    models.  The 'rust' package <https://cran.r-project.org/package=rust> is\n    used to simulate a random sample from the required posterior distribution.\n    The functionality of 'revdbayes' is similar to the 'evdbayes' package\n    <https://cran.r-project.org/package=evdbayes>, which uses Markov Chain\n    Monte Carlo ('MCMC') methods for posterior simulation.  In addition, there\n    are functions for making inferences about the extremal index, using \n    the models for threshold inter-exceedance times of Suveges and Davison \n    (2010) <doi:10.1214/09-AOAS292> and Holesovsky and Fusek (2020) \n    <doi:10.1007/s10687-020-00374-3>. Also provided are d,p,q,r functions for \n    the Generalised Extreme Value ('GEV') and Generalised Pareto ('GP') \n    distributions that deal appropriately with cases where the shape parameter \n    is very close to zero."}, "pco": {"categories": ["Econometrics"], "description": "Computation of the Pedroni (1999) panel cointegration test statistics.  Reported are the empirical and the standardized values. "}, "reclin": {"categories": ["OfficialStatistics"], "description": "Functions to assist in performing probabilistic record linkage and\n    deduplication: generating pairs, comparing records, em-algorithm for\n    estimating m- and u-probabilities, forcing one-to-one matching. Can also be\n    used for pre- and post-processing for machine learning methods for record\n    linkage."}, "RadData": {"categories": ["ChemPhys"], "description": "Nuclear Decay Data for Dosimetric Calculations from the \n    International Commission on Radiological Protection from ICRP \n    Publication 107. Ann. ICRP 38 (3). Eckerman, Keith and Endo, Akira 2008 \n    <doi:10.1016/j.icrp.2008.10.004> \n    <https://www.icrp.org/publication.asp?id=ICRP%20Publication%20107>. \n    This is a database of the physical data needed in calculations of \n    radionuclide-specific protection and operational quantities. The \n    data is prescribed by the ICRP, the international authority on \n    radiation dose standards, for estimating dose from the intake of or \n    exposure to radionuclides in the workplace and the environment. \n    The database contains information on the half-lives, decay chains, \n    and yields and energies of radiations emitted in nuclear transformations \n    of 1252 radionuclides of 97 elements. "}, "kitagawa": {"categories": ["Hydrology"], "description": "Provides tools to calculate the theoretical hydrodynamic response\n    of an aquifer undergoing harmonic straining or pressurization, or analyze\n    measured responses. There are\n    two classes of models here, designed for use with confined\n    aquifers: (1) for sealed wells, based on the model of \n    Kitagawa et al (2011, <doi:10.1029/2010JB007794>), \n    and (2) for open wells, based on the models of\n    Cooper et al (1965, <doi:10.1029/JZ070i016p03915>), \n    Hsieh et al (1987, <doi:10.1029/WR023i010p01824>), \n    Rojstaczer (1988, <doi:10.1029/JB093iB11p13619>), \n    Liu et al (1989, <doi:10.1029/JB094iB07p09453>), and\n    Wang et al (2018, <doi:10.1029/2018WR022793>). Wang's \n    solution is a special exception which\n    allows for leakage out of the aquifer \n    (semi-confined); it is equivalent to Hsieh's model\n    when there is no leakage (the confined case).\n    These models treat strain (or aquifer head) as an input to the\n    physical system, and fluid-pressure (or water height) as the output. The\n    applicable frequency band of these models is characteristic of seismic\n    waves, atmospheric pressure fluctuations, and solid earth tides."}, "xgboost": {"categories": ["HighPerformanceComputing", "MachineLearning", "ModelDeployment", "Survival"], "description": "Extreme Gradient Boosting, which is an efficient implementation\n    of the gradient boosting framework from Chen & Guestrin (2016) <doi:10.1145/2939672.2939785>.\n    This package is its R interface. The package includes efficient linear\n    model solver and tree learning algorithms. The package can automatically\n    do parallel computation on a single machine which could be more than 10\n    times faster than existing gradient boosting packages. It supports\n    various objective functions, including regression, classification and ranking.\n    The package is made to be extensible, so that users are also allowed to define\n    their own objectives easily."}, "lcopula": {"categories": ["Distributions"], "description": "Collections of functions allowing random number generations and\n    estimation of 'Liouville' copulas, as described in Belzile and Neslehova (2017) <doi:10.1016/j.jmva.2017.05.008>."}, "MAMS": {"categories": ["ExperimentalDesign"], "description": "Designing multi-arm multi-stage studies with (asymptotically) normal endpoints and known variance."}, "gifti": {"categories": ["MedicalImaging"], "description": "Functions to read in the geometry format under the \n    'Neuroimaging' 'Informatics' Technology Initiative ('NIfTI'), called \n    'GIFTI' <https://www.nitrc.org/projects/gifti/>. \n    These files contain surfaces of brain imaging data."}, "samplingbook": {"categories": ["OfficialStatistics"], "description": "Sampling procedures from the book 'Stichproben - Methoden und praktische Umsetzung mit R' by Goeran Kauermann and Helmut Kuechenhoff (2010)."}, "monobin": {"categories": ["Finance"], "description": "Performs monotonic binning of numeric risk factor in credit rating models (PD, LGD, EAD) \n\tdevelopment. All functions handle both binary and continuous target variable. \n\tFunctions that use isotonic regression in the first stage of binning process have an additional \n\tfeature for correction of minimum percentage of observations and minimum target rate per bin. \t\n\tAdditionally, monotonic trend can be identified based on raw data or, if known in advance,\n\tforced by functions' argument. Missing values and other possible special values are treated \n\tseparately from so-called complete cases."}, "emplik2": {"categories": ["Survival"], "description": "Calculates the p-value for a mean-type hypothesis (or multiple mean-type hypotheses) based on \n        two samples with possible censored data."}, "etrm": {"categories": ["Finance"], "description": "Provides a collection of functions to perform core tasks within\n    Energy Trading and Risk Management (ETRM). Calculation of maximum smoothness \n    forward price curves for electricity and natural gas contracts with flow delivery, as presented in\n    F. E. Benth, S. Koekebakker, and F. Ollmar (2007) <doi:10.3905/jod.2007.694791>\n    and F. E. Benth,  J. S. Benth,  and S. Koekebakker (2008) <doi:10.1142/6811>.\n    Portfolio insurance trading strategies for price risk management in the forward market, see\n    F. Black (1976) <doi:10.1016/0304-405X(76)90024-6>, \n    T. Bjork (2009) <https://EconPapers.repec.org/RePEc:oxp:obooks:9780199574742>,  \n    F. Black and R. W. Jones (1987) <doi:10.3905/jpm.1987.409131> and\n    H. E. Leland (1980) <http://www.jstor.org/stable/2327419>."}, "cocorresp": {"categories": ["Environmetrics", "Psychometrics"], "description": "Fits predictive and symmetric co-correspondence analysis (CoCA) models to relate one data matrix to another data matrix. More specifically, CoCA maximises the weighted covariance between the weighted averaged species scores of one community and the weighted averaged species scores of another community. CoCA attempts to find patterns that are common to both communities."}, "tsrobprep": {"categories": ["MissingData", "TimeSeries"], "description": "Methods for handling the missing values outliers are introduced in\n    this package. The recognized missing values and outliers are replaced \n    using a model-based approach. The model may consist of both autoregressive\n    components and external regressors. The methods work robust and efficient,\n    and they are fully tunable. The primary motivation for writing the package\n    was preprocessing of the energy systems data, e.g. power plant production\n    time series, but the package could be used with any time series data. For \n    details, see Narajewski et al. (2021) <doi:10.1016/j.softx.2021.100809>."}, "parallelly": {"categories": ["HighPerformanceComputing"], "description": "Utility functions that enhance the 'parallel' package and support the built-in parallel backends of the 'future' package.  For example, availableCores() gives the number of CPU cores available to your R process as given by the operating system, 'cgroups' and Linux containers, R options, and environment variables, including those set by job schedulers on high-performance compute clusters. If none is set, it will fall back to parallel::detectCores(). Another example is makeClusterPSOCK(), which is backward compatible with parallel::makePSOCKcluster() while doing a better job in setting up remote cluster workers without the need for configuring the firewall to do port-forwarding to your local computer."}, "distrEx": {"categories": ["Distributions"], "description": "Extends package 'distr' by functionals, distances, and conditional distributions."}, "mitools": {"categories": ["MissingData"], "description": "Tools to perform analyses and combine results from\n        multiple-imputation datasets."}, "ROptEst": {"categories": ["Robust"], "description": "Optimally robust estimation in general smoothly parameterized models using S4\n            classes and methods."}, "sparseMVN": {"categories": ["Distributions"], "description": "Computes multivariate normal (MVN) densities, and\n    samples from MVN distributions, when the covariance or\n    precision matrix is sparse."}, "did": {"categories": ["CausalInference"], "description": "The standard Difference-in-Differences (DID) setup involves two periods and two groups \u2013 a treated group and untreated group.  Many applications of DID methods involve more than two periods and have individuals that are treated at different points in time.  This package contains tools for computing average treatment effect parameters in Difference in Differences setups with more than two periods and with variation in treatment timing using the methods developed in Callaway and Sant'Anna (2021) <doi:10.1016/j.jeconom.2020.12.001>.  The main parameters are group-time average treatment effects which are the average treatment effect for a particular group at a a particular time.  These can be aggregated into a fewer number of treatment effect parameters, and the package deals with the cases where there is selective treatment timing, dynamic treatment effects, calendar time effects, or combinations of these.  There are also functions for testing the Difference in Differences assumption, and plotting group-time average treatment effects."}, "ipumsr": {"categories": ["OfficialStatistics"], "description": "An easy way to import census, survey and geographic data provided by 'IPUMS'\n    into R plus tools to help use the associated metadata to make analysis easier. 'IPUMS'\n    data describing 1.4 billion individuals drawn from over 750 censuses and surveys is\n    available free of charge from our website <https://www.ipums.org>."}, "tractor.base": {"categories": ["MedicalImaging"], "description": "Functions for working with magnetic resonance images. Reading and\n    writing of popular file formats (DICOM, Analyze, NIfTI-1, NIfTI-2, MGH);\n    interactive and non-interactive visualisation; flexible image manipulation;\n    metadata and sparse image handling."}, "robumeta": {"categories": ["MetaAnalysis", "Robust"], "description": "Functions for conducting robust variance estimation (RVE) meta-regression using both large and small sample RVE estimators under various weighting schemes. These methods are distribution free and provide valid point estimates, standard errors and hypothesis tests even when the degree and structure of dependence between effect sizes is unknown. Also included are functions for conducting sensitivity analyses under correlated effects weighting and producing RVE-based forest plots. "}, "theft": {"categories": ["TimeSeries"], "description": "Consolidates and calculates different sets of time-series features from multiple\n    'R' and 'Python' packages including 'Rcatch22' Henderson, T. (2021) <doi:10.5281/zenodo.5546815>,\n    'feasts' O'Hara-Wild, M., Hyndman, R., and Wang, E. (2021) <https://CRAN.R-project.org/package=feasts>,\n    'tsfeatures' Hyndman, R., Kang, Y., Montero-Manso, P., Talagala, T., Wang, E., Yang, Y., and O'Hara-Wild, M. (2020)\n    <https://CRAN.R-project.org/package=tsfeatures>, 'tsfresh' Christ, M., Braun, N., Neuffer, J.,\n    and Kempa-Liehr A.W. (2018) <doi:10.1016/j.neucom.2018.03.067>, 'TSFEL' Barandas, M., et al. (2020)\n    <doi:10.1016/j.softx.2020.100456>, and 'Kats' Facebook Infrastructure Data Science (2021)\n    <https://facebookresearch.github.io/Kats/>. Provides a standardised workflow from feature calculation to\n    feature processing, machine learning classification procedures, and the production of statistical graphics."}, "crmPack": {"categories": ["ClinicalTrials", "ExperimentalDesign"], "description": "Implements a wide range of model-based dose\n    escalation designs, ranging from classical and modern continual\n    reassessment methods (CRMs) based on dose-limiting toxicity endpoints to\n    dual-endpoint designs taking into account a biomarker/efficacy outcome. The\n    focus is on Bayesian inference, making it very easy to setup a new design\n    with its own JAGS code. However, it is also possible to implement 3+3\n    designs for comparison or models with non-Bayesian estimation. The whole\n    package is written in a modular form in the S4 class system, making it very\n    flexible for adaptation to new models, escalation or stopping rules."}, "pmultinom": {"categories": ["Distributions"], "description": "Implements multinomial CDF (P(N1<=n1, ..., Nk<=nk)) and tail probabilities (P(N1>n1, ..., Nk>nk)), as well as probabilities with both constraints (P(l1<N1<=u1, ..., lk<Nk<=uk)). Uses a method suggested by Bruce Levin (1981) <doi:10.1214/aos/1176345593>."}, "stinepack": {"categories": ["NumericalMathematics"], "description": "A consistently well behaved method of interpolation based on piecewise rational functions using Stineman's algorithm."}, "tvgeom": {"categories": ["Distributions"], "description": "Probability mass (d), distribution (p), quantile (q), and random \n    number generating (r and rt) functions for the time-varying right-truncated \n    geometric (tvgeom) distribution. Also provided are functions to calculate the first \n    and second central moments of the distribution. The tvgeom distribution\n    is similar to the geometric distribution, but the probability \n    of success is allowed to vary at each time step, and there are a limited \n    number of trials. This distribution is essentially a Markov chain, and it\n    is useful for modeling Markov chain systems with a set \n    number of time steps."}, "vcd": {"categories": ["TeachingStatistics"], "description": "Visualization techniques, data sets, summary and inference\n        procedures aimed particularly at categorical data. Special\n        emphasis is given to highly extensible grid graphics. The\n        package was package was originally inspired by the book \n\t\"Visualizing Categorical Data\" by Michael Friendly and is \n\tnow the main support package for a new book, \n\t\"Discrete Data Analysis with R\" by Michael Friendly and \n\tDavid Meyer (2015)."}, "fingerprint": {"categories": ["ChemPhys"], "description": "Functions to manipulate binary fingerprints\n of arbitrary length. A fingerprint is represented by an object of S4 class 'fingerprint'\n which is internally represented a vector of integers, such\n that each element represents the position in the fingerprint that is set to 1.\n The bitwise logical functions in R are overridden so that they can be used directly\n with 'fingerprint' objects. A number of distance metrics are also\n available (many contributed by Michael Fadock). Fingerprints \n can be converted to Euclidean vectors (i.e., points on the unit hypersphere) and\n can also be folded using OR.  Arbitrary fingerprint formats can be handled via line\n handlers. Currently handlers are provided for CDK, MOE and BCI fingerprint data."}, "covid19us": {"categories": ["Epidemiology"], "description": "A wrapper around the 'COVID Tracking Project API'\n    <https://covidtracking.com/api/> providing data on cases of COVID-19\n    in the US."}, "svd": {"categories": ["NumericalMathematics"], "description": "R bindings to SVD and eigensolvers (PROPACK, nuTRLan)."}, "rlme": {"categories": ["Robust"], "description": "Estimates robust rank-based fixed effects and predicts robust\n    random effects in two- and three- level random effects nested models.\n    The methodology is described in Bilgic & Susmann (2013) <https://journal.r-project.org/archive/2013/RJ-2013-027/>."}, "clusterRepro": {"categories": ["Cluster"], "description": "This is a function for validating microarray clusters via reproducibility,\n based on the paper referenced below."}, "EpiCurve": {"categories": ["Epidemiology"], "description": "Creates simple or stacked epidemic curves for hourly, daily, weekly or monthly outcome data."}, "rioja": {"categories": ["Environmetrics"], "description": "Constrained clustering, transfer functions, and other methods for analysing Quaternary science data."}, "fGarch": {"categories": ["Finance", "TimeSeries"], "description": "Analyze and model heteroskedastic behavior in financial time series."}, "NMOF": {"categories": ["Finance", "Optimization", "ReproducibleResearch"], "description": "Functions, examples and data from the first and\n  the second edition of \"Numerical Methods and Optimization\n  in Finance\" by M. Gilli, D. Maringer and E. Schumann\n  (2019, ISBN:978-0128150658).  The package provides\n  implementations of optimisation heuristics (Differential\n  Evolution, Genetic Algorithms, Particle Swarm\n  Optimisation, Simulated Annealing and Threshold\n  Accepting), and other optimisation tools, such as grid\n  search and greedy search.  There are also functions for\n  the valuation of financial instruments such as bonds and\n  options, for portfolio selection and functions that help\n  with stochastic simulations."}, "deepnet": {"categories": ["MachineLearning"], "description": "Implement some deep learning architectures and neural network\n    algorithms, including BP,RBM,DBN,Deep autoencoder and so on."}, "bgmm": {"categories": ["Cluster"], "description": "Two partially supervised mixture modeling methods: \n        soft-label and belief-based modeling are implemented.\n        For completeness, we equipped the package also with the\n        functionality of unsupervised, semi- and fully supervised\n        mixture modeling.  The package can be applied also to selection\n        of the best-fitting from a set of models with different\n        component numbers or constraints on their structures.\n        For detailed introduction see:\n        Przemyslaw Biecek, Ewa Szczurek, Martin Vingron, Jerzy\n        Tiuryn (2012), The R Package bgmm: Mixture Modeling with\n        Uncertain Knowledge, Journal of Statistical Software \n        <doi:10.18637/jss.v047.i03>."}, "uncmbb": {"categories": ["SportsAnalytics"], "description": "Dataset contains select attributes for each match result since 1949-1950 season for UNC men's basketball team."}, "crul": {"categories": ["WebTechnologies"], "description": "A simple HTTP client, with tools for making HTTP requests,\n    and mocking HTTP requests. The package is built on R6, and takes\n    inspiration from Ruby's 'faraday' gem (<https://rubygems.org/gems/faraday>).\n    The package name is a play on curl, the widely used command line tool\n    for HTTP, and this package is built on top of the R package 'curl', an\n    interface to 'libcurl' (<https://curl.se/libcurl/>)."}, "multiway": {"categories": ["Psychometrics"], "description": "Fits multi-way component models via alternating least squares algorithms with optional constraints. Fit models include N-way Canonical Polyadic Decomposition, Individual Differences Scaling, Multiway Covariates Regression, Parallel Factor Analysis (1 and 2), Simultaneous Component Analysis, and Tucker Factor Analysis."}, "represtools": {"categories": ["ReproducibleResearch"], "description": "Reproducible research tools automates the creation of an analysis directory structure and work flow. There are R markdown\n  skeletons which encapsulate typical analytic work flow steps. Functions will create appropriate modules which may\n  pass data from one step to another."}, "frostr": {"categories": ["Hydrology"], "description": "An R API to MET Norway's 'Frost' API <https://frost.met.no/index.html> \n    to retrieve data as data frames. The 'Frost' API, and the underlying data, is \n    made available by the Norwegian Meteorological Institute (MET Norway). The data\n    and products are distributed under the \n    Norwegian License for Open Data 2.0 (NLOD) <https://data.norge.no/nlod/en/2.0>\n    and Creative Commons 4.0 <https://creativecommons.org/licenses/by/4.0/>."}, "SortedEffects": {"categories": ["CausalInference"], "description": "Implements the estimation and inference methods for sorted causal effects and \n    classification analysis as in Chernozhukov, Fernandez-Val and Luo (2018) <doi:10.3982/ECTA14415>."}, "MOCCA": {"categories": ["Cluster"], "description": "Provides methods to analyze cluster alternatives based on multi-objective optimization of cluster validation indices. For details see Kraus et al. (2011) <doi:10.1007/s00180-011-0244-6>."}, "clusterSim": {"categories": ["Cluster"], "description": "Distance measures (GDM1, GDM2,\tSokal-Michener, Bray-Curtis, for symbolic interval-valued data), cluster quality indices (Calinski-Harabasz, Baker-Hubert, Hubert-Levine, Silhouette, Krzanowski-Lai, Hartigan, Gap,\tDavies-Bouldin),\tdata normalization formulas (metric data, interval-valued symbolic data), data generation (typical and non-typical data), HINoV method,\treplication analysis, linear ordering methods, spectral clustering, agreement indices between two partitions, plot functions (for categorical and symbolic interval-valued data). \n (MILLIGAN, G.W., COOPER, M.C. (1985) <doi:10.1007/BF02294245>, \n HUBERT, L., ARABIE, P. (1985) <doi:10.1007%2FBF01908075>, \n RAND, W.M. (1971) <doi:10.1080/01621459.1971.10482356>, \n JAJUGA, K., WALESIAK, M. (2000) <doi:10.1007/978-3-642-57280-7_11>, \n MILLIGAN, G.W., COOPER, M.C. (1988) <doi:10.1007/BF01897163>, \n JAJUGA, K., WALESIAK, M., BAK, A. (2003) <doi:10.1007/978-3-642-55721-7_12>, \n DAVIES, D.L., BOULDIN, D.W. (1979) <doi:10.1109/TPAMI.1979.4766909>, \n CALINSKI, T., HARABASZ, J. (1974) <doi:10.1080/03610927408827101>,\n HUBERT, L. (1974) <doi:10.1080/01621459.1974.10480191>, \n TIBSHIRANI, R., WALTHER, G., HASTIE, T. (2001) <doi:10.1111/1467-9868.00293>, \n BRECKENRIDGE, J.N. (2000) <doi:10.1207/S15327906MBR3502_5>, \n WALESIAK, M., DUDEK, A. (2008) <doi:10.1007/978-3-540-78246-9_11>)."}, "tsbox": {"categories": ["TimeSeries"], "description": "Time series toolkit with identical behavior for all\n  time series classes: 'ts','xts', 'data.frame', 'data.table', 'tibble', 'zoo',\n  'timeSeries', 'tsibble', 'tis' or 'irts'. Also converts reliably between these classes."}, "ROptSpace": {"categories": ["MissingData"], "description": "Matrix reconstruction, also known as matrix completion, is the task of inferring missing entries of a partially observed matrix. This package provides a method called OptSpace, which was proposed by Keshavan, R.H., Oh, S., and Montanari, A. (2009) <doi:10.1109/ISIT.2009.5205567> for a case under low-rank assumption."}, "psidR": {"categories": ["Econometrics"], "description": "Makes it easy to build panel data in wide format from Panel Survey\n    of Income Dynamics ('PSID') delivered raw data. Downloads data directly from\n    the PSID server using the 'SAScii' package. 'psidR' takes care of merging\n    data from each wave onto a cross-period index file, so that individuals can be\n    followed over time. The user must specify which years they are interested in,\n    and the 'PSID' variable names (e.g. ER21003) for each year (they differ in each\n    year). The package offers helper functions to retrieve variable names from different\n    waves. There are different panel data designs and sample subsetting criteria\n    implemented (\"SRC\", \"SEO\", \"immigrant\" and \"latino\" samples)."}, "gdalcubes": {"categories": ["Spatial"], "description": "Processing collections of Earth observation images as on-demand multispectral, multitemporal raster data cubes. Users\n    define cubes by spatiotemporal extent, resolution, and spatial reference system and let 'gdalcubes' automatically apply cropping, reprojection, and \n    resampling using the 'Geospatial Data Abstraction Library' ('GDAL'). Implemented functions on data cubes include reduction over space and time, \n    applying arithmetic expressions on pixel band values, moving window aggregates over time, filtering by space, time, bands, and predicates on pixel values, \n    exporting data cubes as 'netCDF' or 'GeoTIFF' files, plotting, and extraction from spatial and or spatiotemporal features.  \n    All computational parts are implemented in C++, linking to the 'GDAL', 'netCDF', 'CURL', and 'SQLite' libraries. \n    See Appel and Pebesma (2019) <doi:10.3390/data4030092> for further details."}, "diagram": {"categories": ["GraphicalModels"], "description": "Visualises simple graphs (networks) based on a transition matrix, utilities to plot flow diagrams, \n     visualising webs, electrical networks, etc.\n     Support for the book \"A practical guide to ecological modelling -\n     using R as a simulation platform\"\n     by Karline Soetaert and Peter M.J. Herman (2009), Springer.\n     and the book \"Solving Differential Equations in R\"\n     by Karline Soetaert, Jeff Cash and Francesca Mazzia (2012), Springer.\n     Includes demo(flowchart), demo(plotmat), demo(plotweb)."}, "mcmcse": {"categories": ["Bayesian"], "description": "Provides tools for computing Monte Carlo standard\n        errors (MCSE) in Markov chain Monte Carlo (MCMC) settings. MCSE\n        computation for expectation and quantile estimators is\n        supported as well as multivariate estimations. The package also provides \n\tfunctions for computing effective sample size and for plotting\n\tMonte Carlo estimates versus sample size."}, "regmedint": {"categories": ["CausalInference"], "description": "This is an extension of the regression-based causal mediation analysis first proposed by Valeri and VanderWeele (2013) <doi:10.1037/a0031034> and Valeri and VanderWeele (2015) <doi:10.1097/EDE.0000000000000253>). It supports including effect measure modification by covariates(treatment-covariate and mediator-covariate product terms in mediator and outcome regression models). It also accommodates the original 'SAS' macro and 'PROC CAUSALMED' procedure in 'SAS' when there is no effect measure modification. Linear and logistic models are supported for the mediator model. Linear, logistic, loglinear, Poisson, negative binomial, Cox, and accelerated failure time (exponential and Weibull) models are supported for the outcome model."}, "Renext": {"categories": ["Distributions", "ExtremeValue"], "description": "Peaks Over Threshold (POT) or 'methode du renouvellement'. The distribution for the exceedances can be chosen, and heterogeneous data (including historical data or block data) can be used in a Maximum-Likelihood framework. "}, "baseballr": {"categories": ["SportsAnalytics"], "description": "Provides numerous utilities for acquiring and analyzing\n    baseball data from online sources such as Baseball Reference <https://www.baseball-reference.com/>,\n    FanGraphs <https://www.fangraphs.com/>, and the MLB Stats API <https://www.mlb.com/>."}, "zipfextR": {"categories": ["Distributions"], "description": "Implementation of four extensions of the Zipf distribution: the Marshall-Olkin \n  Extended Zipf (MOEZipf) P\u00e9rez-Casany, M., & Casellas, A. (2013) <arXiv:1304.4540>, the Zipf-Poisson Extreme (Zipf-PE), the \n  Zipf-Poisson Stopped Sum (Zipf-PSS) and the Zipf-Polylog distributions. \n  In log-log scale, the two first extensions allow for top-concavity \n  and top-convexity while the third one only allows for top-concavity. \n  All the extensions maintain the linearity associated with the Zipf model in the tail."}, "CDM": {"categories": ["Psychometrics"], "description": "\n    Functions for cognitive diagnosis modeling and multidimensional item response modeling \n    for dichotomous and polytomous item responses. This package enables the estimation of \n    the DINA and DINO model (Junker & Sijtsma, 2001, <doi:10.1177/01466210122032064>),\n    the multiple group (polytomous) GDINA model (de la Torre, 2011, \n    <doi:10.1007/s11336-011-9207-7>), the multiple choice DINA model (de la Torre, 2009, \n    <doi:10.1177/0146621608320523>), the general diagnostic model (GDM; von Davier, 2008, \n    <doi:10.1348/000711007X193957>), the structured latent class model (SLCA; Formann, 1992, \n    <doi:10.1080/01621459.1992.10475229>) and regularized latent class analysis \n    (Chen, Li, Liu, & Ying, 2017, <doi:10.1007/s11336-016-9545-6>). \n    See George, Robitzsch, Kiefer, Gross, and Uenlue (2017) <doi:10.18637/jss.v074.i02> \n    or Robitzsch and George (2019, <doi:10.1007/978-3-030-05584-4_26>)     \n    for further details on estimation and the package structure.\n    For tutorials on how to use the CDM package see \n    George and Robitzsch (2015, <doi:10.20982/tqmp.11.3.p189>) as well as\n    Ravand and Robitzsch (2015)."}, "cvar": {"categories": ["Finance"], "description": "Compute expected shortfall (ES) and Value at Risk (VaR) from a\n    quantile function, distribution function, random number generator or\n    probability density function.  ES is also known as Conditional Value at\n    Risk (CVaR). Virtually any continuous distribution can be specified.\n    The functions are vectorized over the arguments. The computations are\n    done directly from the definitions, see e.g. Acerbi and Tasche (2002)\n    <doi:10.1111/1468-0300.00091>. Some support for GARCH models is provided,\n    as well."}, "stationaRy": {"categories": ["Hydrology"], "description": "Acquire hourly meteorological data from stations located all over\n    the world. There is a wealth of data available, with historic weather data\n    accessible from nearly 30,000 stations. The available data is automatically\n    downloaded from a data repository and processed into a 'tibble' for the\n    exact range of years requested. A relative humidity approximation is\n    provided using the 'August-Roche-Magnus' formula, which was adapted from\n    Alduchov and Eskridge (1996) <doi:10.1175%2F1520-0450%281996%29035%3C0601%3AIMFAOS%3E2.0.CO%3B2>."}, "rBayesianOptimization": {"categories": ["Optimization"], "description": "A Pure R implementation of Bayesian Global Optimization with Gaussian Processes."}, "random": {"categories": ["Distributions"], "description": "The true random number service provided by the RANDOM.ORG\n website created by Mads Haahr samples atmospheric noise via radio tuned to\n an unused broadcasting frequency together with a skew correction algorithm\n due to John von Neumann.  More background is available in the included\n vignette based on an essay by Mads Haahr.  In its current form, the package\n offers functions to retrieve random integers, randomized sequences and\n random strings."}, "GroupSeq": {"categories": ["ClinicalTrials", "ExperimentalDesign"], "description": "Computes probabilities related to group sequential designs for\n    normally distributed test statistics. Enables to derive critical\n    boundaries, power, drift, and confidence intervals of such designs.\n    Supports the alpha spending approach by Lan-DeMets."}, "echor": {"categories": ["Hydrology"], "description": "An R interface to United States Environmental \n    Protection Agency (EPA) Environmental Compliance \n    History Online ('ECHO') Application Program Interface\n    (API). 'ECHO' provides information about EPA permitted \n    facilities, discharges, and other reporting info \n    associated with permitted entities. Data are obtained \n    from <https://echo.epa.gov/>. "}, "bmgarch": {"categories": ["Finance"], "description": "Fit Bayesian multivariate GARCH models using 'Stan' for full Bayesian inference. Generate (weighted) forecasts for means, variances (volatility) and correlations. Currently DCC(P,Q), CCC(P,Q), pdBEKK(P,Q), and BEKK(P,Q) parameterizations are implemented, based either on a multivariate gaussian normal or student-t distribution. DCC and CCC models are based on Engle (2002) <doi:10.1198/073500102288618487> and Bollerslev (1990). The BEKK parameterization follows Engle and Kroner (1995) <doi:10.1017/S0266466600009063> while the pdBEKK as well as the estimation approach for this package is described in Rast et al. (2020) <doi:10.31234/osf.io/j57pk>. The fitted models contain 'rstan' objects and can be examined with 'rstan' functions.  "}, "cfdecomp": {"categories": ["CausalInference"], "description": "Provides a set of functions for counterfactual decomposition (cfdecomp). The functions available in this package decompose differences in an outcome attributable to a mediating variable (or sets of mediating variables) between groups based on counterfactual (causal inference) theory. By using Monte Carlo (MC) integration (simulations based on empirical estimates from multivariable models) we provide added flexibility compared to existing (analytical) approaches, at the cost of computational power or time. The added flexibility means that we can decompose difference between groups in any outcome or and with any mediator (any variable type and distribution). See Sudharsanan & Bijlsma (2019) <doi:10.4054/MPIDR-WP-2019-004> for more information."}, "geonames": {"categories": ["Spatial"], "description": "The web service at <https://www.geonames.org/> provides a number of spatial data queries, including \n    administrative area hierarchies, city locations and some country postal code queries. A (free) username\n    is required and rate limits exist."}, "webshot": {"categories": ["WebTechnologies"], "description": "Takes screenshots of web pages, including Shiny applications and R\n    Markdown documents."}, "CaDENCE": {"categories": ["Distributions"], "description": "Parameters of a user-specified probability distribution are modelled by a multi-layer perceptron artificial neural network. This framework can be used to implement probabilistic nonlinear models including mixture density networks, heteroscedastic regression models, zero-inflated models, etc. following Cannon (2012) <doi:10.1016/j.cageo.2011.08.023>."}, "MinEDfind": {"categories": ["ClinicalTrials"], "description": "The nonparametric two-stage Bayesian adaptive design is a novel phase II clinical trial design for finding the minimum effective dose (MinED). This design is motivated by the top priority and concern of clinicians when testing a new drug, which is to effectively treat patients and minimize the chance of exposing them to subtherapeutic or overly toxic doses. It is used to design single-agent trials. "}, "rpdo": {"categories": ["Hydrology"], "description": "Monthly Pacific Decadal Oscillation (PDO) index\n    values from January 1900 to September 2018. \n    Superseded by 'rsoi' package which includes the historical and \n    most recent monthly PDO index values together with related climate indices."}, "RSAlgaeR": {"categories": ["Hydrology"], "description": "Assists in processing reflectance data, developing empirical models using stepwise regression and a generalized linear modeling approach, cross-\n    validation, and analysis of trends in water quality conditions (specifically chl-a) and climate conditions using the Theil-Sen estimator."}, "onion": {"categories": ["NumericalMathematics"], "description": "\n  Quaternions and Octonions are four- and eight- dimensional\n  extensions of the complex numbers.  They are normed division\n  algebras over the real numbers and find applications in spatial\n  rotations (quaternions), and string theory and relativity\n  (octonions).  The quaternions are noncommutative and the octonions\n  nonassociative.  See the package vignette for more details."}, "BigVAR": {"categories": ["TimeSeries"], "description": "Estimates VAR and VARX models with Structured Penalties using the methods developed by Nicholson et al (2017)<doi:10.1016/j.ijforecast.2017.01.003> and Nicholson et al (2020) <doi:10.48550/arXiv.1412.5250>."}, "profoc": {"categories": ["TimeSeries"], "description": "Combine probabilistic forecasts using CRPS learning algorithms proposed in Berrisch, Ziel (2021) <arXiv:2102.00968> <doi:10.1016/j.jeconom.2021.11.008>. The package implements multiple online learning algorithms like Bernstein online aggregation; see Wintenberger (2014) <arXiv:1404.1356>. Quantile regression is also implemented for comparison purposes. Model parameters can be tuned automatically with respect to the loss of the forecast combination. Methods like predict(), update(), plot() and print() are available for convenience. This package utilizes the optim C++ library for numeric optimization <https://github.com/kthohr/optim>."}, "BACCO": {"categories": ["Bayesian"], "description": "The BACCO bundle of packages is replaced by the BACCO\n package, which provides a vignette that illustrates the constituent\n packages (emulator, approximator, calibrator) in use."}, "biwavelet": {"categories": ["TimeSeries"], "description": "This is a port of the WTC MATLAB package written by Aslak Grinsted\n    and the wavelet program written by Christopher Torrence and Gibert P.\n    Compo. This package can be used to perform univariate and bivariate\n    (cross-wavelet, wavelet coherence, wavelet clustering) analyses."}, "optimsimplex": {"categories": ["Optimization"], "description": "Provides a building block for optimization algorithms\n        based on a simplex. The 'optimsimplex' package may be used in the\n        following optimization methods: the simplex method of Spendley\n        et al. (1962) <doi:10.1080/00401706.1962.10490033>, the method of \n        Nelder and Mead (1965) <doi:10.1093/comjnl/7.4.308>, Box's algorithm for\n        constrained optimization (1965) <doi:10.1093/comjnl/8.1.42>, the \n        multi-dimensional search by Torczon (1989) \n        <https://www.cs.wm.edu/~va/research/thesis.pdf>, etc..."}, "BayesCTDesign": {"categories": ["Bayesian"], "description": "A set of functions to help clinical trial researchers calculate power and sample size for two-arm Bayesian randomized clinical trials that do or do not incorporate historical control data.  At some point during the design process, a clinical trial researcher who is designing a basic two-arm Bayesian randomized clinical trial needs to make decisions about power and sample size within the context of hypothesized treatment effects.  Through simulation, the simple_sim() function will estimate power and other user specified clinical trial characteristics at user specified sample sizes given user defined scenarios about treatment effect,control group characteristics, and outcome.  If the clinical trial researcher has access to historical control data, then the researcher can design a two-arm Bayesian randomized clinical trial that incorporates the historical data.  In such a case, the researcher needs to work through the potential consequences of historical and randomized control differences on trial characteristics, in addition to working through issues regarding power in the context of sample size, treatment effect size, and outcome.  If a researcher designs a clinical trial that will incorporate historical control data, the researcher needs the randomized controls to be from the same population as the historical controls.  What if this is not the case when the designed trial is implemented?  During the design phase, the researcher needs to investigate the negative effects of possible historic/randomized control differences on power, type one error, and other trial characteristics.  Using this information, the researcher should design the trial to mitigate these negative effects.  Through simulation, the historic_sim() function will estimate power and other user specified clinical trial characteristics at user specified sample sizes given user defined scenarios about historical and randomized control differences as well as treatment effects and outcomes.  The results from historic_sim() and simple_sim() can be printed with print_table() and graphed with plot_table() methods.  Outcomes considered are Gaussian, Poisson, Bernoulli, Lognormal, Weibull, and Piecewise Exponential.  The methods are described in Eggleston et al. (2021) <doi:10.18637/jss.v100.i21>.  "}, "mfx": {"categories": ["Econometrics"], "description": "Estimates probit, logit, Poisson, negative binomial, \n  and beta regression models, returning their marginal effects, odds ratios, \n  or incidence rate ratios as an output.\n  Greene (2008, pp. 780-7) provides a textbook introduction to this topic."}, "HI": {"categories": ["Bayesian", "Distributions"], "description": "Simulation from distributions supported by nested\n        hyperplanes, using the algorithm described in Petris &\n        Tardella, \"A geometric approach to transdimensional Markov\n        chain Monte Carlo\", Canadian Journal of Statistics, v.31, n.4,\n        (2003).  Also random direction multivariate Adaptive Rejection\n        Metropolis Sampling."}, "textplot": {"categories": ["NaturalLanguageProcessing"], "description": "Visualise complex relations in texts. This is done by providing functionalities for displaying \n    text co-occurrence networks, text correlation networks, dependency relationships as well as text clustering and semantic text 'embeddings'. \n    Feel free to join the effort of providing interesting text visualisations."}, "divest": {"categories": ["MedicalImaging"], "description": "Provides tools to sort DICOM-format medical image files, and\n    convert them to NIfTI-1 format."}, "WrightMap": {"categories": ["Psychometrics"], "description": "A powerful yet simple graphical tool available in the field of psychometrics is the Wright Map (also known as item maps or item-person maps), which presents the location of both respondents and items on the same scale. Wright Maps are commonly used to present the results of dichotomous or polytomous item response models. The 'WrightMap' package provides functions to create these plots from item parameters and person estimates stored as R objects. Although the package can be used in conjunction with any software used to estimate the IRT model (e.g. 'TAM', 'mirt', 'eRm' or 'IRToys' in 'R', or 'Stata', 'Mplus', etc.),  'WrightMap' features special integration with 'ConQuest' to facilitate reading and plotting its output directly.The 'wrightMap' function creates Wright Maps based on person estimates and item parameters produced by an item response analysis. The 'CQmodel' function reads output files created using 'ConQuest' software and creates a set of data frames for easy data manipulation, bundled in a 'CQmodel' object. The 'wrightMap' function can take a 'CQmodel' object as input or it can be used to create Wright Maps directly from data frames of person and item parameters."}, "copulaData": {"categories": ["Finance"], "description": "Data sets used for copula modeling in addition to those in\n the package 'copula'. These include a random subsample from the US National\n Education Longitudinal Study (NELS) of 1988 and nursing home data from\n Wisconsin."}, "LSMRealOptions": {"categories": ["Finance"], "description": "The least-squares Monte Carlo (LSM) simulation method is a popular method for the approximation of the value of early and multiple exercise options. 'LSMRealOptions' provides implementations of the LSM simulation method to value American option products and capital investment projects through real options analysis. 'LSMRealOptions' values capital investment projects with cash flows dependent upon underlying state variables that are stochastically evolving, providing analysis into the timing and critical values at which investment is optimal. 'LSMRealOptions' provides flexibility in the stochastic processes followed by underlying assets, the number of state variables, basis functions and underlying asset characteristics to allow a broad range of assets to be valued through the LSM simulation method. Real options projects are further able to be valued whilst considering construction periods, time-varying initial capital expenditures and path-dependent operational flexibility including the ability to temporarily shutdown or permanently abandon projects after initial investment has occurred. The LSM simulation method was first presented in the prolific work of Longstaff and Schwartz (2001) <doi:10.1093/rfs/14.1.113>."}, "wildmeta": {"categories": ["MetaAnalysis"], "description": "Conducts single coefficient tests and multiple-contrast hypothesis tests of meta-regression models using cluster wild bootstrapping, based on methods examined in Joshi, Pustejovsky, and Beretvas (2022) <doi:10.1002/jrsm.1554>. "}, "dstat": {"categories": ["CausalInference"], "description": "A d-statistic tests the null hypothesis of no treatment effect in a matched, nonrandomized study of the effects caused by treatments.  A d-statistic focuses on subsets of matched pairs that demonstrate insensitivity to unmeasured bias in such an observational study, correcting for double-use of the data by conditional inference. This conditional inference can, in favorable circumstances, substantially increase the power of a sensitivity analysis (Rosenbaum (2010) <doi:10.1007/978-1-4419-1213-8_14>).  There are two examples, one concerning unemployment from Lalive et al. (2006) <doi:10.1111/j.1467-937X.2006.00406.x>, the other concerning smoking and periodontal disease from Rosenbaum (2017) <doi:10.1214/17-STS621>.  "}, "ggspatial": {"categories": ["Spatial"], "description": "Spatial data plus the power of the ggplot2 framework means easier mapping when input \n  data are already in the form of spatial objects."}, "investr": {"categories": ["ChemPhys"], "description": "Functions to facilitate inverse estimation (e.g., calibration) in\n    linear, generalized linear, nonlinear, and (linear) mixed-effects models. A\n    generic function is also provided for plotting fitted regression models with\n    or without confidence/prediction bands that may be of use to the general\n    user. For a general overview of these methods, see Greenwell and Schubert \n    Kabban (2014) <doi:10.32614/RJ-2014-009>."}, "endtoend": {"categories": ["Epidemiology"], "description": "Computes the expectation of the number of transmissions and receptions considering an End-to-End transport model with limited number of retransmissions per packet. It provides theoretical results and also estimated values based on Monte Carlo simulations. It is also possible to consider random data and ACK probabilities."}, "CoSMoS": {"categories": ["Hydrology"], "description": "Makes univariate, multivariate, or random fields simulations precise and simple. Just select the desired time series or random fields\u2019 properties and it will do the rest. CoSMoS is based on the framework described in Papalexiou (2018, <doi:10.1016/j.advwatres.2018.02.013>), extended for random fields in Papalexiou and Serinaldi (2020, <doi:10.1029/2019WR026331>), and further advanced in Papalexiou et al. (2021, <doi:10.1029/2020WR029466>) to allow fine-scale space-time simulation of storms (or even cyclone-mimicking fields)."}, "ECOSolveR": {"categories": ["Optimization"], "description": "R interface to the Embedded COnic Solver (ECOS), an efficient\n\t     and robust C library for convex problems. Conic and equality\n\t     constraints can be specified in addition to integer and\n\t     boolean variable constraints for mixed-integer problems. This\n\t     R interface is inspired by the python interface and has\n\t     similar calling conventions."}, "DiceEval": {"categories": ["ExperimentalDesign"], "description": "Estimation, validation and prediction of models of different types : linear models, additive models, MARS,PolyMARS and Kriging."}, "RXshrink": {"categories": ["MachineLearning"], "description": "Functions are provided to calculate and display ridge TRACE Diagnostics for a \n  variety of alternative Shrinkage Paths. While all methods focus on Maximum Likelihood \n  estimation of unknown true effects under Normal-distribution theory, some estimates are \n  modified to be Unbiased or to have \"Correct Range\" when estimating either [1] the noncentrality \n  of the F-ratio for testing that true Beta coefficients are Zeros or [2] the \"relative\" MSE \n  Risk (i.e. MSE divided by true sigma-square, where the \"relative\" variance of OLS is known.) \n  The eff.ridge() function implements the \"Efficient Shrinkage Path\" introduced in Obenchain\n  (2022) <Open Statistics>. This new \"p-Parameter\" Shrinkage-Path always passes through the\n  vector of regression coefficient estimates Most-Likely to achieve the overall Optimal\n  Variance-Bias Trade-Off and is the shortest Path with this property. Functions eff.aug() and\n  eff.biv() augment the calculations made by eff.ridge() to provide plots of the bivariate\n  confidence ellipses corresponding to any of the p*(p-1) possible ordered pairs of shrunken\n  regression coefficients. "}, "arrangements": {"categories": ["NumericalMathematics"], "description": "Fast generators and iterators for permutations, combinations,\n    integer partitions and compositions. The arrangements are in\n    lexicographical order and generated iteratively in a memory efficient manner. \n    It has been demonstrated that 'arrangements' outperforms most existing\n    packages of similar kind. Benchmarks could be found at\n    <https://randy3k.github.io/arrangements/articles/benchmark.html>."}, "emplik": {"categories": ["Survival"], "description": "Empirical likelihood ratio tests for means/quantiles/hazards\n \tfrom possibly censored and/or truncated data. Now does regression too.\n\tThis version contains some C code."}, "changepoint.np": {"categories": ["TimeSeries"], "description": "Implements the multiple changepoint algorithm PELT with a nonparametric cost function based on the empirical distribution of the data. This package extends the changepoint package (see Killick, R and Eckley, I (2014) <doi:10.18637/jss.v058.i03> ).  "}, "bizdays": {"categories": ["Finance"], "description": "Business days calculations based on a list of holidays and\n    nonworking weekdays. Quite useful for fixed income and derivatives pricing."}, "bayescount": {"categories": ["Bayesian"], "description": "A set of functions to allow analysis of count data (such\n        as faecal egg count data) using Bayesian MCMC methods.  Returns\n        information on the possible values for mean count, coefficient\n        of variation and zero inflation (true prevalence) present in\n        the data.  A complete faecal egg count reduction test (FECRT)\n        model is implemented, which returns inference on the true\n        efficacy of the drug from the pre- and post-treatment data\n        provided, using non-parametric bootstrapping as well as using\n        Bayesian MCMC.  Functions to perform power analyses for faecal\n        egg counts (including FECRT) are also provided."}, "QUALYPSO": {"categories": ["MissingData"], "description": "These functions use data augmentation and Bayesian techniques for the assessment of single-member and incomplete ensembles of climate projections. It provides unbiased estimates of climate change responses of all simulation chains and of all uncertainty variables. It additionally propagates uncertainty due to missing information in the estimates.\n  - Evin, G., B. Hingray, J. Blanchet, N. Eckert, S. Morin, and D. Verfaillie. (2019) <doi:10.1175/JCLI-D-18-0606.1>."}, "cancensus": {"categories": ["OfficialStatistics"], "description": "Integrated, convenient, and uniform access to Canadian\n    Census data and geography retrieved using the 'CensusMapper' API. This package produces analysis-ready \n    tidy data frames and spatial data in multiple formats, as well as convenience functions\n    for working with Census variables, variable hierarchies, and region selection. API\n    keys are freely available with free registration at <https://censusmapper.ca/api>.\n    Census data and boundary geometries are reproduced and distributed on an \"as\n    is\" basis with the permission of Statistics Canada (Statistics Canada 2001; 2006;\n    2011; 2016)."}, "ChemoSpec": {"categories": ["ChemPhys"], "description": "A collection of functions for top-down exploratory data analysis\n    of spectral data including nuclear magnetic resonance (NMR), infrared (IR),\n    Raman, X-ray fluorescence (XRF) and other similar types of spectroscopy.\n    Includes functions for plotting and inspecting spectra, peak alignment,\n    hierarchical cluster analysis (HCA), principal components analysis (PCA) and\n    model-based clustering. Robust methods appropriate for this type of\n    high-dimensional data are available. ChemoSpec is designed for structured\n    experiments, such as metabolomics investigations, where the samples fall into\n    treatment and control groups. Graphical output is formatted consistently for\n    publication quality plots. ChemoSpec is intended to be very user friendly and\n    to help you get usable results quickly. A vignette covering typical operations\n    is available."}, "IDE": {"categories": ["SpatioTemporal"], "description": "The Integro-Difference Equation model is a linear, dynamical model used to model\n   phenomena that evolve in space and in time; see, for example, Cressie and Wikle (2011,\n   ISBN:978-0-471-69274-4) or Dewar et al. (2009) <doi:10.1109/TSP.2008.2005091>. At the\n   heart of the model is the kernel, which dictates how the process evolves from one time\n   point to the next. Both process and parameter reduction are used to facilitate computation,\n   and spatially-varying kernels are allowed. Data used to estimate the parameters are assumed\n   to be readings of the process corrupted by Gaussian measurement error. Parameters are fitted\n   by maximum likelihood, and estimation is carried out using an evolution algorithm. "}, "smcure": {"categories": ["Survival"], "description": "An R-package for Estimating Semiparametric PH and AFT Mixture Cure Models."}, "primefactr": {"categories": ["NumericalMathematics"], "description": "Use Prime Factorization for simplifying computations,\n    for instance for ratios of large factorials."}, "compositions": {"categories": ["ChemPhys", "Distributions"], "description": "Provides functions for the consistent analysis of compositional \n  data (e.g. portions of substances) and positive numbers (e.g. concentrations) \n  in the way proposed by J. Aitchison and V. Pawlowsky-Glahn."}, "nhdR": {"categories": ["Hydrology"], "description": "Tools for working with the National Hydrography Dataset, with \n    functions for querying, downloading, and networking both the NHD \n    <https://www.usgs.gov/national-hydrography> \n    and NHDPlus <https://www.epa.gov/waterdata/nhdplus-national-hydrography-dataset-plus> datasets. "}, "BayesCombo": {"categories": ["Bayesian", "MetaAnalysis"], "description": "Combine diverse evidence across multiple studies to test a high level scientific theory. The methods can also be used as an alternative to a standard meta-analysis."}, "elliptic": {"categories": ["NumericalMathematics"], "description": "\n A suite of elliptic and related functions including Weierstrass and\n Jacobi forms.  Also includes various tools for manipulating and\n visualizing complex functions."}, "FLAME": {"categories": ["CausalInference"], "description": "Efficient implementations of the algorithms in the \n    Almost-Matching-Exactly framework for interpretable matching in causal\n    inference. These algorithms match units via a learned, weighted Hamming\n    distance that determines which covariates are more important to match on.\n    For more information and examples, see the Almost-Matching-Exactly website. "}, "pwt": {"categories": ["Econometrics"], "description": "The Penn World Table provides purchasing power parity and\n\tnational income accounts converted to international prices for\n\t189 countries for some or all of the years 1950-2010."}, "bartCause": {"categories": ["Bayesian", "CausalInference"], "description": "Contains a variety of methods to generate typical causal inference estimates using Bayesian Additive Regression Trees (BART) as the underlying regression model (Hill (2012) <doi:10.1198/jcgs.2010.08162>)."}, "nosoi": {"categories": ["Epidemiology"], "description": "The aim of 'nosoi' (pronounced no.si) is to provide a flexible agent-based stochastic transmission chain/epidemic simulator (Lequime et al. Methods in Ecology and Evolution 11:1002-1007). It is named after the daimones of plague, sickness and disease that escaped Pandora's jar in the Greek mythology. 'nosoi' is able to take into account the influence of multiple variable on the transmission process (e.g. dual-host systems (such as arboviruses), within-host viral dynamics, transportation, population structure), alone or taken together, to create complex but relatively intuitive epidemiological simulations."}, "metaplus": {"categories": ["MetaAnalysis", "Robust"], "description": "Performs meta-analysis and meta-regression using standard and robust methods with confidence intervals based on the profile likelihood. Robust methods are based on alternative distributions for the random effect, either the t-distribution (Lee and Thompson, 2008 <doi:10.1002/sim.2897> or Baker and Jackson, 2008 <doi:10.1007/s10729-007-9041-8>) or mixtures of normals (Beath, 2014 <doi:10.1002/jrsm.1114>)."}, "symengine": {"categories": ["NumericalMathematics"], "description": "\n    Provides an R interface to 'SymEngine' <https://github.com/symengine/>,\n    a standalone 'C++' library for fast symbolic manipulation. The package has functionalities\n    for symbolic computation like calculating exact mathematical expressions, solving\n    systems of linear equations and code generation."}, "Spbsampling": {"categories": ["Spatial"], "description": "Selection of spatially balanced samples. In particular, the implemented sampling designs allow to select probability samples well spread over the population of interest, in any dimension and using any distance function (e.g. Euclidean distance, Manhattan distance). For more details, Benedetti R and Piersimoni F (2017) <doi:10.1002/bimj.201600194> and Benedetti R and Piersimoni F (2017) <arXiv:1710.09116>. The implementation has been done in C++ through the use of 'Rcpp' and 'RcppArmadillo'. "}, "dng": {"categories": ["Distributions"], "description": "Provides density, distribution function, quantile function and random\n             generation for the split normal and split-t distributions, and computes their\n             mean, variance, skewness and kurtosis for the two distributions (Li, F,\n             Villani, M. and Kohn, R. (2010) <doi:10.1016/j.jspi.2010.04.031>)."}, "timeSeries": {"categories": ["Finance", "MissingData", "TimeSeries"], "description": "'S4' classes and various tools for financial time series:\n  Basic functions such as scaling and sorting, subsetting,\n  mathematical operations and statistical functions."}, "BootPR": {"categories": ["TimeSeries"], "description": "Contains functions for bias-Corrected Forecasting and Bootstrap Prediction Intervals for Autoregressive Time Series."}, "tensorBF": {"categories": ["MissingData"], "description": "Bayesian Tensor Factorization for decomposition of tensor data sets using the trilinear CANDECOMP/PARAFAC (CP) factorization, with automatic component selection. The complete data analysis pipeline is provided, including functions and recommendations for data normalization and model definition, as well as missing value prediction and model visualization. The method performs factorization for three-way tensor datasets and the inference is implemented with Gibbs sampling."}, "BANOVA": {"categories": ["Bayesian"], "description": "It covers several Bayesian Analysis of Variance (BANOVA) models used in analysis of experimental designs in which both within- and between- subjects factors are manipulated. They can be applied to data that are common in the behavioral and social sciences. The package includes: Hierarchical Bayes ANOVA models with normal response, t response, Binomial (Bernoulli) response, Poisson response, ordered multinomial response and multinomial response variables. All models accommodate unobserved heterogeneity by including a normal distribution of the parameters across individuals. Outputs of the package include tables of sums of squares, effect sizes and p-values, and tables of predictions, which are easily interpretable for behavioral and social researchers. The floodlight analysis and mediation analysis based on these models are also provided. BANOVA uses 'Stan' and 'JAGS' as the computational platform. References: Dong and Wedel (2017) <doi:10.18637/jss.v081.i09>; Wedel and Dong (2020) <doi:10.1002/jcpy.1111>."}, "RavenR": {"categories": ["Hydrology"], "description": "Utilities for processing input and output files associated with the Raven Hydrological Modelling Framework. Includes various plotting functions, model diagnostics, reading output files into extensible time series format, and support for writing Raven input files. \n    The 'RavenR' package is also archived at Chlumsky et al. (2020) <doi:10.5281/zenodo.4248183>.\n    The Raven Hydrologic Modelling Framework method can be referenced with Craig et al. (2020) <doi:10.1016/j.envsoft.2020.104728>."}, "msos": {"categories": ["TeachingStatistics"], "description": "Multivariate Analysis methods and data sets used\n    in John Marden's book Multivariate Statistics: Old School (2015) <ISBN:978-1456538835>.\n    This also serves as a companion package for the \n    STAT 571: Multivariate Analysis course offered by the Department of Statistics\n    at the University of Illinois at Urbana-Champaign ('UIUC'). "}, "EGRETci": {"categories": ["Hydrology"], "description": "Collection of functions to evaluate uncertainty of results from\n    water quality analysis using the Weighted Regressions on Time Discharge and\n    Season (WRTDS) method. This package is an add-on to the EGRET package that\n    performs the WRTDS analysis. The WRTDS modeling\n    method was initially introduced and discussed in Hirsch et al. (2010) <doi:10.1111/j.1752-1688.2010.00482.x>,\n    and expanded in Hirsch and De Cicco (2015) <doi:10.3133/tm4A10>. The \n    paper describing the uncertainty and confidence interval calculations \n    is Hirsch et al. (2015) <doi:10.1016/j.envsoft.2015.07.017>."}, "gslnls": {"categories": ["Optimization"], "description": "An R interface to nonlinear least-squares optimization with the GNU Scientific Library (GSL), see M. Galassi et al. (2009, ISBN:0954612078). The available trust region methods include the Levenberg-Marquadt algorithm with and without geodesic acceleration, the Steihaug-Toint conjugate gradient algorithm for large systems and several variants of Powell's dogleg algorithm. Bindings are provided to tune a number of parameters affecting the low-level aspects of the trust region algorithms. The interface mimics R's nls() function and returns model objects inheriting from the same class."}, "pcse": {"categories": ["Econometrics"], "description": "A function to estimate\n        panel-corrected standard errors. Data may contain balanced or\n        unbalanced panels."}, "ssgraph": {"categories": ["Bayesian", "HighPerformanceComputing", "MachineLearning"], "description": "Bayesian estimation for undirected graphical models using spike-and-slab priors. The package handles continuous, discrete, and mixed data. "}, "linprog": {"categories": ["Optimization"], "description": "Can be used to solve Linear Programming / Linear\n   Optimization problems by using the simplex algorithm."}, "mudfold": {"categories": ["Psychometrics"], "description": "Nonparametric unfolding item response theory (IRT) model for dichotomous data (see W.H. Van Schuur (1984). Structure in Political Beliefs: A New Model for Stochastic Unfolding with Application to European Party Activists, and W.J.Post (1992). Nonparametric Unfolding Models: A Latent Structure Approach). The package implements MUDFOLD (Multiple UniDimensional unFOLDing), an iterative item selection algorithm that constructs unfolding scales from dichotomous preferential-choice data without explicitly assuming a parametric form of the item response functions. Scale diagnostics from Post(1992) and estimates for the person locations proposed by Johnson(2006) and Van Schuur(1984) are also available. This model can be seen as the unfolding variant of Mokken(1971) scaling method."}, "bayesQR": {"categories": ["Bayesian"], "description": "Bayesian quantile regression using the asymmetric Laplace distribution, both continuous as well as binary dependent variables are supported. The package consists of implementations of the methods of Yu & Moyeed (2001) <doi:10.1016/S0167-7152(01)00124-9>, Benoit & Van den Poel (2012) <doi:10.1002/jae.1216> and Al-Hamzawi, Yu & Benoit (2012) <doi:10.1177/1471082X1101200304>. To speed up the calculations, the Markov Chain Monte Carlo core of all algorithms is programmed in Fortran and called from R."}, "ROCR": {"categories": ["MachineLearning"], "description": "ROC graphs, sensitivity/specificity curves, lift charts,\n  and precision/recall plots are popular examples of trade-off\n  visualizations for specific pairs of performance measures. ROCR is a\n  flexible tool for creating cutoff-parameterized 2D performance curves\n  by freely combining two from over 25 performance measures (new\n  performance measures can be added using a standard interface).\n  Curves from different cross-validation or bootstrapping runs can be\n  averaged by different methods, and standard deviations, standard\n  errors or box plots can be used to visualize the variability across\n  the runs. The parameterization can be visualized by printing cutoff\n  values at the corresponding curve positions, or by coloring the\n  curve according to cutoff. All components of a performance plot can\n  be quickly adjusted using a flexible parameter dispatching\n  mechanism. Despite its flexibility, ROCR is easy to use, with only\n  three commands and reasonable default values for all optional\n  parameters."}, "robustDA": {"categories": ["Robust"], "description": "Robust mixture discriminant analysis (RMDA), proposed in Bouveyron & Girard, 2009 <doi:10.1016/j.patcog.2009.03.027>, allows to build a robust supervised classifier from learning data with label noise. The idea of the proposed method is to confront an unsupervised modeling of the data with the supervised information carried by the labels of the learning data in order to detect inconsistencies. The method is able afterward to build a robust classifier taking into account the detected inconsistencies into the labels."}, "inlmisc": {"categories": ["Spatial"], "description": "A collection of functions for creating high-level graphics,\n    performing raster-based analysis, processing MODFLOW-based models,\n    selecting subsets using a genetic algorithm, creating interactive web maps,\n    accessing color palettes, etc. Used to support packages and scripts written\n    by researchers at the United States Geological Survey (USGS)\n    Idaho National Laboratory (INL) Project Office."}, "vdg": {"categories": ["ExperimentalDesign"], "description": "Facilities for constructing variance dispersion graphs, fraction-\n    of-design-space plots and similar graphics for exploring the properties of\n    experimental designs. The design region is explored via random sampling, which\n    allows for more flexibility than traditional variance dispersion graphs. A\n    formula interface is leveraged to provide access to complex model formulae.\n    Graphics can be constructed simultaneously for multiple experimental designs\n    and/or multiple model formulae. Instead of using pointwise optimization to\n    find the minimum and maximum scaled prediction variance curves, which can be\n    inaccurate and time consuming, this package uses quantile regression as an\n    alternative."}, "pushoverr": {"categories": ["WebTechnologies"], "description": "Send push notifications to mobile devices or the desktop\n    using 'Pushover' <https://pushover.net>. These notifications can\n    display things such as results, job status, plots, or any other text\n    or numeric data."}, "nhlscrape": {"categories": ["SportsAnalytics"], "description": "Add game events to a database file to use for statistical analysis of hockey games. This means we only call the 'NHL' API\n    once for each game we want to add. We will have very fast retrieval of data once games have been added since the data is stored locally.\n    We use the API located at <https://statsapi.web.nhl.com/api/v1/teams> with supplemental data from <https://www.nhl.com/scores/>.\n    Other endpoints can be found at <https://gitlab.com/dword4/nhlapi>."}, "metapower": {"categories": ["MetaAnalysis"], "description": "A simple and effective tool for computing and visualizing statistical power for meta-analysis,\n    including power analysis of main effects (Jackson & Turner, 2017)<doi:10.1002/jrsm.1240>, \n    test of homogeneity (Pigott, 2012)<doi:10.1007/978-1-4614-2278-5>, subgroup analysis, \n    and categorical moderator analysis (Hedges & Pigott, 2004)<doi:10.1037/1082-989X.9.4.426>."}, "hermite": {"categories": ["Distributions"], "description": "Probability functions and other utilities for the generalized Hermite distribution."}, "turboEM": {"categories": ["NumericalMathematics"], "description": "Algorithms for accelerating the convergence of slow,\n        monotone sequences from smooth, contraction mapping such as the\n        EM and MM algorithms. It can be used to accelerate any smooth,\n        linearly convergent acceleration scheme.  A tutorial style\n        introduction to this package is available in a vignette on the\n        CRAN download page or, when the package is loaded in an R\n        session, with vignette(\"turboEM\")."}, "PearsonDS": {"categories": ["Distributions"], "description": "Implementation of the Pearson distribution system, including full\n  support for the (d,p,q,r)-family of functions for probability distributions \n  and fitting via method of moments and maximum likelihood method."}, "memapp": {"categories": ["Epidemiology"], "description": "The Moving Epidemic Method, created by T Vega and JE Lozano (2012, 2015) <doi:10.1111/j.1750-2659.2012.00422.x>, <doi:10.1111/irv.12330>, allows the weekly assessment of the epidemic and intensity status to help in routine respiratory infections surveillance in health systems. Allows the comparison of different epidemic indicators, timing and shape with past epidemics and across different regions or countries with different surveillance systems. Also, it gives a measure of the performance of the method in terms of sensitivity and specificity of the alert week. 'memapp' is a web application created in the Shiny framework for the 'mem' R package."}, "rcbalance": {"categories": ["CausalInference"], "description": "Tools for large, sparse optimal matching of treated units\n\tand control units in observational studies.  Provisions are\n\tmade for refined covariate balance constraints, which include\n\tfine and near-fine balance as special cases.  Matches are \n\toptimal in the sense that they are computed as solutions to\n\tnetwork optimization problems rather than greedy algorithms.\n\tSee Pimentel, et al.(2015) <doi:10.1080/01621459.2014.997879> \n\tand Pimentel (2016), Obs. Studies 2(1):4-23. The rrelaxiv \n\tpackage, which provides an alternative solver for\n\tthe underlying network flow problems, carries an\n\tacademic license and is not available on CRAN, but\n\tmay be downloaded from Github at \n\t<https://github.com/josherrickson/rrelaxiv/>."}, "oai": {"categories": ["WebTechnologies"], "description": "A general purpose client to work with any 'OAI-PMH'\n    (Open Archives Initiative Protocol for 'Metadata' Harvesting) service.\n    The 'OAI-PMH' protocol is described at\n    <http://www.openarchives.org/OAI/openarchivesprotocol.html>.\n    Functions are provided to work with the 'OAI-PMH' verbs: 'GetRecord',\n    'Identify', 'ListIdentifiers', 'ListMetadataFormats', 'ListRecords', and\n    'ListSets'."}, "metaDigitise": {"categories": ["MetaAnalysis"], "description": "High-throughput, flexible and reproducible extraction of data from figures in primary research papers. metaDigitise() can extract data and / or automatically calculate summary statistics for users from box plots, bar plots (e.g., mean and errors), scatter plots and histograms."}, "RcppDE": {"categories": ["Optimization"], "description": "An efficient C++ based implementation of the 'DEoptim'\n function which performs global optimization by differential evolution.  \n Its creation was motivated by trying to see if the old approximation \"easier,\n shorter, faster: pick any two\" could in fact be extended to achieving all\n three goals while moving the code from plain old C to modern C++.  The\n initial version did in fact do so, but a good part of the gain was due to \n an implicit code review which eliminated a few inefficiencies which have\n since been eliminated in 'DEoptim'."}, "miceFast": {"categories": ["MissingData"], "description": "\n  Fast imputations under the object-oriented programming paradigm. \t\n  Moreover there are offered a few functions built to work with popular R packages such as 'data.table' or 'dplyr'.\n  The biggest improvement in time performance could be achieve for a calculation where a grouping variable have to be used.\n  A single evaluation of a quantitative model for the multiple imputations is another major enhancement.\n  A new major improvement is one of the fastest predictive mean matching in the R world because of presorting and binary search."}, "rvest": {"categories": ["WebTechnologies"], "description": "Wrappers around the 'xml2' and 'httr' packages to\n    make it easy to download, then manipulate, HTML and XML."}, "CHNOSZ": {"categories": ["ChemPhys"], "description": "An integrated set of tools for thermodynamic calculations in\n  aqueous geochemistry and geobiochemistry. Functions are provided for writing\n  balanced reactions to form species from user-selected basis species and for\n  calculating the standard molal properties of species and reactions, including\n  the standard Gibbs energy and equilibrium constant. Calculations of the\n  non-equilibrium chemical affinity and equilibrium chemical activity of species\n  can be portrayed on diagrams as a function of temperature, pressure, or\n  activity of basis species; in two dimensions, this gives a maximum affinity or\n  predominance diagram. The diagrams have formatted chemical formulas and axis\n  labels, and water stability limits can be added to Eh-pH, oxygen fugacity-\n  temperature, and other diagrams with a redox variable. The package has been\n  developed to handle common calculations in aqueous geochemistry, such as\n  solubility due to complexation of metal ions, mineral buffers of redox or pH,\n  and changing the basis species across a diagram (\"mosaic diagrams\"). CHNOSZ\n  also implements a group additivity algorithm for the standard thermodynamic\n  properties of proteins."}, "splines2": {"categories": ["NumericalMathematics"], "description": "Constructs basis matrix of B-splines, M-splines,\n    I-splines, convex splines (C-splines), periodic M-splines,\n    natural cubic splines, generalized Bernstein polynomials,\n    and their integrals (except C-splines) and derivatives\n    of given order by close-form recursive formulas.\n    It also contains a C++ head-only library integrated with Rcpp.\n    See Wang and Yan (2021) <doi:10.6339/21-JDS1020> for details."}, "sp": {"categories": ["Spatial", "SpatioTemporal"], "description": "Classes and methods for spatial\n  data; the classes document where the spatial location information\n  resides, for 2D or 3D data. Utility functions are provided, e.g. for\n  plotting data as maps, spatial selection, as well as methods for\n  retrieving coordinates, for subsetting, print, summary, etc."}, "htmltools": {"categories": ["ReproducibleResearch", "WebTechnologies"], "description": "Tools for HTML generation and output."}, "pmml": {"categories": ["ModelDeployment"], "description": "The Predictive Model Markup Language (PMML) is an XML-based language which provides a way for applications to define machine learning, statistical and data mining models and to share models between PMML compliant applications. More information about the PMML industry standard and the Data Mining Group can be found at <http://dmg.org/>. The generated PMML can be imported into any PMML consuming application, such as Zementis Predictive Analytics products. The package isofor (used for anomaly detection) can be installed with devtools::install_github(\"gravesee/isofor\")."}, "lira": {"categories": ["ChemPhys"], "description": "Performs Bayesian linear regression and forecasting in astronomy. The method accounts for heteroscedastic errors in both the independent and the dependent variables, intrinsic scatters (in both variables) and scatter correlation, time evolution of slopes, normalization, scatters, Malmquist and Eddington bias, upper limits and break of linearity. The posterior distribution of the regression parameters is sampled with a Gibbs method exploiting the JAGS library."}, "dexterMST": {"categories": ["Psychometrics"], "description": "Conditional Maximum Likelihood Calibration and data management of multistage tests. \n  Supports polytomous items and incomplete designs with linear as well as multistage tests.\n  Extended Nominal Response and Interaction models, DIF and profile analysis.\n  See Robert J. Zwitser and Gunter Maris (2015)<doi:10.1007/s11336-013-9369-6>."}, "datarobot": {"categories": ["WebTechnologies"], "description": "For working with the 'DataRobot' predictive modeling platform's API <https://www.datarobot.com/>."}, "HBV.IANIGLA": {"categories": ["Hydrology"], "description": "The HBV hydrological model (Bergstr\u00f6m, S. and Lindstr\u00f6m, G., (2015) <doi:10.1002/hyp.10510>) has been split in modules to allow the user to build his/her own model. This version was developed by the author in IANIGLA-CONICET (Instituto Argentino de Nivologia, Glaciologia y Ciencias Ambientales - Consejo Nacional de Investigaciones Cientificas y Tecnicas) for hydroclimatic studies in the Andes. HBV.IANIGLA incorporates routines for clean and debris covered glacier melt simulations. "}, "RSmartlyIO": {"categories": ["WebTechnologies"], "description": "Aims at loading Facebook and Instagram advertising data from\n    'Smartly.io' into R. 'Smartly.io' is an online advertising service that enables\n    advertisers to display commercial ads on social media networks (see <http://www.smartly.io/> for more information).\n    The package offers an interface to query the 'Smartly.io' API and loads data directly into R for further data processing and data analysis."}, "metagear": {"categories": ["MetaAnalysis", "MissingData"], "description": "Functionalities for facilitating systematic reviews, data\n    extractions, and meta-analyses. It includes a GUI (graphical user interface)\n    to help screen the abstracts and titles of bibliographic data; tools to assign\n    screening effort across multiple collaborators/reviewers and to assess inter-\n    reviewer reliability; tools to help automate the download and retrieval of\n    journal PDF articles from online databases; figure and image extractions \n    from PDFs; web scraping of citations; automated and manual data extraction \n    from scatter-plot and bar-plot images; PRISMA (Preferred Reporting Items for\n    Systematic Reviews and Meta-Analyses) flow diagrams; simple imputation tools\n    to fill gaps in incomplete or missing study parameters; generation of random\n    effects sizes for Hedges' d, log response ratio, odds ratio, and correlation\n    coefficients for Monte Carlo experiments; covariance equations for modelling\n    dependencies among multiple effect sizes (e.g., effect sizes with a common\n    control); and finally summaries that replicate analyses and outputs from \n    widely used but no longer updated meta-analysis software (i.e., metawin).\n\tFunding for this package was supported by National Science Foundation (NSF) \n\tgrants DBI-1262545 and DEB-1451031. CITE: Lajeunesse, M.J. (2016) \n\tFacilitating systematic reviews, data extraction and meta-analysis with the \n\tmetagear package for R. Methods in Ecology and Evolution 7, 323-330 \n\t<doi:10.1111/2041-210X.12472>."}, "argo": {"categories": ["Epidemiology"], "description": "Augmented Regression with General Online data (ARGO) for accurate estimation of influenza epidemics in United States on national level, regional level and state level. It replicates the method introduced in paper Yang, S., Santillana, M. and Kou, S.C. (2015) <doi:10.1073/pnas.1515373112>; Ning, S., Yang, S. and Kou, S.C. (2019) <doi:10.1038/s41598-019-41559-6>; Yang, S., Ning, S. and Kou, S.C. (2021) <doi:10.1038/s41598-021-83084-5>."}, "nloptr": {"categories": ["Optimization"], "description": "\n    Solve optimization problems using an R interface to NLopt. NLopt is a \n    free/open-source library for nonlinear optimization, providing a common\n    interface for a number of different free optimization routines available\n    online as well as original implementations of various other algorithms.\n    See <https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/> for more\n    information on the available algorithms. Building from included sources \n    requires 'CMake'. On Linux and 'macOS', if a suitable system build of \n    NLopt (2.7.0 or later) is found, it is used; otherwise, it is built \n    from included sources via 'CMake'. On Windows, NLopt is obtained through \n    'rwinlib' for 'R <= 4.1.x' or grabbed from the 'Rtools42 toolchain' for \n    'R >= 4.2.0'."}, "ThreeWay": {"categories": ["Psychometrics"], "description": "Component analysis for three-way data arrays by means of Candecomp/Parafac, Tucker3, Tucker2 and Tucker1 models."}, "mev": {"categories": ["ExtremeValue"], "description": "Various tools for the analysis of univariate, multivariate and functional extremes. Exact simulation from max-stable processes [Dombry, Engelke and Oesting (2016) <doi:10.1093/biomet/asw008>, R-Pareto processes for various parametric models, including Brown-Resnick (Wadsworth and Tawn, 2014, <doi:10.1093/biomet/ast042>) and Extremal Student (Thibaud and Opitz, 2015, <doi:10.1093/biomet/asv045>). Threshold selection methods, including Wadsworth (2016) <doi:10.1080/00401706.2014.998345>, and Northrop and Coleman (2014) <doi:10.1007/s10687-014-0183-z>. Multivariate extreme diagnostics. Estimation and likelihoods for univariate extremes, e.g., Coles (2001) <doi:10.1007/978-1-4471-3675-0>."}, "intccr": {"categories": ["Survival"], "description": "Semiparametric regression models on the cumulative incidence function for interval-censored competing risks data as described in Bakoyannis, Yu, & Yiannoutsos (2017) /doi{10.1002/sim.7350} and the models with missing event types as described in Park, Bakoyannis, Zhang, & Yiannoutsos (2021) \\doi{10.1093/biostatistics/kxaa052}. The proportional subdistribution hazards model (Fine-Gray model), the proportional odds model, and other models that belong to the class of semiparametric generalized odds rate transformation models."}, "osmplotr": {"categories": ["WebTechnologies"], "description": "Bespoke images of 'OpenStreetMap' ('OSM') data and data\n    visualisation using 'OSM' objects."}, "tm.plugin.alceste": {"categories": ["NaturalLanguageProcessing"], "description": "This package provides a tm Source to create corpora from\n  a corpus prepared in the format used by the Alceste application (i.e.\n  a single text file with inline meta-data). It is able to import both\n  text contents and meta-data (starred) variables."}, "renv": {"categories": ["ReproducibleResearch"], "description": "A dependency management toolkit for R. Using 'renv', you can create\n    and manage project-local R libraries, save the state of these libraries to\n    a 'lockfile', and later restore your library as required. Together, these\n    tools can help make your projects more isolated, portable, and reproducible."}, "ctmle": {"categories": ["CausalInference"], "description": "Implements the general template for collaborative targeted maximum likelihood estimation. It also provides several commonly used C-TMLE instantiation, like the vanilla/scalable variable-selection C-TMLE (Ju et al. (2017) <doi:10.1177/0962280217729845>) and the glmnet-C-TMLE algorithm (Ju et al. (2017) <arXiv:1706.10029>). "}, "epibasix": {"categories": ["ClinicalTrials", "Epidemiology"], "description": "Contains elementary tools for analysis of\n        common epidemiological problems, ranging from sample size\n        estimation, through 2x2 contingency table analysis and basic\n        measures of agreement (kappa, sensitivity/specificity).\n        Appropriate print and summary statements are also written to\n        facilitate interpretation wherever possible.  Source code is \tcommented throughout to facilitate modification.  The target audience includes advanced undergraduate and graduate students\n        in epidemiology or biostatistics courses, and clinical researchers."}, "psyphy": {"categories": ["Psychometrics"], "description": "An assortment of functions that could be useful in analyzing data from psychophysical experiments. It includes functions for calculating d' from several different experimental designs, links for m-alternative forced-choice (mafc) data to be used with the binomial family in glm (and possibly other contexts) and self-Start functions for estimating gamma values for CRT screen calibrations."}, "filehashSQLite": {"categories": ["Databases"], "description": "Simple key-value database using SQLite as the back end."}, "ammiBayes": {"categories": ["Bayesian"], "description": "Flexible multi-environment trials analysis via MCMC method for Additive Main Effects and Multiplicative Model (AMMI) for continuous data. \n Biplot with the averages and regions of confidence can be generated. The chains run in parallel on Linux systems and run serially on Windows."}, "vitality": {"categories": ["Survival"], "description": "Provides fitting routines for four versions of the\n        Vitality family of mortality models."}, "ecespa": {"categories": ["Spatial"], "description": "Some wrappers, functions and data sets for for spatial point pattern analysis (mainly based on 'spatstat'), used in the book \"Introduccion al Analisis Espacial de Datos en Ecologia y Ciencias Ambientales: Metodos y Aplicaciones\" and in the papers by De la Cruz et al. (2008) <doi:10.1111/j.0906-7590.2008.05299.x> and Olano et al. (2009) <doi:10.1051/forest:2008074>."}, "cbinom": {"categories": ["Distributions"], "description": "Implementation of the d/p/q/r family of functions for a continuous analog to the standard discrete binomial with continuous size parameter and continuous support with x in [0, size + 1], following Ilienko (2013) <arXiv:1303.5990>."}, "VarSelLCM": {"categories": ["Cluster", "MissingData"], "description": "Full model selection (detection of the relevant features and estimation of the number of clusters) for model-based clustering (see reference here <doi:10.1007/s11222-016-9670-1>). Data to analyze can be continuous, categorical, integer or mixed. Moreover, missing values can occur and do not necessitate any pre-processing. Shiny application permits an easy interpretation of the results."}, "Dowd": {"categories": ["Finance"], "description": "'Kevin Dowd's' book Measuring Market Risk is a widely read book \n          in the area of risk measurement by students and \n          practitioners alike. As he claims, 'MATLAB' indeed might have been the most \n          suitable language when he originally wrote the functions, but,\n          with growing popularity of R it is not entirely \n\t  valid. As 'Dowd's' code was not intended to be error free and were mainly \n\t  for reference, some functions in this package have inherited those \n\t  errors. An attempt will be made in future releases to identify and correct \n\t  them. 'Dowd's' original code can be downloaded from www.kevindowd.org/measuring-market-risk/. \n          It should be noted that 'Dowd' offers both\n          'MMR2' and 'MMR1' toolboxes. Only 'MMR2' was ported to R. 'MMR2' is more \n          recent version of 'MMR1' toolbox and they both have mostly similar \n          function. The toolbox mainly contains different parametric and non \n\t  parametric methods for measurement of market risk as well as \n\t  backtesting risk measurement methods."}, "labdsv": {"categories": ["Environmetrics", "Psychometrics"], "description": "A variety of ordination and community analyses\n   useful in analysis of data sets in community ecology.  \n   Includes many of the common ordination methods, with \n   graphical routines to facilitate their interpretation, \n   as well as several novel analyses."}, "maptree": {"categories": ["Environmetrics", "MachineLearning"], "description": "Functions with example data for graphing, pruning, and\n        mapping models from hierarchical clustering, and classification\n        and regression trees."}, "xtable": {"categories": ["ReproducibleResearch"], "description": "Coerce data to LaTeX and HTML tables."}, "pitchRx": {"categories": ["SportsAnalytics"], "description": "With 'pitchRx', one can easily obtain Major League Baseball Advanced\n    Media's 'Gameday' data (as well as store it in a remote database). The\n    'Gameday' website hosts a wealth of data in XML format, but perhaps most\n    interesting is 'pitchfx'. Among other things, 'pitchfx' data can be used to\n    recreate a baseball's flight path from a pitcher's hand to home plate. With\n    pitchRx, one can easily create animations and interactive 3D 'scatterplots'\n    of the baseball's flight path. 'pitchfx' data is also commonly used to\n    generate a static plot of baseball locations at the moment they cross home\n    plate. These plots, sometimes called strike-zone plots, can also refer to a\n    plot of event probabilities over the same region. 'pitchRx' provides an easy\n    and robust way to generate strike-zone plots using the 'ggplot2' package."}, "qualmap": {"categories": ["Spatial"], "description": "Provides a set of functions for taking qualitative GIS data, hand drawn on a map, and \n   converting it to a simple features object. These tools are focused on data that are drawn on a map\n   that contains some type of polygon features. For each area identified on the map, the id numbers\n   of these polygons can be entered as vectors and transformed using qualmap."}, "geepack": {"categories": ["Econometrics"], "description": "Generalized estimating equations solver for parameters in\n    mean, scale, and correlation structures, through mean link,\n    scale link, and correlation link. Can also handle clustered\n    categorical responses. See e.g. Halekoh and H\u00f8jsgaard, (2005,\n    <doi:10.18637/jss.v015.i02>), for details. "}, "clusterGeneration": {"categories": ["Cluster"], "description": "We developed the clusterGeneration package to provide functions \n        for generating random clusters, generating random \n        covariance/correlation matrices,\n        calculating a separation index (data and population version)\n        for pairs of clusters or cluster distributions, and 1-D and 2-D\n        projection plots to visualize clusters.  The package also\n        contains a function to generate random clusters based on\n        factorial designs with factors such as degree of separation,\n        number of clusters, number of variables, number of noisy\n        variables."}, "psbcGroup": {"categories": ["Survival"], "description": "Algorithms to implement various Bayesian penalized survival regression models including: semiparametric proportional hazards models with lasso priors (Lee et al., Int J Biostat, 2011 <doi:10.2202/1557-4679.1301>) and three  other shrinkage and group priors (Lee et al., Stat Anal Data Min, 2015 <doi:10.1002/sam.11266>); parametric accelerated failure time models with group/ordinary lasso prior (Lee et al. Comput Stat Data Anal, 2017 <doi:10.1016/j.csda.2017.02.014>)."}, "dendextend": {"categories": ["Cluster"], "description": "Offers a set of functions for extending\n    'dendrogram' objects in R, letting you visualize and compare trees of\n    'hierarchical clusterings'. You can (1) Adjust a tree's graphical parameters\n    - the color, size, type, etc of its branches, nodes and labels. (2)\n    Visually and statistically compare different 'dendrograms' to one another."}, "graphicalVAR": {"categories": ["Psychometrics", "TimeSeries"], "description": "Estimates within and between time point interactions in experience sampling data, using the Graphical vector autoregression model in combination with regularization. See also Epskamp, Waldorp, Mottus & Borsboom (2018) <doi:10.1080/00273171.2018.1454823>."}, "psychmeta": {"categories": ["MetaAnalysis"], "description": "Tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and d values. Includes tools for converting effect sizes, computing sporadic artifact corrections, reshaping meta-analytic databases, computing multivariate corrections for range variation, and more. Bugs can be reported to <https://github.com/psychmeta/psychmeta/issues> or <issues@psychmeta.com>."}, "survSNP": {"categories": ["Survival"], "description": "Conduct asymptotic and empirical power and sample size calculations for Single-Nucleotide Polymorphism (SNP) association studies with right censored time to event outcomes."}, "DALEX": {"categories": ["TeachingStatistics"], "description": "Any unverified black box model is the path to failure. Opaqueness leads to distrust. \n  Distrust leads to ignoration. Ignoration leads to rejection. \n  DALEX package xrays any model and helps to explore and explain its behaviour.\n  Machine Learning (ML) models are widely used and have various applications in classification \n  or regression. Models created with boosting, bagging, stacking or similar techniques are often\n  used due to their high performance. But such black-box models usually lack direct interpretability.\n  DALEX package contains various methods that help to understand the link between input variables \n  and model output. Implemented methods help to explore the model on the level of a single instance \n  as well as a level of the whole dataset.\n  All model explainers are model agnostic and can be compared across different models.\n  DALEX package is the cornerstone for 'DrWhy.AI' universe of packages for visual model exploration.\n  Find more details in (Biecek 2018) <arXiv:1806.08915>."}, "wbstats": {"categories": ["Econometrics"], "description": "Search and download data from the World Bank Data API."}, "blavaan": {"categories": ["Bayesian", "Psychometrics"], "description": "Fit a variety of Bayesian latent variable models, including confirmatory\n   factor analysis, structural equation models, and latent growth curve models. References: Merkle & Rosseel (2018) <doi:10.18637/jss.v085.i04>; Merkle et al. (2021) <doi:10.18637/jss.v100.i06>."}, "condSURV": {"categories": ["Survival"], "description": "Method to implement some newly developed methods for the\n    estimation of the conditional survival function."}, "RcmdrPlugin.temis": {"categories": ["NaturalLanguageProcessing"], "description": "An 'R Commander' plug-in providing an integrated solution to perform\n    a series of text mining tasks such as importing and cleaning a corpus, and\n    analyses like terms and documents counts, vocabulary tables, terms\n    co-occurrences and documents similarity measures, time series analysis,\n    correspondence analysis and hierarchical clustering. Corpora can be imported\n    from spreadsheet-like files, directories of raw text files, 'Twitter' queries,\n    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files."}, "gsarima": {"categories": ["TimeSeries"], "description": "Write SARIMA models in (finite) AR representation and simulate \n\tgeneralized multiplicative seasonal autoregressive moving average (time) series \n\twith Normal / Gaussian, Poisson or negative binomial distribution. \n\tThe methodology of this method is described in Briet OJT, Amerasinghe PH, and \n\tVounatsou P (2013) <doi:10.1371/journal.pone.0065761>."}, "CausalImpact": {"categories": ["Bayesian", "CausalInference"], "description": "Implements a Bayesian approach to causal impact estimation in time\n  series, as described in Brodersen et al. (2015) <doi:10.1214/14-AOAS788>.\n  See the package documentation on GitHub\n  <https://google.github.io/CausalImpact/> to get started."}, "bootstrap": {"categories": ["Econometrics"], "description": "Software (bootstrap, cross-validation, jackknife) and data\n        for the book \"An Introduction to the Bootstrap\" by B. Efron and\n        R. Tibshirani, 1993, Chapman and Hall. This package is\n        primarily provided for projects already based on it, and for\n        support of the book. New projects should preferentially use the\n        recommended package \"boot\"."}, "moveWindSpeed": {"categories": ["SpatioTemporal", "Tracking"], "description": "Estimating wind speed from trajectories of individually tracked birds using a maximum likelihood approach."}, "bioOED": {"categories": ["ExperimentalDesign"], "description": "Extends the bioinactivation package with functions for Sensitivity\n    Analysis and Optimum Experiment Design."}, "mvglmmRank": {"categories": ["SportsAnalytics"], "description": "Maximum likelihood estimates are obtained via an EM algorithm with either a first-order or a fully exponential Laplace approximation. "}, "covid19sf": {"categories": ["Epidemiology"], "description": "Provides a verity of summary tables of the Covid19 cases in San Francisco. Data source: San Francisco, Department of Public Health - Population Health Division <https://datasf.org/opendata/>."}, "rcartocolor": {"categories": ["Spatial"], "description": "Provides color schemes for maps and other graphics\n    designed by 'CARTO' as described at <https://carto.com/carto-colors/>.\n    It includes four types of palettes: aggregation, diverging, qualitative, \n    and quantitative."}, "Hmisc": {"categories": ["Bayesian", "ClinicalTrials", "Databases", "Econometrics", "MissingData", "ReproducibleResearch"], "description": "Contains many functions useful for data\n\tanalysis, high-level graphics, utility operations, functions for\n\tcomputing sample size and power, simulation, importing and annotating datasets,\n\timputing missing values, advanced table making, variable clustering,\n\tcharacter string manipulation, conversion of R objects to LaTeX and html code,\n\tand recoding variables."}, "bvls": {"categories": ["ChemPhys", "Optimization"], "description": "An R interface to the Stark-Parker implementation of an\n        algorithm for bounded-variable least squares"}, "dagitty": {"categories": ["CausalInference"], "description": "A port of the web-based software 'DAGitty', available at \n    <http://dagitty.net>, for analyzing structural causal models \n    (also known as directed acyclic graphs or DAGs).\n    This package computes covariate adjustment sets for estimating causal\n    effects, enumerates instrumental variables, derives testable\n    implications (d-separation and vanishing tetrads), generates equivalent\n    models, and includes a simple facility for data simulation. "}, "pbo": {"categories": ["Finance"], "description": "Following the method of Bailey et al., computes for a collection\n    of candidate models the probability of backtest overfitting, the\n    performance degradation and probability of loss, and the stochastic\n    dominance."}, "QPmin": {"categories": ["Optimization"], "description": "Active set method solver for the solution of indefinite quadratic programs, subject to lower bounds on linear functions of the variables and simple bounds on the variables themselves. The function QPmin() implements an algorithm similar to the one described in Gould (1991) <doi:10.1093/imanum/11.3.299>  with the exception that an efficient sparse internal representation of the basis matrix is maintained thus allowing the solution of somewhat large problems."}, "semPlot": {"categories": ["Psychometrics"], "description": "Path diagrams and visual analysis of various SEM packages' output."}, "pec": {"categories": ["Survival"], "description": "Validation of risk predictions obtained from survival models and\n    competing risk models based on censored data using inverse weighting and\n    cross-validation. Most of the 'pec' functionality has been moved to 'riskRegression'."}, "RefManageR": {"categories": ["ReproducibleResearch"], "description": "Provides tools for importing and working with bibliographic\n    references. It greatly enhances the 'bibentry' class by providing a class\n    'BibEntry' which stores 'BibTeX' and 'BibLaTeX' references, supports 'UTF-8'\n    encoding, and can be easily searched by any field, by date ranges, and by\n    various formats for name lists (author by last names, translator by full names,\n    etc.). Entries can be updated, combined, sorted, printed in a number of styles,\n    and exported. 'BibTeX' and 'BibLaTeX' '.bib' files can be read into 'R' and\n    converted to 'BibEntry' objects. Interfaces to 'NCBI Entrez', 'CrossRef', and\n    'Zotero' are provided for importing references and references can be created\n    from locally stored 'PDF' files using 'Poppler'. Includes functions for citing\n    and generating a bibliography with hyperlinks for documents prepared with\n    'RMarkdown' or 'RHTML'."}, "esemifar": {"categories": ["TimeSeries"], "description": "The nonparametric trend and its derivatives in equidistant time \n    series (TS) with long-memory errors can be estimated. The \n    estimation is conducted via local polynomial regression using an \n    automatically selected bandwidth obtained by a built-in iterative plug-in \n    algorithm or a bandwidth fixed by the user.\n    The smoothing methods of the package are described in Letmathe, S., Beran,\n    J. and Feng, Y., (2021) <https://ideas.repec.org/p/pdn/ciepap/145.html>."}, "MODIStsp": {"categories": ["Hydrology"], "description": "Allows automating the creation of time series of rasters derived\n    from MODIS satellite land products data. It performs several typical\n    preprocessing steps such as download, mosaicking, reprojecting and resizing\n    data acquired on a specified time period. All processing parameters\n    can be set using a user-friendly GUI. Users can select which layers of\n    the original MODIS HDF files they want to process, which additional\n    quality indicators should be extracted from aggregated MODIS quality\n    assurance layers and, in the case of surface reflectance products,\n    which spectral indexes should be computed from the original reflectance\n    bands. For each output layer, outputs are saved as single-band raster\n    files corresponding to each available acquisition date. Virtual files\n    allowing access to the entire time series as a single file are also created.\n    Command-line execution exploiting a previously saved processing options\n    file is also possible, allowing users to automatically update time series\n    related to a MODIS product whenever a new image is available.\n    For additional documentation refer to the following article: \n    Busetto and Ranghetti (2016) <doi:10.1016/j.cageo.2016.08.020>."}, "isoWater": {"categories": ["Hydrology"], "description": "The wiDB...() functions provide an interface to the public API \n    of the wiDB <https://github.com/SPATIAL-Lab/isoWater/blob/master/Protocol.md>: \n    build, check and submit queries, and receive and \n    unpack responses. Data analysis functions support Bayesian \n    inference of the source and source isotope composition of water \n    samples that may have experienced evaporation. Algorithms \n    adapted from Bowen et al. (2018, <doi:10.1007/s00442-018-4192-5>)."}, "ssfa": {"categories": ["Econometrics"], "description": "Spatial Stochastic Frontier Analysis (SSFA) is an original method for controlling the spatial heterogeneity in Stochastic Frontier Analysis (SFA) models, for cross-sectional data, by splitting the inefficiency term into three terms: the first one related to spatial peculiarities of the territory in which each single unit operates, the second one related to the specific production features and the third one representing the error term."}, "networktree": {"categories": ["Psychometrics"], "description": "Network trees recursively partition the data with respect to covariates. Two network tree algorithms are available: model-based trees based on a multivariate normal model and nonparametric trees based on covariance structures. After partitioning, correlation-based networks (psychometric networks) can be fit on the partitioned data. For details see Jones, Mair, Simon, & Zeileis (2020) <doi:10.1007/s11336-020-09731-4>. "}, "discgolf": {"categories": ["WebTechnologies"], "description": "Client for the Discourse API. Discourse is a open source\n    discussion forum platform (<https://www.discourse.org/>). It comes with 'RESTful'\n    API access to an installation. This client requires that you are authorized\n    to access a Discourse installation, either yours or another."}, "robustrao": {"categories": ["MissingData"], "description": "A collection of functions to compute the Rao-Stirling diversity index\n\t(Porter and Rafols, 2009) <doi:10.1007/s11192-008-2197-2> and its extension to\n\tacknowledge missing data (i.e.,\tuncategorized references) by calculating its\n\tinterval of uncertainty using\tmathematical optimization as proposed in Calatrava\n\tet al. (2016) <doi:10.1007/s11192-016-1842-4>.\n\tThe Rao-Stirling diversity index is a well-established bibliometric indicator\n\tto measure the interdisciplinarity of scientific publications. Apart from the\n\tobligatory dataset of publications with their respective references and\ta\n\ttaxonomy of disciplines that categorizes references as well as a measure of\n\tsimilarity between the disciplines, the Rao-Stirling diversity index requires\n\ta complete categorization of all references of a publication into disciplines.\n\tThus, it fails for a incomplete categorization; in this case, the robust\n\textension has to be used, which encodes the uncertainty caused by missing\n\tbibliographic data as an uncertainty interval.\n\tClassification / ACM - 2012: Information systems ~ Similarity measures,\n\tTheory of computation ~ Quadratic\tprogramming, Applied computing ~ Digital\n\tlibraries and archives."}, "hydroroute": {"categories": ["Hydrology"], "description": "Implements an empirical approach referred to as PeakTrace which uses multiple hydrographs to detect and follow hydropower plant-specific hydropeaking waves at the sub-catchment scale and to describe how hydropeaking flow parameters change along the longitudinal flow path. The method is based on the identification of associated events and uses (linear) regression models to describe translation and retention processes between neighboring hydrographs. Several regression model results are combined to arrive at a power plant-specific model. The approach is proposed and validated in Greimel et al. (2022, accepted with minor revisions). The identification of associated events is based on the event detection implemented in 'hydropeak'. "}, "notifyme": {"categories": ["WebTechnologies"], "description": "Functions to flash your hue lights, or text yourself, from R. Designed to be used with long running scripts."}, "Brobdingnag": {"categories": ["NumericalMathematics"], "description": "Very large numbers in R.  Real numbers are held\n        using their natural logarithms, plus a logical flag indicating\n        sign.  Functionality for complex numbers is also provided.  The\n        package includes a vignette that gives a step-by-step\n        introduction to using S4 methods."}, "bujar": {"categories": ["Survival"], "description": "Buckley-James regression for right-censoring survival data with high-dimensional covariates. Implementations for survival data include boosting with componentwise linear least squares, componentwise smoothing splines, regression trees and MARS. Other high-dimensional tools include penalized regression for survival data. See Wang and Wang (2010) <doi:10.2202/1544-6115.1550>."}, "mipfp": {"categories": ["OfficialStatistics"], "description": "An implementation of the iterative proportional fitting (IPFP), \n    maximum likelihood, minimum chi-square and weighted least squares procedures\n    for updating a N-dimensional array with respect to given target marginal \n    distributions (which, in turn can be multidimensional). The package also\n    provides an application of the IPFP to simulate multivariate Bernoulli\n    distributions."}, "EpiEstim": {"categories": ["Epidemiology"], "description": "Tools to quantify transmissibility throughout\n    an epidemic from the analysis of time series of incidence as described in\n    Cori et al. (2013) <doi:10.1093/aje/kwt133> and Wallinga and Teunis (2004) \n    <doi:10.1093/aje/kwh255>."}, "TAM": {"categories": ["MissingData", "Psychometrics"], "description": "\n    Includes marginal maximum likelihood estimation and joint maximum\n    likelihood estimation for unidimensional and multidimensional \n    item response models. The package functionality covers the \n    Rasch model, 2PL model, 3PL model, generalized partial credit model, \n    multi-faceted Rasch model, nominal item response model, \n    structured latent class model, mixture distribution IRT models, \n    and located latent class models. Latent regression models and \n    plausible value imputation are also supported. For details see\n    Adams, Wilson and Wang, 1997 <doi:10.1177/0146621697211001>,\n    Adams, Wilson and Wu, 1997 <doi:10.3102/10769986022001047>,\n    Formann, 1982 <doi:10.1002/bimj.4710240209>,\n    Formann, 1992 <doi:10.1080/01621459.1992.10475229>."}, "VARDetect": {"categories": ["TimeSeries"], "description": "Implementations of Thresholded Block Segmentation Scheme (TBSS) and Low-rank plus Sparse Two Step Procedure (LSTSP) algorithms for detecting multiple changes in structural VAR models. The package aims to address the problem of change point detection in piece-wise stationary VAR models, under different settings regarding the structure of their transition matrices (autoregressive dynamics); specifically, the following cases are included: (i) (weakly) sparse, (ii) structured sparse, and (iii) low rank plus sparse. It includes multiple algorithms and related extensions from Safikhani and Shojaie (2020) <doi:10.1080/01621459.2020.1770097> and Bai, Safikhani and Michailidis (2020) <doi:10.1109/TSP.2020.2993145>."}, "ExceedanceTools": {"categories": ["Spatial"], "description": "Provides methods for constructing confidence or credible regions\n    for exceedance sets and contour lines."}, "LWFBrook90R": {"categories": ["Hydrology"], "description": "Provides a flexible and easy-to use interface for the soil vegetation \n    atmosphere transport (SVAT) model LWF-BROOK90, written in Fortran.\n    The model simulates daily transpiration, interception, soil and snow evaporation, \n    streamflow and soil water fluxes through a soil profile covered with vegetation, \n    as described in Hammel & Kennel (2001, ISBN:978-3-933506-16-0) and Federer et al. (2003) \n    <doi:10.1175/1525-7541(2003)004%3C1276:SOAETS%3E2.0.CO;2>. A set of high-level functions\n    for model set up, execution and parallelization provides easy access to plot-level SVAT \n    simulations, as well as multi-run and large-scale applications. "}, "jqr": {"categories": ["WebTechnologies"], "description": "Client for 'jq', a 'JSON' processor (<https://stedolan.github.io/jq/>), \n    written in C. 'jq' allows the following with 'JSON' data: index into, parse, \n    do calculations, cut up and filter, change key names and values, perform \n    conditionals and comparisons, and more."}, "clustMixType": {"categories": ["Cluster"], "description": "Functions to perform k-prototypes partitioning clustering for\n    mixed variable-type data according to Z.Huang (1998): Extensions to the k-Means\n    Algorithm for Clustering Large Data Sets with Categorical Variables, Data Mining\n    and Knowledge Discovery 2, 283-304, <doi:10.1023/A:1009769707641>."}, "CovidMutations": {"categories": ["Epidemiology"], "description": "A feasible framework for mutation analysis and reverse transcription \n  polymerase chain reaction (RT-PCR) assay evaluation of COVID-19, including \n  mutation profile visualization, statistics and mutation ratio of each assay. \n  The mutation ratio is conducive to evaluating the coverage of RT-PCR assays in \n  large-sized samples. Mercatelli, D. and Giorgi, F. M. (2020) \n  <doi:10.20944/preprints202004.0529.v1>."}, "minqa": {"categories": ["Optimization"], "description": "Derivative-free optimization by quadratic approximation\n        based on an interface to Fortran implementations by M. J. D.\n        Powell."}, "RootsExtremaInflections": {"categories": ["NumericalMathematics"], "description": "Implementation of Taylor Regression Estimator (TRE), \n   Tulip Extreme Finding Estimator (TEFE), Bell Extreme Finding Estimator (BEFE),\n   Integration Extreme Finding Estimator (IEFE) and \n   Integration Root Finding Estimator (IRFE) for roots, extrema and inflections of a curve .     \n   Christopoulos, DT (2019) <doi:10.13140/RG.2.2.17158.32324> .\n   Christopoulos, DT (2016) <doi:10.2139/ssrn.3043076> .\n   Christopoulos, DT (2016) <https://veltech.edu.in/wp-content/uploads/2016/04/Paper-04-2016.pdf> .\n   Christopoulos, DT (2014) <arXiv:1206.5478v2 [math.NA]> ."}, "ECLRMC": {"categories": ["MissingData"], "description": "Ensemble correlation-based low-rank matrix completion method (ECLRMC) is an extension to the LRMC based methods. Traditionally, the LRMC based methods give identical importance to the whole data which results in emphasizing on the commonality of the data and overlooking the subtle but crucial differences. This method aims to overcome the equality assumption problem that exists in the current LRMS based methods. Ensemble correlation-based low-rank matrix completion (ECLRMC) takes consideration of the specific characteristic of each sample and performs LRMC on the set of samples with a strong correlation. It uses an ensemble learning method to improve the imputation performance. Since each sample is analyzed independently this method can be parallelized by distributing imputation across many computation units or GPU platforms. This package provides three different methods (LRMC, CLRMC and ECLRMC) for data imputation. There is also an NRMS function for evaluating the result. Chen, Xiaobo, et al (2017) <doi:10.1016/j.knosys.2017.06.010>."}, "modelbased": {"categories": ["CausalInference"], "description": "Implements a general interface for model-based estimations\n    for a wide variety of models (see list of supported models using the\n    function 'insight::supported_models()'), used in the computation of\n    marginal means, contrast analysis and predictions."}, "qpNCA": {"categories": ["Pharmacokinetics"], "description": "Computes noncompartmental pharmacokinetic parameters\n for drug concentration profiles.  For each profile, data\n imputations and adjustments are made as necessary and\n basic parameters are estimated. Supports single dose, multi-dose,\n and multi-subject data.  Supports steady-state calculations\n and various routes of drug administration. See ?qpNCA and vignettes.\n Methodology follows Rowland and Tozer (2011, ISBN:978-0-683-07404-8), \n Gabrielsson and Weiner (1997, ISBN:978-91-9765-100-4), and\n Gibaldi and Perrier (1982, ISBN:978-0824710422)."}, "getmstatistic": {"categories": ["MetaAnalysis"], "description": "Quantifying systematic heterogeneity in meta-analysis using R.\n    The M statistic aggregates heterogeneity information across multiple\n    variants to, identify systematic heterogeneity patterns and their direction\n    of effect in meta-analysis. It's primary use is to identify outlier studies,\n    which either show \"null\" effects or consistently show stronger or weaker\n    genetic effects than average across, the panel of variants examined in a\n    GWAS meta-analysis. In contrast to conventional heterogeneity metrics\n    (Q-statistic, I-squared and tau-squared) which measure random heterogeneity\n    at individual variants, M measures systematic (non-random)\n    heterogeneity across multiple independently associated variants. Systematic\n    heterogeneity can arise in a meta-analysis due to differences in the study\n    characteristics of participating studies. Some of the differences may\n    include: ancestry, allele frequencies, phenotype definition, age-of-disease\n    onset, family-history, gender, linkage disequilibrium and quality control\n    thresholds. See <https://magosil86.github.io/getmstatistic/> for statistical\n    statistical theory, documentation and examples."}, "JumpeR": {"categories": ["SportsAnalytics"], "description": "Primarily used to convert human readable track and field results into dataframes for further analysis.  Results can come from central repositories like <https://www.flashresults.com/> or <http://www.deltatiming.com/>, or from individual team sites, like those for colleges.   Also contains functions useful for working with track and field data."}, "profileR": {"categories": ["Psychometrics"], "description": "A suite of multivariate methods and data visualization \n      tools to implement profile analysis and cross-validation techniques described in \n      Davison & Davenport (2002) <doi:10.1037/1082-989X.7.4.468>, Bulut (2013), and other published and unpublished resources.\n      The package includes routines to perform criterion-related profile analysis, profile analysis \n      via multidimensional scaling, moderated profile analysis, profile analysis by group, and a \n      within-person factor model to derive score profiles."}, "allestimates": {"categories": ["CausalInference"], "description": "Estimates and plots effect estimates from models with all possible \n    combinations of a list of variables. It can be used for assessing treatment \n    effects in clinical trials or risk factors in bio-medical and epidemiological \n    research. Like Stata command 'confall' (Wang Z. Stata Journal 2007; 7, Number 2, pp. 183\u2013196), \n    'allestimates' calculates and stores all effect estimates, and plots them against p values or \n    Akaike information criterion (AIC) values. It currently has functions for linear \n    regression: all_lm(), logistic and Poisson regression: all_glm() and all_speedglm(), \n    and Cox proportional hazards regression: all_cox(). "}, "cOde": {"categories": ["DifferentialEquations"], "description": "Generates all necessary C functions allowing the user to work with\n    the compiled-code interface of ode() and bvptwp(). The implementation supports\n    \"forcings\" and \"events\". Also provides functions to symbolically compute\n    Jacobians, sensitivity equations and adjoint sensitivities being the basis for\n    sensitivity analysis."}, "ramps": {"categories": ["Bayesian", "Spatial"], "description": "Bayesian geostatistical modeling of Gaussian processes using a\n    reparameterized and marginalized posterior sampling (RAMPS) algorithm\n    designed to lower autocorrelation in MCMC samples.  Package performance is\n    tuned for large spatial datasets."}, "LambertW": {"categories": ["Distributions"], "description": "Lambert W x F distributions are a generalized framework to analyze\n    skewed, heavy-tailed data. It is based on an input/output system, where the\n    output random variable (RV) Y is a non-linearly transformed version of an input\n    RV X ~ F with similar properties as X, but slightly skewed (heavy-tailed).\n    The transformed RV Y has a Lambert W x F distribution. This package contains\n    functions to model and analyze skewed, heavy-tailed data the Lambert Way:\n    simulate random samples, estimate parameters, compute quantiles, and plot/\n    print results nicely. The most useful function is 'Gaussianize',\n    which works similarly to 'scale', but actually makes the data Gaussian.\n    A do-it-yourself toolkit allows users to define their own Lambert W x\n    'MyFavoriteDistribution' and use it in their analysis right away."}, "targets": {"categories": ["HighPerformanceComputing", "ReproducibleResearch"], "description": "As a pipeline toolkit for Statistics and data science in R,\n  the 'targets' package brings together function-oriented programming and\n  'Make'-like declarative workflows.\n  It analyzes the dependency relationships among the tasks of a workflow,\n  skips steps that are already up to date, runs the necessary\n  computation with optional parallel workers, abstracts files as\n  R objects, and provides tangible evidence that the results match\n  the underlying code and data. The methodology in this package\n  borrows from GNU 'Make' (2015, ISBN:978-9881443519)\n  and 'drake' (2018, <doi:10.21105/joss.00550>)."}, "tsibble": {"categories": ["MissingData", "TimeSeries"], "description": "Provides a 'tbl_ts' class (the 'tsibble') for\n    temporal data in an data- and model-oriented format. The 'tsibble'\n    provides tools to easily manipulate and analyse temporal data, such as\n    filling in time gaps and aggregating over calendar periods."}, "prodigenr": {"categories": ["ReproducibleResearch"], "description": "Create a project directory structure, along with typical files\n    for that project.  This allows projects to be quickly and easily created,\n    as well as for them to be standardized. Designed specifically with scientists\n    in mind (mainly bio-medical researchers, but likely applies to other fields)."}, "pdfetch": {"categories": ["TimeSeries"], "description": "Download economic and financial time series from public sources, \n  including the St Louis Fed's FRED system, Yahoo Finance, the US Bureau of Labor Statistics, \n  the US Energy Information Administration, the World Bank, Eurostat, the European Central Bank,\n  the Bank of England, the UK's Office of National Statistics, Deutsche Bundesbank, and INSEE."}, "spdep": {"categories": ["Spatial"], "description": "A collection of functions to create spatial weights matrix\n  objects from polygon 'contiguities', from point patterns by distance and\n  tessellations, for summarizing these objects, and for permitting their\n  use in spatial data analysis, including regional aggregation by minimum\n  spanning tree; a collection of tests for spatial 'autocorrelation',\n  including global 'Morans I' and 'Gearys C' proposed by 'Cliff' and 'Ord'\n  (1973, ISBN: 0850860369) and (1981, ISBN: 0850860814), 'Hubert/Mantel'\n  general cross product statistic, Empirical Bayes estimates and\n  'Assun\u00e7\u00e3o/Reis' (1999) <doi:10.1002/(SICI)1097-0258(19990830)18:16%3C2147::AID-SIM179%3E3.0.CO;2-I> Index, 'Getis/Ord' G ('Getis' and 'Ord' 1992)\n  <doi:10.1111/j.1538-4632.1992.tb00261.x> and multicoloured\n  join count statistics, 'APLE' ('Li 'et al.' )\n  <doi:10.1111/j.1538-4632.2007.00708.x>, local 'Moran's I', 'Gearys C' \n  ('Anselin' 1995) <doi:10.1111/j.1538-4632.1995.tb00338.x> and\n  'Getis/Ord' G ('Ord' and 'Getis' 1995)\n  <doi:10.1111/j.1538-4632.1995.tb00912.x>,\n  'saddlepoint' approximations ('Tiefelsdorf' 2002)\n  <doi:10.1111/j.1538-4632.2002.tb01084.x> and exact tests\n  for global and local 'Moran's I' ('Bivand et al.' 2009)\n  <doi:10.1016/j.csda.2008.07.021> and 'LOSH' local indicators\n  of spatial heteroscedasticity ('Ord' and 'Getis')\n  <doi:10.1007/s00168-011-0492-y>. The implementation of most of\n  the measures is described in 'Bivand' and 'Wong' (2018)\n  <doi:10.1007/s11749-018-0599-x>, with further extensions in 'Bivand' (2022)\n  <doi:10.1111/gean.12319>.\n  From 'spdep' and 'spatialreg' versions >= 1.2-1, the model fitting functions\n  previously present in this package are defunct in 'spdep' and may be found\n  in 'spatialreg'."}, "RProtoBuf": {"categories": ["HighPerformanceComputing"], "description": "Protocol Buffers are a way of encoding structured data in an\n efficient yet extensible format. Google uses Protocol Buffers for almost all\n of its internal 'RPC' protocols and file formats.  Additional documentation\n is available in two included vignettes one of which corresponds to our 'JSS'\n paper (2016, <doi:10.18637/jss.v071.i02>. A sufficiently recent version of\n 'Protocol Buffers' library is required; currently version 3.3.0 from 2017\n is the stated minimum."}, "OPDOE": {"categories": ["ExperimentalDesign"], "description": "Several function related to Experimental Design are implemented here, see\n  \"Optimal Experimental Design with R\" by Rasch D. et. al (ISBN 9781439816974)."}, "knitr": {"categories": ["ReproducibleResearch"], "description": "Provides a general-purpose tool for dynamic report generation in R\n    using Literate Programming techniques."}, "MOTE": {"categories": ["MetaAnalysis"], "description": "Measure of the Effect ('MOTE') is an effect size calculator, including a \n    wide variety of effect sizes in the mean differences family (all versions of d) and \n    the variance overlap family (eta, omega, epsilon, r). 'MOTE' provides non-central \n    confidence intervals for each effect size, relevant test statistics, and output \n    for reporting in APA Style (American Psychological Association, 2010, \n    <ISBN:1433805618>) with 'LaTeX'. In research, an over-reliance on p-values \n    may conceal the fact that a study is under-powered (Halsey, Curran-Everett, \n    Vowler, & Drummond, 2015 <doi:10.1038/nmeth.3288>). A test may be statistically \n    significant, yet practically inconsequential (Fritz, Scherndl, & K\u00fchberger, 2012 \n    <doi:10.1177/0959354312436870>). Although the American Psychological Association \n    has long advocated for the inclusion of effect sizes (Wilkinson & American \n    Psychological Association Task Force on Statistical Inference, 1999 \n    <doi:10.1037/0003-066X.54.8.594>), the vast majority of peer-reviewed, \n    published academic studies stop short of reporting effect sizes and confidence \n    intervals (Cumming, 2013, <doi:10.1177/0956797613504966>). 'MOTE' simplifies \n    the use and interpretation of effect sizes and confidence intervals. For more \n    information, visit <https://www.aggieerin.com/shiny-server>."}, "mixAK": {"categories": ["Cluster", "Survival"], "description": "Contains a mixture of statistical methods including the MCMC methods to analyze normal mixtures. Additionally, model based clustering methods are implemented to perform classification based on (multivariate) longitudinal (or otherwise correlated) data. The basis for such clustering is a mixture of multivariate generalized linear mixed models."}, "interp": {"categories": ["NumericalMathematics", "Spatial"], "description": "Bivariate data interpolation on regular and irregular\n  grids, either linear or using splines are the main part of this\n  package.  It is intended to provide FOSS replacement functions for\n  the ACM licensed akima::interp and tripack::tri.mesh functions.\n  Linear interpolation is implemented in \n  interp::interp(..., method=\"linear\"), this corresponds to the call \n  akima::interp(..., linear=TRUE) which is the default setting and \n  covers most of akima::interp use cases in depending packages.  \n  A re-implementation of Akimas irregular grid spline\n  interpolation (akima::interp(..., linear=FALSE)) is now also\n  available via interp::interp(..., method=\"akima\").\n  Estimators for partial derivatives are now also available in \n  interp::locpoly(), these are a prerequisite for the spline interpolation.  \n  The basic part is a GPLed triangulation algorithm (sweep hull \n  algorithm by David Sinclair) providing the starting point for the\n  irregular grid interpolator. As side effect this algorithm is also\n  used to provide replacements for almost all functions of the tripack\n  package which also suffers from the same ACM license restrictions.  \n  All functions are designed to be backward compatible with their \n  akima / tripack counterparts."}, "Sleuth3": {"categories": ["TeachingStatistics"], "description": "Data sets from Ramsey, F.L. and Schafer, D.W. (2013), \"The\n    Statistical Sleuth: A Course in Methods of Data Analysis (3rd\n    ed)\", Cengage Learning. "}, "cointReg": {"categories": ["TimeSeries"], "description": "Cointegration methods are widely used in empirical macroeconomics\n    and empirical finance. It is well known that in a cointegrating\n    regression the ordinary least squares (OLS) estimator of the\n    parameters is super-consistent, i.e. converges at rate equal to the\n    sample size T. When the regressors are endogenous, the limiting\n    distribution of the OLS estimator is contaminated by so-called second\n    order bias terms, see e.g. Phillips and Hansen (1990) <doi:10.2307/2297545>.\n    The presence of these bias terms renders inference difficult. Consequently,\n    several modifications to OLS that lead to zero mean Gaussian mixture\n    limiting distributions have been proposed, which in turn make\n    standard asymptotic inference feasible. These methods include\n    the fully modified OLS (FM-OLS) approach of Phillips and Hansen\n    (1990) <doi:10.2307/2297545>, the dynamic OLS (D-OLS) approach of Phillips\n    and Loretan (1991) <doi:10.2307/2298004>, Saikkonen (1991)\n    <doi:10.1017/S0266466600004217> and Stock and Watson (1993)\n    <doi:10.2307/2951763> and the new estimation approach called integrated\n    modified OLS (IM-OLS) of Vogelsang and Wagner (2014)\n    <doi:10.1016/j.jeconom.2013.10.015>. The latter is based on an augmented\n    partial sum (integration) transformation of the regression model. IM-OLS is\n    similar in spirit to the FM- and D-OLS approaches, with the key difference\n    that it does not require estimation of long run variance matrices and avoids\n    the need to choose tuning parameters (kernels, bandwidths, lags). However,\n    inference does require that a long run variance be scaled out.\n    This package provides functions for the parameter estimation and inference\n    with all three modified OLS approaches. That includes the automatic\n    bandwidth selection approaches of Andrews (1991) <doi:10.2307/2938229> and\n    of Newey and West (1994) <doi:10.2307/2297912> as well as the calculation of\n    the long run variance."}, "OptimaRegion": {"categories": ["ExperimentalDesign"], "description": "Computes confidence regions on the location of response surface optima."}, "DICOMread": {"categories": ["MedicalImaging"], "description": "This function provides an interface between 'Matlab' and 'R' in facilitating fast processing for reading and saving DICOM images."}, "SCI": {"categories": ["Distributions", "Hydrology"], "description": "Functions for generating Standardized Climate Indices (SCI). \n\t\tSCI is a transformation of (smoothed) climate (or environmental)\n\t\ttime series that  removes seasonality and forces the data to\n\t\ttake values of the standard normal distribution. SCI was \n\t\toriginally developed for precipitation. In this case it is \n\t\tknown as the Standardized Precipitation Index (SPI)."}, "StatMatch": {"categories": ["MissingData", "OfficialStatistics"], "description": "Integration of two data sources referred to the same target population which share a number of variables. Some functions can also be used to impute missing values in data sets through hot deck imputation methods. Methods to perform statistical matching when dealing  with data from complex sample surveys are available too."}, "minimalRSD": {"categories": ["ExperimentalDesign"], "description": "Generate central composite designs (CCD)with full as well \n    as fractional factorial points (half replicate) and Box Behnken \n    designs (BBD) with minimally changed run sequence."}, "sampleSelection": {"categories": ["CausalInference", "Econometrics"], "description": "Two-step\n   and maximum likelihood estimation\n   of Heckman-type sample selection models:\n   standard sample selection models (Tobit-2),\n   endogenous switching regression models (Tobit-5),\n   sample selection models with binary dependent outcome variable,\n   interval regression with sample selection (only ML estimation),\n   and endogenous treatment effects models.\n   These methods are described in the three vignettes\n   that are included in this package \n   and in econometric textbooks such as\n   Greene (2011, Econometric Analysis, 7th edition, Pearson)."}, "RSKC": {"categories": ["Robust"], "description": "This RSKC package contains a function RSKC which runs the robust sparse K-means clustering algorithm."}, "urltools": {"categories": ["WebTechnologies"], "description": "A toolkit for all URL-handling needs, including encoding and decoding,\n    parsing, parameter extraction and modification. All functions are\n    designed to be both fast and entirely vectorised. It is intended to be\n    useful for people dealing with web-related datasets, such as server-side\n    logs, although may be useful for other situations involving large sets of\n    URLs."}, "Compositional": {"categories": ["Distributions"], "description": "Regression, classification, contour plots, hypothesis testing and fitting of distributions for compositional data are some of the functions included. \n             The standard textbook for such data is John Aitchison's (1986) \"The statistical analysis of compositional data\". Relevant papers include:\n\t\t\t       a) Tsagris M.T., Preston S. and Wood A.T.A. (2011). A data-based power transformation for compositional data. Fourth International International Workshop on Compositional Data Analysis. \n\t\t\t       b) Tsagris M. (2014). The k-NN algorithm for compositional data: a revised approach with and without zero values present. Journal of Data Science, 12(3):519\u2013534. \n\t\t\t       c) Tsagris M. (2015). A novel, divergence based, regression for compositional data. Proceedings of the 28th Panhellenic Statistics Conference, 15-18 April 2015, Athens, Greece, 430\u2013444. \n\t\t\t       d) Tsagris M. (2015). Regression analysis with compositional data containing zero values. Chilean Journal of Statistics, 6(2):47\u201357. \n\t\t\t       e) Tsagris M., Preston S. and Wood A.T.A. (2016). Improved supervised classification for compositional data using the alpha-transformation. Journal of Classification, 33(2):243\u2013261. <doi:10.1007/s00357-016-9207-5>. \n\t\t\t       f) Tsagris M., Preston S. and Wood A.T.A. (2017). Nonparametric hypothesis testing for equality of means on the simplex. Journal of Statistical Computation and Simulation, 87(2): 406\u2013422. <doi:10.1080/00949655.2016.1216554>. \n\t\t\t       g) Tsagris M. and Stewart C. (2018). A Dirichlet regression model for compositional data with zeros. Lobachevskii Journal of Mathematics, 39(3): 398\u2013412. <doi:10.1134/S1995080218030198>. \n\t\t\t       h) Alenazi A. (2019). Regression for compositional data with compositional data as predictor variables with or without zero values. Journal of Data Science, 17(1): 219\u2013238. <doi:10.6339/JDS.201901_17(1).0010>. \n\t\t         i) Tsagris M. and Stewart C. (2020). A folded model for compositional data analysis. Australian and New Zealand Journal of Statistics, 62(2):249\u2013277. <doi:10.1111/anzs.12289>. \n\t\t\t       j) Tsagris M., Alenazi A. and Stewart C. (2021). Non-parametric regression models for compositional data. <arXiv:2002.05137>. \n             k) Alenazi A. (2021). Alenazi, A. (2021). A review of compositional data analysis and recent advances. Communications in Statistics-Theory and Methods (Accepted for publication). <doi:10.1080/03610926.2021.2014890>.\n\t\t\t       We further include functions for percentages (or proportions)."}, "GPArotation": {"categories": ["Psychometrics"], "description": "Gradient Projection Algorithm Rotation for Factor Analysis. See '?GPArotation.Intro' for more details."}, "dclone": {"categories": ["Bayesian", "GraphicalModels", "HighPerformanceComputing", "Optimization"], "description": "Low level functions for implementing\n    maximum likelihood estimating procedures for\n    complex models using data cloning and Bayesian\n    Markov chain Monte Carlo methods\n    as described in Solymos 2010 (R Journal 2(2):29\u201337).\n    Sequential and parallel MCMC support\n    for 'JAGS', 'WinBUGS', 'OpenBUGS', and 'Stan'."}, "minpack.lm": {"categories": ["ChemPhys", "Optimization"], "description": "The nls.lm function provides an R interface to lmder and lmdif from the MINPACK library, for solving nonlinear least-squares problems by a modification of the Levenberg-Marquardt algorithm, with support for lower and upper parameter bounds.  The implementation can be used via nls-like calls using the nlsLM function.  "}, "fclust": {"categories": ["Cluster"], "description": "Algorithms for fuzzy clustering, cluster validity indices and plots for cluster validity and visualizing fuzzy clustering results."}, "spectralGraphTopology": {"categories": ["GraphicalModels"], "description": "In the era of big data and hyperconnectivity, learning\n    high-dimensional structures such as graphs from data has become a prominent\n    task in machine learning and has found applications in many fields such as\n    finance, health care, and networks. 'spectralGraphTopology' is an open source,\n    documented, and well-tested R package for learning graphs from data. It\n    provides implementations of state of the art algorithms such as Combinatorial\n    Graph Laplacian Learning (CGL), Spectral Graph Learning (SGL), Graph Estimation\n    based on Majorization-Minimization (GLE-MM), and Graph Estimation based on\n    Alternating Direction Method of Multipliers (GLE-ADMM). In addition, graph\n    learning has been widely employed for clustering, where specific algorithms\n    are available in the literature. To this end, we provide an implementation of\n    the Constrained Laplacian Rank (CLR) algorithm."}, "nlme": {"categories": ["ChemPhys", "Econometrics", "Environmetrics", "Finance", "OfficialStatistics", "Psychometrics", "Spatial", "SpatioTemporal"], "description": "Fit and compare Gaussian linear and nonlinear mixed-effects models."}, "nflplotR": {"categories": ["SportsAnalytics"], "description": "A set of functions to visualize National Football League\n    analysis in 'ggplot2'."}, "polynom": {"categories": ["NumericalMathematics"], "description": "A collection of functions to implement a class for univariate\n  polynomial manipulations."}, "NbClust": {"categories": ["Cluster"], "description": "It provides 30 indexes for determining the optimal number of clusters in a data set and offers the best clustering scheme from different results to the user. "}, "onnx": {"categories": ["ModelDeployment"], "description": "R Interface to 'ONNX' - Open Neural Network Exchange <https://onnx.ai/>. \n             'ONNX' provides an open source format for machine learning models. \n             It defines an extensible computation graph model, as well as definitions\n             of built-in operators and standard data types."}, "carx": {"categories": ["TimeSeries"], "description": "A censored time series class is designed. An estimation procedure\n    is implemented to estimate the Censored AutoRegressive time series with\n    eXogenous covariates (CARX), assuming normality of the innovations. Some other\n    functions that might be useful are also included."}, "SPEI": {"categories": ["Hydrology"], "description": "A set of functions for computing potential evapotranspiration and several widely used drought indices including the Standardized Precipitation-Evapotranspiration Index (SPEI)."}, "ThreeArmedTrials": {"categories": ["CausalInference", "ClinicalTrials", "ExperimentalDesign"], "description": "Design and analyze three-arm non-inferiority or superiority trials\n    which follow a gold-standard design, i.e. trials with an experimental treatment,\n    an active, and a placebo control. Method for the following distributions are implemented:\n    Poisson (Mielke and Munk (2009) <arXiv:0912.4169>), negative binomial (Muetze et al. (2016) <doi:10.1002/sim.6738>),\n    normal (Pigeot et al. (2003) <doi:10.1002/sim.1450>; Hasler et al. (2009) <doi:10.1002/sim.3052>), \n    binary (Friede and Kieser (2007) <doi:10.1002/sim.2543>), nonparametric (Muetze et al. (2017) <doi:10.1002/sim.7176>),\n    exponential (Mielke and Munk (2009) <arXiv:0912.4169>)."}, "PowerTOST": {"categories": ["ClinicalTrials"], "description": "Contains functions to calculate power and sample size for\n    various study designs used in bioequivalence studies. Use known.designs() to\n    see the designs supported. Power and sample size can be obtained based on\n    different methods, amongst them prominently the TOST procedure (two one-sided\n    t-tests). See README and NEWS for further information."}, "sym.arma": {"categories": ["TimeSeries"], "description": "Functions for fitting the Autoregressive and Moving Average Symmetric Model for univariate time series introduced by Maior and Cysneiros (2018), <doi:10.1007/s00362-016-0753-z>. Fitting method: conditional maximum likelihood estimation. For details see: Wei (2006), Time Series Analysis: Univariate and Multivariate Methods, Section 7.2."}, "analogue": {"categories": ["Environmetrics"], "description": "Fits Modern Analogue Technique and Weighted Averaging transfer\n  \t     function models for prediction of environmental data from species\n\t     data, and related methods used in palaeoecology."}, "flexiblas": {"categories": ["HighPerformanceComputing"], "description": "Provides functions to switch the 'BLAS'/'LAPACK' optimized backend\n    and change the number of threads without leaving the R session, which needs\n    to be linked against the 'FlexiBLAS' wrapper library\n    <https://www.mpi-magdeburg.mpg.de/projects/flexiblas>."}, "tfio": {"categories": ["Databases"], "description": "Interface to 'TensorFlow IO', Datasets and filesystem extensions maintained by 'TensorFlow SIG-IO' <https://github.com/tensorflow/community/blob/master/sigs/io/CHARTER.md>."}, "RNiftyReg": {"categories": ["MedicalImaging"], "description": "Provides an 'R' interface to the 'NiftyReg' image registration tools\n    <https://github.com/KCL-BMEIS/niftyreg>. Linear and nonlinear registration\n    are supported, in two and three dimensions."}, "qpmadr": {"categories": ["Optimization"], "description": "Efficiently solve quadratic problems with linear inequality, equality and box constraints. The method used is outlined in D. Goldfarb, and A. Idnani (1983) <doi:10.1007/BF02591962>."}, "bspec": {"categories": ["Bayesian", "TimeSeries"], "description": "Bayesian inference on the (discrete) power spectrum of time series."}, "ssanv": {"categories": ["ClinicalTrials"], "description": "A set of functions to calculate sample size for two-sample difference in means tests. Does adjustments for either nonadherence or variability that comes from using data to estimate parameters."}, "HistogramTools": {"categories": ["Distributions", "HighPerformanceComputing"], "description": "Provides a number of utility functions useful for manipulating large histograms.  This includes methods to trim, subset, merge buckets, merge histograms, convert to CDF, and calculate information loss due to binning.  It also provides a protocol buffer representations of the default R histogram class to allow histograms over large data sets to be computed and manipulated in a MapReduce environment."}, "trustOptim": {"categories": ["Optimization"], "description": "Trust region algorithm for nonlinear optimization. Efficient when\n    the Hessian of the objective function is sparse (i.e., relatively few nonzero\n    cross-partial derivatives). See Braun, M. (2014) <doi:10.18637/jss.v060.i04>."}, "LinRegInteractive": {"categories": ["Econometrics"], "description": "Interactive visualization of effects, response functions \n    and marginal effects for different kinds of regression models. In this version \n    linear regression models, generalized linear models, generalized additive\n    models and linear mixed-effects models are supported.  \n    Major features are the interactive approach and the handling of the effects of categorical covariates: \n    if two or more factors are used as covariates every combination of the levels of each \n    factor is treated separately. The automatic calculation of \n    marginal effects and a number of possibilities to customize the graphical output \n    are useful features as well."}, "rmapshaper": {"categories": ["Spatial"], "description": "Edit and simplify 'geojson', 'Spatial', and 'sf'\n    objects.  This is wrapper around the 'mapshaper' 'JavaScript' library\n    by Matthew Bloch <https://github.com/mbloch/mapshaper/> to perform\n    topologically-aware polygon simplification, as well as other\n    operations such as clipping, erasing, dissolving, and converting\n    'multi-part' to 'single-part' geometries.  It relies on the\n    'geojsonio' package for working with 'geojson' objects, the 'sf'\n    package for working with 'sf' objects, and the 'sp' and 'rgdal'\n    packages for working with 'Spatial' objects."}, "poibin": {"categories": ["Distributions"], "description": "Implementation of both the exact and approximation methods for computing the cdf of the Poisson binomial distribution as described in Hong (2013) <doi:10.1016/j.csda.2012.10.006>. It also provides the pmf, quantile function, and random number generation for the Poisson binomial distribution. The C code for fast Fourier transformation (FFT) is written by R Core Team (2019)<https://www.R-project.org/>, which implements the FFT algorithm in Singleton (1969) <doi:10.1109/TAU.1969.1162042>."}, "publipha": {"categories": ["MetaAnalysis"], "description": "Tools for Bayesian estimation of meta-analysis models that account \n    for publications bias or p-hacking. For publication bias, this package \n    implements a variant of the p-value based selection model of Hedges (1992)\n    <doi:10.1214/ss/1177011364> with discrete selection probabilities. It also \n    implements the mixture of truncated normals model for p-hacking described \n    in Moss and De Bin (2019) <arXiv:1911.12445>."}, "dblcens": {"categories": ["Survival"], "description": "Use EM algorithm to compute the NPMLE of CDF and also the\n        two censoring distributions. For doubly censored data (as\n        described in Chang and Yang (1987) Ann. Stat. 1536-47). You can\n        also specify a constraint, it will return the constrained NPMLE\n        and the -2 log empirical likelihood ratio. This can be used to\n        test the hypothesis about the constraint and find confidence\n        intervals for probability or quantile via empirical likelihood\n        ratio theorem. Influence function of hat F may also be\n        calculated (but may be slow)."}, "kfigr": {"categories": ["ReproducibleResearch"], "description": "A streamlined cross-referencing system for R Markdown documents\n    generated with 'knitr'. R Markdown is  an authoring format for generating\n    dynamic content from R. 'kfigr' provides a hook for anchoring code\n    chunks and a function to cross-reference document elements generated from\n    said chunks, e.g. figures and tables."}, "pseudo": {"categories": ["Survival"], "description": "Various functions for computing pseudo-observations for censored data regression. Computes pseudo-observations for modeling: competing risks based on the cumulative incidence function, survival function based on the restricted mean,  survival function based on the Kaplan-Meier estimator see Klein et al. (2008) <doi:10.1016/j.cmpb.2007.11.017>. "}, "furrr": {"categories": ["HighPerformanceComputing"], "description": "Implementations of the family of map() functions from 'purrr'\n    that can be resolved using any 'future'-supported backend, e.g.\n    parallel on the local machine or distributed on a compute cluster."}, "localsolver": {"categories": ["Optimization"], "description": "The package converts R data onto input and data for LocalSolver,\n    executes optimization and exposes optimization results as R data.\n    LocalSolver (http://www.localsolver.com/) is an optimization engine\n    developed by Innovation24 (http://www.innovation24.fr/). It is designed to\n    solve large-scale mixed-variable non-convex optimization problems.  The\n    localsolver package is developed and maintained by WLOG Solutions\n    (http://www.wlogsolutions.com/en/) in collaboration with Decision Support\n    and Analysis Division at Warsaw School of Economics\n    (http://www.sgh.waw.pl/en/)."}, "SPADAR": {"categories": ["ChemPhys"], "description": "Provides easy to use functions to create all-sky grid plots of widely used astronomical coordinate systems (equatorial, ecliptic, galactic) and scatter plots of data on any of these systems including on-the-fly system conversion. It supports any type of spherical projection to the plane defined by the 'mapproj' package."}, "CEoptim": {"categories": ["Optimization"], "description": "Optimization solver based on the Cross-Entropy method."}, "whisker": {"categories": ["WebTechnologies"], "description": "Implements 'Mustache' logicless templating."}, "multDM": {"categories": ["TimeSeries"], "description": "Allows to perform the multivariate version of the Diebold-Mariano test for equal predictive ability of multiple forecast comparison. Main reference: Mariano, R.S., Preve, D. (2012) <doi:10.1016/j.jeconom.2012.01.014>. "}, "risksetROC": {"categories": ["Survival"], "description": "Compute time-dependent Incident/dynamic accuracy measures\n        (ROC curve, AUC, integrated AUC )from censored survival data\n        under proportional or non-proportional hazard assumption of\n        Heagerty & Zheng (Biometrics, Vol 61 No 1, 2005, PP 92-105)."}, "saeSim": {"categories": ["OfficialStatistics"], "description": "Tools for the simulation of data in the context of small area\n    estimation. Combine all steps of your simulation - from data generation\n    over drawing samples to model fitting - in one object. This enables easy\n    modification and combination of different scenarios. You can store your\n    results in a folder or start the simulation in parallel."}, "garchx": {"categories": ["Finance"], "description": "Flexible and robust estimation and inference of generalised autoregressive conditional heteroscedasticity (GARCH) models with covariates ('X') based on the results by Francq and Thieu (2018) <doi:10.1017/S0266466617000512>. Coefficients can straightforwardly be set to zero by omission, and quasi maximum likelihood methods ensure estimates are generally consistent and inference valid, even when the standardised innovations are non-normal and/or dependent over time."}, "Matching": {"categories": ["CausalInference", "HighPerformanceComputing"], "description": "Provides functions for multivariate and propensity score matching \n             and for finding optimal balance based on a genetic search algorithm. \n             A variety of univariate and multivariate metrics to\n             determine if balance has been obtained are also provided. For\n             details, see the paper by Jasjeet Sekhon \n             (2007, <doi:10.18637/jss.v042.i07>)."}, "analogsea": {"categories": ["WebTechnologies"], "description": "Provides a set of functions for interacting with the 'Digital\n    Ocean' API <https://www.digitalocean.com/>, including\n    creating images, destroying them, rebooting, getting details on regions, and\n    available images."}, "DistributionUtils": {"categories": ["Distributions"], "description": "Utilities are provided which are of use in the\n    packages I have developed for dealing with\n    distributions. Currently these packages are GeneralizedHyperbolic,\n    VarianceGamma, and SkewHyperbolic and NormalLaplace. Each of these\n    packages requires DistributionUtils. Functionality includes sample\n    skewness and kurtosis, log-histogram, tail plots, moments by\n    integration, changing the point about which a moment is\n    calculated, functions for testing distributions using inversion\n    tests and the Massart inequality. Also includes an implementation\n    of the incomplete Bessel K function."}, "pcts": {"categories": ["TimeSeries"], "description": "Classes and methods for modelling and simulation of\n    periodically correlated (PC) and periodically integrated time\n    series.  Compute theoretical periodic autocovariances and related\n    properties of PC autoregressive moving average models. Some original\n    methods including Boshnakov & Iqelan (2009)\n    <doi:10.1111/j.1467-9892.2009.00617.x>, Boshnakov (1996)\n    <doi:10.1111/j.1467-9892.1996.tb00281.x>."}, "convergEU": {"categories": ["MissingData"], "description": "Indicators and measures by country and time describe\n    what happens at economic and social levels. This package provides\n    functions to calculate several measures of convergence after imputing\n    missing values. The automated downloading of Eurostat data,\n    followed by the production of country fiches and indicator fiches,\n    makes possible to produce automated reports.\n    The Eurofound report (<doi:10.2806/68012>)\n    \"Upward convergence in the EU: Concepts, measurements and indicators\", \n    2018, is a detailed  presentation of  convergence."}, "subscore": {"categories": ["Psychometrics"], "description": "Functions for computing test subscores using different\n    methods in both classical test theory (CTT) and item response theory (IRT). This\n    package enables three types of subscoring methods within the framework of CTT\n    and IRT, including (1) Wainer's augmentation method (Wainer et. al., 2001) \n    <doi:10.4324/9781410604729>, (2) Haberman's subscoring methods (Haberman, 2008) \n    <doi:10.3102/1076998607302636>, and (3) Yen's objective performance index (OPI; Yen, 1987) \n    <https://www.ets.org/research/policy_research_reports/publications/paper/1987/hrap>. \n    It also includes functions to compute Proportional Reduction \n    of Mean Squared Errors (PRMSEs) in Haberman's methods which are used to \n    examine whether test subscores are of added value. In addition, the package includes \n    a function to assess the local independence assumption of IRT with\n    Yen's Q3 statistic (Yen, 1984 <doi:10.1177/014662168400800201>; Yen, 1993 \n    <doi:10.1111/j.1745-3984.1993.tb00423.x>)."}, "bayesian": {"categories": ["Bayesian"], "description": "Fit Bayesian models using 'brms'/'Stan' with 'parsnip'/'tidymodels'\n    via 'bayesian' <doi:10.5281/zenodo.6654386>. 'tidymodels' is a collection of\n    packages for machine learning; see Kuhn and Wickham (2020) <https://www.tidymodels.org>).\n    The technical details of 'brms' and 'Stan' are described in B\u00fcrkner (2017)\n    <doi:10.18637/jss.v080.i01>, B\u00fcrkner (2018) <doi:10.32614/RJ-2018-017>,\n    and Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>."}, "GDINA": {"categories": ["Psychometrics"], "description": "A set of psychometric tools for cognitive diagnosis modeling based on the generalized deterministic inputs, noisy and gate (G-DINA) model by de la Torre (2011) <doi:10.1007/s11336-011-9207-7> and its extensions, including the sequential G-DINA model by Ma and de la Torre (2016) <doi:10.1111/bmsp.12070> for polytomous responses, and the polytomous G-DINA model by Chen and de la Torre <doi:10.1177/0146621613479818> for polytomous attributes. Joint attribute distribution can be independent, saturated, higher-order, loglinear smoothed or structured. Q-matrix validation, item and model fit statistics, model comparison at test and item level and differential item functioning can also be conducted. A graphical user interface is also provided. For tutorials, please check Ma and de la Torre (2020) <doi:10.18637/jss.v093.i14>, Ma and de la Torre (2019) <doi:10.1111/emip.12262>, Ma (2019) <doi:10.1007/978-3-030-05584-4_29> and de la Torre and Akbay (2019). "}, "bpcp": {"categories": ["Survival"], "description": "Calculates nonparametric pointwise confidence intervals for the survival distribution for right censored data, and for medians [Fay and Brittain <doi:10.1002/sim.6905>]. Has two-sample tests for dissimilarity (e.g., difference, ratio or odds ratio) in survival at a fixed time, and differences in medians [Fay, Proschan, and Brittain <doi:10.1111/biom.12231>]. Basically, the package gives exact inference methods for one- and two-sample exact inferences for Kaplan-Meier curves (e.g., generalizing Fisher's exact test to allow for right censoring), which are especially important for latter parts of the survival curve, small sample sizes or heavily censored data. Includes mid-p options."}, "locits": {"categories": ["TimeSeries"], "description": "Provides test of second-order stationarity for time\n\tseries (for dyadic and arbitrary-n length data). Provides\n\tlocalized autocovariance, with confidence intervals,\n\tfor locally stationary (nonstationary) time series.\n\tSee Nason, G P (2013) \"A test for second-order stationarity and\n\tapproximate confidence intervals for localized autocovariance\n\tfor locally stationary time series.\" Journal of the Royal Statistical\n\tSociety, Series B, 75, 879-904.  <doi:10.1111/rssb.12015>."}, "lspls": {"categories": ["ChemPhys"], "description": "Implements the LS-PLS (least squares - partial least squares)\n  method described in for instance J\u00f8rgensen, K., Segtnan, V. H., Thyholt, K.,\n  N\u00e6s, T. (2004)  \"A Comparison of Methods for Analysing Regression Models with\n  Both Spectral and Designed Variables\"\n  Journal of Chemometrics, 18(10), 451\u2013464, <doi:10.1002/cem.890>."}, "TransModel": {"categories": ["Survival"], "description": "A unified estimation procedure for the analysis of right censored data using linear transformation models. An introduction can be found in Jie Zhou et al. (2022) <doi:10.18637/jss.v101.i09>. "}, "forestploter": {"categories": ["MetaAnalysis"], "description": "Create forest plot based on the layout of the data. Confidence interval in multiple columns by groups can be done easily. Editing plot, inserting/adding text, applying theme to the plot and much more."}, "skmeans": {"categories": ["Cluster", "NaturalLanguageProcessing"], "description": "Algorithms to compute spherical k-means partitions.\n  Features several methods, including a genetic and a fixed-point\n  algorithm and an interface to the CLUTO vcluster program."}, "samplingVarEst": {"categories": ["OfficialStatistics"], "description": "Functions to calculate some point estimators and estimating their variance under unequal probability sampling without replacement. Single and two stage sampling designs are considered. Some approximations for the second order inclusion probabilities (joint inclusion probabilities) are available (sample and population based). A variety of Jackknife variance estimators are implemented. Almost every function is written in C (compiled) code for faster results. The functions incorporate some performance improvements for faster results with large datasets."}, "sendmailR": {"categories": ["WebTechnologies"], "description": "Package contains a simple SMTP client which provides a\n        portable solution for sending email, including attachment, from\n        within R."}, "xxIRT": {"categories": ["Psychometrics"], "description": "A suite of psychometric analysis tools for research and operation, including:\n    (1) computation of probability, information, and likelihood for the 3PL, GPCM, and GRM;\n    (2) parameter estimation using joint or marginal likelihood estimation method;\n    (3) simulation of computerized adaptive testing using built-in or customized algorithms;\n    (4) assembly and simulation of multistage testing. \n    The full documentation and tutorials are at <https://github.com/xluo11/xxIRT>."}, "joineRML": {"categories": ["Survival"], "description": "Fits the joint model proposed by Henderson and colleagues (2000) \n    <doi:10.1093/biostatistics/1.4.465>, but extended to the case of multiple \n    continuous longitudinal measures. The time-to-event data is modelled using a \n    Cox proportional hazards regression model with time-varying covariates. The \n    multiple longitudinal outcomes are modelled using a multivariate version of the \n    Laird and Ware linear mixed model. The association is captured by a multivariate\n    latent Gaussian process. The model is estimated using a Monte Carlo Expectation \n    Maximization algorithm. This project was funded by the Medical Research Council \n    (Grant number MR/M013227/1)."}, "selectr": {"categories": ["WebTechnologies"], "description": "Translates a CSS3 selector into an equivalent XPath\n  expression. This allows us to use CSS selectors when working with\n  the XML package as it can only evaluate XPath expressions. Also\n  provided are convenience functions useful for using CSS selectors on\n  XML nodes. This package is a port of the Python package 'cssselect'\n  (<https://cssselect.readthedocs.io/>)."}, "tensorflow": {"categories": ["HighPerformanceComputing", "MachineLearning", "ModelDeployment"], "description": "Interface to 'TensorFlow' <https://www.tensorflow.org/>,\n  an open source software library for numerical computation using data\n  flow graphs. Nodes in the graph represent mathematical operations,\n  while the graph edges represent the multidimensional data arrays\n  (tensors) communicated between them. The flexible architecture allows\n  you to deploy computation to one or more 'CPUs' or 'GPUs' in a desktop,\n  server, or mobile device with a single 'API'. 'TensorFlow' was originally\n  developed by researchers and engineers working on the Google Brain Team\n  within Google's Machine Intelligence research organization for the\n  purposes of conducting machine learning and deep neural networks research,\n  but the system is general enough to be applicable in a wide variety\n  of other domains as well."}, "forestmodel": {"categories": ["MetaAnalysis"], "description": "Produces forest plots using 'ggplot2' from models produced by functions\n    such as stats::lm(), stats::glm() and survival::coxph()."}, "MetabolAnalyze": {"categories": ["Cluster"], "description": "Fits probabilistic principal components analysis,\n        probabilistic principal components and covariates analysis and\n        mixtures of probabilistic principal components models to\n        metabolomic spectral data."}, "coxme": {"categories": ["Survival"], "description": "Fit Cox proportional hazards models containing both \n fixed and random effects.  The random effects can have a general form, of which\n familial interactions (a \"kinship\" matrix) is a particular special case. \n Note that the simplest case of a mixed effects Cox model, i.e. a single random \n per-group intercept, is also called a \"frailty\" model.  The approach is based\n on Ripatti and Palmgren, Biometrics 2002."}, "marmap": {"categories": ["Spatial"], "description": "Import xyz data from the NOAA (National Oceanic and Atmospheric Administration, <https://www.noaa.gov>), GEBCO (General Bathymetric Chart of the Oceans, <https://www.gebco.net>) and other sources, plot xyz data to prepare publication-ready figures, analyze xyz data to extract transects, get depth / altitude based on geographical coordinates, or calculate z-constrained least-cost paths."}, "interflex": {"categories": ["CausalInference"], "description": "Performs diagnostic tests of multiplicative interaction models and plots non-linear marginal effects of a treatment on an outcome across different values of a moderator."}, "fCopulae": {"categories": ["Distributions", "ExtremeValue", "Finance"], "description": "Provides a  collection of functions to \n\tmanage, to investigate and to analyze bivariate financial returns by  \n\tCopulae. Included are the families of Archemedean, Elliptical, \n\tExtreme Value, and Empirical Copulae."}, "ipdw": {"categories": ["Spatial"], "description": "Functions are provided to interpolate geo-referenced point data via\n    Inverse Path Distance Weighting. Useful for coastal marine applications where\n    barriers in the landscape preclude interpolation with Euclidean distances."}, "DEoptimR": {"categories": ["Optimization"], "description": "Differential Evolution (DE) stochastic algorithms for global\n    optimization of problems with and without constraints.\n    The aim is to curate a collection of its state-of-the-art variants that\n    (1) do not sacrifice simplicity of design,\n    (2) are essentially tuning-free, and\n    (3) can be efficiently implemented directly in the R language.\n    Currently, it only provides an implementation of the 'jDE' algorithm by\n    Brest et al. (2006) <doi:10.1109/TEVC.2006.872133>."}, "recmap": {"categories": ["Spatial"], "description": "Provides an interface and a C++ implementation of the RecMap MP2\n  construction heuristic (Panse C. (2018) <doi:10.18637/jss.v086.c01>).\n  This algorithm draws maps according to a given statistical value\n  (e.g., election results, population or epidemiological data).\n  The basic idea of the RecMap algorithm is that each map region\n  (e.g., different countries) is represented by a rectangle.\n  The area of each rectangle represents the statistical value given\n  as input (maintain zero cartographic error). Documentation about the usage\n  of the recmap algorithm is provided by a vignette included in this package."}, "BSBT": {"categories": ["SportsAnalytics"], "description": "An implementation of the Bayesian Spatial Bradley\u2013Terry (BSBT) model. It can be used to investigate data sets where judges compared different spatial areas. It constructs a network to describe how the areas are connected, and then places a correlated prior distribution on the quality parameter for each area, based on the network. The package includes MCMC algorithms to estimate the quality parameters. The methodology is published in Seymour et. al. (2020) <arXiv:2010.14128>.  "}, "pcaPP": {"categories": ["ChemPhys", "Psychometrics", "Robust"], "description": "Provides functions for robust PCA by projection pursuit.\n    The methods are described in Croux et al. (2006) <doi:10.2139/ssrn.968376>,\n    Croux et al. (2013) <doi:10.1080/00401706.2012.727746>,\n    Todorov and Filzmoser (2013) <doi:10.1007/978-3-642-33042-1_31>."}, "imputeTS": {"categories": ["MissingData", "TimeSeries"], "description": "Imputation (replacement) of missing values \n             in univariate time series. \n             Offers several imputation functions\n             and missing data plots. \n             Available imputation algorithms include: \n            'Mean', 'LOCF', 'Interpolation', \n            'Moving Average', 'Seasonal Decomposition', \n            'Kalman Smoothing on Structural Time Series models',\n            'Kalman Smoothing on ARIMA models'. Published in Moritz and Bartz-Beielstein (2017) \n            <doi:10.32614/RJ-2017-009>."}, "moveVis": {"categories": ["SpatioTemporal", "Tracking"], "description": "Tools to visualize movement data (e.g. from GPS tracking) and temporal changes of environmental data (e.g. from remote sensing) by creating video animations."}, "denoiseR": {"categories": ["MissingData"], "description": "Estimate a low rank matrix from noisy data using singular values\n    thresholding and shrinking functions. Impute missing values with matrix completion. The method is described in <arXiv:1602.01206>."}, "paws": {"categories": ["WebTechnologies"], "description": "Interface to Amazon Web Services <https://aws.amazon.com>,\n    including storage, database, and compute services, such as 'Simple\n    Storage Service' ('S3'), 'DynamoDB' 'NoSQL' database, and 'Lambda'\n    functions-as-a-service."}, "chess": {"categories": ["SportsAnalytics"], "description": "This is an opinionated wrapper around the\n    python-chess package. It allows users to read and write PGN files as\n    well as create and explore game trees such as the ones seen in chess\n    books."}, "altmeta": {"categories": ["MetaAnalysis"], "description": "Provides alternative statistical methods for meta-analysis, including:\n - bivariate generalized linear mixed models for synthesizing odds ratios, relative risks,\n   and risk differences\n   (Chu et al., 2012 <doi:10.1177/0962280210393712>)\n - heterogeneity tests and measures that are robust to outliers\n   (Lin et al., 2017 <doi:10.1111/biom.12543>);\n - measures, tests, and visualization tools for publication bias or small-study effects\n   (Lin and Chu, 2018 <doi:10.1111/biom.12817>;\n    Lin, 2019 <doi:10.1002/jrsm.1340>;\n    Lin, 2020 <doi:10.1177/0962280220910172>;\n    Shi et al., 2020 <doi:10.1002/jrsm.1415>);\n - meta-analysis of diagnostic tests for synthesizing sensitivities, specificities, etc.\n   (Reitsma et al., 2005 <doi:10.1016/j.jclinepi.2005.02.022>;\n    Chu and Cole, 2006 <doi:10.1016/j.jclinepi.2006.06.011>);\n - meta-analysis methods for synthesizing proportions\n   (Lin and Chu, 2020 <doi:10.1097/ede.0000000000001232>);\n - models for multivariate meta-analysis, measures of inconsistency degrees of freedom\n   in Bayesian network meta-analysis, and predictive P-score\n   (Lin and Chu, 2018 <doi:10.1002/jrsm.1293>;\n    Lin, 2020 <doi:10.1080/10543406.2020.1852247>;\n    Rosenberger et al., 2021 <doi:10.1186/s12874-021-01397-5>)."}, "R2jags": {"categories": ["Bayesian"], "description": "Providing wrapper functions to implement Bayesian analysis in JAGS.  Some major features include monitoring convergence of a MCMC model using Rubin and Gelman Rhat statistics, automatically running a MCMC model till it converges, and implementing parallel processing of a MCMC model for multiple chains."}, "mixAR": {"categories": ["TimeSeries"], "description": "Model time series using mixture autoregressive (MAR)\n             models.  Implemented are frequentist (EM) and Bayesian\n             methods for estimation, prediction and model\n             evaluation. See Wong and Li (2002)\n             <doi:10.1111/1467-9868.00222>, Boshnakov (2009)\n             <doi:10.1016/j.spl.2009.04.009>), and the extensive\n             references in the documentation."}, "truncdist": {"categories": ["Distributions"], "description": "A collection of tools to evaluate probability density\n        functions, cumulative distribution functions, quantile functions\n        and random numbers for truncated random variables.  These functions are\n        provided to also compute the expected value and variance. Nadarajah\n        and Kotz (2006) developed most of the functions. QQ plots can be produced.\n        All the probability functions in the stats, stats4 and evd\n        packages are automatically available for truncation.."}, "SAEval": {"categories": ["OfficialStatistics"], "description": "Allows users to produce diagnostic procedures and graphic tools for the evaluation of Small Area estimators."}, "fflr": {"categories": ["SportsAnalytics"], "description": "Format the raw data from the ESPN fantasy football API\n    <https://fantasy.espn.com/apis/v3/games/ffl/> as data frames.\n    Retrieve data on public leagues, rosters, athletes, and matches."}, "celestial": {"categories": ["ChemPhys"], "description": "Contains a number of common astronomy conversion routines, particularly the HMS and degrees schemes, which can be fiddly to convert between on mass due to the textural nature of the former. It allows users to coordinate match datasets quickly. It also contains functions for various cosmological calculations."}, "moments": {"categories": ["Distributions"], "description": "Functions to calculate: moments, Pearson's kurtosis,\n        Geary's kurtosis and skewness; tests related to them\n        (Anscombe-Glynn, D'Agostino, Bonett-Seier)."}, "rapiclient": {"categories": ["WebTechnologies"], "description": "Access services specified in OpenAPI (formerly Swagger) format.\n  It is not a code generator. Client is generated dynamically as a list of R \n  functions."}, "Rmpi": {"categories": ["HighPerformanceComputing"], "description": "An interface (wrapper) to MPI. It also \n\t     provides interactive R manager and worker environment."}, "ouch": {"categories": ["Environmetrics"], "description": "Fit and compare Ornstein-Uhlenbeck models for evolution along a phylogenetic tree."}, "AeRobiology": {"categories": ["MissingData"], "description": "Different tools for managing databases of airborne particles, elaborating the main calculations and visualization of results. In a first step, data are checked using tools for quality control and all missing gaps are completed. Then, the main parameters of the pollen season are calculated and represented graphically. Multiple graphical tools are available: pollen calendars, phenological plots, time series, tendencies, interactive plots, abundance plots..."}, "robustX": {"categories": ["Robust"], "description": "Robustness \u2013 'eXperimental', 'eXtraneous', or 'eXtraordinary'\n  Functionality for Robust Statistics.  Hence methods which are not well established,\n  often related to methods in package 'robustbase'.  Amazingly, 'BACON()', originally by\n  Billor, Hadi, and Velleman (2000) <doi:10.1016/S0167-9473(99)00101-2>\n  has become established in places.  The \"barrow wheel\" 'rbwheel()' is from\n  Stahel and M\u00e4chler (2009) <doi:10.1111/j.1467-9868.2009.00706.x>."}, "mtsdi": {"categories": ["TimeSeries"], "description": "This is an EM algorithm based method for imputation of missing values in multivariate normal time series. The imputation algorithm accounts for both spatial and temporal correlation structures. Temporal patterns can be modeled using an ARIMA(p,d,q), optionally with seasonal components, a non-parametric cubic spline or generalized additive models with exogenous covariates. This algorithm is specially tailored for climate data with missing measurements from several monitors along a given region."}, "yhatr": {"categories": ["ModelDeployment", "WebTechnologies"], "description": "Deploy, maintain, and invoke models via the Yhat\n    REST API."}, "RTL": {"categories": ["Finance"], "description": "A toolkit for Commodities 'analytics', risk management and\n    trading professionals. Includes functions for API calls to\n    'Morningstar Commodities' and 'Genscape'."}, "neldermead": {"categories": ["Optimization"], "description": "Provides several direct search optimization algorithms based on the\n  simplex method. The provided algorithms are direct search algorithms, i.e.\n  algorithms which do not use the derivative of the cost function. They are\n  based on the update of a simplex. The following algorithms are available: the\n  fixed shape simplex method of Spendley, Hext and Himsworth (unconstrained\n  optimization with a fixed shape simplex, 1962) <doi:10.1080/00401706.1962.10490033>, \n  the variable shape simplex method of Nelder and Mead (unconstrained optimization\n  with a variable shape simplex made, 1965) <doi:10.1093/comjnl/7.4.308>, and \n  Box's complex method (constrained optimization with a variable shape simplex,\n  1965) <doi:10.1093/comjnl/8.1.42>."}, "dbscan": {"categories": ["Cluster"], "description": "A fast reimplementation of several density-based algorithms of\n    the DBSCAN family. Includes the clustering algorithms DBSCAN (density-based \n    spatial clustering of applications with noise) and HDBSCAN (hierarchical \n    DBSCAN), the ordering algorithm OPTICS (ordering points to identify the \n    clustering structure), shared nearest neighbor clustering, and the outlier \n    detection algorithms LOF (local outlier factor) and GLOSH (global-local \n    outlier score from hierarchies). The implementations use the kd-tree data \n    structure (from library ANN) for faster k-nearest neighbor search. An R \n    interface to fast kNN and fixed-radius NN search is also provided. \n    Hahsler, Piekenbrock and Doran (2019) <doi:10.18637/jss.v091.i01>."}, "pcalg": {"categories": ["CausalInference", "GraphicalModels"], "description": "Functions for causal structure\n  learning and causal inference using graphical models. The main algorithms\n  for causal structure learning are PC (for observational data without hidden\n  variables), FCI and RFCI (for observational data with hidden variables),\n  and GIES (for a mix of data from observational studies\n  (i.e. observational data) and data from experiments\n  involving interventions (i.e. interventional data) without hidden\n  variables). For causal inference the IDA algorithm, the Generalized\n  Backdoor Criterion (GBC), the Generalized Adjustment Criterion (GAC)\n  and some related functions are implemented. Functions for incorporating\n  background knowledge are provided."}, "bootUR": {"categories": ["TimeSeries"], "description": "Set of functions to perform various bootstrap unit root tests for both individual time series\n  (including augmented Dickey-Fuller test and union tests), multiple time series and panel data; see\n  Palm, Smeekes and Urbain (2008) <doi:10.1111/j.1467-9892.2007.00565.x>,\n  Palm, Smeekes and Urbain (2011) <doi:10.1016/j.jeconom.2010.11.010>, \n  Moon and Perron (2012) <doi:10.1016/j.jeconom.2012.01.008>, \n  Smeekes and Taylor (2012) <doi:10.1017/S0266466611000387> and \n  Smeekes (2015) <doi:10.1111/jtsa.12110> for key references. "}, "infer": {"categories": ["TeachingStatistics"], "description": "The objective of this package is to perform\n    inference using an expressive statistical grammar that coheres with\n    the tidy design framework."}, "openintro": {"categories": ["TeachingStatistics"], "description": "Supplemental functions and data for 'OpenIntro' resources, which \n    includes open-source textbooks and resources for introductory statistics \n    (<https://www.openintro.org/>). The package contains data sets used in our \n    open-source textbooks along with custom plotting functions for reproducing \n    book figures. Note that many functions and examples include color \n    transparency; some plotting elements may not show up properly (or at all) \n    when run in some versions of Windows operating system."}, "bayesm": {"categories": ["Bayesian", "Cluster", "Distributions", "Econometrics"], "description": "Covers many important models used\n  in marketing and micro-econometrics applications. \n  The package includes:\n  Bayes Regression (univariate or multivariate dep var),\n  Bayes Seemingly Unrelated Regression (SUR),\n  Binary and Ordinal Probit,\n  Multinomial Logit (MNL) and Multinomial Probit (MNP),\n  Multivariate Probit,\n  Negative Binomial (Poisson) Regression,\n  Multivariate Mixtures of Normals (including clustering),\n  Dirichlet Process Prior Density Estimation with normal base,\n  Hierarchical Linear Models with normal prior and covariates,\n  Hierarchical Linear Models with a mixture of normals prior and covariates,\n  Hierarchical Multinomial Logits with a mixture of normals prior\n     and covariates,\n  Hierarchical Multinomial Logits with a Dirichlet Process prior and covariates,\n  Hierarchical Negative Binomial Regression Models,\n  Bayesian analysis of choice-based conjoint data,\n  Bayesian treatment of linear instrumental variables models,\n  Analysis of Multivariate Ordinal survey data with scale\n     usage heterogeneity (as in Rossi et al, JASA (01)),\n  Bayesian Analysis of Aggregate Random Coefficient Logit Models as in BLP (see\n  Jiang, Manchanda, Rossi 2009)\n  For further reference, consult our book, Bayesian Statistics and\n  Marketing by Rossi, Allenby and McCulloch (Wiley 2005) and Bayesian Non- and Semi-Parametric\n  Methods and Applications (Princeton U Press 2014)."}, "Zseq": {"categories": ["NumericalMathematics"], "description": "Generates well-known integer sequences. 'gmp' package is adopted for computing with arbitrarily large numbers. Every function has hyperlink to its corresponding item in OEIS (The On-Line Encyclopedia of Integer Sequences) in the function help page. For interested readers, see Sloane and Plouffe (1995, ISBN:978-0125586306)."}, "seleniumPipes": {"categories": ["WebTechnologies"], "description": "The W3C WebDriver specification defines a way for out-of-process\n    programs to remotely instruct the behaviour of web browsers. It is detailed\n    at <https://w3c.github.io/webdriver/webdriver-spec.html>. This package provides\n    an R client implementing the W3C specification."}, "epiflows": {"categories": ["Epidemiology"], "description": "Provides functions and classes designed to handle and visualise\n    epidemiological flows between locations. Also contains a statistical method\n    for predicting disease spread from flow data initially described in\n    Dorigatti et al. (2017) <doi:10.2807/1560-7917.ES.2017.22.28.30572>.\n    This package is part of the RECON (<http://www.repidemicsconsortium.org/>)\n    toolkit for outbreak analysis."}, "OrgMassSpecR": {"categories": ["ChemPhys"], "description": "Organic/biological mass spectrometry data analysis."}, "ie2misc": {"categories": ["MissingData"], "description": "A collection of Irucka Embry's miscellaneous USGS functions\n    (processing .exp and .psf files, statistical error functions,\n    \"+\" dyadic operator for use with NA, creating ADAPS and QW\n    spreadsheet files, calculating saturated enthalpy). Irucka created these\n    functions while a Cherokee Nation Technology Solutions (CNTS) United States\n    Geological Survey (USGS) Contractor and/or USGS employee."}, "nasapower": {"categories": ["Hydrology"], "description": "Client for 'NASA' 'POWER' global meteorology, surface solar\n    energy and climatology data 'API'.  'POWER' (Prediction Of Worldwide Energy \n    Resource) data are freely available for download with varying spatial\n    resolutions dependent on the original data and with several temporal\n    resolutions depending on the POWER parameter and community.  This work is\n    funded through the 'NASA' Earth Science Directorate Applied Science Program.\n    For more on the data themselves, the methodologies used in creating, a web-\n    based data viewer and web access, please see <https://power.larc.nasa.gov/>."}, "hdnom": {"categories": ["Survival"], "description": "Creates nomogram visualizations for penalized Cox regression\n    models, with the support of reproducible survival model building,\n    validation, calibration, and comparison for high-dimensional data."}, "TexExamRandomizer": {"categories": ["TeachingStatistics"], "description": "Randomizing exams with 'LaTeX'.\n    If you can compile your main document with 'LaTeX', the program should be able to compile the randomized\n    versions without much extra effort when creating the document."}, "EstCRM": {"categories": ["Psychometrics"], "description": "Estimates item and person parameters for the Samejima's Continuous Response Model (CRM), computes item fit residual statistics, draws empirical 3D item category response curves, draws theoretical 3D item category response curves, and generates data under the CRM for simulation studies."}, "matchMulti": {"categories": ["CausalInference"], "description": "Performs multilevel matches for data with cluster-\n\tlevel treatments and individual-level outcomes using a network \n\toptimization algorithm.  Functions for checking balance at the \n\tcluster and individual levels are also provided, as are methods \n\tfor permutation-inference-based outcome analysis.  Details in \n\tPimentel et al. (2018) <doi:10.1214/17-AOAS1118>.  The optmatch \n\tpackage, which is useful for running many of the provided \n\tfunctions, may be downloaded from Github at \n\t<https://github.com/markmfredrickson/optmatch> if not available on \n\tCRAN."}, "ROracle": {"categories": ["Databases"], "description": "Oracle Database interface (DBI) driver for R.\n   This is a DBI-compliant Oracle driver based on the OCI."}, "spikeSlabGAM": {"categories": ["Bayesian"], "description": "Bayesian variable selection, model choice, and regularized\n    estimation for (spatial) generalized additive mixed regression models\n    via stochastic search variable selection with spike-and-slab priors."}, "BCHM": {"categories": ["CausalInference"], "description": "Users can estimate the treatment effect for multiple subgroups basket trials based on the Bayesian Cluster Hierarchical Model (BCHM). In this model, a Bayesian non-parametric method is applied to dynamically calculate the number of clusters by conducting the multiple cluster classification based on subgroup outcomes. Hierarchical model is used to compute the posterior probability of treatment effect with the borrowing strength determined by the Bayesian non-parametric clustering and the similarities between subgroups. To use this package, 'JAGS' software and 'rjags' package are required, and users need to pre-install them."}, "tolerance": {"categories": ["Distributions"], "description": "Statistical tolerance limits provide the limits between which we can expect to find a specified proportion of a sampled population with a given level of confidence.  This package provides functions for estimating tolerance limits (intervals) for various univariate distributions (binomial, Cauchy, discrete Pareto, exponential, two-parameter exponential, extreme value, hypergeometric, Laplace, logistic, negative binomial, negative hypergeometric, normal, Pareto, Poisson-Lindley, Poisson, uniform, and Zipf-Mandelbrot), Bayesian normal tolerance limits, multivariate normal tolerance regions, nonparametric tolerance intervals, tolerance bands for regression settings (linear regression, nonlinear regression, nonparametric regression, and multivariate regression), and analysis of variance tolerance intervals.  Visualizations are also available for most of these settings."}, "epiR": {"categories": ["Epidemiology", "MetaAnalysis", "Survival"], "description": "Tools for the analysis of epidemiological and surveillance data. Contains functions for directly and indirectly adjusting measures of disease frequency, quantifying measures of association on the basis of single or multiple strata of count data presented in a contingency table, computation of confidence intervals around incidence risk and incidence rate estimates and sample size calculations for cross-sectional, case-control and cohort studies. Surveillance tools include functions to calculate an appropriate sample size for 1- and 2-stage representative freedom surveys, functions to estimate surveillance system sensitivity and functions to support scenario tree modelling analyses.   "}, "ragtop": {"categories": ["Finance"], "description": "Algorithms to price American and European\n    equity options, convertible bonds and a\n    variety of other financial derivatives. It uses an\n    extension of the usual Black-Scholes model in which\n    jump to default may occur at a probability specified\n    by a power-law link between stock price and hazard\n    rate as found in the paper by Takahashi, Kobayashi,\n    and Nakagawa (2001) <doi:10.3905/jfi.2001.319302>.  We\n    use ideas and techniques from Andersen and\n    Buffum (2002) <doi:10.2139/ssrn.355308> and\n    Linetsky (2006) <doi:10.1111/j.1467-9965.2006.00271.x>."}, "RcmdrPlugin.MA": {"categories": ["MetaAnalysis"], "description": "Easy to use interface for conducting meta-analysis in R. This\n    package is an Rcmdr-plugin, which allows the user to conduct analyses in a\n    menu-driven, graphical user interface environment (e.g., CMA, SPSS). It\n    uses recommended procedures as described in The Handbook of Research\n    Synthesis and Meta-Analysis (Cooper, Hedges, & Valentine, 2009)."}, "metamisc": {"categories": ["MetaAnalysis"], "description": "Facilitate frequentist and Bayesian meta-analysis of diagnosis and prognosis research studies. It includes functions to  summarize multiple estimates of prediction model discrimination and calibration performance (Debray et al., 2019) <doi:10.1177/0962280218785504>. It also includes functions to evaluate funnel plot asymmetry (Debray et al., 2018) <doi:10.1002/jrsm.1266>. Finally, the package provides functions for developing multivariable prediction models from datasets with clustering (de Jong et al., 2021) <doi:10.1002/sim.8981>. "}, "gSEM": {"categories": ["Psychometrics"], "description": "Conducts a semi-gSEM statistical analysis (semi-supervised generalized structural equation modeling) on a data frame of coincident observations of multiple predictive or intermediate variables and a final continuous, outcome variable, via two functions sgSEMp1() and sgSEMp2(), representing fittings based on two statistical principles. Principle 1 determines all sensible univariate relationships in the spirit of the Markovian process. The relationship between each pair of variables, including predictors and the final outcome variable, is determined with the Markovian property that the value of the current predictor is sufficient in relating to the next level variable, i.e., the relationship is independent of the specific value of the preceding-level variables to the current predictor, given the current value. Principle 2 resembles the multiple regression principle in the way multiple predictors are considered simultaneously. Specifically, the relationship of the first-level predictors (such as Time and irradiance etc) to the outcome variable (such as, module degradation or yellowing)  is fit by a supervised additive model. Then each significant intermediate variable is taken as the new outcome variable and the other variables (except the final outcome variable) as the predictors in investigating the next-level multivariate relationship by a supervised additive model. This fitting process is continued until all sensible models are investigated."}, "osrm": {"categories": ["WebTechnologies"], "description": "An interface between R and the 'OSRM' API. 'OSRM' is a routing\n    service based on 'OpenStreetMap' data. See <http://project-osrm.org/> for more\n    information. This package allows to compute routes, trips, isochrones and\n    travel distances matrices (travel time and kilometric distance)."}, "rgeolocate": {"categories": ["WebTechnologies"], "description": "Connectors to online and offline sources for taking IP addresses\n    and geolocating them to country, city, timezone and other geographic ranges. For\n    individual connectors, see the package index."}, "geosapi": {"categories": ["Spatial", "WebTechnologies"], "description": "Provides an R interface to the GeoServer REST API, allowing to upload \n and publish data in a GeoServer web-application and expose data to OGC Web-Services. \n The package currently supports all CRUD (Create,Read,Update,Delete) operations\n on GeoServer workspaces, namespaces, datastores (stores of vector data), featuretypes,\n layers, styles, as well as vector data upload operations. For more information about \n the GeoServer REST API, see <https://docs.geoserver.org/stable/en/user/rest/>."}, "Rcrawler": {"categories": ["WebTechnologies"], "description": "Performs parallel web crawling and web scraping. It is designed to crawl, parse and store web pages to produce data that can be directly used for analysis application. For details see Khalil and Fakir (2017) <doi:10.1016/j.softx.2017.04.004>."}, "RobStatTM": {"categories": ["Robust"], "description": "Companion package for the book: \"Robust Statistics: Theory and Methods, second edition\", <http://www.wiley.com/go/maronna/robust>.  This package contains code that implements the robust estimators discussed in the recent second edition of the book above, as well as the scripts reproducing all the examples in the book."}, "SSRMST": {"categories": ["Survival"], "description": "Calculates the power and sample size based on the difference in Restricted Mean Survival Time."}, "boa": {"categories": ["Bayesian", "GraphicalModels"], "description": "A menu-driven program and library of functions for carrying out\n    convergence diagnostics and statistical and graphical analysis of Markov\n    chain Monte Carlo sampling output."}, "geos": {"categories": ["Spatial"], "description": "Provides an R API to the Open Source Geometry Engine\n  ('GEOS') library (<https://trac.osgeo.org/geos/>) and a vector format \n  with which to efficiently store 'GEOS' geometries. High-performance functions \n  to extract information from, calculate relationships between, and\n  transform geometries are provided. Finally, facilities to import \n  and export geometry vectors to other spatial formats are provided."}, "FedData": {"categories": ["Hydrology"], "description": "Functions to automate downloading geospatial data available from\n    several federated data sources (mainly sources maintained by the US Federal\n    government). Currently, the package enables extraction from seven datasets:\n    The National Elevation Dataset digital elevation models (1 and 1/3 arc-second;\n    USGS); The National Hydrography Dataset (USGS); The Soil Survey Geographic\n    (SSURGO) database from the National Cooperative Soil Survey (NCSS), which is\n    led by the Natural Resources Conservation Service (NRCS) under the USDA; the\n    Global Historical Climatology Network (GHCN), coordinated by National Climatic\n    Data Center at NOAA; the Daymet gridded estimates of daily weather parameters \n    for North America, version 3, available from the Oak Ridge National Laboratory's\n    Distributed Active Archive Center (DAAC); the International Tree Ring Data Bank; \n    and the National Land Cover Database (NLCD)."}, "questionr": {"categories": ["OfficialStatistics"], "description": "Set of functions to make the processing and analysis of\n    surveys easier : interactive shiny apps and addins for data recoding,\n    contingency tables, dataset metadata handling, and several convenience\n    functions."}, "ReacTran": {"categories": ["DifferentialEquations"], "description": "Routines for developing models that describe reaction and advective-diffusive transport in one, two or three dimensions.\n  Includes transport routines in porous media, in estuaries, and in bodies with variable shape."}, "WikidataR": {"categories": ["WebTechnologies"], "description": "Read from, interogate, and write to Wikidata <https://www.wikidata.org> -\n    the multilingual, interdisciplinary, semantic knowledgebase. Includes functions to:\n    read from wikidata (single items, properties, or properties); query wikidata (retrieving\n    all items that match a set of criterial via Wikidata SPARQL query service); write to\n    Wikidata (adding new items or statements via QuickStatements); and handle and manipulate\n    Wikidata objects (as lists and tibbles). Uses the Wikidata and Quickstatements APIs. "}, "OOR": {"categories": ["Optimization"], "description": "Implementation of optimistic optimization methods for global optimization of deterministic or stochastic functions. The algorithms feature guarantees of the convergence to a global optimum. They require minimal assumptions on the (only local) smoothness, where the smoothness parameter does not need to be known. They are expected to be useful for the most difficult functions when we have no information on smoothness and the gradients are unknown or do not exist. Due to the weak assumptions, however, they can be mostly effective only in small dimensions, for example, for hyperparameter tuning."}, "dtwclust": {"categories": ["TimeSeries"], "description": "Time series clustering along with optimized techniques related\n    to the Dynamic Time Warping distance and its corresponding lower bounds.\n    Implementations of partitional, hierarchical, fuzzy, k-Shape and TADPole\n    clustering are available. Functionality can be easily extended with\n    custom distance measures and centroid definitions. Implementations of\n    DTW barycenter averaging, a distance based on global alignment kernels,\n    and the soft-DTW distance and centroid routines are also provided. \n    All included distance functions have custom loops optimized for the \n    calculation of cross-distance matrices, including parallelization support.\n    Several cluster validity indices are included."}, "SurvRegCensCov": {"categories": ["Survival"], "description": "The function SurvRegCens() of this package allows estimation of a Weibull Regression for a right-censored endpoint, one interval-censored covariate, and an arbitrary number of non-censored covariates. Additional functions allow to switch between different parametrizations of Weibull regression used by different R functions, inference for the mean difference of two arbitrarily censored Normal samples, and estimation of canonical parameters from censored samples for several distributional assumptions. Hubeaux, S. and Rufibach, K. (2014) <arXiv:1402.0432>."}, "tidyquant": {"categories": ["Finance"], "description": "Bringing business and financial analysis to the 'tidyverse'. The 'tidyquant' \n    package provides a convenient wrapper to various 'xts', 'zoo', 'quantmod', 'TTR' \n    and 'PerformanceAnalytics' package \n    functions and returns the objects in the tidy 'tibble' format. The main \n    advantage is being able to use quantitative functions with the 'tidyverse'\n    functions including 'purrr', 'dplyr', 'tidyr', 'ggplot2', 'lubridate', etc. See \n    the 'tidyquant' website for more information, documentation and examples."}, "epiDisplay": {"categories": ["Epidemiology"], "description": "Package for data exploration and result presentation. Full 'epicalc' package with data management functions is available at '<https://medipe.psu.ac.th/epicalc/>'."}, "RGreenplum": {"categories": ["Databases"], "description": "\n    Fully 'DBI'-compliant interface to 'Greenplum' <https://greenplum.org/>,\n    an open-source parallel database. This is an extension of the 'RPostgres'\n    package <https://github.com/r-dbi/RPostgres>."}, "matrixcalc": {"categories": ["NumericalMathematics"], "description": "A collection of functions to support matrix calculations\n        for probability, econometric and numerical analysis. There are\n        additional functions that are comparable to APL functions which\n        are useful for actuarial models such as pension mathematics.\n        This package is used for teaching and research purposes at the\n        Department of Finance and Risk Engineering, New York\n        University, Polytechnic Institute, Brooklyn, NY 11201.\n        Horn, R.A. (1990) Matrix Analysis. ISBN 978-0521386326.\n        Lancaster, P. (1969) Theory of Matrices. ISBN 978-0124355507.\n        Lay, D.C. (1995) Linear Algebra: And Its Applications. ISBN 978-0201845563."}, "mixmeta": {"categories": ["MetaAnalysis"], "description": "A collection of functions to perform various meta-analytical models\n  through a unified mixed-effects framework, including standard univariate\n  fixed and random-effects meta-analysis and meta-regression, and non-standard\n  extensions such as multivariate, multilevel, longitudinal, and dose-response\n  models."}, "optimParallel": {"categories": ["Optimization"], "description": "Provides a parallel version of the L-BFGS-B method of optim(). The main function of the package is optimParallel(), which has the same usage and output as optim(). Using optimParallel() can significantly reduce the optimization time."}, "ICsurv": {"categories": ["Survival"], "description": "Currently using the proportional hazards (PH) model. More methods under other semiparametric regression models will be included in later versions. "}, "rdhs": {"categories": ["OfficialStatistics"], "description": "Provides a client for (1) querying the DHS API for survey indicators\n  and metadata (<https://api.dhsprogram.com/#/index.html>), (2) identifying surveys\n  and datasets for analysis, (3) downloading survey datasets from the DHS website,\n  (4) loading datasets and associate metadata into R, and (5) extracting variables\n  and combining datasets for pooled analysis."}, "adehabitatHR": {"categories": ["Spatial", "Tracking"], "description": "A collection of tools for the estimation of animals home range."}, "GenOrd": {"categories": ["Distributions"], "description": "A gaussian copula based procedure for generating samples from discrete random variables with prescribed correlation matrix and marginal distributions."}, "RRTCS": {"categories": ["OfficialStatistics"], "description": "Point and interval estimation of linear parameters with data\n    obtained from complex surveys (including stratified and clustered samples)\n    when randomization techniques are used. The randomized response technique\n    was developed to obtain estimates that are more valid when studying\n    sensitive topics. Estimators and variances for 14 randomized response\n    methods for qualitative variables and 7 randomized response methods for\n    quantitative variables are also implemented. In addition, some data sets\n    from surveys with these randomization methods are included in the package."}, "DBI": {"categories": ["Databases"], "description": "A database interface definition for communication between R\n    and relational database management systems.  All classes in this\n    package are virtual and need to be extended by the various R/DBMS\n    implementations."}, "lmm": {"categories": ["Bayesian"], "description": "It implements Expectation/Conditional Maximization Either (ECME)\n             and rapidly converging algorithms as well as\n             Bayesian inference for linear mixed models, \n             which is described in Schafer, J.L. (1998)\n             \"Some improved procedures for linear mixed models\".\n             Dept. of Statistics, The Pennsylvania State University."}, "desplot": {"categories": ["ExperimentalDesign"], "description": "A function for plotting maps of agricultural field experiments that\n    are laid out in grids."}, "bayesboot": {"categories": ["Bayesian"], "description": "Functions for performing the Bayesian bootstrap as introduced by\n    Rubin (1981) <doi:10.1214/aos/1176345338> and for summarizing the result.\n    The implementation can handle both summary statistics that works on a\n    weighted version of the data and summary statistics that works on a\n    resampled data set."}, "fdANOVA": {"categories": ["FunctionalData"], "description": "Performs analysis of variance testing procedures for univariate and multivariate functional data (Cuesta-Albertos and Febrero-Bande (2010) <doi:10.1007/s11749-010-0185-3>, Gorecki and Smaga (2015) <doi:10.1007/s00180-015-0555-0>, Gorecki and Smaga (2017) <doi:10.1080/02664763.2016.1247791>, Zhang et al. (2018) <doi:10.1016/j.csda.2018.05.004>)."}, "survMisc": {"categories": ["Survival"], "description": "A collection of functions to help in the analysis of\n    right-censored survival data. These extend the methods available in\n    package:survival."}, "rcrossref": {"categories": ["WebTechnologies"], "description": "Client for various 'CrossRef' 'APIs', including 'metadata' search\n    with their old and newer search 'APIs', get 'citations' in various formats\n    (including 'bibtex', 'citeproc-json', 'rdf-xml', etc.), convert 'DOIs'\n    to 'PMIDs', and 'vice versa', get citations for 'DOIs', and get links to\n    full text of articles when available."}, "regress": {"categories": ["Spatial"], "description": "Functions to fit Gaussian linear model by maximising the\n        residual log likelihood where the covariance structure can be\n        written as a linear combination of known matrices.  Can be used\n        for multivariate models and random effects models.  Easy\n        straight forward manner to specify random effects models,\n        including random interactions. Code now optimised to use\n        Sherman Morrison Woodbury identities for matrix inversion in\n        random effects models. We've added the ability to fit models\n        using any kernel as well as a function to return the mean and\n        covariance of random effects conditional on the data (best\n        linear unbiased predictors, BLUPs).\n        Clifford and McCullagh (2006)\n        <https://www.r-project.org/doc/Rnews/Rnews_2006-2.pdf>."}, "RMark": {"categories": ["Environmetrics"], "description": "An interface to the software package MARK that constructs input\n    files for MARK and extracts the output. MARK was developed by Gary White\n    and is freely available at <http://www.phidot.org/software/mark/downloads/>\n    but is not open source."}, "RcppEigen": {"categories": ["NumericalMathematics"], "description": "R and 'Eigen' integration using 'Rcpp'.\n 'Eigen' is a C++ template library for linear algebra: matrices, vectors,\n numerical solvers and related algorithms.  It supports dense and sparse\n matrices on integer, floating point and complex numbers, decompositions of\n such matrices, and solutions of linear systems. Its performance on many\n algorithms is comparable with some of the best implementations based on\n 'Lapack' and level-3 'BLAS'. The 'RcppEigen' package includes the header\n files from the 'Eigen' C++ template library (currently version 3.3.4). Thus\n users do not need to install 'Eigen' itself in order to use 'RcppEigen'.\n Since version 3.1.1, 'Eigen' is licensed under the Mozilla Public License\n (version 2); earlier version were licensed under the GNU LGPL version 3 or\n later. 'RcppEigen' (the 'Rcpp' bindings/bridge to 'Eigen') is licensed under\n the GNU GPL version 2 or later, as is the rest of 'Rcpp'."}, "emdbook": {"categories": ["Distributions"], "description": "Auxiliary functions and data sets for \"Ecological Models and Data\", a book presenting maximum likelihood estimation and related topics for ecologists (ISBN 978-0-691-12522-0)."}, "amanida": {"categories": ["MetaAnalysis"], "description": "Combination of results for meta-analysis using significance\n    and effect size only. P-values and fold-change are combined to obtain\n    a global significance on each metabolite. Produces a volcano plot\n    summarising the relevant results from meta-analysis. Vote-counting\n    reports for metabolites. And explore plot to detect discrepancies\n    between studies at a first glance. Methodology is described in the\n    Llambrich et al. (2021) <doi:10.1093/bioinformatics/btab591>.  "}, "itscalledsoccer": {"categories": ["SportsAnalytics"], "description": "Provides a wrapper around the same API <https://app.americansocceranalysis.com/api/v1/__docs__/>\n    that powers the American Soccer Analysis app."}, "CKAT": {"categories": ["CausalInference"], "description": "Composite Kernel Association Test (CKAT) is a flexible and robust kernel machine based approach to jointly test the genetic main effect and gene-treatment interaction effect for a set of single-nucleotide polymorphisms (SNPs) in pharmacogenetics (PGx) assessments embedded within randomized clinical trials."}, "superpc": {"categories": ["Survival"], "description": "Does prediction in the case of a censored survival outcome, or a regression outcome, using the \"supervised principal component\" approach. 'Superpc' is especially useful for high-dimensional data when the number of features p dominates the number of samples n (p >> n paradigm), as generated, for instance, by high-throughput technologies."}, "MTS": {"categories": ["TimeSeries"], "description": "Multivariate Time Series (MTS) is a general package for analyzing multivariate linear time series and estimating multivariate volatility models. It also handles factor models, constrained factor models, asymptotic principal component analysis commonly used in finance and econometrics, and principal volatility component analysis.  (a) For the multivariate linear time series analysis, the package performs model specification, estimation, model checking, and prediction for many widely used models, including vector AR models, vector MA models, vector ARMA models, seasonal vector ARMA models, VAR models with exogenous variables, multivariate regression models with time series errors, augmented VAR models, and Error-correction VAR models for co-integrated time series. For model specification, the package performs structural specification to overcome the difficulties of identifiability of VARMA models. The methods used for structural specification include Kronecker indices and Scalar Component Models.  (b) For multivariate volatility modeling, the MTS package handles several commonly used models, including multivariate exponentially weighted moving-average volatility, Cholesky decomposition volatility models, dynamic conditional correlation (DCC) models, copula-based volatility models, and low-dimensional BEKK models. The package also considers multiple tests for conditional heteroscedasticity, including rank-based statistics.  (c) Finally, the MTS package also performs forecasting using diffusion index , transfer function analysis, Bayesian estimation of VAR models, and multivariate time series analysis with missing values.Users can also use the package to simulate VARMA models, to compute impulse response functions of a fitted VARMA model, and to calculate theoretical cross-covariance matrices of a given VARMA model. "}, "hydrostats": {"categories": ["Hydrology"], "description": "Calculates a suite of hydrologic indices for daily time series data that are widely used in hydrology and stream ecology."}, "bayesDccGarch": {"categories": ["Bayesian"], "description": "Bayesian estimation of dynamic conditional correlation GARCH model for multivariate time series volatility (Fioruci, J.A., Ehlers, R.S. and Andrade-Filho, M.G., (2014). <doi:10.1080/02664763.2013.839635>."}, "tsdb": {"categories": ["TimeSeries"], "description": "A terribly-simple data base for numeric\n  time series, written purely in R, so no external\n  database-software is needed. Series are stored in\n  plain-text files (the most-portable and enduring file\n  type) in CSV format. Timestamps are encoded using R's\n  native numeric representation for 'Date'/'POSIXct',\n  which makes them fast to parse, but keeps them\n  accessible with other software. The package provides\n  tools for saving and updating series in this\n  standardised format, for retrieving and joining data,\n  for summarising files and directories, and for\n  coercing series from and to other data types (such as\n  'zoo' series)."}, "nCopula": {"categories": ["Distributions"], "description": "Construct and manipulate hierarchical Archimedean copulas with multivariate compound distributions. The model used is the one of Cossette et al. (2017) <doi:10.1016/j.insmatheco.2017.06.001>. "}, "BayesPiecewiseICAR": {"categories": ["Survival"], "description": "Fits a piecewise exponential hazard to survival data using a\n    Hierarchical Bayesian model with an Intrinsic Conditional Autoregressive\n    formulation for the spatial dependency in the hazard rates for each piece.\n    This function uses Metropolis- Hastings-Green MCMC to allow the number of split\n    points to vary. This function outputs graphics that display the histogram of\n    the number of split points and the trace plots of the hierarchical parameters.\n    The function outputs a list that contains the posterior samples for the number\n    of split points, the location of the split points, and the log hazard rates\n    corresponding to these splits. Additionally, this outputs the posterior samples\n    of the two hierarchical parameters, Mu and Sigma^2."}, "nmarank": {"categories": ["MetaAnalysis"], "description": "Derives the most frequent hierarchies along with their probability of occurrence. One can also define complex hierarchy criteria and calculate their probability. Methodology based on Papakonstantinou et al. (2021) <doi:10.21203/rs.3.rs-858140/v1>."}, "AzureRMR": {"categories": ["WebTechnologies"], "description": "A lightweight but powerful R interface to the 'Azure Resource Manager' REST API. The package exposes a comprehensive class framework and related tools for creating, updating and deleting 'Azure' resource groups, resources and templates. While 'AzureRMR' can be used to manage any 'Azure' service, it can also be extended by other packages to provide extra functionality for specific services. Part of the 'AzureR' family of packages."}, "EFAutilities": {"categories": ["Psychometrics"], "description": "A number of utility function for exploratory factor analysis\n    are included in this package. In particular, it computes standard errors for\n    parameter estimates and factor correlations under a variety of conditions."}, "huxtable": {"categories": ["ReproducibleResearch"], "description": "Creates styled tables for data presentation. Export to HTML, LaTeX,\n  RTF, 'Word', 'Excel', and 'PowerPoint'. Simple, modern interface to manipulate \n  borders, size, position, captions, colours, text styles and number formatting.\n  Table cells can span multiple rows and/or columns.\n  Includes  a 'huxreg' function for creation of regression tables, and 'quick_*' \n  one-liners to print data to a new document."}, "Carlson": {"categories": ["NumericalMathematics"], "description": "Evaluation of the Carlson elliptic integrals and the\n    incomplete elliptic integrals with complex arguments. The\n    implementations use Carlson's algorithms <doi:10.1007/BF02198293>.\n    Applications of elliptic integrals include probability distributions,\n    geometry, physics, mechanics, electrodynamics, statistical mechanics,\n    astronomy, geodesy, geodesics on conics, and magnetic field\n    calculations."}, "imputeTestbench": {"categories": ["MissingData", "TimeSeries"], "description": "Provides a test bench for the comparison of missing data imputation \n    methods in uni-variate time series. Imputation methods are compared using \n    different error metrics. Proposed imputation methods and alternative error \n    metrics can be used."}, "stR": {"categories": ["TimeSeries"], "description": "Methods for decomposing seasonal data: STR (a Seasonal-Trend decomposition procedure based on Regression) and Robust STR. In some ways, STR is similar to Ridge Regression and Robust STR can be related to LASSO. They allow for multiple seasonal components, multiple linear covariates with constant, flexible and seasonal influence. Seasonal patterns (for both seasonal components and seasonal covariates) can be fractional and flexible over time; moreover they can be either strictly periodic or have a more complex topology. The methods provide confidence intervals for the estimated components. The methods can be used for forecasting."}, "dfoptim": {"categories": ["Optimization"], "description": "Derivative-Free optimization algorithms. These algorithms do not require gradient information. More importantly, they can be used to solve non-smooth optimization problems."}, "geigen": {"categories": ["NumericalMathematics"], "description": "Functions to compute generalized eigenvalues and eigenvectors,\n             the generalized Schur decomposition and\n             the generalized Singular Value Decomposition of a matrix pair,\n             using Lapack routines."}, "micromap": {"categories": ["Spatial"], "description": "This group of functions simplifies the creation of linked micromap\n    plots."}, "BiasedUrn": {"categories": ["Distributions"], "description": "Statistical models of biased sampling in the form of \n  univariate and multivariate noncentral hypergeometric distributions, \n  including Wallenius' noncentral hypergeometric distribution and\n  Fisher's noncentral hypergeometric distribution \n  (also called extended hypergeometric distribution).  \n  See vignette(\"UrnTheory\") for explanation of these distributions."}, "isotone": {"categories": ["Optimization"], "description": "Contains two main functions: one for\n        solving general isotone regression problems using the\n        pool-adjacent-violators algorithm (PAVA); another one provides\n        a framework for active set methods for isotone optimization\n        problems with arbitrary order restrictions. Various types of\n        loss functions are prespecified."}, "biclustermd": {"categories": ["MissingData"], "description": "Biclustering is a statistical learning technique that simultaneously \n    partitions and clusters rows and columns of a data matrix. Since the solution \n    space of biclustering is in infeasible to completely search with current \n    computational mechanisms, this package uses a greedy heuristic. The algorithm \n    featured in this package is, to the best our knowledge, the first biclustering \n    algorithm to work on data with missing values. Li, J., Reisner, J., Pham, H., \n    Olafsson, S., and Vardeman, S. (2020) Biclustering with Missing Data. Information \n    Sciences, 510, 304\u2013316."}, "FlexScan": {"categories": ["Spatial"], "description": "An easy way to conduct flexible scan.\n    Monte-Carlo method is used to test the spatial clusters given the cases, population, and shapefile.\n    A table with formal style and a map with clusters are included in the result report.\n    The method can be referenced at: Toshiro Tango and Kunihiko Takahashi (2005) <doi:10.1186/1476-072X-4-11>."}, "viridis": {"categories": ["Spatial"], "description": "Color maps designed to improve graph readability for readers with \n    common forms of color blindness and/or color vision deficiency. The color \n    maps are also perceptually-uniform, both in regular form and also when \n    converted to black-and-white for printing. This package also contains \n    'ggplot2' bindings for discrete and continuous color and fill scales. A lean\n    version of the package called 'viridisLite' that does not include the \n    'ggplot2' bindings can be found at \n    <https://cran.r-project.org/package=viridisLite>."}, "clusteredinterference": {"categories": ["CausalInference"], "description": "Estimating causal effects from observational studies assuming \n    clustered (or partial) interference. These inverse probability-weighted\n    estimators target new estimands arising from population-level treatment \n    policies. The estimands and estimators are introduced in Barkley et al. \n    (2017) <arXiv:1711.04834>."}, "poistweedie": {"categories": ["Distributions"], "description": "Simulation of models Poisson-Tweedie."}, "WaveletComp": {"categories": ["TimeSeries"], "description": "Wavelet analysis and reconstruction of time series, cross-wavelets and phase-difference (with filtering options), significance with simulation algorithms."}, "RPresto": {"categories": ["Databases"], "description": "Implements a 'DBI' compliant interface to Presto. Presto is\n    an open source distributed SQL query engine for running interactive\n    analytic queries against data sources of all sizes ranging from\n    gigabytes to petabytes: <https://prestodb.io/>."}, "RMTstat": {"categories": ["Distributions"], "description": "\n    Functions for working with the Tracy-Widom laws and other distributions \n    related to the eigenvalues of large Wishart matrices.\n    The tables for computing the Tracy-Widom densities and distribution\n    functions were computed by functions were computed by Momar Dieng's MATLAB package \"RMLab\". This package is part of a collaboration between Iain Johnstone, Zongming Ma, Patrick Perry, and Morteza Shahram."}, "MBA": {"categories": ["Spatial"], "description": "Functions to interpolate irregularly and regularly spaced data using Multilevel B-spline Approximation (MBA). Functions call portions of the SINTEF Multilevel B-spline Library written by \u00d8yvind Hjelle which implements methods developed by Lee, Wolberg and Shin (1997; <doi:10.1109/2945.620490>)."}, "funtimes": {"categories": ["TimeSeries"], "description": "Nonparametric estimators and tests for time series analysis. The functions use bootstrap techniques and robust nonparametric difference-based estimators to test for the presence of possibly nonmonotonic trends and for synchronism of trends in multiple time series."}, "ugatsdb": {"categories": ["TimeSeries"], "description": "An R API providing easy access to a relational database with macroeconomic, \n             financial and development related time series data for Uganda. \n             Overall more than 5000 series at varying frequency (daily, monthly, \n             quarterly, annual in fiscal or calendar years) can be accessed through \n             the API. The data is provided by the Bank of Uganda, \n             the Ugandan Ministry of Finance, Planning and Economic Development,\n             the IMF and the World Bank. The database is being updated once a month. "}, "obAnalytics": {"categories": ["Finance"], "description": "Data processing, visualisation and analysis of Limit Order Book\n    event data."}, "HSAUR3": {"categories": ["TeachingStatistics"], "description": "Functions, data sets, analyses and examples from the \n  third edition of the book \n  \u201dA Handbook of Statistical Analyses Using R\u201d (Torsten Hothorn and Brian S.\n  Everitt, Chapman & Hall/CRC, 2014). The first chapter\n  of the book, which is entitled \u201dAn Introduction to R\u201d, \n  is completely included in this package, for all other chapters,\n  a vignette containing all data analyses is available. In addition,\n  Sweave source code for slides of selected chapters is included in \n  this package (see HSAUR3/inst/slides). The publishers web page is \n  '<http://www.crcpress.com/product/isbn/9781482204582>'."}, "cec2013": {"categories": ["Optimization"], "description": "This package provides R wrappers for the C implementation of 28 benchmark functions defined for the  Special Session and Competition on Real-Parameter Single Objective Optimization at CEC-2013. The focus of this package is to provide an open-source and multi-platform implementation of the CEC2013 benchmark functions, in order to make easier for researchers to test the performance of new optimization algorithms in a reproducible way. The original C code (Windows only) was provided by Jane Jing Liang, while GNU/Linux comments were made by Janez Brest. This package was gently authorised for publication on CRAN by Ponnuthurai Nagaratnam Suganthan. The official documentation is available at http://www.ntu.edu.sg/home/EPNSugan/index_files/CEC2013/CEC2013.htm. Bugs reports/comments/questions are very welcomed (in English, Spanish or Italian)."}, "tikzDevice": {"categories": ["ReproducibleResearch"], "description": "Provides a graphics output device for R that records plots\n        in a LaTeX-friendly format. The device transforms plotting\n        commands issued by R functions into LaTeX code blocks. When\n        included in a LaTeX document, these blocks are interpreted with\n        the help of 'TikZ'\u2014a graphics package for TeX and friends\n        written by Till Tantau. Using the 'tikzDevice', the text of R\n        plots can contain LaTeX commands such as mathematical formula.\n        The device also allows arbitrary LaTeX code to be inserted into\n        the output stream."}, "candisc": {"categories": ["Psychometrics"], "description": "Functions for computing and visualizing \n\tgeneralized canonical discriminant analyses and canonical correlation analysis\n\tfor a multivariate linear model.\n\tTraditional canonical discriminant analysis is restricted to a one-way 'MANOVA'\n\tdesign and is equivalent to canonical correlation analysis between a set of quantitative\n\tresponse variables and a set of dummy variables coded from the factor variable.\n\tThe 'candisc' package generalizes this to higher-way 'MANOVA' designs\n\tfor all factors in a multivariate linear model,\n\tcomputing canonical scores and vectors for each term. The graphic functions provide low-rank (1D, 2D, 3D) \n\tvisualizations of terms in an 'mlm' via the 'plot.candisc' and 'heplot.candisc' methods. Related plots are\n\tnow provided for canonical correlation analysis when all predictors are quantitative."}, "rrefine": {"categories": ["WebTechnologies"], "description": "'OpenRefine' (formerly 'Google Refine') is a popular, open source data cleaning software. This package enables users to programmatically trigger data transfer between R and 'OpenRefine'. Available functionality includes project import, export and deletion."}, "dipm": {"categories": ["MachineLearning"], "description": "An implementation by Chen, Li, and Zhang (2022) <doi:10.1093/bioadv/vbac041> of the Depth Importance in Precision Medicine (DIPM) method \n             in Chen and Zhang (2022) <doi:10.1093/biostatistics/kxaa021> and Chen and \n             Zhang (2020) <doi:10.1007/978-3-030-46161-4_16>. The DIPM method is a classification \n             tree that searches for subgroups with especially poor or strong performance in a given treatment group."}, "fasstr": {"categories": ["Hydrology"], "description": "The Flow Analysis Summary Statistics Tool for R, 'fasstr', provides various \n    functions to tidy and screen daily stream discharge data, calculate and visualize various summary statistics\n    and metrics, and compute annual trending and volume frequency analyses. It features useful function arguments \n    for filtering of and handling dates, customizing data and metrics, and the ability to pull daily data directly \n    from the Water Survey of Canada hydrometric database (<https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/>)."}, "mada": {"categories": ["MetaAnalysis"], "description": "Provides functions for diagnostic meta-analysis. Next to basic analysis and visualization the bivariate Model of Reitsma et al. (2005) that is equivalent to the HSROC of Rutter & Gatsonis (2001) can be fitted. A new approach based to diagnostic meta-analysis of Holling et al. (2012) is also available. Standard methods like summary, plot and so on are provided."}, "yorkr": {"categories": ["SportsAnalytics"], "description": "Analyzing performances of cricketers and cricket teams\n             based on 'yaml' match data from Cricsheet <https://cricsheet.org/>."}, "zTree": {"categories": ["Econometrics"], "description": "Read '.xls' and '.sbj' files which are written by the\n   Microsoft Windows program 'z-Tree'. The latter is a software for\n   developing and carrying out economic experiments\n   (see <http://www.ztree.uzh.ch/> for more information)."}, "finreportr": {"categories": ["Finance"], "description": "Download and display company financial data from the U.S. Securities\n    and Exchange Commission's EDGAR database. It contains a suite of functions with\n    web scraping and XBRL parsing capabilities that allows users to extract data from EDGAR \n    in an automated and scalable manner. See <https://www.sec.gov/edgar/searchedgar/companysearch.html>\n    for more information."}, "BenfordTests": {"categories": ["Distributions", "Finance"], "description": "Several specialized statistical tests and support functions \n\t\t\tfor determining if numerical data could conform to Benford's law."}, "WASP": {"categories": ["Hydrology"], "description": "The wavelet-based variance transformation method is used for system modelling and prediction. It refines predictor spectral representation using Wavelet Theory, which leads to improved model specifications and prediction accuracy. Details of methodologies used in the package can be found in Jiang, Z., Sharma, A., & Johnson, F. (2020) <doi:10.1029/2019WR026962>, Jiang, Z., Rashid, M. M., Johnson, F., & Sharma, A. (2020) <doi:10.1016/j.envsoft.2020.104907>, and Jiang, Z., Sharma, A., & Johnson, F. (2021) <doi:10.1016/J.JHYDROL.2021.126816>."}, "ssmrob": {"categories": ["Econometrics", "Robust"], "description": "Package provides a set of tools for robust estimation and inference for models with sample selectivity and endogenous treatment model. For details, see Zhelonkin and Ronchetti (2021) <doi:10.18637/jss.v099.i04>. "}, "MetaAnalyser": {"categories": ["MetaAnalysis"], "description": "An interactive application to visualise meta-analysis data as a\n    physical weighing machine. The interface is based on the Shiny web application\n    framework, though can be run locally and with the user's own data."}, "rerddapXtracto": {"categories": ["SpatioTemporal", "Tracking"], "description": "Contains three functions that access\n    environmental data from any 'ERDDAP' data web service. The rxtracto() function extracts\n    data along a trajectory for a given \"radius\" around the point. The\n    rxtracto_3D() function extracts data in a box. The rxtractogon() function\n    extracts data in a polygon. All of those three function use the 'rerddap' package\n    to extract the data, and should work with any 'ERDDAP' server.\n    There are also two functions, plotBBox() and plotTrack() that use the 'plotdap'\n    package to simplify the creation of maps of the data."}, "YPmodel": {"categories": ["Survival"], "description": "Inference procedures accommodate a flexible range of hazard ratio patterns with a two-sample semi-parametric model. This model contains the proportional hazards model and the proportional odds model as sub-models, and accommodates non-proportional hazards situations to the extreme of having crossing hazards and crossing survivor functions. Overall, this package has four major functions: 1) the parameter estimation, namely short-term and long-term hazard ratio parameters; 2) 95 percent and 90 percent point-wise confidence intervals and simultaneous confidence bands for the hazard ratio function; 3) p-value of the adaptive weighted log-rank test; 4) p-values of two lack-of-fit tests for the model. See the included \"read_me_first.pdf\" for brief instructions. In this version (1.1), there is no need to sort the data before applying this package."}, "densEstBayes": {"categories": ["Bayesian"], "description": "Bayesian density estimates for univariate continuous random samples are provided using the Bayesian inference engine paradigm. The engine options are: Hamiltonian Monte Carlo, the no U-turn sampler, semiparametric mean field variational Bayes and slice sampling. The methodology is described in Wand and Yu (2020) <arXiv:2009.06182>. "}, "fastGHQuad": {"categories": ["NumericalMathematics"], "description": "Fast, numerically-stable Gauss-Hermite quadrature rules and\n    utility functions for adaptive GH quadrature. See Liu, Q. and Pierce, D. A.\n    (1994) <doi:10.2307/2337136> for a reference on these methods."}, "funData": {"categories": ["FunctionalData"], "description": "S4 classes for univariate and multivariate functional data with\n    utility functions. See <doi:10.18637/jss.v093.i05> for a detailed description \n    of the package functionalities and its interplay with the MFPCA package for \n    multivariate functional principal component analysis \n    <https://CRAN.R-project.org/package=MFPCA>. "}, "DCluster": {"categories": ["Spatial"], "description": "A set of functions for the detection of spatial clusters\n        of disease using count data. Bootstrap is used to estimate\n        sampling distributions of statistics."}, "tinyProject": {"categories": ["ReproducibleResearch"], "description": "Creates useful files and folders for data\n  analysis  projects and provides functions to manage data,\n  scripts and output files. Also provides a project\n  template for 'Rstudio'."}, "combinedevents": {"categories": ["SportsAnalytics"], "description": "Includes functions to calculate scores and marks for track and\n    field combined events competitions. The functions are based on the scoring\n    tables for combined events set forth by the International Association of \n    Athletics Federation (2001)."}, "StempCens": {"categories": ["MissingData"], "description": "It estimates the parameters of a censored or missing data in spatio-temporal models using the SAEM algorithm (Delyon et al., 1999). This algorithm is a stochastic approximation of the widely used EM algorithm and an important tool for models in which the E-step does not have an analytic form. Besides the expressions obtained to estimate the parameters to the proposed model, we include the calculations for the observed information matrix using the method developed by Louis (1982). To examine the performance of the fitted model, case-deletion measure are provided."}, "resampledata": {"categories": ["TeachingStatistics"], "description": "Package of data sets from \"Mathematical Statistics\n    with Resampling in R\" (1st Ed. 2011, 2nd Ed. 2018) by Laura Chihara and Tim Hesterberg."}, "R.rsp": {"categories": ["ReproducibleResearch"], "description": "The RSP markup language makes any text-based document come alive.  RSP provides a powerful markup for controlling the content and output of LaTeX, HTML, Markdown, AsciiDoc, Sweave and knitr documents (and more), e.g. 'Today's date is <%=Sys.Date()%>'.  Contrary to many other literate programming languages, with RSP it is straightforward to loop over mixtures of code and text sections, e.g. in month-by-month summaries.  RSP has also several preprocessing directives for incorporating static and dynamic contents of external files (local or online) among other things.  Functions rstring() and rcat() make it easy to process RSP strings, rsource() sources an RSP file as it was an R script, while rfile() compiles it (even online) into its final output format, e.g. rfile('report.tex.rsp') generates 'report.pdf' and rfile('report.md.rsp') generates 'report.html'.  RSP is ideal for self-contained scientific reports and R package vignettes.  It's easy to use - if you know how to write an R script, you'll be up and running within minutes."}, "approxmatch": {"categories": ["CausalInference"], "description": "Tools for constructing a matched design with multiple comparison groups.\n Further specifications of refined covariate balance restriction and exact match on \n covariate can be imposed. Matches are approximately optimal in  the sense that the \n cost of the solution is at most twice the optimal cost, Crama and Spieksma (1992) \n <doi:10.1016/0377-2217(92)90078-N>, Karmakar, Small and Rosenbaum (2019)\n <doi:10.1080/10618600.2019.1584900>."}, "ufRisk": {"categories": ["Finance"], "description": "Enables the user to calculate Value at Risk (VaR) and Expected \n    Shortfall (ES) by means of various parametric and semiparametric \n    GARCH-type models. For the latter the estimation of the nonparametric scale\n    function is carried out by means of a data-driven smoothing approach. Model\n    quality, in terms of forecasting VaR and ES, can be assessed by means of \n    various backtesting methods such as the traffic light test for VaR and a \n    newly developed traffic light test for ES. The approaches implemented in \n    this package are described in e.g. Feng Y., Beran J., Letmathe S. and \n    Ghosh S. (2020) <https://ideas.repec.org/p/pdn/ciepap/137.html> as well as \n    Letmathe S., Feng Y. and Uhde A. (2021) \n    <https://ideas.repec.org/p/pdn/ciepap/141.html>. "}, "engsoccerdata": {"categories": ["SportsAnalytics"], "description": "Analyzing English & European soccer results data from 1871-2016."}, "vardiag": {"categories": ["Spatial"], "description": "Interactive variogram diagnostics."}, "UComp": {"categories": ["TimeSeries"], "description": "Comprehensive analysis and forecasting \n             of univariate time series using automatic \n             unobserved components models and algorithms.\n             Harvey, AC (1989) <doi:10.1017/CBO9781107049994>.\n             Pedregal, DJ and Young PC (2002) <doi:10.1002/9780470996430>.\n             Durbin J and Koopman SJ (2012) <doi:10.1093/acprof:oso/9780199641178.001.0001>."}, "psqn": {"categories": ["Optimization"], "description": "Provides quasi-Newton methods to minimize partially separable\n    functions. The methods are largely described by  \n    Nocedal and Wright (2006) <doi:10.1007/978-0-387-40065-5>."}, "ecodist": {"categories": ["Environmetrics", "Psychometrics"], "description": "Dissimilarity-based analysis functions including ordination and Mantel test functions, intended for use with spatial and community data. The original package description is in Goslee and Urban (2007) <doi:10.18637/jss.v022.i07>, with further statistical detail in Goslee (2010) <doi:10.1007/s11258-009-9641-0>."}, "transcribeR": {"categories": ["WebTechnologies"], "description": "Transcribes audio to text with the HP IDOL API. Includes functions to upload files, \n\t     retrieve transcriptions, and monitor jobs. "}, "bayesImageS": {"categories": ["Bayesian", "MedicalImaging"], "description": "Various algorithms for segmentation of 2D and 3D images, such\n    as computed tomography and satellite remote sensing. This package implements\n    Bayesian image analysis using the hidden Potts model with external field\n    prior of Moores et al. (2015) <doi:10.1016/j.csda.2014.12.001>.\n    Latent labels are sampled using chequerboard updating or Swendsen-Wang.\n    Algorithms for the smoothing parameter include pseudolikelihood, path sampling,\n    the exchange algorithm, approximate Bayesian computation (ABC-MCMC and ABC-SMC),\n    and the parametric functional approximate Bayesian (PFAB) algorithm. Refer to\n    <doi:10.1007/978-3-030-42553-1_6> for an overview and also to <doi:10.1007/s11222-014-9525-6>\n    and <doi:10.1214/18-BA1130> for further details of specific algorithms."}, "sampling": {"categories": ["OfficialStatistics"], "description": "Functions to draw random samples using different sampling schemes are available. Functions are also provided to obtain (generalized) calibration weights, different estimators, as well some variance estimators.  "}, "gwrr": {"categories": ["Spatial"], "description": "Fits geographically weighted regression (GWR) models and has tools to diagnose and remediate collinearity in the GWR models. Also fits geographically weighted ridge regression (GWRR) and geographically weighted lasso (GWL) models. See Wheeler (2009) <doi:10.1068/a40256> and Wheeler (2007) <doi:10.1068/a38325> for more details."}, "eefAnalytics": {"categories": ["CausalInference"], "description": "Analysing data from evaluations of educational interventions using a randomised controlled trial design. Various analytical tools to perform sensitivity analysis using different methods are supported (e.g. frequentist models with bootstrapping and permutations options, Bayesian models). The included commands can be used for simple randomised trials, cluster randomised trials and multisite trials. The methods can also be used more widely beyond education trials. This package can be used to evaluate other intervention designs using Frequentist and Bayesian multilevel models."}, "randtoolbox": {"categories": ["Distributions"], "description": "Provides (1) pseudo random generators - general linear congruential generators, \n    multiple recursive generators and generalized feedback shift register (SF-Mersenne Twister\n    algorithm and WELL generators); (2) quasi random generators - the Torus algorithm, the\n    Sobol sequence, the Halton sequence (including the Van der Corput sequence) and (3) some\n    generator tests - the gap test, the serial test, the poker test.\n    See e.g. Gentle (2003) <doi:10.1007/b97336>. The package can be provided\n    without the rngWELL dependency on demand.\n    Take a look at the Distribution task view of types and tests of random number generators.\n    Package in Memoriam of Diethelm and Barbara Wuertz."}, "NMAoutlier": {"categories": ["MetaAnalysis"], "description": "A set of functions providing several outlier (i.e., studies with extreme findings) and influential detection measures and methodologies in network meta-analysis :\n               - simple outlier and influential detection measures\n               - outlier and influential detection measures by considering study deletion (shift the mean)\n               - plots for outlier and influential detection measures\n\t       - Q-Q plot for network meta-analysis\n               - Forward Search algorithm in network meta-analysis. \n               - forward plots to monitor statistics in each step of the forward search algorithm\n               - forward plots for summary estimates and their confidence intervals in each step of forward search algorithm.   "}, "mgcv": {"categories": ["Bayesian", "Econometrics", "Environmetrics", "Spatial"], "description": "Generalized additive (mixed) models, some of their extensions and \n             other generalized ridge regression with multiple smoothing \n             parameter estimation by (Restricted) Marginal Likelihood, \n             Generalized Cross Validation and similar, or using iterated \n             nested Laplace approximation for fully Bayesian inference. See \n             Wood (2017) <doi:10.1201/9781315370279> for an overview. \n             Includes a gam() function, a wide variety of smoothers, 'JAGS' \n             support and distributions beyond the exponential family. "}, "FinTS": {"categories": ["TimeSeries"], "description": "R companion to Tsay (2005)\n Analysis of Financial Time Series, second edition (Wiley).\n Includes data sets, functions and script files\n required to work some of the examples.  Version 0.3-x\n includes R objects for all data files used in the text\n and script files to recreate most of the analyses in\n chapters 1-3 and 9 plus parts of chapters 4 and 11."}, "poweRlaw": {"categories": ["Distributions"], "description": "An implementation of maximum likelihood estimators\n    for a variety of heavy tailed distributions, including both the\n    discrete and continuous power law distributions. Additionally, a\n    goodness-of-fit based approach is used to estimate the lower cut-off\n    for the scaling region."}, "GillespieSSA": {"categories": ["DifferentialEquations"], "description": "Provides a simple to use, intuitive, and\n  extensible interface to several stochastic simulation\n  algorithms for generating simulated trajectories of finite\n  population continuous-time model. Currently it implements\n  Gillespie's exact stochastic simulation algorithm (Direct\n  method) and several approximate methods (Explicit tau-leap,\n  Binomial tau-leap, and Optimized tau-leap). The package also\n  contains a library of template models that can be run as demo\n  models and can easily be customized and extended. Currently the\n  following models are included, 'Decaying-Dimerization' reaction\n  set, linear chain system, logistic growth model, 'Lotka'\n  predator-prey model, Rosenzweig-MacArthur predator-prey model,\n  'Kermack-McKendrick' SIR model, and a 'metapopulation' SIRS model.\n  Pineda-Krch et al. (2008) <doi:10.18637/jss.v025.i12>."}, "restimizeapi": {"categories": ["Finance"], "description": "Provides the user with functions to develop their trading strategy,\n    uncover actionable trading ideas, and monitor consensus shifts with\n    crowdsourced earnings and economic estimate data directly from\n    <www.estimize.com>. Further information regarding the web services this\n    package invokes can be found at <www.estimize.com/api>."}, "HydroMe": {"categories": ["Hydrology"], "description": "This version 2 of the HydroMe v.1 package estimates the parameters\n  in infiltration and water retention models by curve-fitting methods <doi:10.1016/j.cageo.2008.08.011>. The\n  models considered are those commonly used in soil science.  It has new models\n  for water retention and characteristic curves."}, "PoissonBinomial": {"categories": ["Distributions"], "description": "Efficient implementations of multiple exact and approximate methods as described in Hong (2013) <doi:10.1016/j.csda.2012.10.006>, Biscarri, Zhao & Brunner (2018) <doi:10.1016/j.csda.2018.01.007> and Zhang, Hong & Balakrishnan (2018) <doi:10.1080/00949655.2018.1440294> for computing the probability mass, cumulative distribution and quantile functions, as well as generating random numbers for both the ordinary and generalized Poisson binomial distribution."}, "TSLSTM": {"categories": ["TimeSeries"], "description": "The LSTM (Long Short-Term Memory) model is a Recurrent Neural Network (RNN) based architecture that is widely used for time series forecasting. Min-Max transformation has been used for data preparation. Here, we have used one LSTM layer as a simple LSTM model and a Dense layer is used as the output layer. Then, compile the model using the loss function, optimizer and metrics. This package is based on Keras and TensorFlow modules and the algorithm of Paul and Garai (2021) <doi:10.1007/s00500-021-06087-4>."}, "NNS": {"categories": ["Econometrics"], "description": "Nonlinear nonparametric statistics using partial moments.  Partial moments are the elements of variance and asymptotically approximate the area of f(x).  These robust statistics provide the basis for nonlinear analysis while retaining linear equivalences.  NNS offers: Numerical integration, Numerical differentiation, Clustering, Correlation, Dependence, Causal analysis, ANOVA, Regression, Classification, Seasonality, Autoregressive modeling, Normalization and Stochastic dominance.  All routines based on: Viole, F. and Nawrocki, D. (2013), Nonlinear Nonparametric Statistics: Using Partial Moments (ISBN: 1490523995)."}, "smoof": {"categories": ["Optimization"], "description": "Provides generators for a high number of both single- and multi-\n    objective test functions which are frequently used for the benchmarking of\n    (numerical) optimization algorithms. Moreover, it offers a set of convenient\n    functions to generate, plot and work with objective functions."}, "FMC": {"categories": ["ExperimentalDesign"], "description": "Generate cost effective minimally changed run sequences \n              for symmetrical as well as asymmetrical factorial \n              designs."}, "rpms": {"categories": ["OfficialStatistics"], "description": "Functions to allow users to build and analyze design consistent \n    tree and random forest models using survey data from a complex sample \n    design.  The tree model algorithm can fit a linear model to survey data \n    in each node obtained by recursively partitioning the data.  The splitting \n    variables and selected splits are obtained using a randomized permutation \n    test procedure which adjusted for complex sample design features used to \n    obtain the data. Likewise the model fitting algorithm produces \n    design-consistent coefficients to any specified least squares linear model \n    between the dependent and independent variables used in the end nodes.\n    The main functions return the resulting binary tree or random forest as \n    an object of \"rpms\" or \"rpms_forest\" type. The package also provides methods\n    modeling a \"boosted\" tree or forest model and a tree model for zero-inflated\n    data as well as a number of functions and methods available for use with \n    these object types."}, "hdm": {"categories": ["CausalInference", "Econometrics", "MachineLearning"], "description": "Implementation of selected high-dimensional statistical and\n    econometric methods for estimation and inference. Efficient estimators and\n    uniformly valid confidence intervals for various low-dimensional causal/\n    structural parameters are provided which appear in high-dimensional\n    approximately sparse models. Including functions for fitting heteroscedastic\n    robust Lasso regressions with non-Gaussian errors and for instrumental variable\n    (IV) and treatment effect estimation in a high-dimensional setting. Moreover,\n    the methods enable valid post-selection inference and rely on a theoretically\n    grounded, data-driven choice of the penalty.\n    Chernozhukov, Hansen, Spindler (2016) <arXiv:1603.01700>."}, "XRPython": {"categories": ["NumericalMathematics"], "description": "A 'Python' interface structured according to the general\n    form described in package 'XR' and in the book \"Extending R\"."}, "condMVNorm": {"categories": ["Distributions"], "description": "Computes conditional multivariate normal densities, probabilities, and random deviates."}, "causaldata": {"categories": ["CausalInference"], "description": "Example data sets to run the example\n    problems from causal inference textbooks. Currently, contains data\n    sets for Huntington-Klein, Nick (2021) \"The Effect\" <https://theeffectbook.net>,\n    Cunningham, Scott (2021, ISBN-13: 978-0-300-25168-5) \"Causal Inference: The Mixtape\", \n    and Hern\u00e1n, Miguel and James Robins (2020) \"Causal Inference: What If\" \n    <https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/>."}, "effectsize": {"categories": ["MetaAnalysis"], "description": "Provide utilities to work with indices of effect size and\n    standardized parameters for a wide variety of models (see list of\n    supported models using the function 'insight::supported_models()'),\n    allowing computation of and conversion between indices such as Cohen's\n    d, r, odds, etc."}, "difR": {"categories": ["Psychometrics"], "description": "Provides a collection of standard methods to detect differential item functioning among dichotomously scored items. Methods for uniform and non-uniform DIF, based on test-score or IRT methods, for comparing two or more than two groups of respondents, are available (Magis, Beland, Tuerlinckx and De Boeck,A General Framework and an R Package for the Detection of Dichotomous Differential Item Functioning, Behavior Research Methods, 42, 2010, 847-862 <doi:10.3758/BRM.42.3.847>)."}, "DoE.wrapper": {"categories": ["ExperimentalDesign"], "description": "Various kinds of designs for\n (industrial) experiments can be created. The package uses, and sometimes enhances,\n        design generation routines from other packages. \n        So far, response surface designs from package rsm, latin hypercube\n        samples from packages lhs and DiceDesign, and \n        D-optimal designs from package AlgDesign have been implemented."}, "EloChoice": {"categories": ["SportsAnalytics"], "description": "Allows calculating global scores for characteristics of visual stimuli as assessed by human raters. Stimuli are presented as sequence of pairwise comparisons ('contests'), during each of which a rater expresses preference for one stimulus over the other (forced choice). The algorithm for calculating global scores is based on Elo rating, which updates individual scores after each single pairwise contest. Elo rating is widely used to rank chess players according to their performance. Its core feature is that dyadic contests with expected outcomes lead to smaller changes of participants' scores than outcomes that were unexpected. As such, Elo rating is an efficient tool to rate individual stimuli when a large number of such stimuli are paired against each other in the context of experiments where the goal is to rank stimuli according to some characteristic of interest. Clark et al (2018) <doi:10.1371/journal.pone.0190393> provide details."}, "GlarmaVarSel": {"categories": ["TimeSeries"], "description": "Performs variable selection in high-dimensional sparse GLARMA models. For further details we refer the reader to the paper Gomtsyan et al. (2020), <arXiv:2007.08623v1>."}, "hda": {"categories": ["MachineLearning"], "description": "Functions to perform dimensionality reduction for classification if the covariance matrices of the classes are unequal. "}, "sanic": {"categories": ["NumericalMathematics"], "description": "Routines for solving large systems of linear equations in R.\n  Direct and iterative solvers from the Eigen C++ library are made available.\n  Solvers include Cholesky, LU, QR, and Krylov subspace methods (Conjugate\n  Gradient, BiCGSTAB). Both dense and sparse problems are supported."}, "googleAuthR": {"categories": ["WebTechnologies"], "description": "Create R functions that interact with OAuth2 Google APIs \n    <https://developers.google.com/apis-explorer/> easily,\n    with auto-refresh and Shiny compatibility."}, "epitrix": {"categories": ["Epidemiology"], "description": "A collection of small functions useful for epidemics analysis and infectious disease modelling. This includes computation of basic reproduction numbers from growth rates, generation of hashed labels to anonymise data, and fitting discretised Gamma distributions."}, "gam": {"categories": ["Econometrics", "Environmetrics"], "description": "Functions for fitting and working with generalized\n\t\tadditive models, as described in chapter 7 of \"Statistical Models in S\" (Chambers and Hastie (eds), 1991), and \"Generalized Additive Models\" (Hastie and Tibshirani, 1990)."}, "simsem": {"categories": ["MissingData", "Psychometrics"], "description": "Provides an easy framework for Monte Carlo simulation in structural equation modeling, which can be used for various purposes, such as such as model fit evaluation, power analysis, or missing data handling and planning. "}, "metarep": {"categories": ["MetaAnalysis"], "description": "User-friendly package for reporting replicability-analysis methods, affixed to meta-analyses summary. This package implements the methods introduced in Jaljuli et. al. (2022) <doi:10.1080/19466315.2022.2050291>. The replicability-analysis output provides an assessment of the investigated intervention, where it offers quantification of effect replicability and assessment of the consistency of findings.\n - Replicability-analysis for fixed-effects and random-effect meta analysis: \n - r(u)-value;\n - lower bounds on the number of studies with replicated positive and\\or negative effect;\n - Allows detecting inconsistency of signals;\n - forest plots with the summary of replicability analysis results;\n - Allows Replicability-analysis with or without the common-effect assumption. "}, "coxphw": {"categories": ["Survival"], "description": "Implements weighted estimation in Cox regression as proposed by\n             Schemper, Wakounig and Heinze (Statistics in Medicine, 2009, \n             <doi:10.1002/sim.3623>) and as described in Dunkler, Ploner, Schemper and \n             Heinze (Journal of Statistical Software, 2018, <doi:10.18637/jss.v084.i02>). \n             Weighted Cox regression provides unbiased average hazard ratio \n             estimates also in case of non-proportional hazards. \n             Approximated generalized concordance probability an effect size measure for clear-cut\n             decisions can be obtained.\n             The package provides options to estimate time-dependent effects conveniently by\n             including interactions of covariates with arbitrary functions of time, with or without\n             making use of the weighting option."}, "maxstat": {"categories": ["Survival"], "description": "Maximally selected rank statistics with\n several p-value approximations."}, "suRtex": {"categories": ["ReproducibleResearch"], "description": "suRtex was designed for easy descriptive statistic reporting of categorical survey data (e.g., Likert scales) in LaTeX. suRtex takes a matrix or data frame and produces the LaTeX code necessary for a sideways table creation. Mean, median, standard deviation, and sample size are optional."}, "knitcitations": {"categories": ["ReproducibleResearch"], "description": "Provides the ability to create dynamic citations\n    in which the bibliographic information is pulled from the web rather\n    than having to be entered into a local database such as 'bibtex' ahead of\n    time. The package is primarily aimed at authoring in the R 'markdown'\n    format, and can provide outputs for web-based authoring such as linked\n    text for inline citations.  Cite using a 'DOI', URL, or\n    'bibtex' file key.  See the package URL for details."}, "ssize.fdr": {"categories": ["ExperimentalDesign"], "description": "Functions that calculate appropriate sample sizes for one-sample t-tests, two-sample t-tests, and F-tests for microarray experiments based on desired power while controlling for false discovery rates. For all tests, the standard deviations (variances) among genes can be assumed fixed or random.  This is also true for effect sizes among genes in one-sample and two sample experiments. Functions also output a chart of power versus sample size, a table of power at different sample sizes, and a table of critical test values at different sample sizes."}, "WRTDStidal": {"categories": ["Hydrology"], "description": "An adaptation for estuaries (tidal waters) of weighted regression\n    on time, discharge, and season to evaluate trends in water quality time series."}, "outbreaks": {"categories": ["Epidemiology"], "description": "Empirical or simulated disease outbreak data, provided either as\n    RData or as text files."}, "splitTools": {"categories": ["MachineLearning"], "description": "Fast, lightweight toolkit for data splitting. Data sets can\n    be partitioned into disjoint groups (e.g. into training, validation,\n    and test) or into (repeated) k-folds for subsequent cross-validation.\n    Besides basic splits, the package supports stratified, grouped as well\n    as blocked splitting. Furthermore, cross-validation folds for time\n    series data can be created. See e.g. Hastie et al. (2001)\n    <doi:10.1007/978-0-387-84858-7> for the basic background on data\n    partitioning and cross-validation."}, "spTimer": {"categories": ["Bayesian", "Spatial", "SpatioTemporal", "TimeSeries"], "description": "Fits, spatially predicts and temporally forecasts large amounts of space-time data using  [1] Bayesian Gaussian Process (GP) Models, [2] Bayesian Auto-Regressive (AR) Models, and [3] Bayesian Gaussian Predictive Processes (GPP) based AR Models for spatio-temporal big-n problems. Bakar and Sahu (2015) <doi:10.18637/jss.v063.i15>."}, "popEpi": {"categories": ["Epidemiology"], "description": "Enables computation of epidemiological statistics, including those \n    where counts or mortality rates of the reference population are used. \n    Currently supported: excess hazard models (Dickman, Sloggett, Hills, and \n    Hakulinen (2012) <doi:10.1002/sim.1597>), rates, mean survival times, \n    relative/net survival (in particular the Ederer II (Ederer and Heise (1959))\n    and Pohar Perme (Pohar Perme, Stare, and Esteve \n    (2012) <doi:10.1111/j.1541-0420.2011.01640.x>) estimators), \n    and standardized incidence and mortality ratios, \n    all of which can be easily adjusted for by covariates such as \n    age. Fast splitting and aggregation of 'Lexis' objects (from package 'Epi') \n    and other computations achieved using 'data.table'. "}, "ordinal": {"categories": ["Econometrics", "Psychometrics"], "description": "Implementation of cumulative link (mixed) models also known\n    as ordered regression models, proportional odds models, proportional\n    hazards models for grouped survival times and ordered logit/probit/...\n    models. Estimation is via maximum likelihood and mixed models are fitted\n    with the Laplace approximation and adaptive Gauss-Hermite quadrature.\n    Multiple random effect terms are allowed and they may be nested, crossed or\n    partially nested/crossed. Restrictions of symmetry and equidistance can be\n    imposed on the thresholds (cut-points/intercepts). Standard model\n    methods are available (summary, anova, drop-methods, step,\n    confint, predict etc.) in addition to profile methods and slice\n    methods for visualizing the likelihood function and checking\n    convergence."}, "concurve": {"categories": ["MetaAnalysis"], "description": "Computes compatibility (confidence) distributions along with their corresponding P-values,\n    S-values, and likelihoods. The intervals can be plotted to form the distributions themselves.  \n    Functions can be compared to one another to see how much they overlap. Results can be\n    exported to Microsoft Word, Powerpoint, and TeX documents. The package currently supports \n    resampling methods, computing differences, generalized linear models, mixed-effects models, \n    survival analysis, and meta-analysis. These methods are discussed \n    by Schweder T, Hjort NL. (2016, ISBN:9781316445051) and \n    Rafi Z, Greenland S. (2020) <doi:10.1186/s12874-020-01105-9>."}, "elastic": {"categories": ["Databases"], "description": "Connect to 'Elasticsearch', a 'NoSQL' database built on the 'Java'\n    Virtual Machine. Interacts with the 'Elasticsearch' 'HTTP' API\n    (<https://www.elastic.co/elasticsearch/>), including functions for\n    setting connection details to 'Elasticsearch' instances, loading bulk data,\n    searching for documents with both 'HTTP' query variables and 'JSON' based body\n    requests. In addition, 'elastic' provides functions for interacting with API's\n    for 'indices', documents, nodes, clusters, an interface to the cat API, and\n    more."}, "psychonetrics": {"categories": ["Psychometrics"], "description": "Multi-group (dynamical) structural equation models in combination with confirmatory network models from cross-sectional, time-series and panel data <doi:10.31234/osf.io/8ha93>. Allows for confirmatory testing and fit as well as exploratory model search."}, "VAR.etp": {"categories": ["TimeSeries"], "description": "A collection of the functions for estimation, hypothesis testing, prediction for stationary vector autoregressive models."}, "nbTransmission": {"categories": ["Epidemiology"], "description": "Estimates the relative transmission probabilities between cases in an infectious disease outbreak or cluster using naive Bayes. Included are various functions to use these probabilities to estimate transmission parameters such as the generation/serial interval and reproductive number as well as finding the contribution of covariates to the probabilities and visualizing results. The ideal use is for an infectious disease dataset with metadata on the majority of cases but more informative data such as contact tracing or pathogen whole genome sequencing on only a subset of cases. For a detailed description of the methods see Leavitt et al. (2020) <doi:10.1093/ije/dyaa031>."}, "rpsftm": {"categories": ["CausalInference"], "description": "Implements methods described by the paper Robins and Tsiatis (1991) <doi:10.1080/03610929108830654>. These use g-estimation to estimate the causal effect of a treatment in a two-armed randomised control trial where non-compliance exists and is measured, under an assumption of an accelerated failure time model and no unmeasured confounders."}, "PermAlgo": {"categories": ["Survival"], "description": "This version of the permutational algorithm generates a\n        dataset in which event and censoring times are conditional on\n        an user-specified list of covariates, some or all of which are\n        time-dependent."}, "kernlab": {"categories": ["Cluster", "MachineLearning", "NaturalLanguageProcessing", "Optimization"], "description": "Kernel-based machine learning methods for classification,\n        regression, clustering, novelty detection, quantile regression\n        and dimensionality reduction.  Among other methods 'kernlab'\n        includes Support Vector Machines, Spectral Clustering, Kernel\n        PCA, Gaussian Processes and a QP solver."}, "distrTeach": {"categories": ["Distributions", "TeachingStatistics"], "description": "Provides flexible examples of LLN and CLT for teaching purposes in secondary\n        school."}, "imp4p": {"categories": ["MissingData"], "description": "Functions to analyse missing value mechanisms and to impute data sets in the context of bottom-up MS-based proteomics."}, "tseries": {"categories": ["Econometrics", "Environmetrics", "Finance", "TimeSeries"], "description": "Time series analysis and computational finance."}, "JuliaCall": {"categories": ["NumericalMathematics"], "description": "Provides an R interface to 'Julia',\n    which is a high-level, high-performance dynamic programming language\n    for numerical computing, see <https://julialang.org/> for more information.\n    It provides a high-level interface as well as a low-level interface.\n    Using the high level interface, you could call any 'Julia' function just like\n    any R function with automatic type conversion. Using the low level interface,\n    you could deal with C-level SEXP directly while enjoying the convenience of\n    using a high-level programming language like 'Julia'."}, "lvnet": {"categories": ["GraphicalModels", "Psychometrics"], "description": "Estimate, fit and compare Structural Equation Models (SEM) and network models (Gaussian Graphical Models; GGM) using OpenMx. Allows for two possible generalizations to include GGMs in SEM: GGMs can be used between latent variables (latent network modeling; LNM) or between residuals (residual network modeling; RNM). For details, see Epskamp, Rhemtulla and Borsboom (2017) <doi:10.1007/s11336-017-9557-x>."}, "pdftables": {"categories": ["WebTechnologies"], "description": "Allows the user to convert PDF tables to formats more amenable to\n  analysis ('.csv', '.xml', or '.xlsx') by wrapping the PDFTables API.\n  In order to use the package, the user needs to sign up for an API account\n  on the PDFTables website (<https://pdftables.com/pdf-to-excel-api>).\n  The package works by taking a PDF file as input, uploading it to PDFTables,\n  and returning a file with the extracted data."}, "adaptivetau": {"categories": ["DifferentialEquations"], "description": "Implements adaptive tau leaping to approximate the\n        trajectory of a continuous-time stochastic process as\n        described by Cao et al. (2007) The Journal of Chemical Physics\n        <doi:10.1063/1.2745299> (aka. the Gillespie stochastic\n        simulation algorithm).  This package is based upon work\n        supported by NSF DBI-0906041 and NIH K99-GM104158 to Philip\n        Johnson and NIH R01-AI049334 to Rustom Antia."}, "searchConsoleR": {"categories": ["WebTechnologies"], "description": "Provides an interface with the Google Search Console,\n    formally called Google Webmaster Tools."}, "DynTxRegime": {"categories": ["CausalInference"], "description": "Methods to estimate dynamic treatment regimes using Interactive\n  Q-Learning, Q-Learning, weighted learning, and value-search methods based on \n  Augmented Inverse Probability Weighted Estimators and Inverse Probability\n  Weighted Estimators. Dynamic Treatment Regimes: Statistical Methods for \n  Precision Medicine, Tsiatis, A. A., Davidian, M. D., Holloway, S. T., and Laber, E. B., \n  Chapman & Hall/CRC Press, 2020, ISBN:978-1-4987-6977-8."}, "CombinS": {"categories": ["ExperimentalDesign"], "description": "Series of partially balanced incomplete block designs (PBIB) based on the combinatory method (S) introduced in (Imane Rezgui et al, 2014) <doi:10.3844/jmssp.2014.45.48>; and it gives their associated U-type design."}, "InvariantCausalPrediction": {"categories": ["CausalInference"], "description": "Confidence intervals for causal effects, using data collected in different experimental or environmental conditions. Hidden variables can be included in the model with a more experimental version. "}, "PCMRS": {"categories": ["Psychometrics"], "description": "Implementation of PCMRS (Partial Credit Model with Response Styles) as proposed in by Tutz, Schauberger and Berger (2018) <doi:10.1177/0146621617748322> .  PCMRS is an extension of the regular partial credit model. PCMRS allows for an additional person parameter that characterizes the response style of the person. By taking the response style into account, the estimates of the item parameters are less biased than in partial credit models."}, "StreamMetabolism": {"categories": ["Environmetrics"], "description": "I provide functions to calculate Gross Primary Productivity, Net Ecosystem Production, and Ecosystem Respiration from single station diurnal Oxygen curves.  "}, "secr": {"categories": ["Environmetrics"], "description": "Functions to estimate the density and size of a spatially distributed animal population sampled with an array of passive detectors, such as traps, or by searching polygons or transects. Models incorporating distance-dependent detection are fitted by maximizing the likelihood. Tools are included for data manipulation and model selection."}, "BuyseTest": {"categories": ["CausalInference"], "description": "Implementation of the Generalized Pairwise Comparisons (GPC)\n             as defined in Buyse (2010) <doi:10.1002/sim.3923> for complete observations,\n             and extended in Peron (2018) <doi:10.1177/0962280216658320> to deal with right-censoring.        \n             GPC compare two groups of observations (intervention vs. control group)\n\t\t\t regarding several prioritized endpoints to estimate the probability that a random observation drawn from\n\t\t\t one group performs better than a random observation drawn from the other group (Mann-Whitney parameter).\n\t\t\t The net benefit and win ratio statistics,\n\t\t\t i.e. the difference and ratio between the probabilities relative to the intervention and control groups,\n\t\t\t can then also be estimated. Confidence intervals and p-values are obtained based on asymptotic results (Ozenne 2021 <doi:10.1177/09622802211037067>),\n\t\t\t non-parametric bootstrap, or permutations.\n\t\t\t The software enables the use of thresholds of minimal importance difference,\n\t\t\t stratification, non-prioritized endpoints (O Brien test), and can handle right-censoring and competing-risks."}, "sqldf": {"categories": ["Databases", "HighPerformanceComputing"], "description": "The sqldf() function is typically passed a single argument which \n\tis an SQL select statement where the table names are ordinary R data \n\tframe names.  sqldf() transparently sets up a database, imports the \n\tdata frames into that database, performs the SQL select or other\n\tstatement and returns the result using a heuristic to determine which \n\tclass to assign to each column of the returned data frame.  The sqldf() \n\tor read.csv.sql() functions can also be used to read filtered files \n\tinto R even if the original files are larger than R itself can handle.\n\t'RSQLite', 'RH2', 'RMySQL' and 'RPostgreSQL' backends are supported."}, "gamlss": {"categories": ["Econometrics"], "description": "Functions for fitting the Generalized Additive Models for Location Scale and Shape introduced by Rigby and Stasinopoulos (2005), <doi:10.1111/j.1467-9876.2005.00510.x>. The models use a distributional regression approach where all the parameters of the conditional distribution of the response variable are modelled using explanatory variables."}, "distrDoc": {"categories": ["Distributions"], "description": "Provides documentation in form of a common vignette to packages 'distr',\n        'distrEx', 'distrMod', 'distrSim', 'distrTEst', 'distrTeach', and 'distrEllipse'."}, "metamedian": {"categories": ["MetaAnalysis"], "description": "Implements several methods to meta-analyze studies that report the \n    sample median of the outcome. When the primary studies are one-group \n    studies, the methods of McGrath et al. (2019) <doi:10.1002/sim.8013> can \n    be applied to estimate the pooled median. In the two-group context, the \n    methods of McGrath et al. (2020) <doi:10.1002/bimj.201900036> can be \n    applied to estimate the pooled raw difference of medians across groups."}, "qualtRics": {"categories": ["WebTechnologies"], "description": "Provides functions to access survey results directly into R\n    using the 'Qualtrics' API. 'Qualtrics'\n    <https://www.qualtrics.com/about/> is an online survey and data\n    collection software platform. See <https://api.qualtrics.com/> for\n    more information about the 'Qualtrics' API.  This package is\n    community-maintained and is not officially supported by 'Qualtrics'."}, "proftools": {"categories": ["HighPerformanceComputing"], "description": "Tools for examining Rprof profile output."}, "grwat": {"categories": ["Hydrology"], "description": "River hydrograph separation and daily runoff time series analysis. Provides\n  various filters to separate baseflow and quickflow using methods by \n  Lyne and Hollick (1979) <https://www.researchgate.net/publication/272491803_Stochastic_Time-Variable_Rainfall-Runoff_Modeling>, \n  Chapman (1991) <doi:10.1029/91WR01007>, \n  Boughton (1993) <https://cir.nii.ac.jp/crid/1572543026556977024>, \n  Jakeman and Hornberger (1993) <doi:10.1029/93WR00877>, \n  Chapman and Maxwell (1996) <https://search.informit.org/doi/10.3316/informit.360361071346753>,\n  and Kudelin (1960) <https://www.worldcat.org/title/printsipy-regionalnoi-otsenki-estestvennykh-resursov-podzemnykh-vod/>.\n  Implements advanced separation technique by Rets et al. (2022) <doi:10.1134/S0097807822010146> \n  which involves meteorological data to reveal genetic components of the runoff: \n  ground, rain, thaw and spring (seasonal thaw). High-performance C++17 computation, \n  annually aggregated variables, statistical testing and numerous plotting functions \n  for high-quality visualization."}, "mvtmeta": {"categories": ["MetaAnalysis"], "description": "Functions to run fixed effects or random effects multivariate meta-analysis."}, "papeR": {"categories": ["ReproducibleResearch"], "description": "A toolbox for writing 'knitr', 'Sweave' or other 'LaTeX'- or 'markdown'-based\n\t     reports and to prettify the output of various estimated models."}, "NPHMC": {"categories": ["Survival"], "description": "An R-package for calculating sample size of a survival trial with or without cure fractions."}, "orloca": {"categories": ["HighPerformanceComputing"], "description": "Objects and methods to handle and solve the min-sum location problem, also known as Fermat-Weber problem. The min-sum location problem search for a point such that the weighted sum of the distances to the demand points are minimized. See \"The Fermat-Weber location problem revisited\" by Brimberg, Mathematical Programming, 1, pg. 71-76, 1995. <doi:10.1007/BF01592245>.\n\t     General global optimization algorithms are used to solve the problem, along with the adhoc Weiszfeld method, see \"Sur le point pour lequel la Somme des distances de n points donnes est minimum\", by Weiszfeld, Tohoku Mathematical Journal, First Series, 43, pg. 355-386, 1937 or \"On the point for which the sum of the distances to n given points is minimum\", by E. Weiszfeld and F. Plastria, Annals of Operations Research, 167, pg. 7-41, 2009. <doi:10.1007/s10479-008-0352-z>."}, "gdpc": {"categories": ["TimeSeries"], "description": "Functions to compute the Generalized Dynamic Principal Components\n    introduced in Pe\u00f1a and Yohai (2016) <doi:10.1080/01621459.2015.1072542>. The implementation\n    includes an automatic procedure proposed in Pe\u00f1a, Smucler and Yohai (2020) <doi:10.18637/jss.v092.c02>\n    for the identification of both the number of lags to be used\n    in the generalized dynamic principal components as well as the number of components required\n    for a given reconstruction accuracy."}, "Rdsdp": {"categories": ["Optimization"], "description": "R interface to DSDP semidefinite programming library. The DSDP software is a free open source implementation of an interior-point method for semidefinite programming. It provides primal and dual solutions, exploits low-rank structure and sparsity in the data, and has relatively low memory requirements for an interior-point method. "}, "rneos": {"categories": ["Optimization"], "description": "Within this package the XML-RPC API to NEOS <https://neos-server.org/neos/> is implemented. This enables the user to pass optimization problems to NEOS and retrieve results within R."}, "reservoir": {"categories": ["Hydrology"], "description": "Measure single-storage water supply system performance using resilience,\n    reliability, and vulnerability metrics; assess storage-yield-reliability\n    relationships; determine no-fail storage with sequent peak analysis; optimize\n    release decisions for water supply, hydropower, and multi-objective reservoirs\n    using deterministic and stochastic dynamic programming; generate inflow\n    replicates using parametric and non-parametric models; evaluate inflow\n    persistence using the Hurst coefficient."}, "rcdklibs": {"categories": ["ChemPhys"], "description": "An R interface to the Chemistry Development Kit, a Java library\n    for chemoinformatics. Given the size of the library itself, this package is\n    not expected to change very frequently. To make use of the CDK within R, it is\n    suggested that you use the 'rcdk' package. Note that it is possible to directly\n    interact with the CDK using 'rJava'. However 'rcdk' exposes functionality in a more\n    idiomatic way. The CDK library itself is released as LGPL and the sources can be\n    obtained from <https://github.com/cdk/cdk>."}, "rCMA": {"categories": ["Optimization"], "description": "Tool for providing access to the Java version 'CMAEvolutionStrategy' of\n    Nikolaus Hansen. 'CMA-ES' is the Covariance Matrix Adaptation Evolution Strategy,\n    see <https://www.lri.fr/~hansen/cmaes_inmatlab.html#java>."}, "quantspec": {"categories": ["TimeSeries"], "description": "Methods to determine, smooth and plot quantile periodograms for\n    univariate and multivariate time series. See Kley (2016) <doi:10.18637/jss.v070.i03>\n    for a description and tutorial."}, "ecd": {"categories": ["Distributions"], "description": "Elliptic lambda distribution and lambda option pricing model\n    have been evolved into a framework of\n    stable-law inspired distributions,\n    such as the extended stable lambda distribution for asset return,\n    stable count distribution for volatility,\n    and Lihn-Laplace process as a leptokurtic extension of Wiener process.\n    This package contains functions for the computation of\n    density, probability, quantile, random variable, fitting procedures,\n    option prices, volatility smile. It also comes with sample financial data,\n    and plotting routines."}, "tmle": {"categories": ["CausalInference"], "description": "Targeted maximum likelihood estimation of point treatment effects (Targeted Maximum Likelihood Learning, The International Journal of Biostatistics, 2(1), 2006.  This version automatically estimates the additive treatment effect among the treated (ATT) and among the controls (ATC).  The tmle() function calculates the adjusted marginal difference in mean outcome associated with a binary point treatment, for continuous or binary outcomes.  Relative risk and odds ratio estimates are also reported for binary outcomes. Missingness in the outcome is allowed, but not in treatment assignment or baseline covariate values.  The population mean is calculated when there is missingness, and no variation in the treatment assignment. The tmleMSM() function estimates the parameters of a marginal structural model for a binary point treatment effect. Effect estimation stratified by a binary mediating variable is also available. An ID argument can be used to identify repeated measures. Default settings call 'SuperLearner' to estimate the Q and g portions of the likelihood, unless values or a user-supplied regression function are passed in as arguments. "}, "DiscreteWeibull": {"categories": ["Distributions"], "description": "Probability mass function, distribution function, quantile function, random generation and parameter estimation for the type I and III discrete Weibull distributions."}, "rJava": {"categories": ["HighPerformanceComputing"], "description": "Low-level interface to Java VM very much like .C/.Call and friends. Allows creation of objects, calling methods and accessing fields."}, "odr": {"categories": ["ExperimentalDesign"], "description": "Calculate the optimal sample allocation that produces the highest statistical power\n    for experimental studies under a budget constraint,\n    perform power analyses with and without accommodating cost structures of sampling, and calculate\n    the relative efficiency between two sample allocations. \n    The references \n    for the proposed methods include: \n    (1) Shen, Z., & Kelcey, B. (2020). \n    Optimal sample allocation under unequal costs in \n    cluster-randomized trials.\n    Journal of Educational and Behavioral Statistics, 45(4): 446-474.\n        <doi:10.3102/1076998620912418>.\n    (2) Shen, Z., & Kelcey, B. (2022). Optimal sampling ratios in three-level \n    multisite experiments. \n    Journal of Research on Educational Effectiveness, 15 (1), 130-150.\n    <doi:10.1080/19345747.2021.1953200>.\n    (3) Shen, Z., & Kelcey, B. (in press). Optimal sample \n    allocation in multisite randomized trials. The Journal of Experimental Education.\n    <doi:10.1080/00220973.2020.1830361>.\n    (4) Champely, S. (2020). pwr: Basic functions for power analysis \n    (Version 1.3-0) [Software]. Available from \n    <https://CRAN.R-project.org/package=pwr>."}, "deducorrect": {"categories": ["OfficialStatistics"], "description": "A collection of methods for automated data cleaning where all actions are logged."}, "sms": {"categories": ["OfficialStatistics", "Spatial"], "description": "Produce small area population estimates by fitting census data to\n    survey data."}, "smapr": {"categories": ["Hydrology"], "description": "\n    Facilitates programmatic access to NASA Soil Moisture Active\n    Passive (SMAP) data with R. It includes functions to search for, acquire,\n    and extract SMAP data."}, "DTRreg": {"categories": ["CausalInference"], "description": "Dynamic treatment regime estimation and inference via G-estimation, dynamic weighted ordinary least squares (dWOLS) and Q-learning. Inference via bootstrap and (for G-estimation) recursive sandwich estimation. Estimation and inference for survival outcomes via Dynamic Weighted Survival Modeling (DWSurv). Extension to continuous treatment variables (gdwols). Wallace et al. (2017) <doi:10.18637/jss.v080.i02>; Simoneau et al. (2020) <doi:10.1080/00949655.2020.1793341>."}, "maic": {"categories": ["CausalInference"], "description": "A generalised workflow for generation of subject weights to be \n    used in Matching-Adjusted Indirect Comparison (MAIC) per Signorovitch et \n    al. (2012) <doi:10.1016/j.jval.2012.05.004>, Signorovitch et al (2010) \n    <doi:10.2165/11538370-000000000-00000>. In MAIC, unbiased \n    comparison between outcomes of two trials is facilitated by weighting the\n    subject-level outcomes of one trial with weights derived such that the \n    weighted aggregate measures of the prognostic or effect modifying variables \n    are equal to those of the sample in the comparator trial. The functions and\n    classes included in this package wrap and abstract the process demonstrated\n    in the UK National Institute for Health and Care Excellence Decision \n    Support Unit (NICE DSU)'s example (Phillippo et al, (2016) [see URL]),\n    providing a repeatable and easily specifiable workflow for producing \n    multiple comparison variable sets against a variety of target studies, with\n    preprocessing for a number of aggregate target forms (e.g. mean, median, \n    domain limits)."}, "ExPosition": {"categories": ["Psychometrics"], "description": "A variety of descriptive multivariate analyses with the singular value decomposition,\n    such as principal components analysis, correspondence analysis, and multidimensional scaling.\n    See An ExPosition of the Singular Value Decomposition in R (Beaton et al 2014) <doi:10.1016/j.csda.2013.11.006>."}, "simsl": {"categories": ["CausalInference"], "description": "An implementation of a single-index regression for optimizing individualized dose rules from an observational study. To model interaction effects between baseline covariates and a treatment variable defined on a continuum, we employ two-dimensional penalized spline regression on an index-treatment domain, where the index is defined as a linear combination of the covariates (a single-index). An unspecified main effect for the covariates is allowed, which can also be modeled through a parametric model. A unique contribution of this work is in the parsimonious single-index parametrization specifically defined for the interaction effect term. We refer to Park, Petkova, Tarpey, and Ogden (2020) <doi:10.1111/biom.13320> (for the case of a discrete treatment) and Park, Petkova, Tarpey, and Ogden (2021) \"A single-index model with a surface-link for optimizing individualized dose rules\" <arXiv:2006.00267v2> for detail of the method. The model can take a member of the exponential family as a response variable and can also take an ordinal categorical response. The main function of this package is simsl(). "}, "rainbow": {"categories": ["FunctionalData"], "description": "Visualizing functional data and identifying functional outliers."}, "landsat": {"categories": ["Spatial"], "description": "Processing of Landsat or other multispectral satellite imagery. Includes relative normalization, image-based radiometric correction, and topographic correction options."}, "nhdplusTools": {"categories": ["Hydrology"], "description": "Tools for traversing and working with National Hydrography Dataset Plus (NHDPlus) data. All methods implemented in 'nhdplusTools' are available in the NHDPlus documentation available from the US Environmental Protection Agency <https://www.epa.gov/waterdata/basic-information>."}, "dr4pl": {"categories": ["Pharmacokinetics"], "description": "Models the relationship between dose levels and responses in a pharmacological experiment using the 4 Parameter Logistic model. Traditional packages on dose-response modelling such as 'drc' and 'nplr' often draw errors due to convergence failure especially when data have outliers or non-logistic shapes. This package provides robust estimation methods that are less affected by outliers and other initialization methods that work well for data lacking logistic shapes. We provide the bounds on the parameters of the 4PL model that prevent parameter estimates from diverging or converging to zero and base their justification in a statistical principle. These methods are used as remedies to convergence failure problems.  Gadagkar, S. R. and Call, G. B. (2015) <doi:10.1016/j.vascn.2014.08.006> Ritz, C. and Baty, F. and Streibig, J. C. and Gerhard, D. (2015) <doi:10.1371/journal.pone.0146021>."}, "dosresmeta": {"categories": ["MetaAnalysis"], "description": "Estimates dose-response relations from summarized dose-response\n    data and to combines them according to principles of (multivariate)\n    random-effects models.  "}, "worldmet": {"categories": ["Hydrology"], "description": "Functions to import data from more than 30,000 surface\n    meteorological sites around the world managed by the National Oceanic and Atmospheric Administration (NOAA) Integrated Surface\n    Database (ISD, see <https://www.ncdc.noaa.gov/isd>)."}, "reqres": {"categories": ["WebTechnologies"], "description": "In order to facilitate parsing of http requests and creating \n    appropriate responses this package provides two classes to handle a lot of\n    the housekeeping involved in working with http exchanges. The infrastructure\n    builds upon the 'rook' specification and is thus well suited to be combined\n    with 'httpuv' based web servers."}, "TLMoments": {"categories": ["Distributions"], "description": "Calculates empirical TL-moments (trimmed L-moments) of arbitrary \n    order and trimming, and converts them to distribution parameters. "}, "ROI.plugin.neos": {"categories": ["Optimization"], "description": "Enhances the 'R' Optimization Infrastructure ('ROI') package\n             with a connection to the 'neos' server. 'ROI' optimization\n             problems can be directly be sent to the 'neos' server\n             and solution obtained in the typical 'ROI' style."}, "mrf": {"categories": ["TimeSeries"], "description": "Forecasting of univariate time series using feature extraction with variable prediction methods is provided. Feature extraction is done with a redundant Haar wavelet transform with filter h = (0.5, 0.5). The advantage of the approach compared to typical Fourier based methods is an dynamic adaptation to varying seasonalities. Currently implemented prediction methods based on the selected wavelets levels and scales are a regression and a multi-layer perceptron. Forecasts can be computed for horizon 1 or higher. Model selection is performed with an evolutionary optimization. Selection criteria are currently the AIC criterion, the Mean Absolute Error or the Mean Root Error. The data is split into three parts for model selection: Training, test, and evaluation dataset. The training data is for computing the weights of a parameter set. The test data is for choosing the best parameter set. The evaluation data is for assessing the forecast performance of the best parameter set on new data unknown to the model. This work is published in Stier, Q.; Gehlert, T.; Thrun, M.C. Multiresolution Forecasting for Industrial Applications. Processes 2021, 9, 1697. <doi:10.3390/pr9101697>."}, "RMixtCompIO": {"categories": ["MissingData"], "description": "Mixture Composer <https://github.com/modal-inria/MixtComp> is a project to build mixture models with\n    heterogeneous data sets and partially missing data management.\n    It includes models for real, categorical, counting, functional and ranking data.\n    This package contains the minimal R interface of the C++ 'MixtComp' library."}, "incidence": {"categories": ["Epidemiology"], "description": "Provides functions and classes to compute, handle and visualise\n  incidence from dated events for a defined time interval. Dates can be provided\n  in various standard formats. The class 'incidence' is used to store computed\n  incidence and can be easily manipulated, subsetted, and plotted. In addition,\n  log-linear models can be fitted to 'incidence' objects using 'fit'. This\n  package is part of the RECON (<https://www.repidemicsconsortium.org/>) toolkit\n  for outbreak analysis."}, "cartogram": {"categories": ["Spatial"], "description": "Construct continuous and non-contiguous area cartograms."}, "solaR": {"categories": ["ChemPhys", "SpatioTemporal"], "description": "Calculation methods of solar radiation and performance of photovoltaic systems from daily and intradaily irradiation data sources."}, "apcluster": {"categories": ["Cluster"], "description": "Implements Affinity Propagation clustering introduced by Frey and\n\tDueck (2007) <doi:10.1126/science.1136800>. The algorithms are largely\n        analogous to the 'Matlab' code published by Frey and Dueck.\n        The package further provides leveraged affinity propagation and an\n        algorithm for exemplar-based agglomerative clustering that can also be\n        used to join clusters obtained from affinity propagation. Various\n        plotting functions are available for analyzing clustering results."}, "bnlearn": {"categories": ["Bayesian", "GraphicalModels", "HighPerformanceComputing"], "description": "Bayesian network structure learning, parameter learning and inference.\n  This package implements constraint-based (PC, GS, IAMB, Inter-IAMB, Fast-IAMB, MMPC,\n  Hiton-PC, HPC), pairwise (ARACNE and Chow-Liu), score-based (Hill-Climbing and Tabu\n  Search) and hybrid (MMHC, RSMAX2, H2PC) structure learning algorithms for discrete,\n  Gaussian and conditional Gaussian networks, along with many score functions and\n  conditional independence tests.\n  The Naive Bayes and the Tree-Augmented Naive Bayes (TAN) classifiers are also implemented.\n  Some utility functions (model comparison and manipulation, random data generation, arc\n  orientation testing, simple and advanced plots) are included, as well as support for\n  parameter estimation (maximum likelihood and Bayesian) and inference, conditional\n  probability queries, cross-validation, bootstrap and model averaging.\n  Development snapshots with the latest bugfixes are available from <https://www.bnlearn.com/>."}, "pgirmess": {"categories": ["Environmetrics"], "description": "Set of tools for reading, writing and transforming spatial and seasonal data, model selection and specific statistical tests for ecologists. It includes functions to interpolate regular positions of points between landmarks, to discretize polylines into regular point positions, link distant observations to points and convert a bounding box in a spatial object. It also provides miscellaneous functions for field ecologists such as spatial statistics and inference on diversity indexes, writing data.frame with Chinese characters."}, "mvp": {"categories": ["NumericalMathematics"], "description": "Fast manipulation of symbolic multivariate polynomials\n  using the 'Map' class of the Standard Template Library.  The package\n  uses print and coercion methods from the 'mpoly' package (Kahle 2013,\n  \"Multivariate polynomials in R\".  The R Journal, 5(1):162), but offers\n  speed improvements.  It is comparable in speed to the 'spray' package\n  for sparse arrays, but retains the symbolic benefits of 'mpoly'."}, "skellam": {"categories": ["Distributions"], "description": "Functions for the Skellam distribution, including: density\n    (pmf), cdf, quantiles and regression."}, "pseval": {"categories": ["MissingData"], "description": "Contains the core methods for the evaluation of principal\n    surrogates in a single clinical trial. Provides a flexible interface for\n    defining models for the risk given treatment and the surrogate, the models\n    for integration over the missing counterfactual surrogate responses, and the\n    estimation methods. Estimated maximum likelihood and pseudo-score can be used\n    for estimation, and the bootstrap for inference. A variety of post-estimation\n    summary methods are provided, including print, summary, plot, and testing."}, "rjstat": {"categories": ["OfficialStatistics"], "description": "Handle 'JSON-stat' format (<https://json-stat.org>) in R.\n    Not all features are supported, especially the extensive metadata\n    features of 'JSON-stat'."}, "optiscale": {"categories": ["Psychometrics"], "description": "Optimal scaling of a data vector, relative to a set of targets, is\n    obtained through a least-squares transformation subject to appropriate measurement\n    constraints. The targets are usually predicted values from a statistical\n    model. If the data are nominal level, then the transformation must be\n    identity-preserving. If the data are ordinal level, then the\n    transformation must be monotonic. If the data are discrete, then tied data\n    values must remain tied in the optimal transformation. If the data are\n    continuous, then tied data values can be untied in the optimal\n    transformation."}, "ManifoldOptim": {"categories": ["Optimization"], "description": "An R interface to version 0.3 of the 'ROPTLIB' optimization library\n    (see <https://www.math.fsu.edu/~whuang2/> for more information). Optimize real-\n    valued functions over manifolds such as Stiefel, Grassmann, and Symmetric\n    Positive Definite matrices. For details see Martin et. al. (2020) <doi:10.18637/jss.v093.i01>. \n    Note that the optional ldr package used in some of this package's examples can be obtained from either JSS \n    <https://www.jstatsoft.org/index.php/jss/article/view/v061i03/2886> or from the CRAN archives \n    <https://cran.r-project.org/src/contrib/Archive/ldr/ldr_1.3.3.tar.gz>."}, "lpirfs": {"categories": ["Econometrics"], "description": "Provides functions to estimate and visualize linear as well as nonlinear impulse \n             responses based on local projections by Jord\u00e0 (2005) <doi:10.1257/0002828053828518>.\n             The methods and the package are explained in detail in Ad\u00e4mmer (2019) <doi:10.32614/RJ-2019-052>."}, "NTS": {"categories": ["TimeSeries"], "description": "Simulation, estimation, prediction procedure, and model identification methods for nonlinear time series analysis, including threshold autoregressive models, Markov-switching models, convolutional functional autoregressive models, nonlinearity tests, Kalman filters and various sequential Monte Carlo methods. More examples and details about this package can be found in the book \"Nonlinear Time Series Analysis\" by Ruey S. Tsay and Rong Chen, John Wiley & Sons, 2018 (ISBN: 978-1-119-26407-1)."}, "OasisR": {"categories": ["Spatial"], "description": "A set of indexes and tests for the analysis of social segregation."}, "DoE.MIParray": {"categories": ["ExperimentalDesign"], "description": "'CRAN' packages 'DoE.base' and 'Rmosek' and non-'CRAN' package 'gurobi' are enhanced with functionality for the creation of optimized arrays for experimentation, where optimization is in terms of generalized minimum aberration. It is also possible to optimally extend existing arrays to larger run size. The package writes 'MPS' (Mathematical Programming System) files for use with any mixed integer optimization software that can process such files. If at least one of the commercial products 'Gurobi' or 'Mosek' (free academic licenses available for both) is available, the package also creates arrays by optimization. For installing 'Gurobi' and its R package 'gurobi', follow instructions at <https://www.gurobi.com/products/gurobi-optimizer/> and <https://www.gurobi.com/documentation/7.5/refman/r_api_overview.html> (or higher version). For installing 'Mosek' and its R package 'Rmosek', follow instructions at <https://www.mosek.com/downloads/> and <https://docs.mosek.com/8.1/rmosek/install-interface.html>, or use the functionality in the stump CRAN R package 'Rmosek'."}, "dittodb": {"categories": ["Databases"], "description": "Testing and documenting code that communicates with remote\n  databases can be painful. Although the interaction with R is usually relatively \n  simple (e.g. data(frames) passed to and from a database), because they rely on \n  a separate service and the data there, testing them can be difficult to set up,\n  unsustainable in a continuous integration environment, or impossible without \n  replicating an entire production cluster. This package addresses that by \n  allowing you to make recordings from your database interactions and then play \n  them back while testing (or in other contexts) all without needing to spin up \n  or have access to the database your code would typically connect to."}, "fastrmodels": {"categories": ["SportsAnalytics"], "description": "A data package that hosts all models for the\n    'nflfastR' package."}, "distrMod": {"categories": ["Distributions"], "description": "Implements S4 classes for probability models based on packages 'distr' and\n            'distrEx'."}, "mixsqp": {"categories": ["Optimization"], "description": "Provides an optimization method based on sequential\n    quadratic programming (SQP) for maximum likelihood estimation of\n    the mixture proportions in a finite mixture model where the\n    component densities are known. The algorithm is expected to obtain\n    solutions that are at least as accurate as the state-of-the-art\n    MOSEK interior-point solver (called by function \"KWDual\" in the\n    'REBayes' package), and they are expected to arrive at solutions\n    more quickly when the number of samples is large and the number of\n    mixture components is not too large. This implements the \"mix-SQP\"\n    algorithm, with some improvements, described in Y. Kim,\n    P. Carbonetto, M. Stephens & M. Anitescu (2020)\n    <doi:10.1080/10618600.2019.1689985>."}, "nFactors": {"categories": ["Psychometrics"], "description": "Indices, heuristics and strategies to help determine the number of factors/components to retain:\n                1. Acceleration factor (af with or without Parallel Analysis);\n                2. Optimal Coordinates (noc with or without Parallel Analysis);\n                3. Parallel analysis (components, factors and bootstrap);\n                4. lambda > mean(lambda) (Kaiser, CFA and related);\n                5. Cattell-Nelson-Gorsuch (CNG);\n                6. Zoski and Jurs multiple regression (b, t and p);\n                7. Zoski and Jurs standard error of the regression coeffcient (sescree);\n                8. Nelson R2;\n                9. Bartlett khi-2;\n               10. Anderson khi-2;\n               11. Lawley khi-2 and\n               12. Bentler-Yuan khi-2."}, "rfigshare": {"categories": ["WebTechnologies"], "description": "An R interface to 'figshare'."}, "bcp": {"categories": ["Bayesian", "HighPerformanceComputing"], "description": "Provides an implementation of the Barry and Hartigan (1993) product\n    partition model for the normal errors change point problem using Markov Chain\n    Monte Carlo. It also extends the methodology to regression models on a connected\n    graph (Wang and Emerson, 2015); this allows estimation of change point models\n    with multivariate responses. Parallel MCMC, previously available in bcp v.3.0.0,\n    is currently not implemented."}, "NetLogoR": {"categories": ["Spatial"], "description": "Build and run spatially explicit\n    agent-based models using only the R platform. 'NetLogoR' follows the same\n    framework as the 'NetLogo' software\n    (Wilensky, 1999 <http://ccl.northwestern.edu/netlogo/>) and is a translation\n    in R of the structure and functions of 'NetLogo'.\n    'NetLogoR' provides new R classes to define model agents and functions to\n    implement spatially explicit agent-based models in the R environment.\n    This package allows benefiting of the fast and easy coding phase from the\n    highly developed 'NetLogo' framework, coupled with the versatility, power\n    and massive resources of the R software.\n    Examples of three models (Ants <http://ccl.northwestern.edu/netlogo/models/Ants>,\n    Butterfly (Railsback and Grimm, 2012) and Wolf-Sheep-Predation\n    <http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation>) written using\n    'NetLogoR' are available. The 'NetLogo' code of the original version of these\n    models is provided alongside.\n    A programming guide inspired from the 'NetLogo' Programming Guide\n    (<https://ccl.northwestern.edu/netlogo/docs/programming.html>) and a dictionary\n    of 'NetLogo' primitives (<https://ccl.northwestern.edu/netlogo/docs/dictionary.html>)\n    equivalences are also available.\n    NOTE: To increment 'time', these functions can use a for loop or can be\n    integrated with a discrete event simulator, such as 'SpaDES'\n    (<https://cran.r-project.org/package=SpaDES>).\n    The suggested package 'fastshp' can be installed with\n    'install.packages(\"fastshp\", repos = \"https://rforge.net\", type = \"source\")'."}, "binomSamSize": {"categories": ["ClinicalTrials"], "description": "\n  A suite of functions to compute confidence intervals and necessary\n  sample sizes for the parameter p of the Bernoulli B(p)\n  distribution under simple random sampling or under pooled\n  sampling. Such computations are e.g. of interest when investigating\n  the incidence or prevalence in populations.\n  The package contains functions to compute coverage probabilities and\n  coverage coefficients of the provided confidence intervals\n  procedures. Sample size calculations are based on expected length."}, "mxkssd": {"categories": ["ExperimentalDesign"], "description": "Generates efficient balanced mixed-level k-circulant supersaturated designs by interchanging the elements of the generator vector. Attempts to generate a supersaturated design that has EfNOD efficiency more than user specified efficiency level (mef). Displays the progress of generation of an efficient mixed-level k-circulant design through a progress bar. The progress of 100 per cent means that one full round of interchange is completed. More than one full round (typically 4-5 rounds) of interchange may be required for larger designs. For more details, please see Mandal, B.N., Gupta V. K. and Parsad, R. (2011). Construction of Efficient Mixed-Level k-Circulant Supersaturated Designs, Journal of Statistical Theory and Practice, 5:4, 627-648, <doi:10.1080/15598608.2011.10483735>."}, "bayeslincom": {"categories": ["Bayesian"], "description": "Computes point estimates, standard deviations, and credible intervals \n             for linear combinations of posterior samples. Optionally performs region\n             practical equivalence (ROPE) tests as described in \n             Kruschke and Liddell (2018) <doi:10.3758/s13423-016-1221-4>."}, "MCMC4Extremes": {"categories": ["ExtremeValue"], "description": "Provides some function to perform posterior estimation for some distribution, with emphasis to extreme value distributions. It contains some extreme datasets, and functions that perform the runs of posterior points of the GPD and GEV distribution. The package calculate some important extreme measures like return level for each t periods of time, and some plots as the predictive distribution, and return level plots. "}, "poolr": {"categories": ["MetaAnalysis"], "description": "Functions for pooling/combining the results (i.e., p-values) from (dependent) hypothesis tests. Included are Fisher's method, Stouffer's method, the inverse chi-square method, the Bonferroni method, Tippett's method, and the binomial test. Each method can be adjusted based on an estimate of the effective number of tests or using empirically derived null distribution using pseudo replicates. For Fisher's, Stouffer's, and the inverse chi-square method, direct generalizations based on multivariate theory are also available (leading to Brown's method, Strube's method, and the generalized inverse chi-square method). An introduction can be found in Cinar and Viechtbauer (2022) <doi:10.18637/jss.v101.i01>. "}, "ncvreg": {"categories": ["MachineLearning"], "description": "Fits regularization paths for linear regression, GLM, and Cox\n  regression models using lasso or nonconvex penalties, in particular the\n  minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD)\n  penalty, with options for additional L2 penalties (the \"elastic net\" idea).\n  Utilities for carrying out cross-validation as well as post-fitting\n  visualization, summarization, inference, and prediction are also provided.\n  For more information, see Breheny and Huang (2011) <doi:10.1214/10-AOAS388>\n  or visit the ncvreg homepage <https://pbreheny.github.io/ncvreg/>."}, "DatabionicSwarm": {"categories": ["Cluster"], "description": "Algorithms implementing populations of agents that interact with one another and sense their environment may exhibit emergent behavior such as self-organization and swarm intelligence. Here, a swarm system called Databionic swarm (DBS) is introduced which was published in Thrun, M.C., Ultsch A.: \"Swarm Intelligence for Self-Organized Clustering\" (2020), Artificial Intelligence, <doi:10.1016/j.artint.2020.103237>. DBS is able to adapt itself to structures of high-dimensional data such as natural clusters characterized by distance and/or density based structures in the data space. The first module is the parameter-free projection method called Pswarm (Pswarm()), which exploits the concepts of self-organization and emergence, game theory, swarm intelligence and symmetry considerations. The second module is the parameter-free high-dimensional data visualization technique, which generates projected points on the topographic map with hypsometric tints defined by the generalized U-matrix (GeneratePswarmVisualization()). The third module is the clustering method itself with non-critical parameters (DBSclustering()). Clustering can be verified by the visualization and vice versa. The term DBS refers to the method as a whole. It enables even a non-professional in the field of data mining to apply its algorithms for visualization and/or clustering to data sets with completely different structures drawn from diverse research fields. The comparison to common projection methods can be found in the book of Thrun, M.C.: \"Projection Based Clustering through Self-Organization and Swarm Intelligence\" (2018) <doi:10.1007/978-3-658-20540-9>. A comparison to 26 common clustering algorithms on 15 datasets is presented on the website."}, "smd": {"categories": ["MetaAnalysis"], "description": "Computes standardized mean differences and confidence intervals for \n    multiple data types based on Yang, D., & Dalton, J. E. (2012) \n    <http://www.lerner.ccf.org/qhs/software/lib/stddiff.pdf>. "}, "easySdcTable": {"categories": ["OfficialStatistics"], "description": "The main function, ProtectTable(), performs table suppression according to a \n frequency rule with a data set as the only required input. Within this function, \n protectTable(), protect_linked_tables() or runArgusBatchFile() in package 'sdcTable' is called. \n Lists of level-hierarchy (parameter 'dimList') and other required input to these functions \n are created automatically. \n The suppression method Gauss (default) is an additional method that is not available in 'sdcTable'.\n The function, PTgui(), starts a graphical user interface based on the 'shiny' package."}, "mhurdle": {"categories": ["Econometrics"], "description": "Estimation of models with zero left-censored variables.\n             Null values may be caused by a selection process\n\t     Cragg (1971) <doi:10.2307/1909582>, insufficient resources\n\t     Tobin (1958) <doi:10.2307/1907382> or infrequency of purchase\n\t     Deaton and Irish (1984) <doi:10.1016/0047-2727(84)90067-7>."}, "gld": {"categories": ["Distributions"], "description": "The generalised lambda distribution, or Tukey lambda distribution, \n  provides a wide variety of shapes with one functional form.   \n  This package provides random numbers, quantiles, probabilities, \n  densities and density quantiles for four different types of the distribution,\n  the FKML (Freimer et al 1988), RS (Ramberg and Schmeiser 1974), GPD (van Staden\n  and Loots 2009) and FM5 - see documentation for details.\n  It provides the density function, distribution function, and Quantile-Quantile \n  plots.  \n  It implements a variety of estimation methods for the distribution, \n  including diagnostic plots. \n  Estimation methods include the starship (all 4 types), \n  method of L-Moments for the GPD and FKML types, and a \n  number of methods for only the FKML type.  \n  These include maximum likelihood, maximum product of spacings, \n  Titterington's method, Moments, Trimmed L-Moments and \n  Distributional Least Absolutes. "}, "synthesis": {"categories": ["Hydrology", "TimeSeries"], "description": "Generate synthetic time series from commonly used statistical models, including linear, nonlinear and chaotic systems. Applications to testing methods can be found in Jiang, Z., Sharma, A., & Johnson, F. (2019) <doi:10.1016/j.advwatres.2019.103430> and Jiang, Z., Sharma, A., & Johnson, F. (2020) <doi:10.1029/2019WR026962> associated with an open-source tool by Jiang, Z., Rashid, M. M., Johnson, F., & Sharma, A. (2020) <doi:10.1016/j.envsoft.2020.104907>."}, "AssetCorr": {"categories": ["Finance"], "description": "Functions for the estimation of intra- and inter-cohort correlations in the Vasicek credit portfolio model. For intra-cohort correlations, the package covers the two method of moments estimators of Gordy (2000) <doi:10.1016/S0378-4266(99)00054-0>, the method of moments estimator of Lucas (1995) <https://jfi.pm-research.com/content/4/4/76> and a Binomial approximation extension of this approach. Moreover, the maximum likelihood estimators of Gordy and Heitfield (2010) <http://elsa.berkeley.edu/~mcfadden/e242_f03/heitfield.pdf> and Duellmann and Gehde-Trapp (2004) <http://hdl.handle.net/10419/19729> are implemented. For inter-cohort correlations, the method of moments estimator of Bluhm and Overbeck (2003) <doi:10.1007/978-3-642-59365-9_2>/Bams et al. (2016) <https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2676595> is provided and the maximum likelihood estimators comprise the approaches of Gordy and Heitfield (2010)/Kalkbrener and Onwunta (2010) <ISBN: 978-1906348250> and Pfeuffer et al. (2020). Bootstrap and Jackknife procedures for bias correction are included as well as the method of moments estimator of Frei and Wunsch (2018) <doi:10.21314/JCR.2017.231> for auto-correlated time series."}, "ifaTools": {"categories": ["Psychometrics"], "description": "Tools, tutorials, and demos of Item Factor Analysis using 'OpenMx'.\n    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>."}, "ajv": {"categories": ["WebTechnologies"], "description": "A thin wrapper around the 'ajv' JSON validation package for\n    JavaScript. See <http://epoberezkin.github.io/ajv/> for details."}, "bmrm": {"categories": ["MachineLearning"], "description": "Bundle methods for minimization of convex and non-convex risk\n    under L1 or L2 regularization. Implements the algorithm proposed by Teo et\n    al. (JMLR 2010) as well as the extension proposed by Do and Artieres (JMLR\n    2012). The package comes with lot of loss functions for machine learning\n    which make it powerful for big data analysis. The applications includes:\n    structured prediction, linear SVM, multi-class SVM, f-beta optimization,\n    ROC optimization, ordinal regression, quantile regression,\n    epsilon insensitive regression, least mean square, logistic regression,\n    least absolute deviation regression (see package examples), etc... all with\n    L1 and L2 regularization."}, "rgeoda": {"categories": ["Spatial"], "description": "Provides spatial data analysis functionalities including Exploratory Spatial Data Analysis, \n    Spatial Cluster Detection and Clustering Analysis, Regionalization, etc. based on the C++ source code \n    of 'GeoDa', which is an open-source software tool that serves as an introduction to spatial data analysis.\n    The 'GeoDa' software and its documentation are available at <https://geodacenter.github.io>."}, "GeneNet": {"categories": ["GraphicalModels"], "description": "Analyzes gene expression\n  (time series) data with focus on the inference of gene networks.\n  In particular, GeneNet implements the methods of Schaefer and \n  Strimmer (2005a,b,c) and Opgen-Rhein and Strimmer (2006, 2007)\n  for learning large-scale gene association networks (including\n  assignment of putative directions).  "}, "imputeFin": {"categories": ["MissingData"], "description": "Missing values often occur in financial data due to a variety \n    of reasons (errors in the collection process or in the processing stage, \n    lack of asset liquidity, lack of reporting of funds, etc.). However, \n    most data analysis methods expect complete data and cannot be employed \n    with missing values. One convenient way to deal with this issue without \n    having to redesign the data analysis method is to impute the missing \n    values. This package provides an efficient way to impute the missing \n    values based on modeling the time series with a random walk or an \n    autoregressive (AR) model, convenient to model log-prices and log-volumes \n    in financial data. In the current version, the imputation is \n    univariate-based (so no asset correlation is used). In addition,\n    outliers can be detected and removed.\n    The package is based on the paper:\n    J. Liu, S. Kumar, and D. P. Palomar (2019). Parameter Estimation of \n    Heavy-Tailed AR Model With Missing Data Via Stochastic EM. IEEE Trans. on \n    Signal Processing, vol. 67, no. 8, pp. 2159-2172. <doi:10.1109/TSP.2019.2899816>."}, "crawl": {"categories": ["SpatioTemporal", "Tracking"], "description": "Fit continuous-time correlated random walk models with time indexed\n    covariates to animal telemetry data. The model is fit using the Kalman-filter on\n    a state space version of the continuous-time stochastic movement process."}, "PCA4TS": {"categories": ["TimeSeries"], "description": "To seek for a contemporaneous linear transformation for\n    a multivariate time series such that the transformed series is segmented\n    into several lower-dimensional subseries, and those subseries are\n    uncorrelated with each other both contemporaneously and serially."}, "sportyR": {"categories": ["SportsAnalytics"], "description": "Create scaled 'ggplot' representations of playing surfaces.\n    Playing surfaces are drawn pursuant to rule-book specifications.\n    This package should be used as a baseline plot for displaying player\n    tracking data."}, "nsarfima": {"categories": ["TimeSeries"], "description": "Routines for fitting and simulating data under autoregressive fractionally integrated moving average (ARFIMA) models, without the constraint of covariance stationarity. Two fitting methods are implemented, a pseudo-maximum likelihood method and a minimum distance estimator. Mayoral, L. (2007) <doi:10.1111/j.1368-423X.2007.00202.x>. Beran, J. (1995) <doi:10.1111/j.2517-6161.1995.tb02054.x>."}, "mvna": {"categories": ["Survival"], "description": "Computes the Nelson-Aalen estimator of the cumulative transition hazard for arbitrary Markov multistate models <ISBN:978-0-387-68560-1>. "}, "HTMLUtils": {"categories": ["ReproducibleResearch"], "description": "Facilitates automated HTML report creation, in particular\n        framed HTML pages and dynamically sortable tables."}, "pharmaRTF": {"categories": ["ReproducibleResearch"], "description": "Enhanced RTF wrapper written in R for use with existing R tables\n    packages such as 'Huxtable' or 'GT'. This package fills a gap where tables in\n    certain packages can be written out to RTF, but cannot add certain metadata\n    or features to the document that are required/expected in a report for a\n    regulatory submission, such as multiple levels of titles and footnotes,\n    making the document landscape, and controlling properties such as margins."}, "classInt": {"categories": ["Spatial"], "description": "Selected commonly used methods for choosing univariate class intervals for mapping or other graphics purposes."}, "Bchron": {"categories": ["ChemPhys"], "description": "Enables quick calibration of radiocarbon dates under various \n  calibration curves (including user generated ones); age-depth modelling \n  as per the algorithm of Haslett and Parnell (2008) <doi:10.1111/j.1467-9876.2008.00623.x>; Relative sea level \n  rate estimation incorporating time uncertainty in polynomial regression \n  models (Parnell and Gehrels 2015) <doi:10.1002/9781118452547.ch32>; non-parametric phase modelling via \n  Gaussian mixtures as a means to determine the activity of a site \n  (and as an alternative to the Oxcal function SUM; currently \n  unpublished), and reverse calibration of dates from calibrated into \n  un-calibrated years (also unpublished)."}, "ivprobit": {"categories": ["Econometrics"], "description": "Compute the instrumental variables probit model using  the Amemiya's Generalized Least Squares estimators (Amemiya, Takeshi, (1978) <doi:10.2307/1911443>)."}, "BayesComm": {"categories": ["Bayesian"], "description": "Bayesian multivariate binary (probit) regression\n    models for analysis of ecological communities."}, "mondate": {"categories": ["TimeSeries"], "description": "Keep track of dates in terms of fractional calendar months \n  per Damien Laker \"Time Calculations for Annualizing Returns: the Need for Standardization\", \n  The Journal of Performance Measurement, 2008.\n  Model dates as of close of business.\n  Perform date arithmetic in units of \"months\" and \"years\".\n  Allow \"infinite\" dates to model \"ultimate\" time."}, "RandVar": {"categories": ["Robust"], "description": "Implements random variables by means of S4 classes and methods."}, "cem": {"categories": ["CausalInference"], "description": "Implementation of the Coarsened Exact Matching algorithm discussed \n\talong with its properties in\n  Iacus, King, Porro (2011) <doi:10.1198/jasa.2011.tm09599>;\n\tIacus, King, Porro (2012) <doi:10.1093/pan/mpr013> and\n\tIacus, King, Porro (2019) <doi:10.1017/pan.2018.29>."}, "simml": {"categories": ["CausalInference"], "description": "A major challenge in estimating treatment decision rules from a randomized clinical trial dataset with covariates measured at baseline lies in detecting relatively small treatment effect modification-related variability (i.e., the treatment-by-covariates interaction effects on treatment outcomes) against a relatively large non-treatment-related variability (i.e., the main effects of covariates on treatment outcomes). The class of Single-Index Models with Multiple-Links is a novel single-index model specifically designed to estimate a single-index (a linear combination) of the covariates associated with the treatment effect modification-related variability, while allowing a nonlinear association with the treatment outcomes via flexible link functions. The models provide a flexible regression approach to developing treatment decision rules based on patients' data measured at baseline. We refer to Park, Petkova, Tarpey, and Ogden (2020) <doi:10.1016/j.jspi.2019.05.008> and Park, Petkova, Tarpey, and Ogden (2020) <doi:10.1111/biom.13320> (that allows an unspecified X main effect) for detail of the method. The main function of this package is simml()."}, "qrmdata": {"categories": ["Finance"], "description": "Various data sets (stocks, stock indices, constituent data, FX,\n zero-coupon bond yield curves, volatility, commodities) for Quantitative\n Risk Management practice."}, "twangMediation": {"categories": ["CausalInference"], "description": "Provides functions for estimating natural direct and indirect effects for mediation analysis. It uses weighting where the weights are functions of estimates of the probability of exposure or treatment assignment (Hong, G (2010). <https://cepa.stanford.edu/sites/default/files/workshops/GH_JSM%20Proceedings%202010.pdf> Huber, M. (2014). <doi:10.1002/jae.2341>). Estimation of probabilities can use generalized boosting or logistic regression. Additional functions provide diagnostics of the model fit and weights. The vignette provides details and examples."}, "bqtl": {"categories": ["Bayesian"], "description": "QTL mapping toolkit for inbred crosses and recombinant\n        inbred lines. Includes maximum likelihood and Bayesian tools."}, "epimdr": {"categories": ["Epidemiology"], "description": "Functions, data sets and shiny apps for \"Epidemics: Models and Data in R\" by Ottar N. Bjornstad (ISBN 978-3-319-97487-3) <https://www.springer.com/gp/book/9783319974866>. The package contains functions to study the S(E)IR model, spatial and age-structured SIR models; time-series SIR and chain-binomial stochastic models; catalytic disease models; coupled map lattice models of spatial transmission and network models for social spread of infection. The package is also an advanced quantitative companion to the coursera Epidemics Massive Online Open Course <https://www.coursera.org/learn/epidemics>."}, "waterData": {"categories": ["Hydrology"], "description": "Imports U.S. Geological Survey (USGS) daily hydrologic data from USGS web services (see <https://waterservices.usgs.gov/> for more information), plots the data, addresses some common data problems, and calculates and plots anomalies.    "}, "runexp": {"categories": ["SportsAnalytics"], "description": "Implements two methods of estimating runs scored in a softball \n    scenario: (1) theoretical expectation using discrete Markov chains and (2) empirical\n    distribution using multinomial random simulation.  Scores are based on player-specific input \n    probabilities (out, single, double, triple, walk, and homerun).  Optional inputs include probability\n    of attempting a steal, probability of succeeding in an attempted steal, and an indicator of whether\n    a player is \"fast\" (e.g. the player could stretch home).  These probabilities may be \n    calculated from common player statistics that are publicly available on team's webpages. \n    Scores are evaluated based on a nine-player lineup and may be used to compare lineups, \n    evaluate base scenarios, and compare the offensive potential of individual players.  \n    Manuscript forthcoming.  See Bukiet & Harold (1997) <doi:10.1287/opre.45.1.14> for \n    implementation of discrete Markov chains. "}, "fRegression": {"categories": ["Finance"], "description": "A collection of functions for linear and non-linear regression modelling. \n\tIt implements a wrapper for several regression models available in the base and\n\tcontributed packages of R. "}, "digitize": {"categories": ["MetaAnalysis"], "description": "Import data from a digital image; it requires user input for\n    calibration and to locate the data points. The end result is similar to\n    'DataThief' and other other programs that 'digitize' published plots or\n    graphs."}, "betategarch": {"categories": ["Finance"], "description": "Simulation, estimation and forecasting of first-order Beta-Skew-t-EGARCH models with leverage (one-component, two-component, skewed versions)."}, "tfdeploy": {"categories": ["ModelDeployment"], "description": "Tools to deploy 'TensorFlow' <https://www.tensorflow.org/> models across \n  multiple services. Currently, it provides a local server for testing 'cloudml' \n  compatible services."}, "chron": {"categories": ["TimeSeries"], "description": "Provides chronological objects which can handle dates and times."}, "jsonlite": {"categories": ["WebTechnologies"], "description": "A reasonably fast JSON parser and generator, optimized for statistical \n    data and the web. Offers simple, flexible tools for working with JSON in R, and\n    is particularly powerful for building pipelines and interacting with a web API. \n    The implementation is based on the mapping described in the vignette (Ooms, 2014).\n    In addition to converting JSON data from/to R objects, 'jsonlite' contains \n    functions to stream, validate, and prettify JSON data. The unit tests included \n    with the package verify that all edge cases are encoded and decoded consistently \n    for use with dynamic data in systems and applications."}, "gamair": {"categories": ["Environmetrics"], "description": "Data sets and scripts used in the book 'Generalized Additive \n             Models: An Introduction with R', Wood (2006,2017) CRC."}, "microsynth": {"categories": ["CausalInference"], "description": "A generalization of the 'Synth' package that is\n    designed for data at a more granular level (e.g., micro-level).\n    Provides functions to construct weights (including propensity\n    score-type weights) and run analyses for synthetic control methods\n    with micro- and meso-level data; see Robbins, Saunders, and Kilmer\n    (2017) <doi:10.1080/01621459.2016.1213634> and Robbins and Davenport\n    (2021) <doi:10.18637/jss.v097.i02>."}, "extremis": {"categories": ["ExtremeValue"], "description": "Conducts inference in statistical models for extreme values (de Carvalho et al (2012), <doi:10.1080/03610926.2012.709905>; de Carvalho and Davison (2014), <10.1080/01621459.2013.872651>; Einmahl et al (2016), <doi:10.1111/rssb.12099>)."}, "bayesSurv": {"categories": ["Survival"], "description": "Contains Bayesian implementations of Mixed-Effects Accelerated Failure Time (MEAFT) models\n             for censored data. Those can be not only right-censored but also interval-censored,\n\t     doubly-interval-censored or misclassified interval-censored."}, "SemNeT": {"categories": ["Psychometrics"], "description": "Implements several functions for the analysis of semantic networks including different network estimation algorithms, partial node bootstrapping (Kenett, Anaki, & Faust, 2014 <doi:10.3389/fnhum.2014.00407>), random walk simulation (Kenett & Austerweil, 2016 <http://alab.psych.wisc.edu/papers/files/Kenett16CreativityRW.pdf>), and a function to compute global network measures. Significance tests and plotting features are also implemented. "}, "divseg": {"categories": ["Spatial"], "description": "Implements common measures of diversity and spatial segregation. This package has tools to compute the majority of measures are reviewed in Douglas and Massey (1988) <doi:10.2307/2579183>. Multiple common measures of within-geography diversity are implemented as well. All functions operate on data frames with a 'tidyselect' based workflow."}, "lomb": {"categories": ["TimeSeries"], "description": "Computes the Lomb-Scargle Periodogram for unevenly sampled time series. Includes a randomization procedure to obtain exact p-values."}, "matrixNormal": {"categories": ["Distributions"], "description": "Computes densities, probabilities, and random deviates of the Matrix Normal (Iranmanesh et al. (2010) <doi:10.7508/ijmsi.2010.02.004>). Also includes simple but useful matrix functions. See the vignette for more information. "}, "ICC": {"categories": ["Psychometrics"], "description": "Assist in the estimation of the Intraclass Correlation Coefficient\n    (ICC) from variance components of a one-way analysis of variance and also\n    estimate the number of individuals or groups necessary to obtain an ICC\n    estimate with a desired confidence interval width."}, "EpiILMCT": {"categories": ["Epidemiology"], "description": "Provides tools for simulating from continuous-time individual level models of disease transmission, and carrying out infectious disease data analyses with the same models. The epidemic models considered are distance-based and/or contact network-based models within Susceptible-Infectious-Removed (SIR) or Susceptible-Infectious-Notified-Removed (SINR) compartmental frameworks. <doi:10.18637/jss.v098.i10>."}, "lavaan": {"categories": ["Econometrics", "MissingData", "OfficialStatistics", "Psychometrics"], "description": "Fit a variety of latent variable models, including confirmatory\n   factor analysis, structural equation modeling and latent growth curve models."}, "gtop": {"categories": ["TimeSeries"], "description": "In hierarchical time series (HTS) forecasting, the hierarchical relation between multiple time series is exploited to make better forecasts. This hierarchical relation implies one or more aggregate consistency constraints that the series are known to satisfy. Many existing approaches, like for example bottom-up or top-down forecasting, therefore attempt to achieve this goal in a way that guarantees that the forecasts will also be aggregate consistent. This package provides with an implementation of the Game-Theoretically OPtimal (GTOP) reconciliation method proposed in van Erven and Cugliari (2015), which is guaranteed to only improve any given set of forecasts. This opens up new possibilities  for constructing the forecasts. For example, it is not necessary to assume that bottom-level forecasts are unbiased, and aggregate forecasts may be constructed by regressing both on bottom-level forecasts and on other covariates that may only be available at the aggregate level."}, "doMPI": {"categories": ["HighPerformanceComputing"], "description": "Provides a parallel backend for the %dopar% function using\n        the Rmpi package."}, "titrationCurves": {"categories": ["ChemPhys"], "description": "A collection of functions to plot acid/base titration \n    curves (pH vs. volume of titrant), complexation titration curves \n    (pMetal vs. volume of EDTA), redox titration curves (potential \n    vs.volume of titrant), and precipitation titration curves (either \n    pAnalyte or pTitrant vs. volume of titrant). Options include the \n    titration of mixtures, the ability to overlay two or more \n    titration curves, and the ability to show equivalence points."}, "HDTSA": {"categories": ["TimeSeries"], "description": "Procedures for high-dimensional time series analysis including factor analysis proposed by Lam and Yao (2012) <doi:10.1214/12-AOS970> and Chang, Guo and Yao (2015) <doi:10.1016/j.jeconom.2015.03.024>, martingale difference test proposed by Chang, Jiang and Shao (2021) preprint, principal component analysis proposed by Chang, Guo and Yao (2018) <doi:10.1214/17-AOS1613>, unit root test proposed by Chang, Cheng and Yao (2021) <arXiv:2006.07551> and white noise test proposed by Chang, Yao and Zhou (2017) <doi:10.1093/biomet/asw066>."}, "miniCRAN": {"categories": ["ReproducibleResearch"], "description": "Makes it possible to create an internally consistent\n    repository consisting of selected packages from CRAN-like repositories.\n    The user specifies a set of desired packages, and 'miniCRAN' recursively\n    reads the dependency tree for these packages, then downloads only this\n    subset. The user can then install packages from this repository directly,\n    rather than from CRAN.  This is useful in production settings, e.g. server\n    behind a firewall, or remote locations with slow (or zero) Internet access."}, "Matrix": {"categories": ["Econometrics", "NumericalMathematics"], "description": "A rich hierarchy of matrix classes, including triangular,\n   symmetric, and diagonal matrices, both dense and sparse and with\n   pattern, logical and numeric entries.   Numerous methods for and\n   operations on these matrices, using 'LAPACK' and 'SuiteSparse' libraries."}, "giscoR": {"categories": ["Spatial"], "description": "Tools to download data from the GISCO (Geographic Information\n    System of the Commission) Eurostat database\n    <https://ec.europa.eu/eurostat/web/gisco>. Global and European map\n    data available.  This package is in no way officially related to or\n    endorsed by Eurostat."}, "ipaddress": {"categories": ["WebTechnologies"], "description": "Classes and functions for working with IP (Internet Protocol)\n    addresses and networks, inspired by the Python 'ipaddress' module.\n    Offers full support for both IPv4 and IPv6 (Internet Protocol versions\n    4 and 6) address spaces. It is specifically designed to work well with\n    the 'tidyverse'."}, "Rcatch22": {"categories": ["TimeSeries"], "description": "Calculate 22 summary statistics coded in C on time-series vectors to enable \n    pattern detection, classification, and regression applications in the \n    feature space as proposed by Lubba et al. (2019) <doi:10.1007/s10618-019-00647-x>."}, "sglg": {"categories": ["Distributions"], "description": "Set of tools to fit a linear multiple or semi-parametric regression\n    models with the possibility of non-informative random right-censoring. \n    Under this setup, the localization parameter of the response variable distribution is modeled by using linear multiple regression\n    or semi-parametric functions, whose non-parametric components may be approximated\n    by natural cubic spline or P-splines. The supported distribution for the model error is a generalized log-gamma distribution which includes\n    the generalized extreme value and standard normal distributions as important special cases. \n    Also, some numerical and graphical devices for diagnostic of the fitted models are offered. "}, "irr": {"categories": ["Psychometrics"], "description": "Coefficients of Interrater Reliability and Agreement for\n        quantitative, ordinal and nominal data: ICC, Finn-Coefficient,\n        Robinson's A, Kendall's W, Cohen's Kappa, ..."}, "pcIRT": {"categories": ["Psychometrics"], "description": "Estimates the multidimensional polytomous Rasch model (Rasch, 1961) with conditional maximum likelihood estimation."}, "magic": {"categories": ["NumericalMathematics"], "description": "A collection of functions for the manipulation and\n analysis of arbitrarily dimensioned arrays.  The original motivation\n for the package was the development of efficient, vectorized\n algorithms for the creation and investigation of magic squares and\n high-dimensional magic hypercubes."}, "mfaces": {"categories": ["FunctionalData"], "description": "Multivariate functional principal component analysis via fast covariance estimation for\n             multivariate sparse functional data or longitudinal data proposed by Li, Xiao, and Luo (2020) <doi:10.1002/sta4.245>."}, "networkDynamic": {"categories": ["GraphicalModels"], "description": "Simple interface routines to facilitate the handling of network objects with complex intertemporal data. This is a part of the \"statnet\" suite of packages for network analysis."}, "Boom": {"categories": ["Bayesian"], "description": "A C++ library for Bayesian modeling, with an emphasis on Markov\n   chain Monte Carlo.  Although boom contains a few R utilities (mainly plotting\n   functions), its primary purpose is to install the BOOM C++ library on your\n   system so that other packages can link against it."}, "FatTailsR": {"categories": ["Distributions", "Finance"], "description": "Kiener distributions K1, K2, K3, K4 and K7 to characterize\n    distributions with left and right, symmetric or asymmetric fat tails in market\n    finance, neuroscience and other disciplines. Two algorithms to estimate with\n    a high accuracy distribution parameters, quantiles, value-at-risk and expected\n    shortfall. Include power hyperbolas and power hyperbolic functions. "}, "GA": {"categories": ["Optimization"], "description": "Flexible general-purpose toolbox implementing genetic algorithms (GAs) for stochastic optimisation. Binary, real-valued, and permutation representations are available to optimize a fitness function, i.e. a function provided by users depending on their objective function. Several genetic operators are available and can be combined to explore the best settings for the current task. Furthermore, users can define new genetic operators and easily evaluate their performances. Local search using general-purpose optimisation algorithms can be applied stochastically to exploit interesting regions. GAs can be run sequentially or in parallel, using an explicit master-slave parallelisation or a coarse-grain islands approach."}, "GramQuad": {"categories": ["NumericalMathematics"], "description": "Numerical integration with Gram polynomials (based on <arXiv:2106.14875> [math.NA] 28 Jun 2021, by Irfan Muhammad [School of Computer Science, University of Birmingham, UK])."}, "PolynomF": {"categories": ["NumericalMathematics"], "description": "Implements univariate polynomial operations in R, including\n  polynomial arithmetic, finding zeros, plotting, and some operations on\n  lists of polynomials."}, "x12": {"categories": ["TimeSeries"], "description": "The 'X13-ARIMA-SEATS' <https://www.census.gov/data/software/x13as.html> methodology and software is a widely used software and developed by the US Census Bureau. It can be accessed from 'R' with this package and 'X13-ARIMA-SEATS' binaries are provided by the 'R' package 'x13binary'."}, "animalTrack": {"categories": ["SpatioTemporal", "Tracking"], "description": "2D and 3D animal tracking data can be used to reconstruct tracks through time/space with correction based on known positions. 3D visualization of animal position and attitude."}, "iai": {"categories": ["MissingData"], "description": "An interface to the algorithms of 'Interpretable AI'\n    <https://www.interpretable.ai> from the R programming language.\n    'Interpretable AI' provides various modules, including 'Optimal Trees' for\n    classification, regression, prescription and survival analysis, 'Optimal\n    Imputation' for missing data imputation and outlier detection, and 'Optimal\n    Feature Selection' for exact sparse regression. The 'iai' package is an\n    open-source project. The 'Interpretable AI' software modules are proprietary\n    products, but free academic and evaluation licenses are available."}, "o2geosocial": {"categories": ["Epidemiology"], "description": "Bayesian reconstruction of who infected whom during past outbreaks using routinely-collected surveillance data. Inference of transmission trees using genotype, age specific social contacts, distance between cases and onset dates of the reported cases. (Robert A, Kucharski AJ, Gastanaduy PA, Paul P, Funk S. 2020 <doi:10.1098/rsif.2020.0084>)."}, "mnormt": {"categories": ["Distributions"], "description": "Functions are provided for computing the density and the\n  distribution function of d-dimensional normal and \"t\" random variables,\n  possibly truncated (on one side or two sides),  and for generating random \n  vectors sampled from these distributions, except sampling from the truncated\n  \"t\". Moments of arbitrary order of a multivariate truncated normal are\n  computed, and converted to cumulants up to order 4. \n  Probabilities are computed via non-Monte Carlo methods; different routines \n  are used in the case d=1, d=2, d=3, d>3, if d denotes the dimensionality."}, "snow": {"categories": ["HighPerformanceComputing"], "description": "Support for simple parallel computing in R."}, "BradleyTerry2": {"categories": ["Psychometrics", "SportsAnalytics"], "description": "Specify and fit the Bradley-Terry model, including structured versions in which the parameters are related to explanatory variables through a linear predictor and versions with contest-specific effects, such as a home advantage."}, "dMod": {"categories": ["DifferentialEquations"], "description": "The framework provides functions to generate ODEs of reaction\n    networks, parameter transformations, observation functions, residual functions,\n    etc. The framework follows the paradigm that derivative information should be\n    used for optimization whenever possible. Therefore, all major functions produce\n    and can handle expressions for symbolic derivatives. The methods used in dMod\n    were published in Kaschek et al, 2019, <doi:10.18637/jss.v088.i10>."}, "randomForestSRC": {"categories": ["HighPerformanceComputing", "MachineLearning", "Survival"], "description": "Fast OpenMP parallel computing of Breiman's random forests for univariate, multivariate, unsupervised, survival, competing risks, class imbalanced classification and quantile regression. New Mahalanobis splitting for correlated outcomes.  Extreme random forests and randomized splitting.  Suite of imputation methods for missing data.  Fast random forests using subsampling. Confidence regions and standard errors for variable importance. New improved holdout importance. Case-specific importance.  Minimal depth variable importance. Visualize trees on your Safari or Google Chrome browser. Anonymous random forests for data privacy."}, "geofd": {"categories": ["FunctionalData"], "description": "Kriging based methods are used for predicting functional data \n             (curves) with spatial dependence."}, "sparktex": {"categories": ["ReproducibleResearch"], "description": "Generate syntax for use with the sparklines package for\n        LaTeX."}, "convey": {"categories": ["OfficialStatistics"], "description": "Variance estimation on indicators of income concentration and\n    poverty using complex sample survey designs. Wrapper around the\n    'survey' package."}, "makepipe": {"categories": ["ReproducibleResearch"], "description": "A suite of tools for transforming an existing workflow into a\n    self-documenting pipeline with very minimal upfront costs. Segments of\n    the pipeline are specified in much the same way a 'Make' rule is, by\n    declaring an executable recipe (which might be an R script), along\n    with the corresponding targets and dependencies. When the entire\n    pipeline is run through, only those recipes that need to be executed\n    will be. Meanwhile, execution metadata is captured behind the scenes\n    for later inspection."}, "RobLoxBioC": {"categories": ["Robust"], "description": "Functions for the determination of optimally robust influence curves and\n            estimators for preprocessing omics data, in particular gene expression data."}, "PortfolioOptim": {"categories": ["Finance"], "description": "Two functions for financial portfolio optimization by linear programming are provided. One function implements Benders decomposition algorithm and can be used for very large data sets. The other, applicable for moderate sample sizes, finds optimal portfolio which has the smallest distance to a given benchmark portfolio."}, "distrEllipse": {"categories": ["Distributions"], "description": "Distribution (S4-)classes for elliptically contoured distributions (based on\n            package 'distr')."}, "dirichletprocess": {"categories": ["Bayesian"], "description": "Perform nonparametric Bayesian analysis using Dirichlet \n    processes without the need to program the inference algorithms. \n    Utilise included pre-built models or specify custom \n    models and allow the 'dirichletprocess' package to handle the \n    Markov chain Monte Carlo sampling. \n    Our Dirichlet process objects can act as building blocks for a variety \n    of statistical models including and not limited to: density estimation, \n    clustering and prior distributions in hierarchical models.\n    See Teh, Y. W. (2011) \n    <https://www.stats.ox.ac.uk/~teh/research/npbayes/Teh2010a.pdf>, \n    among many other sources."}, "abglasso": {"categories": ["Bayesian"], "description": "Implements a Bayesian adaptive graphical lasso data-augmented block Gibbs sampler. The sampler simulates the posterior distribution of precision matrices of a Gaussian Graphical Model. This sampler was adapted from the original MATLAB routine proposed in Wang (2012) <doi:10.1214/12-BA729>."}, "HIMA": {"categories": ["Epidemiology"], "description": "Allows to estimate and test high-dimensional mediation effects based on advanced mediator screening and penalized regression techniques. Methods used in the package refer to Zhang H, Zheng Y, Zhang Z, Gao T, Joyce B, Yoon G, Zhang W, Schwartz J, Just A, Colicino E, Vokonas P, Zhao L, Lv J, Baccarelli A, Hou L, Liu L. Estimating and Testing High-dimensional Mediation Effects in Epigenetic Studies. Bioinformatics. (2016) <doi:10.1093/bioinformatics/btw351>. PMID: 27357171."}, "asymmetry": {"categories": ["Psychometrics"], "description": "Multidimensional scaling models and methods for the visualization and analysis of asymmetric proximity data <doi:10.1111/j.2044-8317.1996.tb01078.x>. An asymmetric data matrix has the same number of rows and columns, and these rows and columns refer to the same set of objects. At least some elements in the upper-triangle are different from the corresponding elements in the lower triangle. An example of an asymmetric matrix is a  student migration table, where the rows correspond to the countries of origin of the students and the columns to the destination countries. This package provides algorithms for three multidimensional scaling models. These are the slide-vector model <doi:10.1007/BF02294474>, a scaling model with unique dimensions and the asymscal model for asymmetric multidimensional scaling. Furthermore, a heat map for skew-symmetric data, and the decomposition of asymmetry are provided for the exploratory analysis of asymmetric tables."}, "nCDunnett": {"categories": ["Distributions"], "description": "Computes the noncentral Dunnett's test distribution (pdf, cdf and quantile) and generates random numbers. "}, "ndjson": {"categories": ["WebTechnologies"], "description": "Streaming 'JSON' ('ndjson') has one 'JSON' record per-line\n        and many modern 'ndjson' files contain large numbers of records.\n        These constructs may not be columnar in nature, but it is often\n        useful to read in these files and \"flatten\" the structure out to\n        enable working with the data in an R 'data.frame'-like context.\n        Functions are provided that make it possible to read in plain\n        'ndjson' files or compressed ('gz') 'ndjson' files and either\n        validate the format of the records or create \"flat\" 'data.table'\n        structures from them."}, "PK": {"categories": ["Pharmacokinetics"], "description": "Estimation of pharmacokinetic parameters using non-compartmental theory."}, "hglm": {"categories": ["Spatial"], "description": "Implemented here are procedures for fitting hierarchical generalized linear models (HGLM). It can be used for linear mixed models and generalized linear mixed models with random effects for a variety of links and a variety of distributions for both the outcomes and the random effects. Fixed effects can also be fitted in the dispersion part of the mean model. As statistical models, HGLMs were initially developed by Lee and Nelder (1996) <https://www.jstor.org/stable/2346105?seq=1>. We provide an implementation (Ronnegard, Alam and Shen 2010) <https://journal.r-project.org/archive/2010-2/RJournal_2010-2_Roennegaard~et~al.pdf> following Lee, Nelder and Pawitan (2006) <ISBN: 9781420011340> with algorithms extended for spatial modeling (Alam, Ronnegard and Shen 2015) <https://journal.r-project.org/archive/2015/RJ-2015-017/RJ-2015-017.pdf>. "}, "geoknife": {"categories": ["Hydrology"], "description": "Processes gridded datasets found on the U.S. Geological Survey\n    Geo Data Portal web application or elsewhere, using a web-enabled workflow\n    that eliminates the need to download and store large datasets that are reliably\n    hosted on the Internet. The package provides access to several data subset and\n    summarization algorithms that are available on remote web processing servers (Read et al. (2015) <doi:10.1111/ecog.01880>)."}, "telemac": {"categories": ["Hydrology"], "description": "An R interface to the TELEMAC suite for modelling\n    of free surface flow. This includes methods for model initialisation, simulation,\n    and visualisation. So far only the TELEMAC-2D module for 2-dimensional hydrodynamic\n    modelling is implemented. "}, "sdcHierarchies": {"categories": ["OfficialStatistics"], "description": "Provides functionality to generate, (interactively) modify (by adding, removing and renaming nodes) and convert nested hierarchies between different formats.\n  These tree like structures can be used to define for example complex hierarchical tables used for statistical disclosure control."}, "sphet": {"categories": ["Econometrics", "Spatial", "SpatioTemporal"], "description": "Functions for fitting Cliff-Ord-type spatial autoregressive models with and without heteroskedastic innovations using Generalized Method of Moments estimation are provided. Some support is available for fitting spatial HAC models, and for fitting with non-spatial endogeneous variables using instrumental variables."}, "icenReg": {"categories": ["MissingData", "Survival"], "description": "Regression models for interval censored data. Currently supports\n    Cox-PH, proportional odds, and accelerated failure time models. Allows for\n    semi and fully parametric models (parametric only for accelerated failure\n    time models) and Bayesian parametric models. Includes functions for easy visual\n    diagnostics of model fits and imputation of censored data."}, "sperrorest": {"categories": ["Spatial"], "description": "Implements spatial error estimation and\n    permutation-based variable importance measures for predictive models\n    using spatial cross-validation and spatial block bootstrap."}, "tsintermittent": {"categories": ["TimeSeries"], "description": "Time series methods for intermittent demand forecasting. Includes Croston's method and its variants (Moving Average, SBA), and the TSB method. Users can obtain optimal parameters on a variety of loss functions, or use fixed ones (Kourenztes (2014) <doi:10.1016/j.ijpe.2014.06.007>). Intermittent time series classification methods and iMAPA that uses multiple temporal aggregation levels are also provided (Petropoulos & Kourenztes (2015) <doi:10.1057/jors.2014.62>)."}, "AzureCosmosR": {"categories": ["WebTechnologies"], "description": "An interface to 'Azure CosmosDB': <https://azure.microsoft.com/en-us/services/cosmos-db/>. On the admin side, 'AzureCosmosR' provides functionality to create and manage 'Cosmos DB' instances in Microsoft's 'Azure' cloud. On the client side, it provides an interface to the 'Cosmos DB' SQL API, letting the user store and query documents and attachments in 'Cosmos DB'. Part of the 'AzureR' family of packages."}, "roll": {"categories": ["TimeSeries"], "description": "Fast and efficient computation of rolling and expanding statistics for time-series data."}, "BivGeo": {"categories": ["Distributions"], "description": "Computes the joint probability mass function (pmf), the joint cumulative function (cdf), the joint survival function (sf), the correlation coefficient, the covariance, the cross-factorial moment and generate random deviates for the Basu-Dhar bivariate geometric distribution as well the joint probability mass, cumulative and survival function assuming the presence of a cure fraction given by the standard bivariate mixture cure fraction model. The package also computes the estimators based on the method of moments."}, "osDesign": {"categories": ["ExperimentalDesign"], "description": "A suite of functions for the design of case-control and two-phase studies, and the analysis of data that arise from them. Functions in this packages provides Monte Carlo based evaluation of operating characteristics such as powers for estimators of the components of a logistic regression model. For additional detail see: Haneuse S, Saegusa T and Lumley T (2011)<doi:10.18637/jss.v043.i11>."}, "adaptTest": {"categories": ["ClinicalTrials"], "description": "The functions defined in this program serve for implementing adaptive\n    two-stage tests. Currently, four tests are included: Bauer and Koehne (1994),\n    Lehmacher and Wassmer (1999), Vandemeulebroecke (2006), and the horizontal conditional\n    error function. User-defined tests can also be implemented. Reference: Vandemeulebroecke,\n    An investigation of two-stage tests, Statistica Sinica 2006."}, "FLSSS": {"categories": ["Optimization"], "description": "Specialized solvers for combinatorial optimization problems in the Subset Sum family. The solvers differ from the mainstream in the options of (i) restricting subset size, (ii) bounding subset elements, (iii) mining real-value multisets with predefined subset sum errors, (iv) finding one or more subsets in limited time. A novel algorithm for mining the one-dimensional Subset Sum induced algorithms for the multi-Subset Sum and the multidimensional Subset Sum. The multi-threaded framework for the latter offers exact algorithms to the multidimensional Knapsack and the Generalized Assignment problems. Historical updates include (a) renewed implementation of the multi-Subset Sum, multidimensional Knapsack and Generalized Assignment solvers; (b) availability of bounding solution space in the multidimensional Subset Sum; (c) fundamental data structure and architectural changes for enhanced cache locality and better chance of SIMD vectorization; (d) option of mapping floating-point instance to compressed 64-bit integer instance with user-controlled precision loss, which could yield substantial speedup due to the dimension reduction and efficient compressed integer arithmetic via bit-manipulations; (e) distributed computing infrastructure for multidimensional subset sum; (f) arbitrary-precision zero-margin-of-error multidimensional Subset Sum accelerated by a simplified Bloom filter. The package contains a copy of xxHash from <https://github.com/Cyan4973/xxHash>. Package vignette (<arXiv:1612.04484v3>) detailed a few historical updates. Functions prefixed with 'aux' (auxiliary) are independent implementations of published algorithms for solving optimization problems less relevant to Subset Sum."}, "robcor": {"categories": ["Robust"], "description": "Robust pairwise correlations based on estimates of scale,\n  particularly on \"FastQn\" one-step M-estimate."}, "OAIHarvester": {"categories": ["WebTechnologies"], "description": "\n  Harvest metadata using the Open Archives Initiative Protocol for Metadata\n  Harvesting (OAI-PMH) version 2.0 (for more information, see\n  <https://www.openarchives.org/OAI/openarchivesprotocol.html>)."}, "climatol": {"categories": ["Hydrology"], "description": "Functions for the quality control, homogenization and missing data infilling of climatological series and to obtain climatological summaries and grids from the results. Also functions to draw wind-roses and Walter&Lieth climate diagrams."}, "equivalence": {"categories": ["Environmetrics"], "description": "Provides statistical tests and graphics for assessing tests\n        of equivalence.  Such tests have similarity as the alternative\n\thypothesis instead of the null.  Sample data sets are included."}, "fdasrvf": {"categories": ["FunctionalData"], "description": "Performs alignment, PCA, and modeling of multidimensional and\n            unidimensional functions using the square-root velocity framework\n            (Srivastava et al., 2011 <arXiv:1103.3817> and\n            Tucker et al., 2014 <doi:10.1016/j.csda.2012.12.001>). This framework\n            allows for elastic analysis of functional data through phase and\n            amplitude separation."}, "SymTS": {"categories": ["Distributions"], "description": "Contains methods for simulation and for evaluating the pdf, cdf, and quantile functions for symmetric stable, symmetric classical tempered stable, and symmetric power tempered stable distributions. "}, "WikidataQueryServiceR": {"categories": ["WebTechnologies"], "description": "An API client for the 'Wikidata Query Service'\n    <https://query.wikidata.org/>."}, "MetaSubtract": {"categories": ["MetaAnalysis"], "description": "If results from a meta-GWAS are used for validation in one of the cohorts that was included in the meta-analysis, this will yield biased (i.e. too optimistic) results. \n      The validation cohort needs to be independent from the meta-Genome-Wide-Association-Study (meta-GWAS) results. \n      'MetaSubtract' will subtract the results of the respective cohort from the meta-GWAS results analytically without having to redo the meta-GWAS analysis using the leave-one-out methodology. \n      It can handle different meta-analyses methods and takes into account if single or double genomic control correction was applied to the original meta-analysis. \n      It can also handle different meta-analysis methods. It can be used for whole GWAS, but also for a limited set of genetic markers. \n      See for application: Nolte I.M. et al. (2017); <doi:10.1038/ejhg.2017.50>."}, "socceR": {"categories": ["SportsAnalytics"], "description": "Functions for evaluating tournament predictions, simulating results from individual soccer matches and tournaments. See <http://sandsynligvis.dk/2018/08/03/world-cup-prediction-winners/> for more information."}, "plgp": {"categories": ["ExperimentalDesign"], "description": "Sequential Monte Carlo (SMC) inference for fully Bayesian\n  Gaussian process (GP) regression and classification models by\n  particle learning (PL) following Gramacy & Polson (2011) <arXiv:0909.5262>.\n  The sequential nature of inference\n  and the active learning (AL) hooks provided facilitate thrifty \n  sequential design (by entropy) and optimization\n  (by improvement) for classification and\n  regression models, respectively.\n  This package essentially provides a generic\n  PL interface, and functions (arguments to the interface) which\n  implement the GP models and AL heuristics.  Functions for \n  a special, linked, regression/classification GP model and \n  an integrated expected conditional improvement (IECI) statistic \n  provide for optimization in the presence of unknown constraints.\n  Separable and isotropic Gaussian, and single-index correlation\n  functions are supported.\n  See the examples section of ?plgp and demo(package=\"plgp\") \n  for an index of demos."}, "exactRankTests": {"categories": ["Survival"], "description": "Computes exact conditional p-values and quantiles using an\n implementation of the Shift-Algorithm by Streitberg & Roehmel."}, "heplots": {"categories": ["Psychometrics"], "description": "Provides HE plot and other functions for visualizing hypothesis\n    tests in multivariate linear models. HE plots represent sums-of-squares-and-products \n    matrices for linear hypotheses and for error using ellipses (in two\n    dimensions) and ellipsoids (in three dimensions). The related 'candisc' package\n    provides visualizations in a reduced-rank canonical discriminant space when\n    there are more than a few response variables."}, "metRology": {"categories": ["ChemPhys"], "description": "Provides classes and calculation and plotting functions \n for metrology applications, including measurement uncertainty estimation\n and inter-laboratory metrology comparison studies. "}, "mvoutlier": {"categories": ["Robust"], "description": "Various methods for multivariate outlier detection: arw, a Mahalanobis-type method with an adaptive outlier cutoff value; locout, a method incorporating local neighborhood; pcout, a method for high-dimensional data; mvoutlier.CoDa, a method for compositional data. References are provided in the corresponding help files."}, "margins": {"categories": ["CausalInference", "Econometrics"], "description": "An R port of Stata's 'margins' command, which can be used to\n    calculate marginal (or partial) effects from model objects."}, "CARBayes": {"categories": ["Bayesian", "Spatial"], "description": "Implements a class of univariate and multivariate spatial generalised linear mixed models for areal unit data, with inference in a Bayesian setting using Markov chain Monte Carlo (MCMC) simulation. The response variable can be binomial, Gaussian, multinomial, Poisson or zero-inflated Poisson (ZIP), and spatial autocorrelation is modelled by a set of random effects that are assigned a conditional autoregressive (CAR) prior distribution. A number of different models are available for univariate spatial data, including models with no random effects as well as random effects modelled by different types of CAR prior, including the BYM model (Besag et al., 1991, <doi:10.1007/BF00116466>) and Leroux model (Leroux et al., 2000, <doi:10.1007/978-1-4612-1284-3_4>). Additionally,  a multivariate CAR (MCAR) model for multivariate spatial data is available, as is a two-level hierarchical model for modelling data relating to individuals within areas. Full details are given in the vignette accompanying this package. The initial creation of this package was supported by the Economic and Social Research Council (ESRC) grant RES-000-22-4256, and on-going development has been supported by the Engineering and Physical Science Research Council (EPSRC) grant EP/J017442/1, ESRC grant ES/K006460/1, Innovate UK / Natural Environment Research Council (NERC) grant NE/N007352/1 and the TB Alliance. "}, "fields": {"categories": ["Spatial"], "description": "For curve, surface and function fitting with an emphasis\n on splines, spatial data, geostatistics,  and spatial statistics. The major methods\n include cubic, and thin plate splines, Kriging, and compactly supported\n covariance functions for large data sets. The splines and Kriging methods are\n supported by functions that can determine the smoothing parameter\n (nugget and sill variance) and other covariance function parameters by cross\n validation and also by restricted maximum likelihood. For Kriging\n there is an easy to use function that also estimates the correlation\n scale (range parameter).  A major feature is that any covariance function\n implemented in R and following a simple format can be used for\n spatial prediction. There are also many useful functions for plotting\n and working with spatial data as images. This package also contains\n an implementation of sparse matrix methods for large spatial data\n sets and currently requires the sparse matrix (spam) package. Use\n help(fields) to get started and for an overview.  The fields source\n code is deliberately commented and provides useful explanations of\n numerical details as a companion to the manual pages. The commented\n source code can be viewed by expanding the source code version\n and looking in the R subdirectory. The reference for fields can be generated\n by the citation function in R and has DOI <doi:10.5065/D6W957CT>. Development\n of this package was supported in part by the National Science Foundation  Grant\n 1417857,  the National Center for Atmospheric Research, and Colorado School of Mines.\n See the Fields URL\n for a vignette on using this package and some background on spatial statistics."}, "surveybootstrap": {"categories": ["OfficialStatistics"], "description": "Tools for using different kinds of bootstrap\n    for estimating sampling variation using complex survey\n    data.  "}, "RobRex": {"categories": ["Robust"], "description": "Functions for the determination of optimally robust influence curves in case of\n            linear regression with unknown scale and standard normal distributed errors where\n            the regressor is random."}, "forplo": {"categories": ["MetaAnalysis"], "description": "Simplifies the creation and customization of forest plots (alternatively called dot-and-whisker plots). Input classes accepted by 'forplo' are data.frame, matrix, lm, glm, and coxph. 'forplo' was written in base R and does not depend on other packages."}, "BayesianLaterality": {"categories": ["Bayesian"], "description": "Functional differences between the cerebral hemispheres \n    are a fundamental characteristic of the human brain. Researchers \n    interested in studying these differences often infer underlying \n    hemispheric dominance for a certain function (e.g., language) from \n    laterality indices calculated from observed performance or brain \n    activation measures . However, any inference from observed measures \n    to latent (unobserved) classes has to consider the prior probability \n    of class membership in the population. The provided functions \n    implement a Bayesian model for predicting hemispheric dominance from\n    observed laterality indices (Sorensen and Westerhausen, Laterality: \n    Asymmetries of Body, Brain and Cognition, 2020, <doi:10.1080/1357650X.2020.1769124>)."}, "msm": {"categories": ["Distributions", "Survival"], "description": "Functions for fitting continuous-time Markov and hidden\n    Markov multi-state models to longitudinal data.  Designed for\n    processes observed at arbitrary times in continuous time (panel data)\n    but some other observation schemes are supported. Both Markov\n    transition rates and the hidden Markov output process can be modelled\n    in terms of covariates, which may be constant or piecewise-constant\n    in time."}, "ssMousetrack": {"categories": ["Bayesian"], "description": "Estimates previously compiled state-space modeling for mouse-tracking experiments using the 'rstan' package, which provides the R interface to the Stan C++ library for Bayesian estimation. "}, "randNames": {"categories": ["WebTechnologies"], "description": "Generates random names with additional information including fake\n    SSNs, gender, location, zip, age, address, and nationality."}, "odpc": {"categories": ["TimeSeries"], "description": "Functions to compute the one-sided dynamic\n\tprincipal components ('odpc') introduced in Pe\u00f1a, Smucler and Yohai (2019)\n\t<doi:10.1080/01621459.2018.1520117>. 'odpc' is a novel dimension\n\treduction technique for multivariate time series, that is useful\n\tfor forecasting. These dynamic principal components are defined as\n\tthe linear combinations of the present and past values of the series\n\tthat minimize the reconstruction mean squared error."}, "BCEE": {"categories": ["CausalInference"], "description": "A Bayesian model averaging approach to causal effect estimation\n    based on the BCEE algorithm. Currently supports binary or continuous\n    exposures and outcomes. For more details, see \n    Talbot et al. (2015) <doi:10.1515/jci-2014-0035>\n    Talbot and Beaudoin (2020) <arXiv:2003.11588>."}, "baycn": {"categories": ["Bayesian"], "description": "A Bayesian hybrid approach for inferring Directed Acyclic Graphs\n    (DAGs) for continuous, discrete, and mixed data. The algorithm can use the \n    graph inferred by another more efficient graph inference method as input;\n    the input graph may contain false edges or undirected edges but can help\n    reduce the search space to a more manageable size. A Bayesian Markov chain\n    Monte Carlo algorithm is then used to infer the probability of direction and\n    absence for the edges in the network.\n    References:\n    Martin and Fu (2019) <arXiv:1909.10678>."}, "prophet": {"categories": ["MissingData", "TimeSeries"], "description": "Implements a procedure for forecasting time series data based on\n    an additive model where non-linear trends are fit with yearly, weekly, and\n    daily seasonality, plus holiday effects. It works best with time series\n    that have strong seasonal effects and several seasons of historical data.\n    Prophet is robust to missing data and shifts in the trend, and typically\n    handles outliers well."}, "BDgraph": {"categories": ["Bayesian", "GraphicalModels", "HighPerformanceComputing", "MachineLearning"], "description": "Statistical tools for Bayesian structure learning in undirected graphical models for continuous, discrete, and mixed data. The package is implemented the recent improvements in the Bayesian graphical models' literature, including Mohammadi and Wit (2015) <doi:10.1214/14-BA889>, Mohammadi et al. (2021) <doi:10.1080/01621459.2021.1996377>, and Mohammadi and Wit (2019) <doi:10.18637/jss.v089.i03>. "}, "ggseas": {"categories": ["TimeSeries"], "description": "Provides 'ggplot2' 'stats' that estimate seasonally adjusted series \n    and rolling summaries such as rolling average on the fly for time series."}, "extremeStat": {"categories": ["ExtremeValue"], "description": "Code to fit, plot and compare several (extreme value)\n    distribution functions. Can also compute (truncated) distribution quantile estimates and\n    draw a plot with return periods on a linear scale."}, "mma": {"categories": ["Epidemiology"], "description": "Used for general multiple mediation analysis. \n\tThe analysis method is described in Yu and Li (2022) (ISBN: 9780367365479) \"Statistical Methods for Mediation, Confounding and Moderation Analysis Using R and SAS\", published by Chapman and Hall/CRC; and Yu et al.(2017) <doi:10.1016/j.sste.2017.02.001> \"Exploring racial disparity in obesity: a mediation analysis considering geo-coded environmental factors\", published on Spatial and Spatio-temporal Epidemiology, 21, 13-23.  "}, "gazepath": {"categories": ["SpatioTemporal"], "description": "Eye-tracking data must be transformed into fixations and saccades before it can be analyzed. This package provides a non-parametric speed-based approach to do this on a trial basis. The method is especially useful when there are large differences in data quality, as the thresholds are adjusted accordingly. The same pre-processing procedure can be applied to all participants, while accounting for individual differences in data quality."}, "deBInfer": {"categories": ["Bayesian"], "description": "A Bayesian framework for parameter inference in differential equations.\n    This approach offers a rigorous methodology for parameter inference as well as\n    modeling the link between unobservable model states and parameters, and\n    observable quantities. Provides templates for the DE model, the\n    observation model and data likelihood, and the model parameters and their prior\n    distributions. A Markov chain Monte Carlo (MCMC) procedure processes these inputs\n    to estimate the posterior distributions of the parameters and any derived\n    quantities, including the model trajectories. Further functionality is provided\n    to facilitate MCMC diagnostics and the visualisation of the posterior distributions\n    of model parameters and trajectories."}, "beanz": {"categories": ["CausalInference"], "description": "It is vital to assess the heterogeneity of treatment effects\n    (HTE) when making health care decisions for an individual patient or a group\n    of patients. Nevertheless, it remains challenging to evaluate HTE based\n    on information collected from clinical studies that are often designed and\n    conducted to evaluate the efficacy of a treatment for the overall population.\n    The Bayesian framework offers a principled and flexible approach to estimate\n    and compare treatment effects across subgroups of patients defined by their\n    characteristics. This package allows users to explore a wide range of Bayesian\n    HTE analysis models, and produce posterior inferences about HTE. See Wang et al.\n    (2018) <doi:10.18637/jss.v085.i07> for further details."}, "hydrogeo": {"categories": ["Hydrology"], "description": "Contains one function for drawing Piper diagrams (also\n    called Piper-Hill diagrams) of water analyses for major ions."}, "TestDesign": {"categories": ["Psychometrics"], "description": "Uses the optimal test design approach by Birnbaum (1968, ISBN:9781593119348) and\n    van der Linden (2018) <doi:10.1201/9781315117430> to construct fixed, adaptive, and parallel tests.\n    Supports the following mixed-integer programming (MIP) solver packages: 'lpsymphony', 'Rsymphony',\n    'gurobi', 'lpSolve', and 'Rglpk'. The 'gurobi' package is not available from CRAN; see <https://www.gurobi.com/downloads/>. "}, "AdvancedBasketballStats": {"categories": ["SportsAnalytics"], "description": "Provides different functionalities and calculations used in the world of basketball to analyze the statistics of the players, the statistics of the teams, the statistics of the quintets and the statistics of the plays. For more details of the calculations included in the package can be found in the book Basketball on Paper written by Dean Oliver."}, "HAC": {"categories": ["Distributions"], "description": "Package provides the estimation of the structure and the parameters, sampling methods and structural plots of Hierarchical Archimedean Copulae (HAC)."}, "implied": {"categories": ["SportsAnalytics"], "description": "Convert between bookmaker odds and probabilities. Eight different\n    algorithms are available, including basic normalization, Shin's method \n    (Hyun Song Shin, (1992) <doi:10.2307/2234526>), and others."}, "sae": {"categories": ["OfficialStatistics"], "description": "Functions for small area estimation."}, "mstate": {"categories": ["Epidemiology", "Survival"], "description": "Contains functions for data preparation, descriptives, hazard estimation and prediction with Aalen-Johansen or simulation in competing risks and multi-state models, see Putter, Fiocco, Geskus (2007) <doi:10.1002/sim.2712>."}, "baggr": {"categories": ["Bayesian", "MetaAnalysis"], "description": "Running and comparing meta-analyses of data with hierarchical \n    Bayesian models in Stan, including convenience functions for formatting\n    data, plotting and pooling measures specific to meta-analysis. This implements many models\n    from Meager (2019) <doi:10.1257/app.20170299>."}, "impimp": {"categories": ["MissingData"], "description": "Imputing blockwise missing data by imprecise imputation,\n    featuring a domain-based, variable-wise, and case-wise strategy. \n    Furthermore, the estimation of lower and upper bounds for \n    unconditional and conditional probabilities based on the obtained\n    imprecise data is implemented.\n    Additionally, two utility functions are supplied: one to check \n    whether variables in a data set contain set-valued observations;\n    and another to merge two already imprecisely imputed data. \n    The method is described in a technical report by Endres, Fink and\n    Augustin (2018, <doi:10.5282/ubm/epub.42423>)."}, "qsub": {"categories": ["HighPerformanceComputing"], "description": "Run lapply() calls in parallel by submitting them to \n    'gridengine' clusters using the 'qsub' command."}, "trtf": {"categories": ["MachineLearning"], "description": "Recursive partytioning of transformation models with\n  corresponding random forest for conditional transformation models \n  as described in 'Transformation Forests' (Hothorn and Zeileis, 2021, <doi:10.1080/10618600.2021.1872581>) \n  and 'Top-Down Transformation Choice' (Hothorn, 2018, <doi:10.1177/1471082X17748081>)."}, "prevalence": {"categories": ["Bayesian"], "description": "The prevalence package provides Frequentist and Bayesian methods for prevalence assessment studies. IMPORTANT: the truePrev functions in the prevalence package call on JAGS (Just Another Gibbs Sampler), which therefore has to be available on the user's system. JAGS can be downloaded from <https://mcmc-jags.sourceforge.io/>."}, "rstpm2": {"categories": ["Survival"], "description": "R implementation of generalized survival models (GSMs), smooth accelerated failure time (AFT) models and Markov multi-state models. For the GSMs, g(S(t|x))=eta(t,x) for a link function g, survival S at time t with covariates x and a linear predictor eta(t,x). The main assumption is that the time effect(s) are smooth <doi:10.1177/0962280216664760>. For fully parametric models with natural splines, this re-implements Stata's 'stpm2' function, which are flexible parametric survival models developed by Royston and colleagues. We have extended the parametric models to include any smooth parametric smoothers for time. We have also extended the model to include any smooth penalized smoothers from the 'mgcv' package, using penalized likelihood. These models include left truncation, right censoring, interval censoring, gamma frailties and normal random effects <doi:10.1002/sim.7451>, and copulas. For the smooth AFTs, S(t|x) = S_0(t*eta(t,x)), where the baseline survival function S_0(t)=exp(-exp(eta_0(t))) is modelled for natural splines for eta_0, and the time-dependent cumulative acceleration factor eta(t,x)=\\int_0^t exp(eta_1(u,x)) du for log acceleration factor eta_1(u,x). The Markov multi-state models allow for a range of models with smooth transitions to predict transition probabilities, length of stay, utilities and costs, with differences, ratios and standardisation."}, "dirmult": {"categories": ["Distributions"], "description": "Estimate parameters in Dirichlet-Multinomial and compute log-likelihoods."}, "pcdpca": {"categories": ["FunctionalData", "TimeSeries"], "description": "Method extends multivariate and functional dynamic principal components\n    to periodically correlated multivariate time series. This package allows you to\n    compute true dynamic principal components in the presence of periodicity. \n    We follow implementation guidelines as described in Kidzinski, Kokoszka and\n    Jouzdani (2017), in Principal component analysis of periodically correlated\n    functional time series <arXiv:1612.00040>."}, "powerbydesign": {"categories": ["ExperimentalDesign"], "description": "Functions for bootstrapping the power of ANOVA designs\n    based on estimated means and standard deviations of the conditions.\n    Please refer to the documentation of the boot.power.anova() function\n    for further details."}, "NIRStat": {"categories": ["MissingData"], "description": "Provides transfusion-related differential tests on Near-infrared spectroscopy (NIRS) time series with detection limit, which contains two testing statistics: Mean Area Under the Curve (MAUC) and slope statistic. This package applied a penalized spline method within imputation setting. Testing is conducted by a nested permutation approach within imputation. Refer to Guo et al (2018) <doi:10.1177/0962280218786302> for further details."}, "FD": {"categories": ["Environmetrics"], "description": "Computes different multidimensional FD indices.  Implements a distance-based framework to measure FD that allows any number and type of functional traits, and can also consider species relative abundances.  Also contains other useful tools for functional ecology."}, "rgugik": {"categories": ["Spatial"], "description": "Automatic open data acquisition from resources of Polish Head Office\n    of Geodesy and Cartography ('G\u0142\u00f3wny Urz\u0105d Geodezji i Kartografii')\n    (<https://www.gov.pl/web/gugik>).\n    Available datasets include various types of numeric, raster and vector data,\n    such as orthophotomaps, digital elevation models (digital terrain models,\n    digital surface model, point clouds), state register of borders, spatial\n    databases, geometries of cadastral parcels, 3D models of buildings, and more.\n    It is also possible to geocode addresses or objects using the geocodePL_get()\n    function."}, "rgee": {"categories": ["Spatial"], "description": "Earth Engine <https://earthengine.google.com/> client library for R. All\n  of the 'Earth Engine' API classes, modules, and functions are made available. Additional\n  functions implemented include importing (exporting) of Earth Engine spatial objects, \n  extraction of time series, interactive map display, assets management interface, \n  and metadata display. See <https://r-spatial.github.io/rgee/> for further details."}, "dti": {"categories": ["MedicalImaging"], "description": "Diffusion Weighted Imaging (DWI) is a Magnetic Resonance Imaging\n             modality, that measures diffusion of water in tissues like the human\n             brain. The package contains R-functions to process diffusion-weighted\n             data. The functionality includes diffusion tensor imaging (DTI),\n             diffusion kurtosis imaging (DKI), modeling for high angular resolution\n             diffusion weighted imaging (HARDI) using Q-ball-reconstruction and\n             tensor mixture models, several methods for structural adaptive\n             smoothing including POAS and msPOAS, and a streamline fiber tracking\n             for tensor and tensor mixture models.\n             The package provides functionality to manipulate and visualize results\n             in 2D and 3D."}, "nonnest2": {"categories": ["Econometrics"], "description": "Testing non-nested models via theory supplied by Vuong (1989) <doi:10.2307/1912557>.\n    Includes tests of model distinguishability and of model fit that can be applied\n    to both nested and non-nested models. Also includes functionality to obtain\n    confidence intervals associated with AIC and BIC. This material is partially based on\n    work supported by the National Science Foundation under Grant Number SES-1061334."}, "geomapdata": {"categories": ["Spatial"], "description": "Data sets included here are for use with package GEOmap. These include world map, USA map, Coso map, Japan Map."}, "nanotime": {"categories": ["TimeSeries"], "description": "Full 64-bit resolution date and time functionality with\n nanosecond granularity is provided, with easy transition to and from\n the standard 'POSIXct' type. Three additional classes offer interval,\n period and duration functionality for nanosecond-resolution timestamps."}, "pdynmc": {"categories": ["Econometrics"], "description": "Linear dynamic panel data modeling based on linear and\n    nonlinear moment conditions as proposed by\n    Holtz-Eakin, Newey, and Rosen (1988) <doi:10.2307/1913103>,\n    Ahn and Schmidt (1995) <doi:10.1016/0304-4076(94)01641-C>,\n    and Arellano and Bover (1995) <doi:10.1016/0304-4076(94)01642-D>.\n    Estimation of the model parameters relies on the Generalized\n    Method of Moments (GMM), numerical optimization (when nonlinear\n    moment conditions are employed) and the computation of closed\n    form solutions (when estimation is based on linear moment\n    conditions). One-step, two-step and iterated estimation is\n    available. For inference and specification\n    testing, Windmeijer (2005) <doi:10.1016/j.jeconom.2004.02.005>\n    and doubly corrected standard errors\n    (Hwang, Kang, Lee, 2021 <doi:10.1016/j.jeconom.2020.09.010>)\n    are available. Additionally, serial correlation tests, tests for\n    overidentification, and Wald tests are provided. Functions for\n    visualizing panel data structures and modeling results obtained\n    from GMM estimation are also available. The plot methods include\n    functions to plot unbalanced panel structure, coefficient ranges\n    and coefficient paths across GMM iterations (the latter is\n    implemented according to the plot shown in\n    Hansen and Lee, 2021 <doi:10.3982/ECTA16274>).\n    For a more detailed description of the functionality, please\n    see Fritsch, Pua, Schnurbus (2021) <doi:10.32614/RJ-2021-035>."}, "aggregation": {"categories": ["MetaAnalysis"], "description": "Contains functionality for performing the following methods of p-value aggregation: Fisher's method [Fisher, RA (1932, ISBN: 9780028447308)], the Lancaster method (weighted Fisher's method) [Lancaster, HO (1961, <doi:10.1111/j.1467-842X.1961.tb00058.x>)], and Sidak correction [Sidak, Z (1967, <doi:10.1080/01621459.1967.10482935>)].  Please cite Yi et al., the manuscript corresponding to this package [Yi, L et al., (2017), <doi:10.1101/190199>]."}, "crunch": {"categories": ["WebTechnologies"], "description": "The Crunch.io service <https://crunch.io/> provides a cloud-based\n    data store and analytic engine, as well as an intuitive web interface.\n    Using this package, analysts can interact with and manipulate Crunch\n    datasets from within R. Importantly, this allows technical researchers to\n    collaborate naturally with team members, managers, and clients who prefer a\n    point-and-click interface."}, "OrthoPanels": {"categories": ["Econometrics"], "description": "Implements the orthogonal reparameterization\n    approach recommended by Lancaster (2002) to estimate dynamic panel\n    models with fixed effects (and optionally: panel specific\n    intercepts). The approach uses a likelihood-based estimator and\n    produces estimates that are asymptotically unbiased as N goes to\n    infinity, with a T as low as 2."}, "tsutils": {"categories": ["TimeSeries"], "description": "Includes: (i) tests and visualisations that can help the modeller explore time series components and perform decomposition; (ii) modelling shortcuts, such as functions to construct lagmatrices and seasonal dummy variables of various forms; (iii) an implementation of the Theta method; (iv) tools to facilitate the design of the forecasting process, such as ABC-XYZ analyses; and (v) \"quality of life\" functions, such as treating time series for trailing and leading values."}, "spBayes": {"categories": ["Bayesian", "Spatial", "SpatioTemporal"], "description": "Fits univariate and multivariate spatio-temporal\n        random effects models for point-referenced data using Markov chain Monte Carlo (MCMC). Details are given in Finley, Banerjee, and Gelfand (2015) <doi:10.18637/jss.v063.i13> and Finley and Banerjee <doi:10.1016/j.envsoft.2019.104608>."}, "s20x": {"categories": ["Distributions"], "description": "A set of functions used in teaching STATS 201/208 Data Analysis at\n    the University of Auckland. The functions are designed to make parts of R more\n    accessible to a large undergraduate population who are mostly not statistics\n    majors."}, "survRM2": {"categories": ["Survival"], "description": "Performs two-sample comparisons using the restricted mean survival time (RMST) as a summary measure of the survival time distribution. Three kinds of between-group contrast metrics (i.e., the difference in RMST, the ratio of RMST and the ratio of the restricted mean time lost (RMTL)) are computed. It performs an ANCOVA-type covariate adjustment as well as unadjusted analyses for those measures. "}, "fPortfolio": {"categories": ["Finance"], "description": "Provides a collection\n\tof functions to optimize portfolios and to analyze them from\n    different points of view."}, "tree": {"categories": ["MachineLearning"], "description": "Classification and regression trees."}, "ClustVarLV": {"categories": ["Psychometrics"], "description": "Functions for the clustering of variables around Latent Variables, for 2-way or 3-way data. Each cluster of variables, which may be defined as a local or directional cluster, is associated with a latent variable. External variables measured on the same observations or/and additional information on the variables can be taken into account. A \"noise\" cluster or sparse latent variables can also be defined."}, "expm": {"categories": ["NumericalMathematics"], "description": "Computation of the matrix exponential, logarithm, sqrt,\n  and related quantities, using traditional and modern methods."}, "accelmissing": {"categories": ["MissingData"], "description": "Imputation for the missing count values in accelerometer data. The methodology includes both parametric and semi-parametric multiple imputations under the zero-inflated Poisson lognormal model. This package also provides multiple functions to pre-process the accelerometer data previous to the missing data imputation. These includes detecting wearing and non-wearing time, selecting valid days and subjects, and creating plots."}, "vardpoor": {"categories": ["OfficialStatistics"], "description": "Generation of domain variables, linearization of several non-linear population statistics (the ratio of two totals, weighted income percentile, relative median income ratio, at-risk-of-poverty rate, at-risk-of-poverty threshold, Gini coefficient, gender pay gap, the aggregate replacement ratio, the relative median income ratio, median income below at-risk-of-poverty gap, income quintile share ratio, relative median at-risk-of-poverty gap), computation of regression residuals in case of weight calibration, variance estimation of sample surveys by the ultimate cluster method (Hansen, Hurwitz and Madow, Sample Survey Methods And Theory, vol. I: Methods and Applications; vol. II: Theory. 1953, New York: John Wiley and Sons), variance estimation for longitudinal, cross-sectional measures and measures of change for single and multistage stage cluster sampling designs (Berger, Y. G., 2015, <doi:10.1111/rssa.12116>). Several other precision measures are derived - standard error, the coefficient of variation, the margin of error, confidence interval, design effect."}, "piratings": {"categories": ["SportsAnalytics"], "description": "Calculate and optimize dynamic performance ratings of association football \n  teams competing in matches, in accordance with the method used in \n  the research paper \"Determining the level of ability of football teams by \n  dynamic ratings based on the relative discrepancies in scores between adversaries\", \n  by Constantinou and Fenton (2013) \n  <doi:10.1515/jqas-2012-0036>    \n  This dynamic rating system has proven to provide superior \n  results for predicting association football outcomes."}, "Morpho": {"categories": ["MedicalImaging"], "description": "A toolset for Geometric Morphometrics and mesh processing. This\n    includes (among other stuff) mesh deformations based on reference points,\n    permutation tests, detection of outliers, processing of sliding\n    semi-landmarks and semi-automated surface landmark placement."}, "ABCoptim": {"categories": ["Optimization"], "description": "An implementation of Karaboga (2005) Artificial Bee Colony\n    Optimization algorithm <http://mf.erciyes.edu.tr/abc/pub/tr06_2005.pdf>.\n    This (working) version is a Work-in-progress, which is\n    why it has been implemented using pure R code. This was developed upon the basic\n    version programmed in C and distributed at the algorithm's official website."}, "ldbounds": {"categories": ["ClinicalTrials"], "description": "Computations related to group sequential boundaries.\n    Includes calculation of bounds using the Lan-DeMets\n    alpha spending function approach. Based on FORTRAN\n        program ld98 implemented by Reboussin, et al. (2000) <doi:10.1016/s0197-2456(00)00057-x>."}, "prefmod": {"categories": ["MissingData", "Psychometrics"], "description": "Generates design matrix for analysing real paired comparisons and derived paired comparison data (Likert type items/ratings or rankings) using a loglinear approach. Fits loglinear Bradley-Terry model (LLBT) exploiting an eliminate feature. Computes pattern models for paired comparisons, rankings, and ratings. Some treatment of missing values (MCAR and MNAR). Fits latent class (mixture) models for paired comparison, rating and ranking patterns using a non-parametric ML approach."}, "bidask": {"categories": ["Finance"], "description": "Implements an efficient estimation procedure of bid-ask spreads from open, high, low, and close prices\n  as described in Ardia-Guidotti-Kroencke <https://www.ssrn.com/abstract=3892335>. \n  Moreover, it provides an implementation of the estimators proposed in \n  Roll (1984) <doi:10.1111/j.1540-6261.1984.tb03897.x>, \n  Corwin-Schultz (2012) <doi:10.1111/j.1540-6261.2012.01729.x>,\n  and Abdi-Ranaldo (2017) <doi:10.1093/rfs/hhx084>."}, "DiscreteLaplace": {"categories": ["Distributions"], "description": "Probability mass function, distribution function, quantile function, random generation and estimation for the skew discrete Laplace distributions."}, "es.dif": {"categories": ["MetaAnalysis"], "description": "Computes various effect sizes of the difference, their variance, and confidence interval. This package treats Cohen's d, Hedges' d, biased/unbiased c (an effect size between a mean and a constant) and e (an effect size between means without assuming the variance equality)."}, "KMsurv": {"categories": ["Survival"], "description": "Data sets and functions for Klein and Moeschberger (1997),\n        \"Survival Analysis, Techniques for Censored and Truncated\n        Data\", Springer."}, "unrtf": {"categories": ["ReproducibleResearch"], "description": "Wraps the 'unrtf' utility to extract text from RTF files. Supports\n    document conversion to HTML, LaTeX or plain text. Output in HTML is recommended\n    because 'unrtf' has limited support for converting between character encodings."}, "nomisr": {"categories": ["OfficialStatistics"], "description": "Access UK official statistics from the 'Nomis' database. \n    'Nomis' includes data from the Census, the Labour Force Survey, DWP benefit \n    statistics and other economic and demographic data from the Office for \n    National Statistics, based around statistical geographies. See \n    <https://www.nomisweb.co.uk/api/v01/help> for full API documentation."}, "MRPC": {"categories": ["CausalInference"], "description": "A PC Algorithm with the Principle of Mendelian Randomization. This package implements the MRPC \n            (PC with the principle of Mendelian randomization) algorithm to infer causal graphs. It also \n            contains functions to simulate data under a certain topology, to visualize a graph in different \n            ways, and to compare graphs and quantify the differences. \n            See Badsha and Fu (2019) <doi:10.3389/fgene.2019.00460>,Badsha, Martin and Fu (2021) <doi:10.3389/fgene.2021.651812>. "}, "reldist": {"categories": ["Econometrics"], "description": "Tools for the comparison of distributions. This includes nonparametric estimation of the relative distribution PDF and CDF and numerical summaries as described in \"Relative Distribution Methods in the Social Sciences\" by Mark S. Handcock and Martina Morris, Springer-Verlag, 1999, Springer-Verlag, ISBN 0387987789."}, "cmfrec": {"categories": ["MissingData"], "description": "Collective matrix factorization (a.k.a. multi-view or multi-way factorization,\n\tSingh, Gordon, (2008) <doi:10.1145/1401890.1401969>) tries to approximate a (potentially very sparse\n\tor having many missing values) matrix 'X' as the product of two low-dimensional matrices,\n\toptionally aided with secondary information matrices about rows and/or columns of 'X',\n\twhich are also factorized using the same latent components.\n\tThe intended usage is for recommender systems, dimensionality reduction, and missing value imputation.\n\tImplements extensions of the original model (Cortes, (2018) <arXiv:1809.00366>) and can produce\n\tdifferent factorizations such as the weighted 'implicit-feedback' model (Hu, Koren, Volinsky,\n\t(2008) <doi:10.1109/ICDM.2008.22>), the 'weighted-lambda-regularization' model,\n\t(Zhou, Wilkinson, Schreiber, Pan, (2008) <doi:10.1007/978-3-540-68880-8_32>),\n\tor the enhanced model with 'implicit features' (Rendle, Zhang,\n\tKoren, (2019) <arXiv:1905.01395>), with or without side information. Can use gradient-based\n\tprocedures or alternating-least squares procedures (Koren, Bell, Volinsky, (2009)\n\t<doi:10.1109/MC.2009.263>), with either a Cholesky solver, a faster conjugate gradient solver\n\t(Takacs, Pilaszy, Tikk, (2011) <doi:10.1145/2043932.2043987>), or a non-negative\n\tcoordinate descent solver (Franc, Hlavac, Navara, (2005) <doi:10.1007/11556121_50>),\n\tproviding efficient methods for sparse and dense data, and mixtures thereof.\n\tSupports L1 and L2 regularization in the main models,\n\toffers alternative most-popular and content-based models, and implements functionality\n\tfor cold-start recommendations and imputation of 2D data."}, "Watersheds": {"categories": ["Spatial"], "description": "Methods for watersheds aggregation and spatial drainage network analysis."}, "BayesLN": {"categories": ["Bayesian"], "description": "Bayesian inference under log-normality assumption must be performed very carefully. In fact, under the common priors for the variance, useful quantities in the original data scale (like mean and quantiles) do not have posterior moments that are finite (Fabrizi et al. 2012 <doi:10.1214/12-BA733>). This package allows to easily carry out a proper Bayesian inferential procedure by fixing a suitable distribution (the generalized inverse Gaussian) as prior for the variance. Functions to estimate several kind of means (unconditional, conditional and conditional under a mixed model) and quantiles (unconditional and conditional) are provided. "}, "tensorTS": {"categories": ["TimeSeries"], "description": "Factor and autoregressive models for matrix and tensor valued time series. We provide functions for estimation, simulation and prediction. The models are discussed in \n    Li et al (2021) <arXiv:2110.00928>, Chen et al (2020) <doi:10.1080/01621459.2021.1912757>, \n    Chen et al (2020) <doi:10.1016/j.jeconom.2020.07.015>, and Xiao et al (2020) <arXiv:2006.02611>."}, "quantification": {"categories": ["OfficialStatistics"], "description": "Provides different functions for quantifying qualitative survey data. It supports the Carlson-Parkin method, the regression approach, the balance approach and the conditional expectations method."}, "bang": {"categories": ["Bayesian"], "description": "Provides functions for the Bayesian analysis of some simple \n    commonly-used models, without using Markov Chain Monte Carlo (MCMC) \n    methods such as Gibbs sampling.  The 'rust' package\n    <https://cran.r-project.org/package=rust> is used to simulate a random \n    sample from the required posterior distribution, using the generalized \n    ratio-of-uniforms method.  See Wakefield, Gelfand and Smith (1991) \n    <doi:10.1007/BF01889987> for details. At the moment three conjugate \n    hierarchical models are available: beta-binomial, gamma-Poisson and a 1-way \n    analysis of variance (ANOVA)."}, "topicmodels": {"categories": ["NaturalLanguageProcessing"], "description": "Provides an interface to the C code for Latent Dirichlet\n\t     Allocation (LDA) models and Correlated Topics Models\n\t     (CTM) by David M. Blei and co-authors and the C++ code\n\t     for fitting LDA models using Gibbs sampling by Xuan-Hieu\n\t     Phan and co-authors."}, "nnet": {"categories": ["Econometrics", "MachineLearning"], "description": "Software for feed-forward neural networks with a single\n  hidden layer, and for multinomial log-linear models."}, "timereg": {"categories": ["Survival"], "description": "Programs for Martinussen and Scheike (2006), \u2018Dynamic Regression\n    Models for Survival Data\u2019, Springer Verlag.  Plus more recent developments.\n    Additive survival model, semiparametric proportional odds model, fast\n    cumulative residuals, excess risk models and more. Flexible competing risks\n    regression including GOF-tests. Two-stage frailty modelling. PLS for the\n    additive risk model. Lasso in the 'ahaz' package."}, "dfcrm": {"categories": ["ClinicalTrials", "ExperimentalDesign"], "description": "Provides functions to run the CRM and\n        TITE-CRM in phase I trials and calibration tools for trial\n        planning purposes."}, "SimInf": {"categories": ["Epidemiology"], "description": "Provides an efficient and very flexible framework to\n    conduct data-driven epidemiological modeling in realistic large\n    scale disease spread simulations. The framework integrates\n    infection dynamics in subpopulations as continuous-time Markov\n    chains using the Gillespie stochastic simulation algorithm and\n    incorporates available data such as births, deaths and movements\n    as scheduled events at predefined time-points. Using C code for\n    the numerical solvers and 'OpenMP' (if available) to divide work\n    over multiple processors ensures high performance when simulating\n    a sample outcome. One of our design goals was to make the package\n    extendable and enable usage of the numerical solvers from other R\n    extension packages in order to facilitate complex epidemiological\n    research. The package contains template models and can be extended\n    with user-defined models. For more details see the paper by\n    Widgren, Bauer, Eriksson and Engblom (2019)\n    <doi:10.18637/jss.v091.i12>. The package also provides\n    functionality to fit models to time series data using the\n    Approximate Bayesian Computation Sequential Monte Carlo\n    ('ABC-SMC') algorithm of Toni and others (2009)\n    <doi:10.1098/rsif.2008.0172>."}, "tsPI": {"categories": ["TimeSeries"], "description": "Prediction intervals for ARIMA and structural time series\n    models using importance sampling approach with uninformative priors for model\n    parameters, leading to more accurate coverage probabilities in frequentist\n    sense. Instead of sampling the future observations and hidden states of the\n    state space representation of the model, only model parameters are sampled,\n    and the method is based solving the equations corresponding to the conditional\n    coverage probability of the prediction intervals. This makes method relatively\n    fast compared to for example MCMC methods, and standard errors of prediction\n    limits can also be computed straightforwardly."}, "RhpcBLASctl": {"categories": ["HighPerformanceComputing"], "description": "Control the number of threads on 'BLAS' (Aka 'GotoBLAS', 'OpenBLAS', 'ACML', 'BLIS' and 'MKL').\n  And possible to control the number of threads in 'OpenMP'.\n  Get a number of logical cores and physical cores if feasible."}, "RSelenium": {"categories": ["WebTechnologies"], "description": "Provides a set of R bindings for the 'Selenium 2.0 WebDriver'\n    (see <https://selenium.dev/documentation/en/>\n    for more information) using the 'JsonWireProtocol' (see\n    <https://github.com/SeleniumHQ/selenium/wiki/JsonWireProtocol> for more\n    information). 'Selenium 2.0 WebDriver' allows driving a web browser\n    natively as a user would either locally or on a remote machine using\n    the Selenium server it marks a leap forward in terms of web browser\n    automation. Selenium automates web browsers (commonly referred to as\n    browsers). Using RSelenium you can automate browsers locally or\n    remotely."}, "unmarked": {"categories": ["Environmetrics"], "description": "Fits hierarchical models of animal abundance and occurrence to data collected using survey methods such as point counts, site occupancy sampling, distance sampling, removal sampling, and double observer sampling. Parameters governing the state and observation processes can be modeled as functions of covariates. Reference: Fiske and Chandler (2011) <doi:10.18637/jss.v043.i10>."}, "spatgraphs": {"categories": ["Spatial"], "description": "Graphs (or networks) and graph component\n        calculations for spatial locations in 1D, 2D, 3D etc."}, "DTRlearn2": {"categories": ["CausalInference"], "description": "We provide a comprehensive software to estimate general K-stage DTRs from SMARTs with Q-learning and a variety of outcome-weighted learning methods. Penalizations are allowed for variable selection and model regularization. With the outcome-weighted learning scheme, different loss functions - SVM hinge loss, SVM ramp loss, binomial deviance loss, and L2 loss - are adopted to solve the weighted classification problem at each stage; augmentation in the outcomes is allowed to improve efficiency. The estimated DTR can be easily applied to a new sample for individualized treatment recommendations or DTR evaluation."}, "semsfa": {"categories": ["Econometrics"], "description": "Semiparametric Estimation of Stochastic Frontier Models following a two step procedure: in the first step semiparametric or nonparametric regression techniques are used to relax parametric restrictions of the functional form representing technology and in the second step variance parameters are obtained by pseudolikelihood estimators or by method of moments."}, "sensitivity": {"categories": ["Environmetrics"], "description": "A collection of functions for factor screening, global sensitivity analysis and robustness analysis. Most of the functions have to be applied on model with scalar output, but several functions support multi-dimensional outputs."}, "roptim": {"categories": ["Optimization"], "description": "Perform general purpose optimization in R using C++. A unified \n    wrapper interface is provided to call C functions of the five optimization \n    algorithms ('Nelder-Mead', 'BFGS', 'CG', 'L-BFGS-B' and 'SANN') underlying \n    optim()."}, "OptionPricing": {"categories": ["Finance"], "description": "Efficient Monte Carlo Algorithms for the price and the sensitivities of Asian and European Options under Geometric Brownian Motion."}, "simexaft": {"categories": ["Survival"], "description": "Implement of the Simulation-Extrapolation (SIMEX) algorithm for the accelerated failure time (AFT) with covariates subject to measurement error."}, "targeted": {"categories": ["MissingData"], "description": "Various methods for targeted and semiparametric inference including\n\t     augmented inverse probability weighted estimators for missing data and\n\t     causal inference (Bang and Robins (2005) <doi:10.1111/j.1541-0420.2005.00377.x>)\n\t     and estimators for risk differences and relative risks (Richardson et al. (2017)\n\t     <doi:10.1080/01621459.2016.1192546>)."}, "MBNMAdose": {"categories": ["MetaAnalysis"], "description": "Fits Bayesian dose-response model-based network meta-analysis (MBNMA) \n    that incorporate multiple doses within an agent by modelling different dose-response functions, as\n    described by Mawdsley et al. (2016) <doi:10.1002/psp4.12091>. \n    By modelling dose-response relationships this can connect networks of evidence that might\n    otherwise be disconnected, and can improve precision on treatment estimates. Several common \n    dose-response functions are provided; others may be added by the user. Various characteristics\n    and assumptions can be flexibly added to the models, such as shared class effects. The consistency \n    of direct and indirect evidence in the network can be assessed using unrelated mean effects models \n    and/or by node-splitting at the treatment level."}, "ashr": {"categories": ["Bayesian"], "description": "The R package 'ashr' implements an Empirical Bayes\n    approach for large-scale hypothesis testing and false discovery\n    rate (FDR) estimation based on the methods proposed in\n    M. Stephens, 2016, \"False discovery rates: a new deal\",\n    <doi:10.1093/biostatistics/kxw041>. These methods can be applied\n    whenever two sets of summary statistics\u2014estimated effects and\n    standard errors\u2014are available, just as 'qvalue' can be applied\n    to previously computed p-values. Two main interfaces are\n    provided: ash(), which is more user-friendly; and ash.workhorse(),\n    which has more options and is geared toward advanced users. The\n    ash() and ash.workhorse() also provides a flexible modeling\n    interface that can accommodate a variety of likelihoods (e.g.,\n    normal, Poisson) and mixture priors (e.g., uniform, normal)."}, "cba": {"categories": ["Cluster"], "description": "Implements clustering techniques such as Proximus and Rock, utility functions for efficient computation of cross distances and data manipulation. "}, "gcerisk": {"categories": ["Survival"], "description": "Generalized competing event model based on Cox PH model and Fine-Gray model.\n    This function is designed to develop optimized risk-stratification methods for competing\n    risks data, such as described in:\n    1. Carmona R, Gulaya S, Murphy JD, Rose BS, Wu J, Noticewala S,McHale MT, Yashar CM, Vaida F,\n    and Mell LK (2014) <doi:10.1016/j.ijrobp.2014.03.047>.\n    2. Carmona R, Zakeri K, Green G, Hwang L, Gulaya S, Xu B, Verma R, Williamson CW, Triplett DP, Rose\n    BS, Shen H, Vaida F, Murphy JD, and Mell LK (2016) <doi:10.1200/JCO.2015.65.0739>.\n    3. Lunn, Mary, and Don McNeil (1995) <doi:10.2307/2532940>."}, "ows4R": {"categories": ["Spatial", "WebTechnologies"], "description": "Provides an Interface to Web-Services defined as standards by the Open Geospatial Consortium (OGC), including Web Feature Service\n (WFS) for vector data, Web Coverage Service (WCS), Catalogue Service (CSW) for ISO/OGC metadata, and associated standards such as the common \n web-service specification (OWS) and OGC Filter Encoding. Partial support is provided for the Web Map Service (WMS) and Web Processing Service (WPS). \n The purpose is to add support for additional OGC service standards such as Web Coverage Processing Service (WCPS) or OGC API."}, "missSBM": {"categories": ["MissingData"], "description": "When a network is partially observed (here, NAs in the adjacency matrix rather than 1 or 0 \n  due to missing information between node pairs), it is possible to account for the underlying process\n  that generates those NAs. 'missSBM', presented in 'Barbillon, Chiquet and Tabouy' (2021) <doi:10.18637/jss.v101.i12>,\n  adjusts the popular stochastic block model from network data sampled under various missing data conditions, \n  as described in 'Tabouy, Barbillon and Chiquet' (2019) <doi:10.1080/01621459.2018.1562934>."}, "bsamGP": {"categories": ["Bayesian"], "description": "Contains functions to perform Bayesian inference\n    using a spectral analysis of Gaussian process priors.\n    Gaussian processes are represented with a Fourier series \n    based on cosine basis functions. Currently the package\n    includes parametric linear models, partial linear additive\n    models with/without shape restrictions, generalized linear\n    additive models with/without shape restrictions, and  \n    density estimation model. To maximize computational \n    efficiency, the actual Markov chain Monte Carlo sampling \n    for each model is done using codes written in FORTRAN 90.\n    This software has been developed using funding supported by\n    Basic Science Research Program through the National Research\n    Foundation of Korea (NRF) funded by the Ministry of Education\n    (no. NRF-2016R1D1A1B03932178 and no. NRF-2017R1D1A3B03035235)."}, "exams": {"categories": ["ReproducibleResearch", "TeachingStatistics"], "description": "Automatic generation of exams based on exercises in Markdown or LaTeX format,\n\tpossibly including R code for dynamic generation of exercise elements.\n\tExercise types include single-choice and multiple-choice questions, arithmetic problems,\n\tstring questions, and combinations thereof (cloze). Output formats include standalone\n\tfiles (PDF, HTML, Docx, ODT, ...), Moodle XML, QTI 1.2, QTI 2.1, Blackboard, Canvas, OpenOLAT,\n\tARSnova, and TCExam. In addition to fully customizable PDF exams, a standardized PDF format\n\t(NOPS) is provided that can be printed, scanned, and automatically evaluated."}, "hdi": {"categories": ["MachineLearning"], "description": "Implementation of multiple approaches to perform inference in high-dimensional models."}, "wehoop": {"categories": ["SportsAnalytics"], "description": "A utility for working with women's basketball data. A scraping and aggregating interface for the WNBA Stats API <https://stats.wnba.com/> and ESPN's <https://www.espn.com> women's college basketball and WNBA statistics. It provides users with the capability to access the game play-by-plays, box scores, standings and results to analyze the data for themselves."}, "MPV": {"categories": ["TeachingStatistics"], "description": "Most of this package consists of data sets from the \n             textbook Introduction\n             to Linear Regression Analysis (3rd ed), by Montgomery, Peck\n             and Vining.\n             Some additional data sets and functions are also included."}, "kohonen": {"categories": ["ChemPhys"], "description": "Functions to train self-organising maps (SOMs). Also interrogation of the maps and prediction using trained maps are supported. The name of the package refers to Teuvo Kohonen, the inventor of the SOM."}, "fExtremes": {"categories": ["Distributions", "ExtremeValue", "Finance"], "description": "Provides functions for analysing\n  and modelling extreme events in financial time Series. The\n  topics include: (i) data pre-processing, (ii) explorative \n  data analysis, (iii) peak over threshold modelling, (iv) block\n  maxima modelling, (v) estimation of VaR and CVaR, and (vi) the\n  computation of the extreme index."}, "riskCommunicator": {"categories": ["Epidemiology"], "description": "Estimates flexible epidemiological effect measures including both differences and ratios using the parametric G-formula developed as an alternative to inverse probability weighting.  It is useful for estimating the impact of interventions in the presence of treatment-confounder-feedback. G-computation was originally described by Robbins (1986) <doi:10.1016/0270-0255(86)90088-6> and has been described in detail by Ahern, Hubbard, and Galea (2009) <doi:10.1093/aje/kwp015>; Snowden, Rose, and Mortimer (2011) <doi:10.1093/aje/kwq472>; and Westreich et al. (2012) <doi:10.1002/sim.5316>."}, "MatchThem": {"categories": ["CausalInference", "MissingData", "OfficialStatistics"], "description": "Provides the necessary tools for the pre-processing techniques of matching and weighting multiply imputed datasets to control for effects of confounders and to reduce the degree of dependence on certain modeling assumptions in studying the causal associations between an exposure and an outcome. This package includes functions to perform matching within and across the multiply imputed datasets using several matching methods, to estimate weights of units in the imputed datasets using several weighting methods, to calculate the causal effect estimate in each matched or weighted dataset using parametric or non-parametric statistical models, and to pool the obtained estimates from these models according to Rubin's rules (please see <https://journal.r-project.org/archive/2021/RJ-2021-073/> for details)."}, "wnl": {"categories": ["Pharmacokinetics"], "description": "This is a set of minimization tools (maximum likelihood estimation and least square fitting) to solve examples in the Johan Gabrielsson and Dan Weiner's book \"Pharmacokinetic and Pharmacodynamic Data Analysis - Concepts and Applications\" 5th ed. (ISBN:9198299107). Examples include linear and nonlinear compartmental model, turn-over model, single or multiple dosing bolus/infusion/oral models, allometry, toxicokinetics, reversible metabolism, in-vitro/in-vivo extrapolation, enterohepatic circulation, metabolite modeling, Emax model, inhibitory model, tolerance model, oscillating response model, enantiomer interaction model, effect compartment model, drug-drug interaction model, receptor occupancy model, and rebound phenomena model. "}, "networktools": {"categories": ["Psychometrics"], "description": "Includes assorted tools for network analysis. Bridge centrality; goldbricker; MDS, PCA, & eigenmodel network plotting."}, "smoots": {"categories": ["TimeSeries"], "description": "The nonparametric trend and its derivatives in equidistant time \n    series (TS) with short-memory stationary errors can be estimated. The \n    estimation is conducted via local polynomial regression using an \n    automatically selected bandwidth obtained by a built-in iterative plug-in \n    algorithm or a bandwidth fixed by the user. A Nadaraya-Watson kernel \n    smoother is also built-in as a comparison. With version 1.1.0, a linearity \n    test for the trend function, forecasting methods and backtesting \n    approaches are implemented as well.\n    The smoothing methods of the package are described in Feng, Y., Gries, T., \n    and Fritz, M. (2020) <doi:10.1080/10485252.2020.1759598>."}, "HyperbolicDist": {"categories": ["Distributions"], "description": "Maintenance has been discontinued for this package. It has been\n\t     superseded by 'GeneralizedHyperbolic'. 'GeneralizedHyperbolic' \n\t     includes all the functionality of 'HyperbolicDist' and more \n\t     and is based on a more rational design. 'HyperbolicDist' \n\t     provides functions for the hyperbolic and related \n\t     distributions. Density, distribution and quantile functions \n\t     and random number generation are provided for the hyperbolic \n\t     distribution, the generalized hyperbolic distribution, \n\t     the generalized inverse Gaussian distribution and the \n\t     skew-Laplace distribution. Additional functionality is \n\t     provided for the hyperbolic distribution, including \n\t     fitting of the hyperbolic to data."}, "eegkit": {"categories": ["MedicalImaging", "Psychometrics"], "description": "Analysis and visualization tools for electroencephalography (EEG) data. Includes functions for (i) plotting EEG data, (ii) filtering EEG data, (iii) smoothing EEG data; (iv) frequency domain (Fourier) analysis of EEG data, (v) Independent Component Analysis of EEG data, and (vi) simulating event-related potential EEG data."}, "wavScalogram": {"categories": ["TimeSeries"], "description": "Provides scalogram based wavelet tools for time series analysis: wavelet power spectrum, scalogram, windowed scalogram, windowed scalogram difference (see Bolos et al. (2017) <doi:10.1016/j.amc.2017.05.046>), scale index and windowed scale index (Benitez et al. (2010) <doi:10.1016/j.camwa.2010.05.010>)."}, "WRS2": {"categories": ["Robust"], "description": "A collection of robust statistical methods based on Wilcox' WRS functions. It implements robust t-tests (independent and dependent samples), robust ANOVA (including between-within subject designs), quantile ANOVA, robust correlation, robust mediation, and nonparametric ANCOVA models based on robust location measures."}, "MultiRNG": {"categories": ["Distributions"], "description": "Pseudo-random number generation for 11 multivariate distributions: Normal, t, Uniform, Bernoulli, Hypergeometric, Beta (Dirichlet), Multinomial, Dirichlet-Multinomial, Laplace, Wishart, and Inverted Wishart. The details of the method are explained in Demirtas (2004) <doi:10.22237/jmasm/1099268340>."}, "ProfessR": {"categories": ["TeachingStatistics"], "description": "Programs to determine student grades and create\n  examinations from Question banks.  Programs will create numerous\n  multiple choice exams, randomly shuffled, for different versions of same question list."}, "dbx": {"categories": ["Databases"], "description": "Provides select, insert, update, upsert, and delete database operations. Supports 'PostgreSQL', 'MySQL', 'SQLite', and more, and plays nicely with the 'DBI' package."}, "optR": {"categories": ["NumericalMathematics"], "description": "Solves linear systems of form Ax=b via Gauss elimination, \n  LU decomposition, Gauss-Seidel, Conjugate Gradient Method (CGM) and Cholesky methods."}, "lqmm": {"categories": ["Robust"], "description": "Functions to fit quantile regression models for hierarchical\n    data (2-level nested designs) as described in Geraci and\n    Bottai (2014, Statistics and Computing) <doi:10.1007/s11222-013-9381-9>.\n    A vignette is given in Geraci (2014, Journal of Statistical Software)\n    <doi:10.18637/jss.v057.i13> and included in the package documents.\n    The packages also provides functions to fit quantile models for\n    independent\tdata and for count responses."}, "GCPM": {"categories": ["Finance"], "description": "Analyze the default risk of credit portfolios. Commonly known models, \n\t\tlike CreditRisk+ or the CreditMetrics model are implemented in their very basic settings.\n\t\tThe portfolio loss distribution can be achieved either by simulation or analytically \n\t\tin case of the classic CreditRisk+ model. Models are only implemented to respect losses\n\t\tcaused by defaults, i.e. migration risk is not included. The package structure is kept\n\t\tflexible especially with respect to distributional assumptions in order to quantify the\n\t\tsensitivity of risk figures with respect to several assumptions. Therefore the package\n\t\tcan be used to determine the credit risk of a given portfolio as well as to quantify\n\t\tmodel sensitivities."}, "mdftracks": {"categories": ["SpatioTemporal"], "description": "'MTrackJ' is an 'ImageJ' plugin for motion tracking and analysis (see \n    <https://imagescience.org/meijering/software/mtrackj/>). This package reads \n    and writes 'MTrackJ Data Files' ('.mdf', see \n    <https://imagescience.org/meijering/software/mtrackj/format/>). It supports\n    2D data and read/writes cluster, point, and channel information. If desired, \n    generates track identifiers that are unique over the clusters.\n    See the project page for more information and examples."}, "webreadr": {"categories": ["WebTechnologies"], "description": "R is used by a vast array of people for a vast array of purposes\n    - including web analytics. This package contains functions for consuming and\n    munging various common forms of request log, including the Common and Combined\n    Web Log formats and various Amazon access logs."}, "XBRL": {"categories": ["Finance", "OfficialStatistics"], "description": "\n  Functions to extract business financial information from\n  an Extensible Business Reporting Language ('XBRL') instance file and the\n  associated collection of files that defines its 'Discoverable' Taxonomy\n  Set ('DTS')."}, "juicr": {"categories": ["MetaAnalysis"], "description": "Provides a GUI interface for automating data extraction from \n  multiple images containing scatter and bar plots, semi-automated tools to tinker \n  with extraction attempts, and a fully-loaded point-and-click manual extractor \n  with image zoom, calibrator, and classifier. Also provides detailed and \n  R-independent extraction reports as fully-embedded .html records."}, "sundialr": {"categories": ["DifferentialEquations"], "description": "Provides a way to call the functions in 'SUNDIALS' C ODE solving library (<https://computing.llnl.gov/projects/sundials>). Currently the serial version of ODE solver, 'CVODE', sensitivity calculator 'CVODES' and differential algebraic solver 'IDA' from the 'SUNDIALS' library are implemented. The package requires ODE to be written as an 'R' or 'Rcpp' function and does not require the 'SUNDIALS' library to be installed on the local machine."}, "airGR": {"categories": ["Hydrology"], "description": "Hydrological modelling tools developed at INRAE-Antony (HYCAR Research Unit, France). The package includes several conceptual rainfall-runoff models (GR4H, GR5H, GR4J, GR5J, GR6J, GR2M, GR1A) that can be applied either on a lumped or semi-distributed way. A snow accumulation and melt model (CemaNeige) and the associated functions for the calibration and evaluation of models are also included. Use help(airGR) for package description and references."}, "rdrobust": {"categories": ["CausalInference", "Econometrics"], "description": "Regression-discontinuity (RD) designs are quasi-experimental research designs popular in social, behavioral and natural sciences. The RD design is usually employed to study the (local) causal effect of a treatment, intervention or policy. This package provides tools for data-driven graphical and analytical statistical inference in RD\tdesigns: rdrobust() to construct local-polynomial point estimators and robust confidence intervals for average treatment effects at the \tcutoff in Sharp, Fuzzy and Kink RD settings, rdbwselect() to perform bandwidth selection for the different procedures implemented, and rdplot() to conduct exploratory data analysis (RD plots)."}, "psychTools": {"categories": ["Psychometrics"], "description": "Support functions,  data sets, and vignettes for the 'psych' package. Contains several of the biggest data sets for the 'psych' package as well as one vignette. A few helper functions for file manipulation are included as well. For more information, see the <https://personality-project.org/r/> web page."}, "extremefit": {"categories": ["Distributions", "ExtremeValue"], "description": "Extreme value theory, nonparametric kernel estimation, tail\n    conditional probabilities, extreme conditional quantile, adaptive estimation,\n    quantile regression, survival probabilities."}, "season": {"categories": ["TimeSeries"], "description": "Routines for the seasonal analysis of health data, including regression models, time-stratified case-crossover, plotting functions and residual checks, see Barnett and Dobson (2010) ISBN 978-3-642-10748-1. Thanks to Yuming Guo for checking the case-crossover code. "}, "Rilostat": {"categories": ["OfficialStatistics"], "description": "Tools to download data from the ilostat database\n    <https://ilostat.ilo.org> together with search and\n    manipulation utilities."}, "astroFns": {"categories": ["ChemPhys"], "description": "Miscellaneous astronomy functions, utilities, and data."}, "RobustAFT": {"categories": ["Robust", "Survival"], "description": "R functions for the computation of the truncated maximum\n\t     likelihood and the robust accelerated failure time regression \n\t     for gaussian and log-Weibull case."}, "wordcloud": {"categories": ["NaturalLanguageProcessing"], "description": "Functionality to create pretty word clouds, visualize differences and similarity between documents, and avoid over-plotting in scatter plots with text."}, "readODS": {"categories": ["ReproducibleResearch"], "description": "Import ODS (OpenDocument Spreadsheet) into R as a data frame. Also support writing data frame into ODS file."}, "edstan": {"categories": ["Psychometrics"], "description": "Provides convenience functions and pre-programmed Stan models\n    related to item response theory. Its purpose is to make fitting\n    common item response theory models using Stan easy."}, "osqp": {"categories": ["Optimization"], "description": "Provides bindings to the 'OSQP' solver. The 'OSQP' solver is a numerical optimization package or solving convex quadratic programs written in 'C' and based on the alternating direction method of multipliers. See <arXiv:1711.08013> for details."}, "movMF": {"categories": ["Cluster", "Distributions", "NaturalLanguageProcessing"], "description": "Fit and simulate mixtures of von Mises-Fisher distributions."}, "flextable": {"categories": ["ReproducibleResearch"], "description": "Create pretty tables for 'HTML', 'PDF', 'Microsoft Word' and 'Microsoft PowerPoint' \n  documents from 'R Markdown'. Functions are provided to let users create tables, modify and format \n  their content. It also extends package 'officer' that does not contain any feature for customized \n  tabular reporting."}, "PBSddesolve": {"categories": ["DifferentialEquations"], "description": "Functions for solving systems of delay differential equations by\n   interfacing with numerical routines written by Simon N. Wood, including\n   contributions from Benjamin J. Cairns. These numerical routines first\n   appeared in Simon Wood's 'solv95' program. This package includes a vignette\n   and a complete user's guide. 'PBSddesolve' originally appeared on CRAN under\n   the name 'ddesolve'. That version is no longer supported. The current name\n   emphasizes a close association with other 'PBS' packages, particularly\n   'PBSmodelling'."}, "fdaPDE": {"categories": ["FunctionalData"], "description": "An implementation of regression models with partial differential regularizations, making use of the Finite Element Method. The models efficiently handle data distributed over irregularly shaped domains and can comply with various conditions at the boundaries of the domain. A priori information about the spatial structure of the phenomenon under study can be incorporated in the model via the differential regularization. See Sangalli, L. M. (2021). Spatial Regression With Partial Differential Equation Regularisation. International Statistical Review, 89(3), 505-531. for an overview. "}, "nflverse": {"categories": ["SportsAnalytics"], "description": "The 'nflverse' is a set of packages dedicated to data of the\n    National Football League. This package is designed to make it easy to\n    install and load multiple 'nflverse' packages in a single step. Learn\n    more about the 'nflverse' at <https://nflverse.nflverse.com/>."}, "mlpack": {"categories": ["MachineLearning"], "description": "A fast, flexible machine learning library, written in C++, that\n             aims to provide fast, extensible implementations of cutting-edge\n             machine learning algorithms.  See also Curtin et al. (2018)\n             <doi:10.21105/joss.00726>."}, "ADPF": {"categories": ["NumericalMathematics"], "description": "This function takes a vector or matrix of data and smooths\n    the data with an improved Savitzky Golay transform. The Savitzky-Golay\n    method for data smoothing and differentiation calculates convolution\n    weights using Gram polynomials that exactly reproduce the results of\n    least-squares polynomial regression. Use of the Savitzky-Golay\n    method requires specification of both filter length and\n    polynomial degree to calculate convolution weights. For maximum\n    smoothing of statistical noise in data, polynomials with\n    low degrees are desirable, while a high polynomial degree\n    is necessary for accurate reproduction of peaks in the data.\n    Extension of the least-squares regression formalism with\n    statistical testing of additional terms of polynomial degree\n    to a heuristically chosen minimum for each data window leads\n    to an adaptive-degree polynomial filter (ADPF). Based on noise\n    reduction for data that consist of pure noise and on signal\n    reproduction for data that is purely signal, ADPF performed\n    nearly as well as the optimally chosen fixed-degree\n    Savitzky-Golay filter and outperformed sub-optimally chosen\n    Savitzky-Golay filters. For synthetic data consisting of noise\n    and signal, ADPF outperformed both optimally chosen and\n    sub-optimally chosen fixed-degree Savitzky-Golay filters. See Barak, P. (1995) <doi:10.1021/ac00113a006> for more information."}, "numbers": {"categories": ["NumericalMathematics"], "description": "\n    Provides number-theoretic functions for factorization, prime \n    numbers, twin primes, primitive roots, modular logarithm and\n    inverses, extended GCD, Farey series and continuous fractions.\n    Includes Legendre and Jacobi symbols, some divisor functions,\n    Euler's Phi function, etc."}, "ThreeGroups": {"categories": ["ClinicalTrials"], "description": "Implements the Maximum Likelihood estimator for baseline, placebo, and treatment groups (three-group) experiments with non-compliance proposed by Gerber, Green, Kaplan, and Kern (2010)."}, "equateMultiple": {"categories": ["Psychometrics"], "description": "Equating of multiple forms using Item Response Theory (IRT) methods (Battauz M. (2017) <doi:10.1007/s11336-016-9517-x> and Haberman S. J. (2009) <doi:10.1002/j.2333-8504.2009.tb02197.x>)."}, "UPMASK": {"categories": ["ChemPhys"], "description": "An implementation of the UPMASK method for performing membership\n    assignment in stellar clusters in R. It is prepared to use photometry and\n    spatial positions, but it can take into account other types of data. The\n    method is able to take into account arbitrary error models, and it is\n    unsupervised, data-driven, physical-model-free and relies on as few\n    assumptions as possible. The approach followed for membership assessment is\n    based on an iterative process, dimensionality reduction, a clustering\n    algorithm and a kernel density estimation."}, "splinetree": {"categories": ["FunctionalData"], "description": "Builds regression trees and random forests for longitudinal or functional data using a spline projection method. Implements and extends the work of Yu and Lambert (1999) <doi:10.1080/10618600.1999.10474847>. This method allows trees and forests to be built while considering either level and shape or only shape of response trajectories. "}, "fable": {"categories": ["TimeSeries"], "description": "Provides a collection of commonly used univariate and multivariate\n    time series forecasting models including automatically selected exponential \n    smoothing (ETS) and autoregressive integrated moving average (ARIMA) models.\n    These models work within the 'fable' framework provided by the 'fabletools'\n    package, which provides the tools to evaluate, visualise, and combine models \n    in a workflow consistent with the tidyverse."}, "kwb.hantush": {"categories": ["Hydrology"], "description": "Calculation groundwater mounding beneath an\n    infiltration basin based on the Hantush (1967) equation\n    (<doi:10.1029/WR003i001p00227>). The correct implementation is shown\n    with a verification example based on a USGS report (page 25,\n    <https://pubs.usgs.gov/sir/2010/5102/support/sir2010-5102.pdf#page=35>)."}, "musica": {"categories": ["Hydrology"], "description": "Provides functions allowing for (1) easy aggregation of multivariate time series into custom time scales, (2) comparison of statistical summaries between different data sets at multiple time scales (e.g. observed and bias-corrected data), (3) comparison of relations between variables and/or different data sets at multiple time scales (e.g. correlation of precipitation and temperature in control and scenario simulation) and (4) transformation of time series at custom time scales."}, "mpath": {"categories": ["MachineLearning"], "description": "Algorithms compute robust estimators for loss functions in the concave convex (CC) family by the iteratively reweighted convex optimization (IRCO), an extension of the iteratively reweighted least squares (IRLS). The IRCO reduces the weight of the observation that leads to a large loss; it also provides weights to help identify outliers. Applications include robust (penalized) generalized linear models and robust support vector machines. The package also contains penalized Poisson, negative binomial, zero-inflated Poisson, zero-inflated negative binomial regression models and robust models with non-convex loss functions. Wang et al. (2014) <doi:10.1002/sim.6314>,\n      Wang et al. (2015) <doi:10.1002/bimj.201400143>,\n      Wang et al. (2016) <doi:10.1177/0962280214530608>,\n      Wang (2021) <doi:10.1007/s11749-021-00770-2>,\n      Wang (2020) <arXiv:2010.02848>."}, "gemtc": {"categories": ["MetaAnalysis"], "description": "Network meta-analyses (mixed treatment comparisons) in the Bayesian\n    framework using JAGS. Includes methods to assess heterogeneity and\n    inconsistency, and a number of standard visualizations.\n    van Valkenhoef et al. (2012) <doi:10.1002/jrsm.1054>;\n    van Valkenhoef et al. (2015) <doi:10.1002/jrsm.1167>."}, "Crossover": {"categories": ["ExperimentalDesign"], "description": "Generate and analyse crossover designs from combinatorial or search algorithms as well as from literature and a GUI to\n    access them."}, "quantmod": {"categories": ["Finance"], "description": "Specify, build, trade, and analyse quantitative financial trading strategies."}, "CARBayesST": {"categories": ["Bayesian", "SpatioTemporal"], "description": "Implements a class of univariate and multivariate spatio-temporal generalised linear mixed models for areal unit data, with inference in a Bayesian setting using Markov chain Monte Carlo (MCMC) simulation. The response variable can be binomial, Gaussian, or Poisson, but for some models only the binomial and Poisson data likelihoods are available. The spatio-temporal autocorrelation is modelled by  random effects, which are assigned conditional autoregressive (CAR) style prior distributions. A number of different random effects structures are available, including models similar to Rushworth et al. (2014) <doi:10.1016/j.sste.2014.05.001>. Full details are given in the vignette accompanying this package. The creation and development of this package was supported by the Engineering and Physical Sciences Research Council  (EPSRC) grants EP/J017442/1 and EP/T004878/1 and the Medical Research Council (MRC) grant MR/L022184/1."}, "mclcar": {"categories": ["TimeSeries"], "description": "The likelihood of direct CAR models and Binomial and Poisson GLM with latent CAR variables are approximated by the Monte Carlo likelihood. The Maximum Monte Carlo likelihood estimator is found either by an iterative procedure of directly maximising the Monte Carlo approximation or by a response surface design method.Reference for the method can be found in the DPhil thesis in Z. Sha (2016). For application a good reference is R.Bivand et.al (2017) <doi:10.1016/j.spasta.2017.01.002>."}, "WebAnalytics": {"categories": ["WebTechnologies"], "description": "Provides Apache and IIS log analytics for transaction performance, client populations and workload definitions."}, "adimpro": {"categories": ["MedicalImaging"], "description": "Implements tools for manipulation of digital\n             \t\timages and the Propagation Separation approach\n             \t\tby Polzehl and Spokoiny (2006) <doi:10.1007/s00440-005-0464-1>\n                        for smoothing digital images, see Polzehl and Tabelow (2007)\n                        <doi:10.18637/jss.v019.i01>."}, "robotstxt": {"categories": ["WebTechnologies"], "description": "Provides functions to download and parse 'robots.txt' files.\n        Ultimately the package makes it easy to check if bots\n        (spiders, crawler, scrapers, ...) are allowed to access specific\n        resources on a domain."}, "XR": {"categories": ["NumericalMathematics"], "description": "Support for interfaces from R to other languages,\n    built around a class for evaluators and a combination of functions, classes and\n    methods for communication. Will be used through a specific language interface\n    package. Described in the book \"Extending R\"."}, "dlm": {"categories": ["Bayesian", "Finance", "TimeSeries"], "description": "Provides routines for Maximum likelihood,\n    Kalman filtering and smoothing, and Bayesian\n    analysis of Normal linear State Space models, also known as \n    Dynamic Linear Models. "}, "trending": {"categories": ["Epidemiology"], "description": "Provides a coherent interface to multiple modelling tools for\n  fitting trends along with a standardised approach for generating confidence\n  and prediction intervals."}, "rsae": {"categories": ["OfficialStatistics"], "description": "Empirical best linear unbiased prediction (EBLUP) and\n    robust prediction of the area-level means under the basic unit-level\n    model. The model can be fitted by maximum likelihood or a (robust)\n    M-estimator. Mean square prediction error is computed by a parametric\n    bootstrap."}, "DescTools": {"categories": ["MissingData"], "description": "A collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author's intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. The package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The 'BigCamelCase' style was consequently applied to functions borrowed from contributed R packages as well."}, "MSSQL": {"categories": ["Databases"], "description": "Tools that extend the functionality of the 'RODBC' package to work\n  with Microsoft SQL Server databases. Makes it easier to browse the database\n  and examine individual tables and views."}, "xpose.nlmixr": {"categories": ["Pharmacokinetics"], "description": "Extension to 'xpose' to support 'nlmixr'. Provides functions to import 'nlmixr' fit data into an 'xpose' data object, allowing the use of 'xpose' for 'nlmixr' model diagnostics.  "}, "tfarima": {"categories": ["TimeSeries"], "description": "Building customized transfer function and ARIMA models with multiple operators and parameter restrictions. Functions for model identification, model estimation (exact or conditional maximum likelihood), model diagnostic checking, automatic outlier detection, calendar effects, forecasting and seasonal adjustment. See Bell and Hillmer (1983) <doi:10.1080/01621459.1983.10478005>, Box, Jenkins, Reinsel and Ljung <ISBN:978-1-118-67502-1>, Box, Pierce and Newbold (1987) <doi:10.1080/01621459.1987.10478430>, Box and Tiao (1975) <doi:10.1080/01621459.1975.10480264>, Chen and Liu (1993) <doi:10.1080/01621459.1993.10594321>."}, "regsem": {"categories": ["Psychometrics"], "description": "Uses both ridge and lasso penalties (and extensions) to penalize\n    specific parameters in structural equation models. The package offers additional\n    cost functions, cross validation, and other extensions beyond traditional structural\n    equation models. Also contains a function to perform exploratory mediation (XMed). "}, "parfm": {"categories": ["Survival"], "description": "Fits Parametric Frailty Models by maximum marginal likelihood.\n             Possible baseline hazards:\n                 exponential, Weibull, inverse Weibull (Fr\u00e9chet),\n                 Gompertz, lognormal, log-skew-normal, and loglogistic.\n             Possible Frailty distributions:\n                gamma, positive stable, inverse Gaussian and lognormal."}, "MPTinR": {"categories": ["Psychometrics"], "description": "Provides a user-friendly way for the analysis of multinomial processing tree (MPT) models (e.g.,  Riefer, D. M., and Batchelder, W. H. [1988]. Multinomial modeling and the measurement of cognitive processes. Psychological Review, 95, 318-339) for single and multiple datasets. The main functions perform model fitting and model selection. Model selection can be done using AIC, BIC, or the Fisher Information Approximation (FIA) a measure based on the Minimum Description Length (MDL) framework. The model and restrictions can be specified in external files or within an R script in an intuitive syntax or using the context-free language for MPTs. The 'classical' .EQN file format for model files is also supported. Besides MPTs, this package can fit a wide variety of other cognitive models such as SDT models (see fit.model). It also supports multicore fitting and FIA calculation (using the snowfall package), can generate or bootstrap data for simulations, and plot predicted versus observed data."}, "plsRbeta": {"categories": ["MissingData"], "description": "Provides Partial least squares Regression for (weighted) beta regression models (Bertrand 2013,  <http://journal-sfds.fr/article/view/215>) and k-fold cross-validation of such models using various criteria. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available."}, "pampe": {"categories": ["Econometrics"], "description": "Implements the Panel Data Approach Method for program evaluation as developed in Hsiao, Ching and Ki Wan (2012). pampe estimates the effect of an intervention by comparing the evolution of the outcome for a unit affected by an intervention or treatment to the evolution of the unit had it not been affected by the intervention."}, "iarm": {"categories": ["Psychometrics"], "description": "Tools to assess model fit and identify misfitting items for Rasch models (RM) and partial credit models (PCM). Included are item fit statistics, item characteristic curves, item-restscore association, conditional likelihood ratio tests, assessment of measurement error, estimates of the reliability and test targeting as described in Christensen et al. (Eds.) (2013, ISBN:978-1-84821-222-0)."}, "evclust": {"categories": ["Cluster"], "description": "Various clustering algorithms that produce a credal partition,\n    i.e., a set of Dempster-Shafer mass functions representing the membership of objects\n    to clusters. The mass functions quantify the cluster-membership uncertainty of the     \n    objects. The algorithms are: Evidential c-Means, Relational Evidential c-Means, \n    Constrained Evidential c-Means, Evidential Clustering, Constrained Evidential \n    Clustering, Evidential K-nearest-neighbor-based Clustering, Bootstrap Model-Based\n    Evidential Clustering, Belief Peak Evidential Clustering, Neural-Network-based\n    Evidential Clustering. "}, "rstanarm": {"categories": ["Survival"], "description": "Estimates previously compiled regression models using the 'rstan'\n    package, which provides the R interface to the Stan C++ library for Bayesian\n    estimation. Users specify models via the customary R syntax with a formula and\n    data.frame plus some additional arguments for priors."}, "backbone": {"categories": ["GraphicalModels"], "description": "An implementation of methods for extracting an unweighted unipartite\n   graph (i.e. a backbone) from an unweighted unipartite graph, a weighted unipartite\n   graph, the projection of an unweighted bipartite graph , or the projection\n   of a weighted bipartite graph (Neal, 2022 <doi:10.1371/journal.pone.0269137>)."}, "globalOptTests": {"categories": ["Optimization"], "description": "This package makes available 50 objective functions for benchmarking the performance of global optimization algorithms "}, "graphTweets": {"categories": ["WebTechnologies"], "description": "Allows building an edge table from data frame of tweets, \n  also provides function to build nodes and another create a temporal graph."}, "cbsodataR": {"categories": ["OfficialStatistics"], "description": "The data and meta data from Statistics\n    Netherlands (<https://www.cbs.nl>) can be browsed and downloaded. The client uses\n    the open data API of Statistics Netherlands."}, "cubature": {"categories": ["NumericalMathematics"], "description": "R wrappers around the cubature C library of Steven\n    G. Johnson for adaptive multivariate integration over hypercubes\n    and the Cuba C library of Thomas Hahn for deterministic and\n    Monte Carlo integration. Scalar and vector interfaces for \n    cubature and Cuba routines are provided; the vector interfaces\n    are highly recommended as demonstrated in the package\n    vignette."}, "bigrquery": {"categories": ["Databases", "WebTechnologies"], "description": "Easily talk to Google's 'BigQuery' database from R."}, "RH2": {"categories": ["Databases"], "description": "DBI/RJDBC interface to h2 database. h2 version 1.3.175 is included."}, "gtrendsR": {"categories": ["WebTechnologies"], "description": "An interface for retrieving and displaying the information\n        returned online by Google Trends is provided. Trends (number of\n        hits) over the time as well as geographic representation of the\n        results can be displayed."}, "fromo": {"categories": ["Distributions"], "description": "Fast, numerically robust computation of weighted moments via 'Rcpp'. \n   Supports computation on vectors and matrices, and Monoidal append of moments. \n   Moments and cumulants over running fixed length windows can be computed, \n   as well as over time-based windows.\n   Moment computations are via a generalization of Welford's method, as described\n   by Bennett et. (2009) <doi:10.1109/CLUSTR.2009.5289161>."}, "word2vec": {"categories": ["NaturalLanguageProcessing"], "description": "Learn vector representations of words by continuous bag of words and skip-gram implementations of the 'word2vec' algorithm. \n    The techniques are detailed in the paper \"Distributed Representations of Words and Phrases and their Compositionality\" by Mikolov et al. (2013), available at <arXiv:1310.4546>."}, "RBesT": {"categories": ["MetaAnalysis"], "description": "Tool-set to support Bayesian evidence synthesis.  This\n    includes meta-analysis, (robust) prior derivation from historical\n    data, operating characteristics and analysis (1 and 2 sample\n    cases). Please refer to Weber et al. (2021) <doi:10.18637/jss.v100.i19>\n    for details on applying this package while Neuenschwander et al. (2010)\n    <doi:10.1177/1740774509356002> and Schmidli et al. (2014)\n    <doi:10.1111/biom.12242> explain details on the methodology."}, "bssm": {"categories": ["TimeSeries"], "description": "Efficient methods for Bayesian inference of state space models \n    via Markov chain Monte Carlo (MCMC) based on parallel \n    importance sampling type weighted estimators \n    (Vihola, Helske, and Franks, 2020, <doi:10.1111/sjos.12492>), \n    particle MCMC, and its delayed acceptance version. \n    Gaussian, Poisson, binomial, negative binomial, and Gamma\n    observation densities and basic stochastic volatility models \n    with linear-Gaussian state dynamics, as well as general non-linear Gaussian \n    models and discretised diffusion models are supported. \n    See Helske and Vihola (2021, <doi:10.32614/RJ-2021-103>) for details."}, "RcppNumerical": {"categories": ["Optimization"], "description": "A collection of open source libraries for numerical computing\n    (numerical integration, optimization, etc.) and their integration with\n    'Rcpp'."}, "stabledist": {"categories": ["Distributions"], "description": "Density, Probability and Quantile functions, and random number\n  generation for (skew) stable distributions, using the parametrizations of\n  Nolan."}, "irace": {"categories": ["Optimization"], "description": "Iterated race is an extension of the Iterated F-race method for\n             the automatic configuration of optimization algorithms, that is,\n             (offline) tuning their parameters by finding the most appropriate\n             settings given a set of instances of an optimization problem.\n             M. L\u00f3pez-Ib\u00e1\u00f1ez, J. Dubois-Lacoste, L. P\u00e9rez C\u00e1ceres, T. St\u00fctzle,\n             and M. Birattari (2016) <doi:10.1016/j.orp.2016.09.002>."}, "tsibbletalk": {"categories": ["TimeSeries"], "description": "A shared tsibble data easily communicates between\n    htmlwidgets on both client and server sides, powered by 'crosstalk'. A\n    shiny module is provided to visually explore periodic/aperiodic\n    temporal patterns."}, "wellknown": {"categories": ["Spatial"], "description": "Convert 'WKT' to 'GeoJSON' and 'GeoJSON' to 'WKT'. Functions\n    included for converting between 'GeoJSON' to 'WKT', creating both\n    'GeoJSON' features, and non-features, creating 'WKT' from R objects\n    (e.g., lists, data.frames, vectors), and linting 'WKT'."}, "fitzRoy": {"categories": ["SportsAnalytics"], "description": "An easy package for scraping and processing Australia Rules Football (AFL)\n    data. 'fitzRoy' provides a range of functions for accessing publicly available data \n    from 'AFL Tables' <https://afltables.com/afl/afl_index.html>, 'Footy Wire' <https://www.footywire.com> and\n    'The Squiggle' <https://squiggle.com.au>. Further functions allow for easy processing, \n    cleaning and transformation of this data into formats that can be used for analysis. "}, "covid19dbcand": {"categories": ["Epidemiology"], "description": "Provides different datasets parsed from 'Drugbank' \n    <https://www.drugbank.ca/covid-19> database using 'dbparser' package. \n    It is a smaller version from 'dbdataset' package. It contains only information\n    about COVID-19 possible treatment."}, "gRain": {"categories": ["Bayesian", "GraphicalModels"], "description": "Probability propagation in graphical independence networks, also\n    known as Bayesian networks or probabilistic expert systems. Documentation\n    of the package is provided in vignettes included in the package and in\n    the paper by H\u00f8jsgaard (2012, <doi:10.18637/jss.v046.i10>).\n    See 'citation(\"gRain\")' for details. "}, "icRSF": {"categories": ["Survival"], "description": "Implements a modification to the Random Survival Forests algorithm for obtaining variable importance in high dimensional datasets. The proposed algorithm is appropriate for settings in which a silent event is observed through sequentially administered, error-prone self-reports or laboratory based diagnostic tests.  The modified algorithm incorporates a formal likelihood framework that accommodates sequentially administered, error-prone self-reports or laboratory based diagnostic tests. The original Random Survival Forests algorithm is modified by the introduction of a new splitting criterion based on a likelihood ratio test statistic."}, "abbyyR": {"categories": ["WebTechnologies"], "description": "Get text from images of text using Abbyy Cloud Optical Character\n    Recognition (OCR) API. Easily OCR images, barcodes, forms, documents with\n    machine readable zones, e.g. passports. Get the results in a variety of formats\n    including plain text and XML. To learn more about the Abbyy OCR API, see \n    <http://ocrsdk.com/>."}, "mmand": {"categories": ["MedicalImaging"], "description": "Provides tools for performing mathematical morphology operations,\n    such as erosion and dilation, on data of arbitrary dimensionality. Can also\n    be used for finding connected components, resampling, filtering, smoothing\n    and other image processing-style operations."}, "RepoGenerator": {"categories": ["ReproducibleResearch"], "description": "Generates a project and repo for easy initialization of a GitHub repo for R workshops. The repo includes a README with instructions to ensure that all users have the needed packages, an 'RStudio' project with the right directories and the proper data. The repo can then be used for hosting code taught during the workshop."}, "EMbC": {"categories": ["SpatioTemporal", "Tracking"], "description": "Unsupervised, multivariate, binary clustering for meaningful annotation of data, taking into account the uncertainty in the data. A specific constructor for trajectory analysis in movement ecology yields behavioural annotation of trajectories based on estimated local measures of velocity and turning angle, eventually with solar position covariate as a daytime indicator, (\"Expectation-Maximization Binary Clustering for Behavioural Annotation\")."}, "postlightmercury": {"categories": ["WebTechnologies"], "description": "This is a wrapper for the Mercury Parser API. The Mercury Parser is \n    a single API endpoint that takes a URL and gives you back the content reliably \n    and easily. \n    With just one API request, Mercury takes any web article and returns \n    only the relevant content \u2014 headline, author, body text, relevant images and \n    more \u2014 free from any clutter. It\u2019s reliable, easy-to-use and free.\n    See the webpage here: <https://mercury.postlight.com/>."}, "ggamma": {"categories": ["Distributions"], "description": "Density, distribution function, quantile function and random generation for the Generalized Gamma proposed in Stacy, E. W. (1962) <doi:10.1214/aoms/1177704481>."}, "bayescopulareg": {"categories": ["Bayesian"], "description": "Tools for Bayesian copula generalized linear models (GLMs). \n             The sampling scheme is based on Pitt, Chan, and Kohn (2006) <doi:10.1093/biomet/93.3.537>. \n             Regression parameters (including coefficients and dispersion parameters) are\n             estimated via the adaptive random walk Metropolis approach developed by\n             Haario, Saksman, and Tamminen (1999) <doi:10.1007/s001800050022>.\n             The prior for the correlation matrix is based on Hoff (2007) <doi:10.1214/07-AOAS107>."}, "nipals": {"categories": ["MissingData"], "description": "Principal Components Analysis of a matrix using Non-linear Iterative Partial Least Squares or weighted Expectation Maximization PCA with Gram-Schmidt orthogonalization of the scores and loadings. Optimized for speed. See Andrecut (2009) <doi:10.1089/cmb.2008.0221>."}, "sem": {"categories": ["Econometrics", "Psychometrics"], "description": "Functions for fitting general linear structural\n    equation models (with observed and latent variables) using the RAM approach, \n    and for fitting structural equations in observed-variable models by two-stage least squares."}, "CORElearn": {"categories": ["MachineLearning"], "description": "A suite of machine learning algorithms written in C++ with the R \n interface contains several learning techniques for classification and regression.\n Predictive models include e.g., classification and regression trees with\n optional constructive induction and models in the leaves, random forests, kNN, \n naive Bayes, and locally weighted regression. All predictions obtained with these\n models can be explained and visualized with the 'ExplainPrediction' package.  \n This package is especially strong in feature evaluation where it contains several variants of\n Relief algorithm and many impurity based attribute evaluation functions, e.g., Gini, \n information gain, MDL, and DKM. These methods can be used for feature selection \n or discretization of numeric attributes.\n The OrdEval algorithm and its visualization is used for evaluation\n of data sets with ordinal features and class, enabling analysis according to the \n Kano model of customer satisfaction. \n Several algorithms support parallel multithreaded execution via OpenMP.  \n The top-level documentation is reachable through ?CORElearn."}, "makeProject": {"categories": ["ReproducibleResearch"], "description": "This package creates an empty framework of files and\n        directories for the \"Load, Clean, Func, Do\" structure described\n        by Josh Reich."}, "vsd": {"categories": ["Survival"], "description": "Provides a shim command for survival analysis graphic generation."}, "csn": {"categories": ["Distributions"], "description": "Provides functions for computing the density\n    and the log-likelihood function of closed-skew normal variates,\n    and for generating random vectors sampled from this distribution.\n    See Gonzalez-Farias, G., Dominguez-Molina, J., and Gupta, A. (2004).\n    The closed skew normal distribution, \n    Skew-elliptical distributions and their applications: a journey beyond normality,\n    Chapman and Hall/CRC, Boca Raton, FL, pp. 25-42."}, "mlogitBMA": {"categories": ["Bayesian"], "description": "Provides a modified function bic.glm of the BMA package that can be applied to multinomial logit (MNL) data. The data is converted to binary logit using the Begg & Gray approximation. The package also contains functions for maximum likelihood estimation of MNL. "}, "SmoothHazard": {"categories": ["Survival"], "description": "Estimation of two-state (survival) models and irreversible illness-\n    death models with possibly interval-censored,left-truncated and right-censored\n    data. Proportional intensities regression models can be specified to allow for\n    covariates effects separately for each transition. We use either a parametric\n    approach with Weibull baseline intensities or a semi-parametric approach with\n    M-splines approximation of baseline intensities in order to obtain smooth\n    estimates of the hazard functions. Parameter estimates are obtained by maximum\n    likelihood in the parametric approach and by penalized maximum likelihood in the\n    semi-parametric approach."}, "BalancedSampling": {"categories": ["OfficialStatistics"], "description": "Select balanced and spatially balanced probability samples in multi-dimensional spaces \n    with any prescribed inclusion probabilities. It contains fast (C++ via Rcpp) implementations of \n    the included sampling methods. The local pivotal method by Grafstr\u00f6m, Lundstr\u00f6m and Schelin (2012)\n    <doi:10.1111/j.1541-0420.2011.01699.x> and spatially correlated Poisson sampling by Grafstr\u00f6m (2012) \n    <doi:10.1016/j.jspi.2011.07.003> are included. Also the cube method (for balanced sampling) and \n    the local cube method (for doubly balanced sampling) are included, see Grafstr\u00f6m and Till\u00e9 (2013) \n    <doi:10.1002/env.2194>. "}, "ParetoPosStable": {"categories": ["Distributions"], "description": "Statistical functions to describe a Pareto Positive Stable (PPS) \n    distribution and fit it to real data. Graphical and statistical tools to \n    validate the fits are included."}, "sp23design": {"categories": ["ExperimentalDesign"], "description": "Provides methods for generating, exploring and executing seamless Phase II-III designs of Lai, Lavori and Shih using generalized likelihood ratio statistics. Includes pdf and source files that describe the entire R implementation with the relevant mathematical details."}, "lidR": {"categories": ["Spatial"], "description": "Airborne LiDAR (Light Detection and Ranging) interface for data\n    manipulation and visualization. Read/write 'las' and 'laz' files, computation\n    of metrics in area based approach, point filtering, artificial point reduction,\n    classification from geographic data, normalization, individual tree segmentation\n    and other manipulations."}, "symmoments": {"categories": ["Distributions"], "description": "Symbolic central and non-central moments of the multivariate normal distribution. Computes a standard representation, LateX code, and values at specified mean and covariance matrices."}, "collapse": {"categories": ["Econometrics", "OfficialStatistics", "TimeSeries"], "description": "A C/C++ based package for advanced data transformation and \n    statistical computing in R that is extremely fast, class-agnostic,\n    and programmer friendly through a flexible and parsimonious syntax.\n    It is well integrated with base R, 'dplyr' / (grouped) 'tibble', \n    'data.table', 'sf', 'plm' (panel-series and data frames), and \n    non-destructively handles other matrix or data frame based classes (like \n    'ts', 'xts' / 'zoo', 'tsibble', ...)\n    \u2014 Key Features: \u2014\n    (1) Advanced statistical programming: A full set of fast statistical functions \n        supporting grouped and weighted computations on vectors, matrices and \n        data frames. Fast and programmable grouping, ordering, unique values/rows, \n        factor generation and interactions. Fast and flexible functions for data \n        manipulation, data object conversions, and memory efficient R programming.\n    (2) Advanced aggregation: Fast and easy multi-data-type, multi-function, weighted \n        and parallelized data aggregation.\n    (3) Advanced transformations: Fast row/column arithmetic, (grouped) replacing \n        and sweeping out of statistics (by reference), (grouped, weighted) scaling/standardizing, \n        (higher-dimensional) between (averaging) and (quasi-)within (demeaning) transformations, \n        linear prediction, model fitting and testing exclusion restrictions.\n    (4) Advanced time-computations: Fast and flexible indexed time series and panel data classes. \n        Fast (sequences of) lags/leads, and  (lagged/leaded, iterated, quasi-, log-) \n        differences and (compounded) growth rates on (irregular) time series and panels. \n        Multivariate auto-, partial- and cross-correlation functions for panel data. \n        Panel data to (ts-)array conversions.\n    (5) List processing: Recursive list search, splitting, \n        extraction/subsetting, apply, and generalized row-binding / unlisting to data frame.\n    (6) Advanced data exploration: Fast (grouped, weighted, panel-decomposed) \n        summary statistics and descriptive tools."}, "dse": {"categories": ["Environmetrics", "Finance", "TimeSeries"], "description": "Tools for multivariate, linear, time-invariant,\n\ttime series models. This includes ARMA and state-space representations,\n\tand methods for converting between them. It also includes simulation\n\tmethods and several estimation functions. The package has functions \n\tfor looking at model roots, stability, and forecasts at different \n\thorizons. The ARMA model representation is general, so that VAR, VARX, \n\tARIMA, ARMAX, ARIMAX can all be considered to be special cases. Kalman\n\tfilter and smoother estimates can be obtained from the state space\n\tmodel, and state-space model reduction techniques are implemented. \n\tAn introduction and User's Guide is available in a vignette."}, "TraMineR": {"categories": ["Survival"], "description": "Toolbox for the manipulation, description and rendering of sequences, and more generally the mining of sequence data in the field of social sciences. Although the toolbox is primarily intended for analyzing state or event sequences that describe life courses such as family formation histories or professional careers, its features also apply to many other kinds of categorical sequence data. It accepts many different sequence representations as input and provides tools for converting sequences from one format to another. It offers several functions for describing and rendering sequences, for computing distances between sequences with different metrics (among which optimal matching), original dissimilarity-based analysis tools, and simple functions for extracting the most frequent subsequences and identifying the most discriminating ones among them. A user's guide can be found on the TraMineR web page."}, "conf.design": {"categories": ["ClinicalTrials", "ExperimentalDesign"], "description": "This small library contains a series of simple tools for\n        constructing and manipulating confounded and fractional\n        factorial designs."}, "smoothHR": {"categories": ["Survival"], "description": "Provides flexible hazard ratio curves allowing non-linear\n  relationships between continuous predictors and survival.\n  To better understand the effects that each continuous covariate\n  has on the outcome, results are ex pressed in terms of hazard\n  ratio curves, taking a specific covariate value as reference.\n  Confidence bands for these curves are also derived."}, "mlr3": {"categories": ["MachineLearning"], "description": "Efficient, object-oriented programming on the\n    building blocks of machine learning. Provides 'R6' objects for tasks,\n    learners, resamplings, and measures. The package is geared towards\n    scalability and larger datasets by supporting parallelization and\n    out-of-memory data-backends like databases. While 'mlr3' focuses on\n    the core computational operations, add-on packages provide additional\n    functionality."}, "soiltexture": {"categories": ["Environmetrics"], "description": "\"The Soil Texture Wizard\" is a set of R functions designed to produce texture triangles (also called texture plots, texture diagrams, texture ternary plots), classify and transform soil textures data. These functions virtually allows to plot any soil texture triangle (classification) into any triangle geometry (isosceles, right-angled triangles, etc.). This set of function is expected to be useful to people using soil textures data from different soil texture classification or different particle size systems. Many (> 15) texture triangles from all around the world are predefined in the package. A simple text based graphical user interface is provided: soiltexture_gui()."}, "diffpriv": {"categories": ["OfficialStatistics"], "description": "An implementation of major general-purpose mechanisms for privatizing\n    statistics, models, and machine learners, within the framework of differential\n    privacy of Dwork et al. (2006) <doi:10.1007/11681878_14>. Example mechanisms\n    include the Laplace mechanism for releasing numeric aggregates, and the \n    exponential mechanism for releasing set elements. A sensitivity sampler \n    (Rubinstein & Alda, 2017) <arXiv:1706.02562> permits sampling target \n    non-private function sensitivity; combined with the generic mechanisms, it \n    permits turn-key privatization of arbitrary programs."}, "lsei": {"categories": ["Optimization"], "description": "It contains functions that solve least squares linear\n\t     regression problems under linear equality/inequality\n\t     constraints. Functions for solving quadratic programming\n\t     problems are also available, which transform such problems\n\t     into least squares ones first. It is developed based on the\n\t     'Fortran' program of Lawson and Hanson (1974, 1995), which is\n\t     public domain and available at\n\t     <http://www.netlib.org/lawson-hanson/>."}, "wrangle": {"categories": ["MissingData"], "description": "Supports systematic scrutiny, modification, and integration of\n    data. The function status() counts rows that have missing values in \n    grouping columns (returned by na() ), have non-unique combinations of \n    grouping columns (returned by dup() ), and that are not locally sorted\n    (returned by unsorted() ). Functions enumerate() and itemize() give \n    sorted unique combinations of columns, with or without occurrence counts,\n    respectively. Function ignore() drops columns in x that are present in y,\n    and informative() drops columns in x that are entirely NA; constant() returns\n    values that are constant, given a key.  Data that have defined unique \n    combinations of grouping values behave more predictably during merge operations."}, "gtheory": {"categories": ["Psychometrics"], "description": "Estimates variance components, generalizability coefficients,\n    universe scores, and standard errors when observed scores contain variation from\n    one or more measurement facets (e.g., items and raters)."}, "nppbib": {"categories": ["ClinicalTrials"], "description": "Implements a nonparametric statistical test for rank or score data\n from partially-balanced incomplete block-design experiments."}, "thregI": {"categories": ["Survival"], "description": "Fit a threshold regression model for Interval Censored Data based on the first-hitting-time of a boundary by the sample path of a Wiener diffusion process. The threshold regression methodology is well suited to applications involving survival and time-to-event data."}, "BayesX": {"categories": ["Bayesian"], "description": "Functions for exploring and visualising estimation results\n  obtained with BayesX, a free software for estimating structured additive\n  regression models (<http://www.BayesX.org>). In addition,\n  functions that allow to read, write and manipulate map objects that are required in spatial\n  analyses performed with BayesX."}, "bigstatsr": {"categories": ["HighPerformanceComputing"], "description": "Easy-to-use, efficient, flexible and scalable statistical tools.\n  Package bigstatsr provides and uses Filebacked Big Matrices via memory-mapping.\n  It provides for instance matrix operations, Principal Component Analysis,\n  sparse linear supervised models, utility functions and more\n  <doi:10.1093/bioinformatics/bty185>."}, "Gmedian": {"categories": ["Robust"], "description": "Fast algorithms for robust estimation with large samples of multivariate observations. Estimation of the geometric median, robust k-Gmedian clustering, and robust PCA based on the Gmedian covariation matrix."}, "hbsae": {"categories": ["Bayesian", "OfficialStatistics"], "description": "Functions to compute small area estimates based on a basic area or\n    unit-level model. The model is fit using restricted maximum likelihood, or\n    in a hierarchical Bayesian way. In the latter case numerical integration is\n    used to average over the posterior density for the between-area variance.\n    The output includes the model fit, small area estimates and corresponding\n    mean squared errors, as well as some model selection measures. Additional functions\n    provide means to compute aggregate estimates and mean squared errors, to minimally\n    adjust the small area estimates to benchmarks at a higher aggregation\n    level, and to graphically compare different sets of small area estimates."}, "lulcc": {"categories": ["Hydrology"], "description": "Classes and methods for spatially explicit land use change\n    modelling in R."}, "tsfknn": {"categories": ["TimeSeries"], "description": "Allows to forecast time series using nearest neighbors regression\n    Francisco Martinez, Maria P. Frias, Maria D. Perez-Godoy and Antonio J.\n    Rivera (2017) <doi:10.1007/s10462-017-9593-z>. When the forecasting horizon\n    is higher than 1, two multi-step ahead forecasting strategies can be used.\n    The model built is autoregressive, that is, it is only based on the \n    observations of the time series. The nearest neighbors used in a prediction\n    can be consulted and plotted."}, "LearnBayes": {"categories": ["Bayesian", "Distributions", "Survival", "TeachingStatistics"], "description": "A collection of functions helpful in learning the basic tenets of Bayesian statistical inference.  It contains functions for summarizing basic one and two parameter posterior distributions and predictive distributions.  It contains MCMC algorithms for summarizing posterior distributions defined by the user.  It also contains functions for regression models, hierarchical models, Bayesian tests, and illustrations of Gibbs sampling."}, "sdcMicro": {"categories": ["OfficialStatistics"], "description": "Data from statistical agencies and other institutions are mostly\n    confidential. This package (see also Templ, Kowarik and Meindl (2017) <doi:10.18637/jss.v067.i04>) can be used for the generation of anonymized\n    (micro)data, i.e. for the creation of public- and scientific-use files.\n    The theoretical basis for the methods implemented can be found in Templ (2017) <doi:10.1007/978-3-319-50272-4>.\n    Various risk estimation and anonymisation methods are included. Note that the package\n    includes a graphical user interface (Meindl and Templ, 2019 <doi:10.3390/a12090191>) that allows to use various methods of this\n    package."}, "Risk": {"categories": ["Finance"], "description": "Computes 26 financial risk measures for any continuous distribution.  The 26 financial risk measures  include value at risk, expected shortfall due to Artzner et al. (1999) <doi:10.1007/s10957-011-9968-2>, tail conditional median due to Kou et al. (2013) <doi:10.1287/moor.1120.0577>, expectiles due to Newey and Powell (1987) <doi:10.2307/1911031>, beyond value at risk due to Longin (2001) <doi:10.3905/jod.2001.319161>, expected proportional shortfall due to Belzunce et al. (2012) <doi:10.1016/j.insmatheco.2012.05.003>, elementary risk measure due to Ahmadi-Javid (2012) <doi:10.1007/s10957-011-9968-2>, omega due to Shadwick and Keating (2002), sortino ratio due to Rollinger and Hoffman (2013), kappa  due to Kaplan and Knowles  (2004), Wang (1998)'s <doi:10.1080/10920277.1998.10595708> risk measures, Stone (1973)'s <doi:10.2307/2978638> risk measures, Luce (1980)'s <doi:10.1007/BF00135033> risk measures, Sarin (1987)'s <doi:10.1007/BF00126387> risk measures, Bronshtein and Kurelenkova (2009)'s risk measures."}, "PowerUpR": {"categories": ["ClinicalTrials"], "description": "\n Includes tools to calculate statistical power, minimum detectable effect size (MDES), MDES difference (MDESD), and minimum required sample size for various multilevel randomized experiments (MRE) with continuous outcomes.\n Accomodates 14 types of MRE designs to detect main treatment effect, seven types of MRE designs to detect moderated treatment effect (2-1-1, 2-1-2, 2-2-1, 2-2-2, 3-3-1, 3-3-2, and 3-3-3 designs; <total.lev> - <trt.lev> - <mod.lev>),\n five types of MRE designs to detect mediated treatment effects (2-1-1, 2-2-1, 3-1-1, 3-2-1, and 3-3-1 designs; <trt.lev> - <med.lev> - <out.lev>), four types of partially nested (PN) design to detect main treatment effect,\n and three types of PN designs to detect mediated treatment effects (2/1, 3/1, 3/2; <trt.arm.lev> / <ctrl.arm.lev>). \n See 'PowerUp!' Excel series at <https://www.causalevaluation.org/>. "}, "odbc": {"categories": ["Databases"], "description": "A DBI-compatible interface to ODBC databases."}, "pairwise": {"categories": ["Psychometrics"], "description": "Performs the explicit calculation\n    \u2013 not estimation! \u2013 of the Rasch item parameters for dichotomous and\n    polytomous item responses, using a pairwise comparison approach. Person\n    parameters (WLE) are calculated according to Warm's weighted likelihood\n    approach."}, "clinsig": {"categories": ["ClinicalTrials"], "description": "Functions for calculating clinical significance."}, "causalweight": {"categories": ["CausalInference"], "description": "Various estimators of causal effects based on inverse probability weighting, doubly robust estimation, and double machine learning. Specifically, the package includes methods for estimating average treatment effects, direct and indirect effects in causal mediation analysis, and dynamic treatment effects. The models refer to studies of Froelich (2007) <doi:10.1016/j.jeconom.2006.06.004>, Huber (2012) <doi:10.3102/1076998611411917>, Huber (2014) <doi:10.1080/07474938.2013.806197>, Huber (2014) <doi:10.1002/jae.2341>, Froelich and Huber (2017) <doi:10.1111/rssb.12232>, Hsu, Huber, Lee, and Lettry (2020)  <doi:10.1002/jae.2765>, and others."}, "OBsMD": {"categories": ["ExperimentalDesign"], "description": "Implements the objective Bayesian methodology proposed in Consonni and Deldossi in order to choose the optimal experiment that better discriminate between competing models, see Deldossi and Nai Ruscone (2020) <doi:10.18637/jss.v094.i02>."}, "LNIRT": {"categories": ["MissingData", "Psychometrics"], "description": "Allows the simultaneous analysis of responses and response times in an Item Response Theory (IRT) modelling framework. Supports variable person speed functions (intercept, trend, quadratic), and covariates for item and person (random) parameters. Data missing-by-design can be specified. Parameter estimation is done with a MCMC algorithm. LNIRT replaces the package CIRT, which was written by Rinke Klein Entink. For reference, see the paper by Fox, Klein Entink and Van der Linden (2007), \"Modeling of Responses and Response Times with the Package cirt\", Journal of Statistical Software, <doi:10.18637/jss.v020.i07>."}, "JuliaConnectoR": {"categories": ["NumericalMathematics"], "description": "Allows to import functions and whole packages from 'Julia' in R.\n    Imported 'Julia' functions can directly be called as R functions.\n    Data structures can be translated between 'Julia' and R.\n    More details can also be found in the corresponding article\n    <doi:10.18637/jss.v101.i06>."}, "BayesSAE": {"categories": ["OfficialStatistics"], "description": "Provides a variety of methods from Rao (2003, ISBN:0-471-41374-7)\n  and some other research articles to deal with several specific small area area-\n  level models in Bayesian framework. Models provided range from the basic Fay-Herriot model to \n  its improvement such as You-Chapman models, unmatched models, spatial models and so on. \n  Different types of priors for specific parameters could be chosen to obtain MCMC posterior \n  draws. The main sampling function is written in C with GSL lab so as to facilitate the \n  computation. Model internal checking and model comparison criteria are also involved."}, "AzureVM": {"categories": ["WebTechnologies"], "description": "Functionality for working with virtual machines (VMs) in Microsoft's 'Azure' cloud: <https://azure.microsoft.com/en-us/services/virtual-machines/>. Includes facilities to deploy, startup, shutdown, and cleanly delete VMs and VM clusters. Deployment configurations can be highly customised, and can make use of existing resources as well as creating new ones. A selection of predefined configurations is provided to allow easy deployment of commonly used Linux and Windows images, including Data Science Virtual Machines. With a running VM, execute scripts and install optional extensions. Part of the 'AzureR' family of packages."}, "pimeta": {"categories": ["MetaAnalysis"], "description": "An implementation of prediction intervals for random-effects meta-analysis:\n  Higgins et al. (2009) <doi:10.1111/j.1467-985X.2008.00552.x>, Partlett and Riley (2017)\n  <doi:10.1002/sim.7140>, and Nagashima et al. (2019) <doi:10.1177/0962280218773520>,\n  <arXiv:1804.01054>."}, "CFC": {"categories": ["Survival"], "description": "Numerical integration of cause-specific survival curves to arrive at cause-specific cumulative incidence functions,\n  with three usage modes: 1) Convenient API for parametric survival regression followed by competing-risk analysis, 2) API for\n  CFC, accepting user-specified survival functions in R, and 3) Same as 2, but accepting survival functions in C++."}, "ata": {"categories": ["Psychometrics"], "description": "Provides a collection of psychometric methods to process item metadata\n and use target assessment and measurement blueprint constraints to assemble a test form. Currently two automatic\n test assembly (ata) approaches are enabled. For example, the weighted (positive) deviations method, wdm(), proposed\n by Swanson and Stocking (1993) <doi:10.1177/014662169301700205> was implemented in its full specification allowing\n for both item selection as well as test form refinement. The linear constraint programming approach, atalp(), uses the \n linear equation solver by Berkelaar et. al (2014) <http://lpsolve.sourceforge.net/5.5/>\n to enable a variety of approaches to select items."}, "CircSpaceTime": {"categories": ["Bayesian"], "description": "Implementation of Bayesian models for spatial and spatio-temporal\n             interpolation of circular data using Gaussian Wrapped and Gaussian Projected distributions.\n             We developed the methods described in Jona Lasinio G. et al. (2012) <doi:10.1214/12-aoas576>, \n             Wang F. et al. (2014) <doi:10.1080/01621459.2014.934454> and \n             Mastrantonio G. et al. (2016) <doi:10.1007/s11749-015-0458-y>."}, "rpca": {"categories": ["Robust"], "description": "Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Candes, E. J., Li, X., Ma, Y., & Wright, J. (2011). Robust principal component analysis?. Journal of the ACM (JACM), 58(3), 11. prove that we can recover each component individually under some suitable assumptions. It is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the L1 norm. This package implements this decomposition algorithm resulting with Robust PCA approach."}, "covid19br": {"categories": ["Epidemiology"], "description": "Set of functions to import COVID-19 pandemic data into R. The Brazilian COVID-19 data, obtained from the official Brazilian repository at <https://covid.saude.gov.br/>, is available at country, region, state, and city-levels. The package also downloads the world-level COVID-19 data from the John Hopkins University's repository."}, "pwt9": {"categories": ["Econometrics"], "description": "The Penn World Table 9.x (<http://www.ggdc.net/pwt/>) provides information\n\ton relative levels of income, output, inputs, and productivity\n\tfor 182 countries between 1950 and 2017."}, "pglm": {"categories": ["Econometrics"], "description": "Estimation of panel models for glm-like models:\n             this includes binomial models (logit and probit), count models (poisson and negbin)\n\t     and ordered models (logit and probit), as described in:\n             Baltagi (2013) Econometric Analysis of Panel Data, ISBN-13:978-1-118-67232-7,\n\t     Hsiao (2014) Analysis of Panel Data  <doi:10.1017/CBO9781139839327> and\n\t     Croissant and Millo (2018), Panel Data Econometrics with R, ISBN:978-1-118-94918-4."}, "Rmixmod": {"categories": ["Cluster"], "description": "Interface of 'MIXMOD' software for supervised, unsupervised and\n    semi-supervised classification with mixture modelling."}, "PBIBD": {"categories": ["ExperimentalDesign"], "description": "The PBIB designs are important type of incomplete block designs having wide area of their applications for example in agricultural experiments, in plant breeding, in sample surveys etc. This package constructs various series of PBIB designs and assists in checking all the necessary conditions of PBIB designs and the association scheme on which these designs are based on. It also assists in calculating the efficiencies of PBIB designs with any number of associate classes. The package also constructs Youden-m square designs which are Row-Column designs for the two-way elimination of heterogeneity. The incomplete columns of these Youden-m square designs constitute PBIB designs. With the present functionality, the package will be of immense importance for the researchers as it will help them to construct PBIB designs, to check if their PBIB designs and association scheme satisfy various necessary conditions for the existence, to calculate the efficiencies of PBIB designs based on any association scheme and to construct Youden-m square designs for the two-way elimination of heterogeneity. R. C. Bose and K. R. Nair (1939) <http://www.jstor.org/stable/40383923>."}, "FILEST": {"categories": ["MissingData"], "description": "A population genetic simulator, which is able to generate synthetic datasets for single-nucleotide polymorphisms (SNP) for multiple populations. The genetic distances among populations can be set according to the Fixation Index (Fst) as explained in Balding and Nichols (1995) <doi:10.1007/BF01441146>. This tool is able to simulate outlying individuals and missing SNPs can be specified. For Genome-wide association study (GWAS), disease status can be set in desired level according risk ratio."}, "reportfactory": {"categories": ["ReproducibleResearch"], "description": "Provides an infrastructure for handling multiple R Markdown\n  reports, including automated curation and time-stamping of outputs,\n  parameterisation and provision of helper functions to manage dependencies."}, "eigenmodel": {"categories": ["Bayesian", "MissingData"], "description": "Estimation of the parameters in a model for\n        symmetric relational data (e.g., the above-diagonal part of a\n        square matrix), using a model-based eigenvalue decomposition\n        and regression. Missing data is accommodated, and a posterior\n        mean for missing data is calculated under the assumption that\n        the data are missing at random. The marginal distribution of\n        the relational data can be arbitrary, and is fit with an\n        ordered probit specification. See Hoff (2007) <arXiv:0711.1146> \n        for details on the model.  "}, "glarma": {"categories": ["TimeSeries"], "description": "Functions are provided for estimation, testing, diagnostic checking and forecasting of generalized linear autoregressive moving average (GLARMA) models for discrete valued time series with regression variables.  These are a class of observation driven non-linear non-Gaussian state space models. The state vector consists of a linear regression component plus an observation driven component consisting of an autoregressive-moving average (ARMA) filter of past predictive residuals. Currently three distributions (Poisson, negative binomial and binomial) can be used for the response series. Three options (Pearson, score-type and unscaled) for the residuals in the observation driven component are available. Estimation is via maximum likelihood (conditional on initializing values for the ARMA process) optimized using Fisher scoring or Newton Raphson iterative methods. Likelihood ratio and Wald tests for the observation driven component allow testing for serial dependence in generalized linear model settings. Graphical diagnostics including model fits, autocorrelation functions and probability integral transform residuals are included in the package. Several standard data sets are included in the package."}, "tis": {"categories": ["Finance", "TimeSeries"], "description": "Functions and S3 classes for time indexes and time indexed\n        series, which are compatible with FAME frequencies."}, "tm.plugin.dc": {"categories": ["NaturalLanguageProcessing"], "description": "A plug-in for the text mining framework tm to support text mining \n             in a distributed way. The package provides a convenient interface for\n             handling distributed corpus objects based on distributed list objects."}, "emg": {"categories": ["Distributions"], "description": "Provides basic distribution functions for a mixture model of a Gaussian and exponential distribution."}, "nmadb": {"categories": ["MetaAnalysis"], "description": "Set of functions for accessing database of network meta-analyses described in \n  Petropoulou M, et al. Bibliographic study showed improving statistical methodology of network\n  meta-analyses published between 1999 and 2015\n  <doi:10.1016/j.jclinepi.2016.11.002>. The database is hosted in a REDcap database at the \n  Institute of Social and Preventive Medicine (ISPM) in the University of Bern."}, "IRTShiny": {"categories": ["Psychometrics"], "description": "Interactive shiny application for running Item Response Theory\n    analysis. Provides graphics for characteristic and information curves."}, "cjoint": {"categories": ["CausalInference"], "description": "An R implementation of the Average Marginal Component-specific\n    Effects (AMCE) estimator presented in Hainmueller, J., Hopkins, D., and Yamamoto\n    T. (2014) <doi:10.1093/pan/mpt024> Causal Inference in Conjoint Analysis: Understanding Multi-Dimensional\n    Choices via Stated Preference Experiments. Political Analysis 22(1):1-30."}, "semTools": {"categories": ["MissingData", "Psychometrics"], "description": "Provides tools for structural equation modeling, many of which extend the 'lavaan' package; for example, to pool results from multiple imputations, probe latent interactions, or test measurement invariance."}, "EvidenceSynthesis": {"categories": ["MetaAnalysis"], "description": "Routines for combining causal effect estimates and study diagnostics across multiple data sites in a distributed study, without sharing patient-level data. \n  Allows for normal and non-normal approximations of the data-site likelihood of the effect parameter. "}, "ff": {"categories": ["HighPerformanceComputing"], "description": "The ff package provides data structures that are stored on\n\tdisk but behave (almost) as if they were in RAM by transparently \n\tmapping only a section (pagesize) in main memory - the effective \n\tvirtual memory consumption per ff object. ff supports R's standard \n\tatomic data types 'double', 'logical', 'raw' and 'integer' and \n\tnon-standard atomic types boolean (1 bit), quad (2 bit unsigned), \n\tnibble (4 bit unsigned), byte (1 byte signed with NAs), ubyte (1 byte \n\tunsigned), short (2 byte signed with NAs), ushort (2 byte unsigned), \n\tsingle (4 byte float with NAs). For example 'quad' allows efficient \n\tstorage of genomic data as an 'A','T','G','C' factor. The unsigned \n\ttypes support 'circular' arithmetic. There is also support for \n\tclose-to-atomic types 'factor', 'ordered', 'POSIXct', 'Date' and \n\tcustom close-to-atomic types. \n\tff not only has native C-support for vectors, matrices and arrays \n\twith flexible dimorder (major column-order, major row-order and \n\tgeneralizations for arrays). There is also a ffdf class not unlike \n\tdata.frames and import/export filters for csv files.\n\tff objects store raw data in binary flat files in native encoding,\n\tand complement this with metadata stored in R as physical and virtual\n\tattributes. ff objects have well-defined hybrid copying semantics, \n\twhich gives rise to certain performance improvements through \n\tvirtualization. ff objects can be stored and reopened across R \n\tsessions. ff files can be shared by multiple ff R objects \n\t(using different data en/de-coding schemes) in the same process \n\tor from multiple R processes to exploit parallelism. A wide choice of \n\tfinalizer options allows to work with 'permanent' files as well as \n\tcreating/removing 'temporary' ff files completely transparent to the \n\tuser. On certain OS/Filesystem combinations, creating the ff files\n\tworks without notable delay thanks to using sparse file allocation.\n\tSeveral access optimization techniques such as Hybrid Index \n\tPreprocessing and Virtualization are implemented to achieve good \n\tperformance even with large datasets, for example virtual matrix \n\ttranspose without touching a single byte on disk. Further, to reduce \n\tdisk I/O, 'logicals' and non-standard data types get stored native and \n\tcompact on binary flat files i.e. logicals take up exactly 2 bits to \n\trepresent TRUE, FALSE and NA. \n\tBeyond basic access functions, the ff package also provides \n\tcompatibility functions that facilitate writing code for ff and ram \n\tobjects and support for batch processing on ff objects (e.g. as.ram, \n\tas.ff, ffapply). ff interfaces closely with functionality from package \n\t'bit': chunked looping, fast bit operations and coercions between \n\tdifferent objects that can store subscript information ('bit', \n\t'bitwhich', ff 'boolean', ri range index, hi hybrid index). This allows\n\tto work interactively with selections of large datasets and quickly \n\tmodify selection criteria. \n\tFurther high-performance enhancements can be made available upon request. "}, "mpoly": {"categories": ["NumericalMathematics"], "description": "Symbolic computing with multivariate polynomials in R."}, "Newdistns": {"categories": ["Distributions"], "description": "Computes the probability density function, cumulative distribution function, quantile function, random numbers and measures of inference for the following general families of  distributions (each family defined in terms of an arbitrary cdf G): Marshall Olkin G distributions, exponentiated G distributions, beta G distributions, gamma G distributions, Kumaraswamy G distributions, generalized beta G distributions, beta extended G distributions, gamma G distributions, gamma uniform G distributions, beta exponential G distributions, Weibull G distributions, log gamma G I distributions, log gamma G II distributions, exponentiated generalized G distributions, exponentiated Kumaraswamy G distributions, geometric exponential Poisson G distributions, truncated-exponential skew-symmetric G distributions, modified beta G distributions, and exponentiated exponential Poisson G distributions."}, "ROOPSD": {"categories": ["Distributions"], "description": "Statistical distribution in OOP (Object Oriented Programming) way.\n             This package proposes a R6 class interface to classic statistical\n             distribution, and new distributions can be easily added with the\n             class AbstractDist. A useful point is the generic fit() method for\n             each class, which uses a maximum likelihood estimation to find the\n             parameters of a dataset, see, e.g. Hastie, T. and al (2009)\n             <isbn:978-0-387-84857-0>. Furthermore, the rv_histogram class\n             gives a non-parametric fit, with the same accessors that for the\n             classic distribution. Finally, three random generators useful to\n             build synthetic data are given: a multivariate normal generator, an\n             orthogonal matrix generator, and a symmetric positive definite\n             matrix generator, see Mezzadri, F. (2007)\n             <arXiv:math-ph/0609050>."}, "ICBayes": {"categories": ["Survival"], "description": "Contains functions to fit Bayesian semiparametric regression survival models (proportional hazards model, proportional odds model, and probit model) to interval-censored time-to-event data."}, "rddensity": {"categories": ["Econometrics"], "description": "Density discontinuity testing (a.k.a. manipulation testing) is commonly employed in regression discontinuity designs and other program evaluation settings to detect perfect self-selection (manipulation) around a cutoff where treatment/policy assignment changes. This package implements manipulation testing procedures using the local polynomial density estimators: rddensity() to construct test statistics and p-values given a prespecified cutoff, rdbwdensity() to perform data-driven bandwidth selection, and rdplotdensity() to construct density plots."}, "RNifti": {"categories": ["MedicalImaging"], "description": "Provides very fast read and write access to images stored in the\n    NIfTI-1, NIfTI-2 and ANALYZE-7.5 formats, with seamless synchronisation\n    of in-memory image objects between compiled C and interpreted R code. Also\n    provides a simple image viewer, and a C/C++ API that can be used by other\n    packages. Not to be confused with 'RNiftyReg', which performs image\n    registration and applies spatial transformations."}, "MNP": {"categories": ["Bayesian", "Econometrics"], "description": "Fits the Bayesian multinomial probit model via Markov chain\n Monte Carlo.  The multinomial probit model is often used to analyze \n the discrete choices made by individuals recorded in survey data. \n Examples where the multinomial probit model may be useful include the \n analysis of product choice by consumers in market research and the \n analysis of candidate or party choice by voters in electoral studies.  \n The MNP package can also fit the model with different choice sets for \n each individual, and complete or partial individual choice orderings \n of the available alternatives from the choice set. The estimation is\n based on the efficient marginal data augmentation algorithm that is \n developed by Imai and van Dyk (2005). \"A Bayesian Analysis of the \n Multinomial Probit Model Using the Data Augmentation.\" Journal of \n Econometrics, Vol. 124, No. 2 (February), pp. 311-334. \n <doi:10.1016/j.jeconom.2004.02.002>  Detailed examples are given in \n Imai and van Dyk (2005). \"MNP: R Package for Fitting the Multinomial \n Probit Model.\"  Journal of Statistical Software, Vol. 14, No. 3 (May), \n pp. 1-32. <doi:10.18637/jss.v014.i03>."}, "mapsapi": {"categories": ["WebTechnologies"], "description": "Interface to the 'Google Maps' APIs: (1) routing directions based on the 'Directions' API, returned as 'sf' objects, either as single feature per alternative route, or a single feature per segment per alternative route; (2) travel distance or time matrices based on the 'Distance Matrix' API; (3) geocoded locations based on the 'Geocode' API, returned as 'sf' objects, either points or bounds; (4) map images using the 'Maps Static' API, returned as 'stars' objects."}, "Cubist": {"categories": ["MachineLearning"], "description": "Regression modeling using rules with added instance-based corrections."}, "ftsa": {"categories": ["FunctionalData", "TimeSeries"], "description": "Functions for visualizing, modeling, forecasting and hypothesis testing of functional time series."}, "animation": {"categories": ["ReproducibleResearch", "TeachingStatistics"], "description": "Provides functions for animations in statistics, covering topics\n    in probability theory, mathematical statistics, multivariate statistics,\n    non-parametric statistics, sampling survey, linear models, time series,\n    computational statistics, data mining and machine learning. These functions\n    may be helpful in teaching statistics and data analysis. Also provided in this\n    package are a series of functions to save animations to various formats, e.g.\n    Flash, 'GIF', HTML pages, 'PDF' and videos. 'PDF' animations can be inserted\n    into 'Sweave' / 'knitr' easily."}, "InfoTrad": {"categories": ["Finance"], "description": "Estimates the probability of informed trading (PIN) initially introduced by Easley et. al. (1996) <doi:10.1111/j.1540-6261.1996.tb04074.x> . Contribution of the package is that it uses likelihood factorizations of Easley et. al. (2010) <doi:10.1017/S0022109010000074> (EHO factorization) and Lin and Ke (2011) <doi:10.1016/j.finmar.2011.03.001> (LK factorization). Moreover, the package uses different estimation algorithms. Specifically, the grid-search algorithm proposed by Yan and Zhang (2012) <doi:10.1016/j.jbankfin.2011.08.003> , hierarchical agglomerative clustering approach proposed by Gan et. al. (2015) <doi:10.1080/14697688.2015.1023336> and later extended by Ersan and Alici (2016) <doi:10.1016/j.intfin.2016.04.001> ."}, "BOIN": {"categories": ["ExperimentalDesign"], "description": "The Bayesian optimal interval (BOIN) design is a novel phase I\n    clinical trial design for finding the maximum tolerated dose (MTD). It can be\n    used to design both single-agent and drug-combination trials. The BOIN design\n    is motivated by the top priority and concern of clinicians when testing a new\n    drug, which is to effectively treat patients and minimize the chance of exposing\n    them to subtherapeutic or overly toxic doses. The prominent advantage of the\n    BOIN design is that it achieves simplicity and superior performance at the same\n    time. The BOIN design is algorithm-based and can be implemented in a simple\n    way similar to the traditional 3+3 design. The BOIN design yields an average\n    performance that is comparable to that of the continual reassessment method\n    (CRM, one of the best model-based designs) in terms of selecting the MTD, but\n    has a substantially lower risk of assigning patients to subtherapeutic or overly\n    toxic doses. For tutorial, please check Yan et al. (2020) <doi:10.18637/jss.v094.i13>."}, "fwildclusterboot": {"categories": ["Econometrics"], "description": "Implementation of the fast algorithm for wild cluster bootstrap \n             inference developed in Roodman et al (2019, STATA Journal) for \n             linear regression models <doi:10.1177/1536867X19830877>, \n             which makes it feasible to quickly calculate bootstrap test \n             statistics based on a large number of bootstrap draws even for \n             large samples. Multiway clustering, regression weights, \n             bootstrap weights, fixed effects and subcluster bootstrapping\n             are supported. Further, both restricted (WCR) and unrestricted\n             (WCU) bootstrap are supported. Methods are provided for a variety \n             of fitted models, including 'lm()', 'feols()' \n             (from package 'fixest') and 'felm()' (from package 'lfe'). \n             Additionally implements a heteroskedasticity-robust (HC1) wild \n             bootstrap.\n             Further, the package provides an R binding to 'WildBootTests.jl',\n             which provides additional speed gains and functionality, \n             including the 'WRE' bootstrap for instrumental variable models \n             (based on models of type 'ivreg()' from package 'ivreg')\n             and hypotheses with q > 1."}, "quantreg": {"categories": ["Econometrics", "Environmetrics", "Optimization", "ReproducibleResearch", "Robust", "Survival"], "description": "Estimation and inference methods for models for conditional quantile functions: \n  Linear and nonlinear parametric and non-parametric (total variation penalized) models \n  for conditional quantiles of a univariate response and several methods for handling\n  censored survival data.  Portfolio selection methods based on expected shortfall\n  risk are also now included. See Koenker, R. (2005) Quantile Regression, Cambridge U. Press,\n  <doi:10.1017/CBO9780511754098> and Koenker, R. et al. (2017) Handbook of Quantile Regression, \n  CRC Press, <doi:10.1201/9781315120256>. "}, "ProbReco": {"categories": ["TimeSeries"], "description": "Training of reconciliation weights for probabilistic forecasts to optimise total energy (or variogram) score using Stochastic Gradient Descent with automatically differentiated gradients. See Panagiotelis, Gamakumara, Athanasopoulos and Hyndman, (2020) <https://www.monash.edu/business/ebs/research/publications/ebs/wp26-2020.pdf> for a description of the methods."}, "metansue": {"categories": ["MetaAnalysis"], "description": "Novel method to unbiasedly include studies with Non-statistically Significant Unreported Effects (NSUEs) in a meta-analysis <doi:10.1001/jamapsychiatry.2015.2196> and <doi:10.1177/0962280218811349>. Briefly, the method first calculates the interval where the unreported effects (e.g. t-values) should be according to the threshold of statistical significance used in each study. Afterwards, maximum likelihood techniques are used to impute the expected effect size of each study with NSUEs, accounting for between-study heterogeneity and potential covariates. Multiple imputations of the NSUEs are then randomly created based on the expected value, variance and statistical significance bounds. Finally, a restricted-maximum likelihood random-effects meta-analysis is separately conducted for each set of imputations, and estimations from these meta-analyses are pooled. Please read the reference in 'metansue' for details of the procedure."}, "tframe": {"categories": ["TimeSeries"], "description": "A kernel of functions for programming \n\ttime series methods in a way that is relatively independently of the \n\trepresentation of time. Also provides plotting, time windowing, \n\tand some\n\tother utility functions which are specifically intended for time series.\n\tSee the Guide distributed as a vignette, or ?tframe.Intro for more\n\tdetails. (User utilities are in package tfplot.)"}, "Ecdat": {"categories": ["Econometrics", "TimeSeries"], "description": "Data sets for econometrics, including political science."}, "tidymodels": {"categories": ["MachineLearning"], "description": "The tidy modeling \"verse\" is a collection of packages for\n    modeling and statistical analysis that share the underlying design\n    philosophy, grammar, and data structures of the tidyverse."}, "depmix": {"categories": ["Cluster", "TimeSeries"], "description": "Fits (multigroup) mixtures of latent or hidden Markov models on mixed categorical and continuous (timeseries) data. The 'Rdonlp2' package can optionally be used for optimization of the log-likelihood and is available from R-forge. See Visser et al. (2009, <doi:10.1007/978-0-387-95922-1_13>) for examples and applications. "}, "glmx": {"categories": ["Econometrics"], "description": "Extended techniques for generalized linear models (GLMs), especially for binary responses,\n             including parametric links and heteroscedastic latent variables."}, "DiceDesign": {"categories": ["ExperimentalDesign"], "description": "Space-Filling Designs and space-filling criteria (distance-based and uniformity-based), with emphasis to computer experiments; <doi:10.18637/jss.v065.i11>."}, "FDboost": {"categories": ["FunctionalData"], "description": "Regression models for functional data, i.e., scalar-on-function,\n    function-on-scalar and function-on-function regression models, are fitted\n    by a component-wise gradient boosting algorithm. \n\tFor a manual on how to use 'FDboost', see Brockhaus, Ruegamer, Greven (2017) <doi:10.18637/jss.v094.i10>."}, "evir": {"categories": ["Distributions", "Environmetrics", "ExtremeValue"], "description": "Functions for extreme value theory, which may be \n  divided into the following groups; exploratory data analysis, \n  block maxima, peaks over thresholds (univariate and bivariate), \n  point processes, gev/gpd distributions."}, "PP": {"categories": ["Psychometrics"], "description": "The PP package includes estimation of (MLE, WLE, MAP, EAP, ROBUST)\n    person parameters for the 1,2,3,4-PL model and the GPCM (generalized\n    partial credit model). The parameters are estimated under the assumption\n    that the item parameters are known and fixed. The package is useful e.g. in\n    the case that items from an item pool / item bank with known item parameters\n    are administered to a new population of test-takers and an ability\n    estimation for every test-taker is needed."}, "BaSTA": {"categories": ["Survival"], "description": "Estimates survival and mortality with covariates from capture-recapture/recovery data in a Bayesian framework when many individuals are of unknown age. It includes tools for data checking, model diagnostics and outputs such as life-tables and plots."}, "SpatialPosition": {"categories": ["Spatial"], "description": "Computes spatial position models: the potential model as defined\n    by Stewart (1941) <doi:10.1126/science.93.2404.89> and catchment areas as\n    defined by Reilly (1931) or Huff (1964) <doi:10.2307/1249154>."}, "NHLData": {"categories": ["SportsAnalytics"], "description": "Each dataset contains scores for every game during a specific season of the NHL."}, "evd": {"categories": ["Distributions", "Environmetrics", "ExtremeValue"], "description": "Extends simulation, distribution, quantile and density\n        functions to univariate and multivariate parametric extreme\n        value distributions, and provides fitting functions which\n        calculate maximum likelihood estimates for univariate and\n        bivariate maxima models, and for univariate and bivariate\n        threshold models."}, "monomvn": {"categories": ["MissingData"], "description": "Estimation of multivariate normal (MVN) and student-t data of \n arbitrary dimension where the pattern of missing data is monotone.\n See Pantaleo and Gramacy (2010) <arXiv:0907.2135>.\n Through the use of parsimonious/shrinkage regressions \n (plsr, pcr, lasso, ridge,  etc.), where standard regressions fail, \n the package can handle a nearly arbitrary amount of missing data. \n The current version supports maximum likelihood inference and \n a full Bayesian approach employing scale-mixtures for Gibbs sampling.\n Monotone data augmentation extends this Bayesian approach to arbitrary \n missingness patterns.  A fully functional standalone interface to the \n Bayesian lasso (from Park & Casella), Normal-Gamma (from Griffin & Brown),\n Horseshoe (from Carvalho, Polson, & Scott), and ridge regression \n with model selection via Reversible Jump, and student-t errors \n (from Geweke) is also provided."}, "cosa": {"categories": ["ClinicalTrials"], "description": "\n  Implements bound constrained optimal sample size allocation (BCOSSA) framework described in Bulus & Dong (2021) <doi:10.1080/00220973.2019.1636197> for power analysis of multilevel regression discontinuity designs (MRDDs) and multilevel randomized trials (MRTs) with continuous outcomes.\n  Minimum detectable effect size (MDES) and power computations for MRDDs allow polynomial functional form specification for the score variable (with or without interaction with the treatment indicator). See Bulus (2021) <doi:10.1080/19345747.2021.1947425>. "}, "cfbfastR": {"categories": ["SportsAnalytics"], "description": "A utility to quickly obtain clean and tidy college football\n    data. Serves as a wrapper around the\n    <https://collegefootballdata.com/> API and provides functions to\n    access live play by play and box score data from ESPN\n    <https://www.espn.com> when available. It provides users the\n    capability to access a plethora of endpoints, and supplement that data\n    with additional information (Expected Points Added/Win Probability\n    added)."}, "ELYP": {"categories": ["Survival"], "description": "Empirical likelihood ratio tests for the Yang and Prentice (short/long term hazards ratio) models. \n             Empirical likelihood tests within a Cox model, for parameters defined via \n\t\t\t both baseline hazard function and regression parameters."}, "nonlinearICP": {"categories": ["CausalInference"], "description": "Performs 'nonlinear Invariant Causal Prediction' to estimate the \n    causal parents of a given target variable from data collected in\n    different experimental or environmental conditions, extending\n    'Invariant Causal Prediction' from Peters, Buehlmann and Meinshausen (2016), \n    <arXiv:1501.01332>, to nonlinear settings. For more details, see C. Heinze-Deml, \n    J. Peters and N. Meinshausen: 'Invariant Causal Prediction for Nonlinear Models', \n    <arXiv:1706.08576>."}, "DT": {"categories": ["ReproducibleResearch"], "description": "Data objects in R can be rendered as HTML tables using the\n    JavaScript library 'DataTables' (typically via R Markdown or Shiny). The\n    'DataTables' library has been included in this R package. The package name\n    'DT' is an abbreviation of 'DataTables'."}, "survPresmooth": {"categories": ["Survival"], "description": "Presmoothed estimators of survival, density, cumulative and non-cumulative hazard functions with right-censored survival data. For details, see Lopez-de-Ullibarri and Jacome (2013) <doi:10.18637/jss.v054.i11>."}, "spatsoc": {"categories": ["Tracking"], "description": "Detects spatial and temporal groups in GPS relocations \n    (Robitaille et al. (2020) <doi:10.1111/2041-210X.13215>). \n    It can be used to convert GPS relocations to \n    gambit-of-the-group format to build proximity-based social networks \n    In addition, the randomizations function provides data-stream \n    randomization methods suitable for GPS data. "}, "AER": {"categories": ["Econometrics", "Survival", "TeachingStatistics", "TimeSeries"], "description": "Functions, data sets, examples, demos, and vignettes for the book\n             Christian Kleiber and Achim Zeileis (2008),\n\t     Applied Econometrics with R, Springer-Verlag, New York.\n\t     ISBN 978-0-387-77316-2. (See the vignette \"AER\" for a package overview.)"}, "bamdit": {"categories": ["Bayesian", "MetaAnalysis"], "description": "Provides a new class of Bayesian meta-analysis models that incorporates a model for internal and external validity bias. In this way, it is possible to combine studies of diverse quality and different types. For example, we can combine the results of randomized control trials (RCTs) with the results of observational studies (OS). "}, "movecost": {"categories": ["SpatioTemporal"], "description": "Provides the facility to calculate non-isotropic accumulated cost surface, least-cost paths, least-cost corridors, least-cost networks using a number of human-movement-related cost functions that can be selected by the user. It just requires a Digital Terrain Model, a start location and (optionally) destination locations. See Alberti (2019) <doi:10.1016/j.softx.2019.100331>."}, "forecastML": {"categories": ["TimeSeries"], "description": "The purpose of 'forecastML' is to simplify the process of multi-step-ahead forecasting with standard machine learning algorithms. 'forecastML' supports lagged, dynamic, static, and grouping features for modeling single and grouped numeric or factor/sequence time series. In addition, simple wrapper functions are used to support model-building with most R packages. This approach to forecasting is inspired by Bergmeir, Hyndman, and Koo's (2018) paper \"A note on the validity of cross-validation for evaluating autoregressive time series prediction\" <doi:10.1016/j.csda.2017.11.003>."}, "SparseM": {"categories": ["Econometrics", "NumericalMathematics"], "description": "Some basic linear algebra functionality for sparse matrices is\n  provided:  including Cholesky decomposition and backsolving as well as \n  standard R subsetting and Kronecker products."}, "surveyplanning": {"categories": ["OfficialStatistics"], "description": "Tools for sample survey planning, including sample size calculation, estimation of expected precision for the estimates of totals, and calculation of optimal sample size allocation."}, "MCMCvis": {"categories": ["Bayesian"], "description": "Performs key functions for MCMC analysis using minimal code - visualizes, manipulates, and summarizes MCMC output. Functions support simple and straightforward subsetting of model parameters within the calls, and produce presentable and 'publication-ready' output. MCMC output may be derived from Bayesian model output fit with 'Stan', 'NIMBLE', 'JAGS', and other software."}, "quickpsy": {"categories": ["Psychometrics"], "description": "Quickly fits and plots psychometric functions (normal, logistic,\n    Weibull or any or any function defined by the user) for multiple groups."}, "colorr": {"categories": ["SportsAnalytics"], "description": "Color palettes for EPL, MLB, NBA, NHL, and NFL teams."}, "IncDTW": {"categories": ["TimeSeries"], "description": "The Dynamic Time Warping (DTW) distance measure for time series allows non-linear alignments of time series to match  similar patterns in time series of different lengths and or different speeds. IncDTW is characterized by (1) the incremental calculation of DTW (reduces runtime complexity to a linear level for updating the DTW distance) - especially for life data streams or subsequence matching, (2) the vector based implementation of DTW which is faster because no matrices are allocated (reduces the space complexity from a quadratic to a linear level in the number of observations) - for all runtime intensive DTW computations, (3) the subsequence matching algorithm runDTW, that efficiently finds the k-NN to a query pattern in a long time series, and (4) C++ in the heart. For details about DTW see the original paper \"Dynamic programming algorithm optimization for spoken word recognition\" by Sakoe and Chiba (1978) <doi:10.1109/TASSP.1978.1163055>. For details about this package, Dynamic Time Warping and Incremental Dynamic Time Warping please see \"IncDTW: An R Package for Incremental Calculation of Dynamic Time Warping\" by Leodolter et al. (2021) <doi:10.18637/jss.v099.i09>."}, "opera": {"categories": ["TimeSeries"], "description": "Misc methods to form online predictions, for regression-oriented \n    time-series, by combining a finite set of forecasts provided by the user. See \n             Cesa-Bianchi and Lugosi (2006) <doi:10.1017/CBO9780511546921> for an overview. "}, "lcmm": {"categories": ["Cluster"], "description": "Estimation of various extensions of the mixed models including latent class mixed models, joint latent latent class mixed models, mixed models for curvilinear outcomes, mixed models for multivariate longitudinal outcomes using a maximum likelihood estimation method (Proust-Lima, Philipps, Liquet (2017) <doi:10.18637/jss.v078.i02>)."}, "leaflet": {"categories": ["Spatial"], "description": "Create and customize interactive maps using the 'Leaflet'\n    JavaScript library and the 'htmlwidgets' package. These maps can be used\n    directly from the R console, from 'RStudio', in Shiny applications and R Markdown\n    documents."}, "eChem": {"categories": ["ChemPhys"], "description": "Simulates cyclic voltammetry, linear-sweep voltammetry \n    (both with and without stirring of the solution), and single-pulse \n    and double-pulse chronoamperometry and chronocoulometry \n    experiments using the implicit finite difference method outlined in \n    Gosser (1993, ISBN: 9781560810261) and in Brown (2015) \n    <doi:10.1021/acs.jchemed.5b00225>. Additional functions provide \n    ways to display and to examine the results of these simulations.\n    The primary purpose of this package is to provide tools for\n    use in courses in analytical chemistry."}, "dynaTree": {"categories": ["ExperimentalDesign"], "description": "Inference by sequential Monte Carlo for \n  dynamic tree regression and classification models\n  with hooks provided for sequential design and optimization, \n  fully online learning with drift, variable selection, and \n  sensitivity analysis of inputs.  Illustrative \n  examples from the original dynamic trees paper \n  (Gramacy, Taddy & Polson (2011); <doi:10.1198/jasa.2011.ap09769>) are facilitated\n  by demos in the package; see demo(package=\"dynaTree\")."}, "LSMonteCarlo": {"categories": ["Finance"], "description": "The package compiles functions for calculating prices of American put options with Least Squares Monte Carlo method. The option types are plain vanilla American put, Asian American put, and Quanto American put. The pricing algorithms include variance reduction techniques such as Antithetic Variates and Control Variates. Additional functions are given to derive \"price surfaces\" at different volatilities and strikes, create 3-D plots, quickly generate Geometric Brownian motion, and calculate prices of European options with Black & Scholes analytical solution."}, "bibtex": {"categories": ["ReproducibleResearch"], "description": "Utility to parse a bibtex file. "}, "dynsurv": {"categories": ["Survival"], "description": "Time-varying coefficient models for interval censored and\n    right censored survival data including\n    1) Bayesian Cox model with time-independent, time-varying or\n    dynamic coefficients for right censored and interval censored data studied by\n    Sinha et al. (1999) <doi:10.1111/j.0006-341X.1999.00585.x> and\n    Wang et al. (2013) <doi:10.1007/s10985-013-9246-8>,\n    2) Spline based time-varying coefficient Cox model for right censored data\n    proposed by Perperoglou et al. (2006) <doi:10.1016/j.cmpb.2005.11.006>, and\n    3) Transformation model with time-varying coefficients for right censored data\n    using estimating equations proposed by\n    Peng and Huang (2007) <doi:10.1093/biomet/asm058>."}, "rscopus": {"categories": ["WebTechnologies"], "description": "Uses Elsevier 'Scopus' API\n    <https://dev.elsevier.com/sc_apis.html> to download \n    information about authors and their citations."}, "moveHMM": {"categories": ["SpatioTemporal", "Tracking"], "description": "Provides tools for animal movement modelling using hidden Markov\n    models. These include processing of tracking data, fitting hidden Markov models\n    to movement data, visualization of data and fitted model, decoding of the state\n    process..."}, "FactoClass": {"categories": ["Cluster"], "description": "Some functions of 'ade4' and 'stats' are combined in order to obtain a partition of the rows of a data table, with columns representing variables of scales: quantitative, qualitative or frequency. \n  First, a principal axes method is performed and then, a combination of Ward agglomerative hierarchical classification and K-means is performed, using some of the first coordinates obtained from the previous principal axes method. See, for example: \n      Lebart, L. and Piron, M. and Morineau, A.  (2006).\n      Statistique Exploratoire Multidimensionnelle, Dunod, Paris.\n  In order to permit to have different weights of the elements to be clustered, the function 'kmeansW', programmed in C++, is included. It is a modification of 'kmeans'.   \n  Some graphical functions include the option: 'gg=FALSE'.  When 'gg=TRUE', they  use the 'ggplot2' and 'ggrepel' packages to avoid  the super-position of the labels.   "}, "trend": {"categories": ["TimeSeries"], "description": "The analysis of environmental data often requires\n\t     the detection of trends and change-points. \n\t     This package includes tests for trend detection\n\t     (Cox-Stuart Trend Test, Mann-Kendall Trend Test, \n\t     (correlated) Hirsch-Slack Test,\n             partial Mann-Kendall Trend Test, multivariate (multisite)\n\t     Mann-Kendall Trend Test, (Seasonal) Sen's slope, \n\t     partial Pearson and Spearman correlation trend test),\n             change-point detection (Lanzante's test procedures, \n\t     Pettitt's test, Buishand Range Test,\n\t     Buishand U Test, Standard Normal Homogeinity Test),\n\t     detection of non-randomness (Wallis-Moore Phase Frequency Test,\n\t     Bartels rank von Neumann's ratio test, Wald-Wolfowitz Test)\n\t     and the two sample Robust Rank-Order Distributional Test."}, "mediation": {"categories": ["CausalInference", "Epidemiology", "Psychometrics"], "description": "We implement parametric and non parametric mediation analysis. This package performs the methods and suggestions in Imai, Keele and Yamamoto (2010) <doi:10.1214/10-STS321>, Imai, Keele and Tingley (2010) <doi:10.1037/a0020761>, Imai, Tingley and Yamamoto (2013) <doi:10.1111/j.1467-985X.2012.01032.x>, Imai and Yamamoto (2013) <doi:10.1093/pan/mps040> and Yamamoto (2013) <http://web.mit.edu/teppei/www/research/IVmediate.pdf>. In addition to the estimation of causal mediation effects, the software also allows researchers to conduct sensitivity analysis for certain parametric models."}, "IBrokers": {"categories": ["Finance"], "description": "Provides native R access to Interactive Brokers Trader Workstation API."}, "drgee": {"categories": ["Robust"], "description": "Fit restricted mean models for the conditional association\n  between an exposure and an outcome, given covariates. Three methods\n  are implemented: O-estimation, where a nuisance model for the\n  association between the covariates and the outcome is used;\n  E-estimation where a nuisance model for the association\n  between the covariates and the exposure is used, and doubly robust (DR)\n  estimation where both nuisance models are used. In DR-estimation,\n  the estimates will be consistent when at least one of the nuisance\n  models is correctly specified, not necessarily both. For more information, see Zetterqvist and Sj\u00f6lander (2015) <doi:10.1515/em-2014-0021>."}, "pdp": {"categories": ["MachineLearning"], "description": "A general framework for constructing partial dependence (i.e.,\n        marginal effect) plots from various types machine learning models\n        in R."}, "logspline": {"categories": ["Survival"], "description": "Contains routines for logspline density estimation.\n\tThe function oldlogspline() uses the same algorithm as the logspline package\n\tversion 1.0.x; i.e. the Kooperberg and Stone (1992) \n\talgorithm (with an improved interface).  The recommended routine logspline()\n\tuses an algorithm from Stone et al (1997)  <doi:10.1214/aos/1031594728>."}, "i2extras": {"categories": ["Epidemiology"], "description": "Provides functions to work with 'incidence2' objects, including a\n  simplified interface for trend fitting and peak estimation. This package is\n  part of the RECON (<https://www.repidemicsconsortium.org/>) toolkit for \n  outbreak analysis (<https://www.reconverse.org/)."}, "welo": {"categories": ["SportsAnalytics"], "description": "Estimates the standard and weighted Elo (WElo, Angelini et al., 2022 <doi:10.1016/j.ejor.2021.04.011>) rates. The current version provides Elo and WElo rates for tennis, according to different systems of weights (games or sets) and scale factors (constant, proportional to the number of matches, with more weight on Grand Slam matches or matches played on a specific surface). Moreover, the package gives the possibility of estimating the (bootstrap) standard errors for the rates. Finally, the package includes betting functions that automatically select the matches on which place a bet."}, "grplasso": {"categories": ["MachineLearning"], "description": "Fits user-specified (GLM-) models with group lasso penalty."}, "trust": {"categories": ["Optimization"], "description": "Does local optimization using two derivatives and trust regions.\n    Guaranteed to converge to local minimum of objective function."}, "latex2exp": {"categories": ["ReproducibleResearch"], "description": "Parses and converts LaTeX math formulas to R's plotmath\n    expressions, used to enter mathematical formulas and symbols to be rendered as\n    text, axis labels, etc. throughout R's plotting system."}, "tidypredict": {"categories": ["ModelDeployment"], "description": "It parses a fitted 'R' model object, and returns a formula in\n    'Tidy Eval' code that calculates the predictions.  It works with\n    several databases back-ends because it leverages 'dplyr' and 'dbplyr'\n    for the final 'SQL' translation of the algorithm. It currently\n    supports lm(), glm(), randomForest(), ranger(), earth(),\n    xgb.Booster.complete(), cubist(), and ctree() models."}, "swmmr": {"categories": ["Hydrology"], "description": "Functions to connect the widely used Storm Water Management Model (SWMM)\n  of the United States Environmental Protection Agency (US EPA) \n  <https://www.epa.gov/water-research/storm-water-management-model-swmm> to R with\n  currently two main goals: (1) Run a SWMM simulation from R and (2) provide fast \n  access to simulation results, i.e. SWMM's binary '.out'-files. High performance is achieved\n  with help of Rcpp. Additionally, reading SWMM's '.inp' and '.rpt' files is supported to \n  glance model structures and to get direct access to simulation summaries."}, "fxregime": {"categories": ["Econometrics"], "description": "Exchange rate regression and structural change tools\n             for estimating, testing, dating, and monitoring\n\t     (de facto) exchange rate regimes."}, "geojsonio": {"categories": ["Spatial"], "description": "Convert data to 'GeoJSON' or 'TopoJSON' from various R classes,\n    including vectors, lists, data frames, shape files, and spatial classes.\n    'geojsonio' does not aim to replace packages like 'sp', 'rgdal', 'rgeos',\n    but rather aims to be a high level client to simplify conversions of data\n    from and to 'GeoJSON' and 'TopoJSON'."}, "Bergm": {"categories": ["Bayesian"], "description": "Bayesian analysis for exponential random graph models using advanced computational algorithms. More information can be found at: <https://acaimo.github.io/Bergm/>."}, "actuar": {"categories": ["Distributions", "Finance"], "description": "Functions and data sets for actuarial science:\n  modeling of loss distributions; risk theory and ruin theory;\n  simulation of compound models, discrete mixtures and compound\n  hierarchical models; credibility theory. Support for many additional\n  probability distributions to model insurance loss size and\n  frequency: 23 continuous heavy tailed distributions; the\n  Poisson-inverse Gaussian discrete distribution; zero-truncated and\n  zero-modified extensions of the standard discrete distributions.\n  Support for phase-type distributions commonly used to compute ruin\n  probabilities. Main reference: <doi:10.18637/jss.v025.i07>.\n  Implementation of the Feller-Pareto family of distributions:\n  <doi:10.18637/jss.v103.i06>."}, "ffsimulator": {"categories": ["SportsAnalytics"], "description": "Uses bootstrap resampling to run fantasy football season\n    simulations supported by historical rankings and 'nflfastR' data,\n    calculating optimal lineups, and returning aggregated results."}, "bvartools": {"categories": ["TimeSeries"], "description": "Assists in the set-up of algorithms for Bayesian inference of vector autoregressive (VAR) and error correction (VEC) models. Functions for posterior simulation, forecasting, impulse response analysis and forecast error variance decomposition are largely based on the introductory texts of Chan, Koop, Poirier and Tobias (2019, ISBN: 9781108437493), Koop and Korobilis (2010) <doi:10.1561/0800000013> and Luetkepohl (2006, ISBN: 9783540262398)."}, "brms": {"categories": ["Bayesian"], "description": "Fit Bayesian generalized (non-)linear multivariate multilevel models\n    using 'Stan' for full Bayesian inference. A wide range of distributions\n    and link functions are supported, allowing users to fit \u2013 among others \u2013\n    linear, robust linear, count data, survival, response times, ordinal,\n    zero-inflated, hurdle, and even self-defined mixture models all in a\n    multilevel context. Further modeling options include non-linear and\n    smooth terms, auto-correlation structures, censored data, meta-analytic\n    standard errors, and quite a few more. In addition, all parameters of the\n    response distribution can be predicted in order to perform distributional\n    regression. Prior specifications are flexible and explicitly encourage\n    users to apply prior distributions that actually reflect their beliefs.\n    Model fit can easily be assessed and compared with posterior predictive\n    checks and leave-one-out cross-validation. References: B\u00fcrkner (2017)\n    <doi:10.18637/jss.v080.i01>; B\u00fcrkner (2018) <doi:10.32614/RJ-2018-017>;\n    B\u00fcrkner (2021) <doi:10.18637/jss.v100.i05>; Carpenter et al. (2017)\n    <doi:10.18637/jss.v076.i01>."}, "ztable": {"categories": ["ReproducibleResearch"], "description": "Makes zebra-striped tables (tables with alternating row colors)\n    in LaTeX and HTML formats easily from a data.frame, matrix, lm, aov, anova,\n    glm, coxph, nls, fitdistr, mytable and cbind.mytable objects."}, "mixreg": {"categories": ["Cluster"], "description": "Fits mixtures of (possibly multivariate) regressions\n\t(which has been described as doing ANCOVA when you don't\n\tknow the levels). Turner (2000) <doi:10.1111/1467-9876.00198>."}, "opendotaR": {"categories": ["SportsAnalytics"], "description": "Enables the usage of the OpenDota API from <https://www.opendota.com/>, get game lists, and download JSON's of parsed replays from\n    the OpenDota API. Also has functionality to execute own code to extract the specific parts of the JSON file."}, "EGAnet": {"categories": ["Psychometrics"], "description": "Implements the Exploratory Graph Analysis (EGA) framework for dimensionality and psychometric assessment. EGA is part of a new area called network psychometrics that uses undirected network models for the assessment of psychometric properties. EGA estimates the number of dimensions (or factors) using graphical lasso or Triangulated Maximally Filtered Graph (TMFG) and a weighted network community detection algorithm. A bootstrap method for verifying the stability of the dimensions and items in those dimensions is available. The fit of the structure suggested by EGA can be verified using Entropy Fit Indices. A novel approach called Unique Variable Analysis (UVA) can be used to identify and reduce redundant variables in multivariate data. Network loadings, which are roughly equivalent to factor loadings when the data generating model is a factor model, are available. Network scores can also be computed using the network loadings. Dynamic EGA (dynEGA) will estimate dimensions from time series data for individual, group, and sample levels. Golino, H., & Epskamp, S. (2017) <doi:10.1371/journal.pone.0174035>. Golino, H., Shi, D., Christensen, A. P., Garrido, L. E., Nieto, M. D., Sadana, R., & Thiyagarajan, J. A. (2020) <doi:10.31234/osf.io/gzcre>. Christensen, A. P., & Golino, H. (under review) <doi:10.31234/osf.io/hz89e>. Golino, H., Moulder, R. G., Shi, D., Christensen, A. P., Garrido, L. E., Nieto, M. D., Nesselroade, J., Sadana, R., Thiyagarajan, J. A., & Boker, S. M. (2020) <doi:10.31234/osf.io/mtka2>. Christensen, A. P. & Golino, H. (2021)  <doi:10.3390/psych3030032>. Christensen, A. P., Garrido, L. E., & Golino, H. (under review) <doi:10.31234/osf.io/4kra2>. Golino, H., Christensen, A. P., Moulder, R. G., Kim, S., & Boker, S. M. (under review) <doi:10.31234/osf.io/tfs7c>."}, "R2PPT": {"categories": ["ReproducibleResearch"], "description": "Provides a simple set of wrappers to easily use \n         RDCOMClient for generating Microsoft PowerPoint presentations. Warning:this package is soon to be archived from CRAN. "}, "smartsizer": {"categories": ["CausalInference"], "description": "A set of tools for determining the necessary sample size\n    in order to identify the optimal dynamic treatment regime\n    in a sequential, multiple assignment, randomized trial (SMART). \n    Utilizes multiple comparisons with the best methodology \n    to adjust for multiple comparisons.\n    Designed for an arbitrary SMART design. Please see Artman (2018) <doi:10.1093/biostatistics/kxy064> for more details."}, "leaps": {"categories": ["ChemPhys"], "description": "Regression subset selection, including exhaustive search."}, "NPBayesImputeCat": {"categories": ["MissingData"], "description": "These routines create multiple imputations of missing at random categorical data, and create multiply imputed synthesis of categorical data, with or without structural zeros. Imputations and syntheses are based on Dirichlet process mixtures of multinomial distributions, which is a non-parametric Bayesian modeling approach that allows for flexible joint modeling, described in Manrique-Vallier and Reiter (2014) <doi:10.1080/10618600.2013.844700>."}, "rspa": {"categories": ["OfficialStatistics"], "description": "Minimally adjust the values of numerical records in a data.frame, such\n    that each record satisfies a predefined set of equality and/or inequality\n    constraints. The constraints can be defined using the 'validate' package. \n    The core algorithms have recently been moved to the 'lintools' package,\n    refer to 'lintools' for a more basic interface and access to a version\n    of the algorithm that works with sparse matrices."}, "MCPMod": {"categories": ["ClinicalTrials"], "description": "Implements a methodology for the design and analysis of dose-response studies that\n             combines aspects of multiple comparison procedures  and modeling approaches\n\t     (Bretz, Pinheiro and Branson, 2005, Biometrics 61, 738-748, <doi:10.1111/j.1541-0420.2005.00344.x>).\n             The package provides tools for the analysis of dose finding trials as well as a variety\n             of tools necessary to plan a trial to be conducted with the MCP-Mod methodology.\n             Please note: The 'MCPMod' package will not be further developed, all future development of \n             the MCP-Mod methodology will be done in the 'DoseFinding' R-package. "}, "timeROC": {"categories": ["Survival"], "description": "Estimation of time-dependent ROC curve and area under time dependent ROC curve (AUC) in the presence of censored data, with or without competing risks. Confidence intervals of AUCs and tests for comparing AUCs of two rival markers measured on the same subjects can be computed, using the iid-representation of the AUC estimator. Plot functions for time-dependent ROC curves and AUC curves are provided. Time-dependent Positive Predictive Values (PPV) and Negative Predictive Values (NPV) can also be computed. See Blanche et al. (2013) <doi:10.1002/sim.5958> and references therein for the details of the methods implemented in the package."}, "DClusterm": {"categories": ["Spatial"], "description": "Model-based methods for the detection of disease clusters\n  using GLMs, GLMMs and zero-inflated models. These methods are described\n  in 'V. G\u00f3mez-Rubio et al.' (2019) <doi:10.18637/jss.v090.i14> and\n  'V. G\u00f3mez-Rubio et al.' (2018) <doi:10.1007/978-3-030-01584-8_1>."}, "choiceDes": {"categories": ["ExperimentalDesign"], "description": "Design functions for DCMs and other types of choice studies (including MaxDiff and other tradeoffs)."}, "tsoutliers": {"categories": ["TimeSeries"], "description": "Detection of outliers in time series following the \n    Chen and Liu (1993) <doi:10.2307/2290724> procedure. \n    Innovational outliers, additive outliers, level shifts, \n    temporary changes and seasonal level shifts are considered."}, "rocker": {"categories": ["Databases"], "description": "'R6' class interface for handling relational database connections using 'DBI' package as backend.\n  The class allows handling of connections to e.g. PostgreSQL, MariaDB and SQLite.\n  The purpose is having an intuitive object allowing straightforward handling of SQL databases."}, "RobLox": {"categories": ["Robust"], "description": "Functions for the determination of optimally robust influence curves and\n            estimators in case of normal location and/or scale."}, "betareg": {"categories": ["Econometrics", "Psychometrics"], "description": "Beta regression for modeling beta-distributed dependent variables, e.g., rates and proportions.\n  In addition to maximum likelihood regression (for both mean and precision of a beta-distributed\n  response), bias-corrected and bias-reduced estimation as well as finite mixture models and\n  recursive partitioning for beta regressions are provided."}, "simMSM": {"categories": ["Survival"], "description": "Simulation of event histories with possibly non-linear baseline hazard rate functions, non-linear (time-varying) covariate effect functions, and dependencies on the past of the history. Random generation of event histories is performed using inversion sampling on the cumulative all-cause hazard rate functions. "}, "partitions": {"categories": ["NumericalMathematics"], "description": "Additive partitions of integers.  Enumerates the\n  partitions, unequal partitions, and restricted partitions of an\n  integer; the three corresponding partition functions are also given.\n  Set partitions and now compositions and riffle shuffles are\n  included."}, "prism": {"categories": ["Hydrology"], "description": "Allows users to access the Oregon State Prism climate data\n    (<https://prism.nacse.org/>). Using the web service API data\n    can easily downloaded in bulk and loaded into R for spatial analysis.\n    Some user friendly visualizations are also provided."}, "fiery": {"categories": ["WebTechnologies"], "description": "A very flexible framework for building server side logic in R. The \n    framework is unopinionated when it comes to how HTTP requests and WebSocket\n    messages are handled and supports all levels of app complexity; from serving\n    static content to full-blown dynamic web-apps. Fiery does not hold your hand\n    as much as e.g. the shiny package does, but instead sets you free to create\n    your web app the way you want."}, "VeryLargeIntegers": {"categories": ["NumericalMathematics"], "description": "Multi-precision library that allows to store and operate with arbitrarily big integers without\n    loss of precision. It includes a large list of tools to work with them, like:\n      - Arithmetic and logic operators\n      - Modular-arithmetic operators\n      - Computer Number Theory utilities\n      - Probabilistic primality tests\n      - Factorization algorithms\n      - Random generators of diferent types of integers."}, "editrules": {"categories": ["OfficialStatistics"], "description": "Facilitates reading and manipulating (multivariate) data restrictions\n    (edit rules) on numerical and categorical data. Rules can be defined with common R syntax\n    and parsed to an internal (matrix-like format). Rules can be manipulated with\n    variable elimination and value substitution methods, allowing for feasibility checks\n    and more. Data can be tested against the rules and erroneous fields can be found based\n    on Fellegi and Holt's generalized principle. Rules dependencies can be visualized with \n    using the 'igraph' package."}, "evclass": {"categories": ["MachineLearning"], "description": "Different evidential distance-based classifiers, which provide\n    outputs in the form of Dempster-Shafer mass functions. The methods are: the\n    evidential K-nearest neighbor rule and the evidential neural network."}, "spgwr": {"categories": ["Spatial"], "description": "Functions for computing geographically weighted\n  regressions are provided, based on work by Chris\n  Brunsdon, Martin Charlton and Stewart Fotheringham. "}, "vars": {"categories": ["Econometrics", "Finance", "TimeSeries"], "description": "Estimation, lag selection, diagnostic testing, forecasting, causality analysis, forecast error variance decomposition and impulse response functions of VAR models and estimation of SVAR and SVEC models."}, "metawho": {"categories": ["MetaAnalysis"], "description": "A tool for implementing so called 'deft' approach\n    (see Fisher, David J., et al. (2017) <doi:10.1136/bmj.j573>) and model\n    visualization."}, "rsoi": {"categories": ["Hydrology"], "description": "Downloads Southern Oscillation Index, Oceanic Nino\n    Index, North Pacific Gyre Oscillation data, North Atlantic Oscillation\n    and Arctic Oscillation. Data sources are described in the help files for each function."}, "gamlss.dist": {"categories": ["Distributions"], "description": "A set of distributions  which can be used  for modelling the response variables in Generalized Additive Models for Location Scale and Shape, Rigby and Stasinopoulos (2005), <doi:10.1111/j.1467-9876.2005.00510.x>. The distributions can be continuous, discrete or mixed  distributions.  Extra distributions can be created, by transforming, any continuous distribution defined on the real line,  to  a distribution defined on ranges 0 to infinity  or  0 to 1,  by using a \u201dlog\u201d or a \u201dlogit' transformation respectively. "}, "spatialCovariance": {"categories": ["Spatial"], "description": "Functions that compute the spatial covariance matrix for the matern and power classes of spatial models, for data that arise on rectangular units.  This code can also be used for the change of support problem and for spatial data that arise on irregularly shaped regions like counties or zipcodes by laying a fine grid of rectangles and aggregating the integrals in a form of Riemann integration."}, "metacor": {"categories": ["MetaAnalysis"], "description": "Implement the DerSimonian-Laird (DSL) and Olkin-Pratt (OP)\n        meta-analytical approaches with correlation coefficients as\n        effect sizes."}, "wkutils": {"categories": ["Spatial"], "description": "Provides extra utilities for well-known formats in the\n  'wk' package that are outside the scope of that package. Utilities\n  to parse coordinates from data frames, plot well-known geometry\n  vectors, extract meta information from well-known geometry vectors,\n  and calculate bounding boxes are provided."}, "DiscreteInverseWeibull": {"categories": ["Distributions"], "description": "Probability mass function, distribution function, quantile function, random generation and parameter estimation for the discrete inverse Weibull distribution."}, "future": {"categories": ["HighPerformanceComputing"], "description": "The purpose of this package is to provide a lightweight and\n    unified Future API for sequential and parallel processing of R\n    expression via futures.  The simplest way to evaluate an expression\n    in parallel is to use 'x %<-% { expression }' with 'plan(multisession)'.\n    This package implements sequential, multicore, multisession, and\n    cluster futures.  With these, R expressions can be evaluated on the\n    local machine, in parallel a set of local machines, or distributed\n    on a mix of local and remote machines.\n    Extensions to this package implement additional backends for\n    processing futures via compute cluster schedulers, etc.\n    Because of its unified API, there is no need to modify any code in order\n    switch from sequential on the local machine to, say, distributed\n    processing on a remote compute cluster.\n    Another strength of this package is that global variables and functions\n    are automatically identified and exported as needed, making it\n    straightforward to tweak existing code to make use of futures."}, "R2HTML": {"categories": ["ReproducibleResearch"], "description": "Includes HTML function and methods to write in an HTML\n        file. Thus, making HTML reports is easy. Includes a function\n        that allows redirection on the fly, which appears to be very\n        useful for teaching purpose, as the student can keep a copy of\n        the produced output to keep all that he did during the course.\n        Package comes with a vignette describing how to write HTML\n        reports for statistical analysis. Finally, a driver for 'Sweave'\n        allows to parse HTML flat files containing R code and to\n        automatically write the corresponding outputs (tables and\n        graphs)."}, "alabama": {"categories": ["Optimization"], "description": "Augmented Lagrangian Adaptive Barrier Minimization\n        Algorithm for optimizing smooth nonlinear objective functions\n        with constraints. Linear or nonlinear equality and inequality\n        constraints are allowed."}, "nanonext": {"categories": ["WebTechnologies"], "description": "R binding for NNG (Nanomsg Next Gen), a successor to ZeroMQ. NNG is \n    a socket library providing high-performance scalability protocols,\n    implementing a cross-platform standard for messaging and communications.\n    Serves as a concurrency framework for building distributed applications,\n    utilising 'Aio' objects which automatically resolve upon completion of\n    asynchronous operations."}, "baseballDBR": {"categories": ["SportsAnalytics"], "description": "A tool for gathering and analyzing data from the Baseball Databank <http://www.baseball-databank.org/>, which includes player performance statistics from major league baseball in the United States beginning in the year 1871."}, "kequate": {"categories": ["Psychometrics"], "description": "Implements the kernel method of test equating as defined in von Davier, A. A., Holland, P. W. and Thayer, D. T. (2004) <doi:10.1007/b97446> and Andersson, B. and Wiberg, M. (2017) <doi:10.1007/s11336-016-9528-7> using the CB, EG, SG, NEAT CE/PSE and NEC designs, supporting Gaussian, logistic and uniform kernels and unsmoothed and pre-smoothed input data."}, "bnclassify": {"categories": ["GraphicalModels"], "description": "State-of-the art algorithms for learning discrete Bayesian network classifiers from data, including a number of those described in Bielza & Larranaga (2014) <doi:10.1145/2576868>, with functions for prediction, model evaluation and inspection."}, "gapfill": {"categories": ["MissingData", "SpatioTemporal"], "description": "Tools to fill missing values in satellite data and to develop new\n    gap-fill algorithms. The methods are tailored to data (images) observed\n    at equally-spaced points in time. The package is illustrated with MODIS\n    NDVI data."}, "meta": {"categories": ["ClinicalTrials", "MetaAnalysis"], "description": "User-friendly general package providing standard methods for meta-analysis and supporting Schwarzer, Carpenter, and R\u00fccker <doi:10.1007/978-3-319-21416-0>, \"Meta-Analysis with R\" (2015):\n - common effect and random effects meta-analysis;\n - several plots (forest, funnel, Galbraith / radial, L'Abbe, Baujat, bubble);\n - statistical tests and trim-and-fill method to evaluate bias in meta-analysis;\n - import data from 'RevMan 5';\n - prediction interval, Hartung-Knapp method for random effects model;\n - cumulative meta-analysis and leave-one-out meta-analysis;\n - meta-regression;\n - generalised linear mixed models;\n - produce forest plot summarising several (subgroup) meta-analyses."}, "rgl": {"categories": ["SpatioTemporal"], "description": "Provides medium to high level functions for 3D interactive graphics, including\n    functions modelled on base graphics (plot3d(), etc.) as well as functions for\n    constructing representations of geometric objects (cube3d(), etc.).  Output\n    may be on screen using OpenGL, or to various standard 3D file formats including\n    WebGL, PLY, OBJ, STL as well as 2D image formats, including PNG, Postscript, SVG, PGF."}, "openNLP": {"categories": ["NaturalLanguageProcessing"], "description": "An interface to the Apache OpenNLP tools (version 1.5.3).\n  The Apache OpenNLP library is a machine learning based toolkit for the\n  processing of natural language text written in Java.\n  It supports the most common NLP tasks, such as tokenization, sentence\n  segmentation, part-of-speech tagging, named entity extraction, chunking,\n  parsing, and coreference resolution.\n  See <https://opennlp.apache.org/> for more information."}, "OVtool": {"categories": ["CausalInference"], "description": "This tool was designed to assess the sensitivity of research findings to omitted variables when estimating causal effects using propensity score (PS) weighting. This tool produces graphics and summary results that will enable a researcher to quantify the impact an omitted variable would have on their results. Burgette et al. (2021) describe the methodology behind the primary function in this package, ov_sim. The method is demonstrated in Griffin et al. (2020) <doi:10.1016/j.jsat.2020.108075>."}, "TSA": {"categories": ["TimeSeries"], "description": "Contains R functions and datasets detailed in the book\n        \"Time Series Analysis with Applications in R (second edition)\" by Jonathan Cryer and Kung-Sik Chan."}, "softImpute": {"categories": ["MissingData"], "description": "Iterative methods for matrix completion that use nuclear-norm regularization. There are two main approaches.The one approach uses iterative soft-thresholded svds to impute the missing values. The second approach uses alternating least squares. Both have an 'EM' flavor, in that at each iteration the matrix is completed with the current estimate. For large matrices there is a special sparse-matrix class named \"Incomplete\" that efficiently handles all computations. The package includes procedures for centering and scaling rows, columns or both, and for computing low-rank SVDs on large sparse centered matrices (i.e. principal components)."}, "n1qn1": {"categories": ["Optimization"], "description": "Provides 'Scilab' 'n1qn1'. This takes more memory than traditional L-BFGS.  The n1qn1 routine is useful since it allows prespecification of a Hessian.\n       If the Hessian is near enough the truth in optimization it can speed up the optimization problem. The algorithm is described in the\n       'Scilab' optimization documentation located at \n       <https://www.scilab.org/sites/default/files/optimization_in_scilab.pdf>. This version uses manually modified code from 'f2c' to make this a C only binary."}, "EnvStats": {"categories": ["Distributions", "Environmetrics", "Epidemiology"], "description": "Graphical and statistical analyses of environmental data, with \n  focus on analyzing chemical concentrations and physical parameters, usually in \n  the context of mandated environmental monitoring.  Major environmental \n  statistical methods found in the literature and regulatory guidance documents, \n  with extensive help that explains what these methods do, how to use them, \n  and where to find them in the literature.  Numerous built-in data sets from \n  regulatory guidance documents and environmental statistics literature.  Includes \n  scripts reproducing analyses presented in the book \"EnvStats:  An R Package for \n  Environmental Statistics\" (Millard, 2013, Springer, ISBN 978-1-4614-8455-4, \n  <https://link.springer.com/book/9781461484554>)."}, "brxx": {"categories": ["MissingData"], "description": "When samples contain missing data, are small, or are suspected of bias,\n    estimation of scale reliability may not be trustworthy.  A recommended solution\n    for this common problem has been Bayesian model estimation.  Bayesian methods\n    rely on user specified information from historical data or researcher intuition \n    to more accurately estimate the parameters.  This package provides a user friendly\n    interface for estimating test reliability.  Here, reliability is modeled as a beta\n    distributed random variable with shape parameters alpha=true score variance and\n    beta=error variance (Tanzer & Harlow, 2020) <doi:10.1080/00273171.2020.1854082>."}, "shinybrms": {"categories": ["Bayesian"], "description": "A graphical user interface (GUI) for fitting Bayesian regression\n    models using the package 'brms' which in turn relies on 'Stan'\n    (<https://mc-stan.org/>). The 'shinybrms' GUI is a 'shiny' app."}, "retroharmonize": {"categories": ["MissingData"], "description": "Assist in reproducible retrospective (ex-post) harmonization\n    of data, particularly individual level survey data, by providing tools\n    for organizing metadata, standardizing the coding of variables, and\n    variable names and value labels, including missing values, and\n    documenting the data transformations, with the help of comprehensive\n    s3 classes."}, "sn": {"categories": ["Distributions"], "description": "Build and manipulate probability distributions of the skew-normal \n  family and some related ones, notably the skew-t and the SUN families. \n  For the skew-normal and the skew-t distributions, statistical methods are \n  provided for data fitting and model diagnostics, in the univariate and the \n  multivariate case."}, "metaRMST": {"categories": ["MetaAnalysis"], "description": "R implementation of a multivariate meta-analysis of randomized controlled trials (RCT) with the difference in restricted mean survival times (RMSTD). Use this package with individual patient level data from an RCT for a time-to-event outcome to determine combined effect estimates according to 4 methods: 1)  a univariate meta-analysis using observed treatment effects, 2) a univariate meta-analysis using effects predicted by fitted Royston-Parmar flexible parametric models, 3) multivariate meta-analysis with analytically derived covariance, 4) multivariate meta-analysis with bootstrap derived covariance. This package computes all combined effects and provides an RMSTD curve with combined effect estimates and their confidence intervals."}, "rsparse": {"categories": ["MissingData"], "description": "Implements many algorithms for statistical learning on \n  sparse matrices - matrix factorizations, matrix completion, \n  elastic net regressions, factorization machines. \n  Also 'rsparse' enhances 'Matrix' package by providing methods for \n  multithreaded <sparse, dense> matrix products and native slicing of \n  the sparse matrices in Compressed Sparse Row (CSR) format.\n  List of the algorithms for regression problems:\n  1) Elastic Net regression via Follow The Proximally-Regularized Leader (FTRL) \n  Stochastic Gradient Descent (SGD), as per McMahan et al(, <doi:10.1145/2487575.2488200>)\n  2) Factorization Machines via SGD, as per Rendle (2010, <doi:10.1109/ICDM.2010.127>)\n  List of algorithms for matrix factorization and matrix completion:\n  1) Weighted Regularized Matrix Factorization (WRMF) via Alternating Least \n  Squares (ALS) - paper by Hu, Koren, Volinsky (2008, <doi:10.1109/ICDM.2008.22>)\n  2) Maximum-Margin Matrix Factorization via ALS, paper by Rennie, Srebro \n  (2005, <doi:10.1145/1102351.1102441>)\n  3) Fast Truncated Singular Value Decomposition (SVD), Soft-Thresholded SVD, \n  Soft-Impute matrix completion via ALS - paper by Hastie, Mazumder \n  et al. (2014, <arXiv:1410.2596>)\n  4) Linear-Flow matrix factorization, from 'Practical linear models for \n  large-scale one-class collaborative filtering' by Sedhain, Bui, Kawale et al \n  (2016, ISBN:978-1-57735-770-4)\n  5) GlobalVectors (GloVe) matrix factorization via SGD, paper by Pennington, \n  Socher, Manning (2014, <https://aclanthology.org/D14-1162/>)\n  Package is reasonably fast and memory efficient - it allows to work with large\n  datasets - millions of rows and millions of columns. This is particularly useful \n  for practitioners working on recommender systems."}, "nonneg.cg": {"categories": ["Optimization"], "description": "Minimize a differentiable function subject to all the variables being non-negative (i.e. >= 0),\n  using a Conjugate-Gradient algorithm based on a modified Polak-Ribiere-Polyak formula as described in\n  (Li, Can, 2013, <https://www.hindawi.com/journals/jam/2013/986317/abs/>)."}, "BGVAR": {"categories": ["TimeSeries"], "description": "Estimation of Bayesian Global Vector Autoregressions (BGVAR) with different prior setups and the possibility to introduce stochastic volatility. Built-in priors include the Minnesota, the stochastic search variable selection and Normal-Gamma (NG) prior. For a reference see also Crespo Cuaresma, J., Feldkircher, M. and F. Huber (2016) \"Forecasting with Global Vector Autoregressive Models: a Bayesian Approach\", Journal of Applied Econometrics, Vol. 31(7), pp. 1371-1391 <doi:10.1002/jae.2504>. Post-processing functions allow for doing predictions, structurally identify the model with short-run or sign-restrictions and compute impulse response functions, historical decompositions and forecast error variance decompositions. Plotting functions are also available."}, "testcorr": {"categories": ["TimeSeries"], "description": "Computes the test statistics for examining the significance of autocorrelation in univariate time series, cross-correlation in bivariate time series, Pearson correlations in multivariate series and test statistics for i.i.d. property of univariate series given in Dalla, Giraitis and Phillips (2020), <https://cowles.yale.edu/sites/default/files/files/pub/d21/d2194-r.pdf>."}, "qrmtools": {"categories": ["Distributions", "Finance"], "description": "Functions and data sets for reproducing selected results from\n  the book \"Quantitative Risk Management: Concepts, Techniques and Tools\".\n  Furthermore, new developments and auxiliary functions for Quantitative\n  Risk Management practice."}, "spselect": {"categories": ["Spatial"], "description": "Fits spatial scale (SS) forward stepwise regression, SS incremental forward stagewise regression, SS least angle regression (LARS), and SS lasso models.  All area-level covariates are considered at all available scales to enter a model, but the SS algorithms are constrained to select each area-level covariate at a single spatial scale."}, "ez": {"categories": ["ExperimentalDesign"], "description": "Facilitates easy analysis of factorial experiments, including\n    purely within-Ss designs (a.k.a. \"repeated measures\"), purely between-Ss\n    designs, and mixed within-and-between-Ss designs. The functions in this package\n    aim to provide simple, intuitive and consistent specification of data analysis\n    and visualization. Visualization functions also include design visualization for\n    pre-analysis data auditing, and correlation matrix visualization. Finally, this\n    package includes functions for non-parametric analysis, including permutation\n    tests and bootstrap resampling. The bootstrap function obtains predictions\n    either by cell means or by more advanced/powerful mixed effects models, yielding\n    predictions and confidence intervals that may be easily visualized at any level\n    of the experiment's design."}, "bundesbank": {"categories": ["TimeSeries"], "description": "Download data from the time-series\n  databases of the Bundesbank, the German central\n  bank. See the overview at the Bundesbank website\n  (<https://www.bundesbank.de/en/statistics/time-series-databases>)\n  for available series. The package provides only a\n  single function, getSeries(), which supports both\n  traditional and real-time datasets; it will also\n  download meta data if available. Downloaded data\n  can automatically be arranged in various formats,\n  such as data frames or 'zoo' series. The data\n  may optionally be cached, so as to avoid repeated\n  downloads of the same series."}, "vrtest": {"categories": ["Finance"], "description": "A  collection of statistical tests for martingale difference hypothesis, including automatic portmanteau test (Escansiano and Lobato, 2009) <doi:10.1016/j.jeconom.2009.03.001> and automatic variance ratio test (Kim, 2009) <doi:10.1016/j.frl.2009.04.003>."}, "landscapemetrics": {"categories": ["Spatial"], "description": "Calculates landscape metrics for categorical landscape patterns in \n    a tidy workflow. 'landscapemetrics' reimplements the most common metrics from\n    'FRAGSTATS' (<https://www.umass.edu/landeco/research/fragstats/fragstats.html>) \n    and new ones from the current literature on landscape metrics.\n    This package supports 'raster' spatial objects and takes \n    RasterLayer, RasterStacks, RasterBricks or lists of RasterLayer from the\n    'raster' package as input arguments. It further provides utility functions\n    to visualize patches, select metrics and building blocks to develop new \n    metrics."}, "inferference": {"categories": ["CausalInference"], "description": "Provides methods for estimating causal effects in the presence of interference described in  B. Saul and M. Hugdens (2017) <doi:10.18637/jss.v082.i02>. Currently it implements the inverse-probability weighted (IPW) estimators proposed by E.J. Tchetgen Tchetgen and T.J. Vanderweele (2012) <doi:10.1177/0962280210386779>."}, "wavethresh": {"categories": ["Finance", "TimeSeries"], "description": "Performs 1, 2 and 3D real and complex-valued wavelet transforms,\n\tnondecimated transforms, wavelet packet transforms, nondecimated\n\twavelet packet transforms, multiple wavelet transforms,\n\tcomplex-valued wavelet transforms, wavelet shrinkage for\n\tvarious kinds of data, locally stationary wavelet time series,\n\tnonstationary multiscale transfer function modeling, density\n\testimation."}, "fda": {"categories": ["FunctionalData"], "description": "These functions were developed to support functional data\n analysis as described in Ramsay, J. O. and Silverman, B. W.\n (2005) Functional Data Analysis. New York: Springer and in \n Ramsay, J. O., Hooker, Giles, and Graves, Spencer (2009). \n Functional Data Analysis with R and Matlab (Springer). \n The package includes data sets and script files working many examples \n including all but one of the 76 figures in this latter book.  Matlab versions \n are available by ftp from \n <https://www.psych.mcgill.ca/misc/fda/downloads/FDAfuns/>."}, "streamR": {"categories": ["WebTechnologies"], "description": "Functions to access Twitter's filter, sample, and user streams, and to\n    parse the output into data frames."}, "sandwich": {"categories": ["Econometrics", "Finance", "Robust"], "description": "Object-oriented software for model-robust covariance matrix estimators. Starting out from the basic \n             robust Eicker-Huber-White sandwich covariance methods include: heteroscedasticity-consistent (HC)\n\t     covariances for cross-section data; heteroscedasticity- and autocorrelation-consistent (HAC)\n\t     covariances for time series data (such as Andrews' kernel HAC, Newey-West, and WEAVE estimators);\n\t     clustered covariances (one-way and multi-way); panel and panel-corrected covariances;\n\t     outer-product-of-gradients covariances; and (clustered) bootstrap covariances. All methods are\n\t     applicable to (generalized) linear model objects fitted by lm() and glm() but can also be adapted\n\t     to other classes through S3 methods. Details can be found in Zeileis et al. (2020) <doi:10.18637/jss.v095.i01>,\n\t     Zeileis (2004) <doi:10.18637/jss.v011.i10> and Zeileis (2006) <doi:10.18637/jss.v016.i09>."}, "EngrExpt": {"categories": ["ExperimentalDesign"], "description": "Datasets from Nelson, Coffin and Copeland \"Introductory\n        Statistics for Engineering Experimentation\" (Elsevier, 2003)\n        with sample code."}, "corporaexplorer": {"categories": ["NaturalLanguageProcessing"], "description": "Facilitates dynamic exploration of text collections through an\n    intuitive graphical user interface and the power of regular expressions.\n    The package contains 1) a helper function to convert a data frame to a\n    'corporaexplorerobject', 2) a 'Shiny' app for fast and flexible exploration\n    of a 'corporaexplorerobject', and 3) a 'Shiny' app for simple\n    retrieval/extraction of documents from a 'corporaexplorerobject' in a\n    reading-friendly format. The package also includes demo apps with which\n    one can explore Jane Austen's novels and the State of the Union Addresses\n    (data from the 'janeaustenr' and 'sotu' packages respectively)."}, "cncaGUI": {"categories": ["Psychometrics"], "description": "A GUI with which users can construct and interact\n        with Canonical Correspondence Analysis and Canonical Non-Symmetrical Correspondence Analysis and provides inferential results by using Bootstrap Methods."}, "sadists": {"categories": ["Distributions"], "description": "Provides the density, distribution, quantile and generation functions of some obscure probability \n    distributions, including the doubly non-central t, F, Beta, and Eta distributions; \n    the lambda-prime and K-prime; the upsilon distribution; the (weighted) sum of \n    non-central chi-squares to a power; the (weighted) sum of log non-central chi-squares;\n    the product of non-central chi-squares to powers; the product of doubly non-central\n    F variables; the product of independent normals."}, "birtr": {"categories": ["Psychometrics"], "description": "R functions for \"The Basics of Item Response Theory Using R\" by Frank B. Baker and Seock-Ho Kim (Springer, 2017, ISBN-13: 978-3-319-54204-1) including iccplot(), icccal(), icc(), iccfit(), groupinv(), tcc(), ability(), tif(), and rasch().  For example, iccplot() plots an item characteristic curve under the two-parameter logistic model."}, "diffeqr": {"categories": ["DifferentialEquations"], "description": "An interface to 'DifferentialEquations.jl' <https://diffeq.sciml.ai/dev/> from the R programming language.\n  It has unique high performance methods for solving ordinary differential equations (ODE), stochastic differential equations (SDE),\n  delay differential equations (DDE), differential-algebraic equations (DAE), and more. Much of the functionality,\n  including features like adaptive time stepping in SDEs, are unique and allow for multiple orders of magnitude speedup over more common methods.\n  'diffeqr' attaches an R interface onto the package, allowing seamless use of this tooling by R users. For more information,\n  see Rackauckas and Nie (2017) <doi:10.5334/jors.151>."}, "clifro": {"categories": ["Hydrology"], "description": "CliFlo is a web portal to the New Zealand National Climate\n    Database and provides public access (via subscription) to around 6,500\n    various climate stations (see <https://cliflo.niwa.co.nz/> for more\n    information). Collating and manipulating data from CliFlo\n    (hence clifro) and importing into R for further analysis, exploration and\n    visualisation is now straightforward and coherent. The user is required to\n    have an internet connection, and a current CliFlo subscription (free) if\n    data from stations, other than the public Reefton electronic weather\n    station, is sought."}, "bayefdr": {"categories": ["Bayesian"], "description": "\n    Implements the Bayesian FDR control described by \n    Newton et al. (2004), <doi:10.1093/biostatistics/5.2.155>.\n    Allows optimisation and visualisation of expected error rates based on\n    tail posterior probability tests.\n    Based on code written by Catalina Vallejos for BASiCS, see\n    Beyond comparisons of means: understanding changes in gene expression at the\n    single-cell level Vallejos et al. (2016) <doi:10.1186/s13059-016-0930-3>."}, "officer": {"categories": ["ReproducibleResearch"], "description": "Access and manipulate 'Microsoft Word' and 'Microsoft PowerPoint' documents from R. \n  The package focuses on tabular and graphical reporting from R; it also provides two functions\n  that let users get document content into data objects. A set of functions \n  lets add and remove images, tables and paragraphs of text in new or existing documents. \n  The package does not require any installation of Microsoft products to be able to write Microsoft \n  files."}, "spaMM": {"categories": ["Spatial"], "description": "Inference based on models with or without spatially-correlated random effects, multivariate responses, or non-Gaussian random effects (e.g., Beta). Variation in residual variance (heteroscedasticity) can itself be represented by a mixed-effect model. Both classical geostatistical models (Rousset and Ferdy 2014 <doi:10.1111/ecog.00566>), and Markov random field models on irregular grids (as considered in the 'INLA' package, <https://www.r-inla.org>), can be fitted, with distinct computational procedures exploiting the sparse matrix representations for the latter case and other autoregressive models. Laplace approximations are used for likelihood or restricted  likelihood. Penalized quasi-likelihood and other variants discussed in the h-likelihood literature (Lee and Nelder 2001 <doi:10.1093/biomet/88.4.987>) are also implemented. "}, "AovBay": {"categories": ["Bayesian"], "description": "It covers various approaches to analysis of variance, provides an assumption testing section in order to provide a decision diagram that allows selecting the most appropriate technique. It provides the classical analysis of variance, the nonparametric equivalent of Kruskal Wallis, and the Bayesian approach. These results are shown in an interactive shiny panel, which allows modifying the arguments of the tests, contains interactive graphics and presents automatic conclusions depending on the tests in order to contribute to the interpretation of these analyzes. 'AovBay' uses 'Stan' and 'FactorBayes' for Bayesian analysis and 'Highcharts' for interactive charts."}, "liftr": {"categories": ["ReproducibleResearch"], "description": "Persistent reproducible reporting by containerization of R Markdown documents."}, "exreport": {"categories": ["ReproducibleResearch"], "description": "Analysis of experimental results and automatic report generation in both interactive HTML and LaTeX. This package ships with a rich interface for data modeling and built in functions for the rapid application of statistical tests and generation of common plots and tables with publish-ready quality."}}